{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.050008829242450994,
  "eval_steps": 500,
  "global_step": 1593,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 3.1392862047991835e-05,
      "grad_norm": 20.763439178466797,
      "learning_rate": 0.0005,
      "loss": 7.2128,
      "step": 1
    },
    {
      "epoch": 6.278572409598367e-05,
      "grad_norm": 1.7951451539993286,
      "learning_rate": 0.0004996861268047708,
      "loss": 0.7872,
      "step": 2
    },
    {
      "epoch": 9.417858614397551e-05,
      "grad_norm": 10.374807357788086,
      "learning_rate": 0.0004993722536095418,
      "loss": 4.2923,
      "step": 3
    },
    {
      "epoch": 0.00012557144819196734,
      "grad_norm": 1.2727655172348022,
      "learning_rate": 0.0004990583804143126,
      "loss": 0.4459,
      "step": 4
    },
    {
      "epoch": 0.00015696431023995918,
      "grad_norm": 2.3697619438171387,
      "learning_rate": 0.0004987445072190835,
      "loss": 0.3065,
      "step": 5
    },
    {
      "epoch": 0.00018835717228795102,
      "grad_norm": 1.3030450344085693,
      "learning_rate": 0.0004984306340238544,
      "loss": 0.3152,
      "step": 6
    },
    {
      "epoch": 0.00021975003433594286,
      "grad_norm": 4.3560791015625,
      "learning_rate": 0.0004981167608286252,
      "loss": 0.8058,
      "step": 7
    },
    {
      "epoch": 0.0002511428963839347,
      "grad_norm": 3.254380702972412,
      "learning_rate": 0.0004978028876333961,
      "loss": 0.8133,
      "step": 8
    },
    {
      "epoch": 0.0002825357584319265,
      "grad_norm": 2.4777097702026367,
      "learning_rate": 0.000497489014438167,
      "loss": 0.6473,
      "step": 9
    },
    {
      "epoch": 0.00031392862047991836,
      "grad_norm": 1.2789767980575562,
      "learning_rate": 0.0004971751412429379,
      "loss": 0.4308,
      "step": 10
    },
    {
      "epoch": 0.0003453214825279102,
      "grad_norm": 0.4175763428211212,
      "learning_rate": 0.0004968612680477087,
      "loss": 0.1086,
      "step": 11
    },
    {
      "epoch": 0.00037671434457590204,
      "grad_norm": 4.407939434051514,
      "learning_rate": 0.0004965473948524797,
      "loss": 1.0235,
      "step": 12
    },
    {
      "epoch": 0.0004081072066238939,
      "grad_norm": 3.20878529548645,
      "learning_rate": 0.0004962335216572505,
      "loss": 0.8575,
      "step": 13
    },
    {
      "epoch": 0.0004395000686718857,
      "grad_norm": 1.8949596881866455,
      "learning_rate": 0.0004959196484620213,
      "loss": 0.4055,
      "step": 14
    },
    {
      "epoch": 0.00047089293071987757,
      "grad_norm": 2.365950345993042,
      "learning_rate": 0.0004956057752667923,
      "loss": 0.581,
      "step": 15
    },
    {
      "epoch": 0.0005022857927678694,
      "grad_norm": 1.6669114828109741,
      "learning_rate": 0.0004952919020715631,
      "loss": 0.6432,
      "step": 16
    },
    {
      "epoch": 0.0005336786548158613,
      "grad_norm": 0.6142949461936951,
      "learning_rate": 0.000494978028876334,
      "loss": 0.1042,
      "step": 17
    },
    {
      "epoch": 0.000565071516863853,
      "grad_norm": 1.3072015047073364,
      "learning_rate": 0.0004946641556811049,
      "loss": 0.5177,
      "step": 18
    },
    {
      "epoch": 0.0005964643789118449,
      "grad_norm": 1.670235276222229,
      "learning_rate": 0.0004943502824858757,
      "loss": 0.7432,
      "step": 19
    },
    {
      "epoch": 0.0006278572409598367,
      "grad_norm": 0.8000158071517944,
      "learning_rate": 0.0004940364092906466,
      "loss": 0.2566,
      "step": 20
    },
    {
      "epoch": 0.0006592501030078286,
      "grad_norm": 0.9492769241333008,
      "learning_rate": 0.0004937225360954174,
      "loss": 0.3736,
      "step": 21
    },
    {
      "epoch": 0.0006906429650558204,
      "grad_norm": 0.625768780708313,
      "learning_rate": 0.0004934086629001883,
      "loss": 0.3862,
      "step": 22
    },
    {
      "epoch": 0.0007220358271038123,
      "grad_norm": 0.7871900200843811,
      "learning_rate": 0.0004930947897049592,
      "loss": 0.4484,
      "step": 23
    },
    {
      "epoch": 0.0007534286891518041,
      "grad_norm": 0.44407743215560913,
      "learning_rate": 0.00049278091650973,
      "loss": 0.2382,
      "step": 24
    },
    {
      "epoch": 0.000784821551199796,
      "grad_norm": 0.9357921481132507,
      "learning_rate": 0.000492467043314501,
      "loss": 0.4392,
      "step": 25
    },
    {
      "epoch": 0.0008162144132477878,
      "grad_norm": 0.943886399269104,
      "learning_rate": 0.0004921531701192718,
      "loss": 0.1385,
      "step": 26
    },
    {
      "epoch": 0.0008476072752957797,
      "grad_norm": 0.5392075777053833,
      "learning_rate": 0.0004918392969240427,
      "loss": 0.285,
      "step": 27
    },
    {
      "epoch": 0.0008790001373437715,
      "grad_norm": 4.086784362792969,
      "learning_rate": 0.0004915254237288136,
      "loss": 1.0923,
      "step": 28
    },
    {
      "epoch": 0.0009103929993917633,
      "grad_norm": 4.979513168334961,
      "learning_rate": 0.0004912115505335845,
      "loss": 0.8425,
      "step": 29
    },
    {
      "epoch": 0.0009417858614397551,
      "grad_norm": 0.9900334477424622,
      "learning_rate": 0.0004908976773383553,
      "loss": 0.2223,
      "step": 30
    },
    {
      "epoch": 0.0009731787234877469,
      "grad_norm": 2.4739837646484375,
      "learning_rate": 0.0004905838041431262,
      "loss": 0.6587,
      "step": 31
    },
    {
      "epoch": 0.0010045715855357387,
      "grad_norm": 0.9001989364624023,
      "learning_rate": 0.0004902699309478971,
      "loss": 0.6983,
      "step": 32
    },
    {
      "epoch": 0.0010359644475837306,
      "grad_norm": 0.7222761511802673,
      "learning_rate": 0.0004899560577526679,
      "loss": 0.3322,
      "step": 33
    },
    {
      "epoch": 0.0010673573096317225,
      "grad_norm": 0.5057505369186401,
      "learning_rate": 0.0004896421845574388,
      "loss": 0.3775,
      "step": 34
    },
    {
      "epoch": 0.0010987501716797144,
      "grad_norm": 2.096632957458496,
      "learning_rate": 0.0004893283113622097,
      "loss": 0.9273,
      "step": 35
    },
    {
      "epoch": 0.001130143033727706,
      "grad_norm": 1.5917242765426636,
      "learning_rate": 0.0004890144381669805,
      "loss": 0.956,
      "step": 36
    },
    {
      "epoch": 0.001161535895775698,
      "grad_norm": 0.637842059135437,
      "learning_rate": 0.0004887005649717515,
      "loss": 0.2886,
      "step": 37
    },
    {
      "epoch": 0.0011929287578236899,
      "grad_norm": 1.4043060541152954,
      "learning_rate": 0.0004883866917765223,
      "loss": 0.8821,
      "step": 38
    },
    {
      "epoch": 0.0012243216198716818,
      "grad_norm": 0.20544353127479553,
      "learning_rate": 0.0004880728185812932,
      "loss": 0.1558,
      "step": 39
    },
    {
      "epoch": 0.0012557144819196734,
      "grad_norm": 1.2993770837783813,
      "learning_rate": 0.000487758945386064,
      "loss": 0.8533,
      "step": 40
    },
    {
      "epoch": 0.0012871073439676653,
      "grad_norm": 0.8029264211654663,
      "learning_rate": 0.0004874450721908349,
      "loss": 0.6446,
      "step": 41
    },
    {
      "epoch": 0.0013185002060156572,
      "grad_norm": 0.2087167352437973,
      "learning_rate": 0.00048713119899560577,
      "loss": 0.1879,
      "step": 42
    },
    {
      "epoch": 0.0013498930680636491,
      "grad_norm": 0.2651229798793793,
      "learning_rate": 0.00048681732580037665,
      "loss": 0.2405,
      "step": 43
    },
    {
      "epoch": 0.0013812859301116408,
      "grad_norm": 0.4504196345806122,
      "learning_rate": 0.00048650345260514753,
      "loss": 0.4246,
      "step": 44
    },
    {
      "epoch": 0.0014126787921596327,
      "grad_norm": 0.16697657108306885,
      "learning_rate": 0.0004861895794099184,
      "loss": 0.1659,
      "step": 45
    },
    {
      "epoch": 0.0014440716542076246,
      "grad_norm": 0.17742466926574707,
      "learning_rate": 0.0004858757062146893,
      "loss": 0.1558,
      "step": 46
    },
    {
      "epoch": 0.0014754645162556163,
      "grad_norm": 1.0815861225128174,
      "learning_rate": 0.0004855618330194602,
      "loss": 0.7241,
      "step": 47
    },
    {
      "epoch": 0.0015068573783036082,
      "grad_norm": 0.23985442519187927,
      "learning_rate": 0.000485247959824231,
      "loss": 0.1788,
      "step": 48
    },
    {
      "epoch": 0.0015382502403516,
      "grad_norm": 1.0087871551513672,
      "learning_rate": 0.0004849340866290019,
      "loss": 0.9007,
      "step": 49
    },
    {
      "epoch": 0.001569643102399592,
      "grad_norm": 0.10466726124286652,
      "learning_rate": 0.0004846202134337728,
      "loss": 0.0948,
      "step": 50
    },
    {
      "epoch": 0.0016010359644475836,
      "grad_norm": 0.48157402873039246,
      "learning_rate": 0.00048430634023854366,
      "loss": 0.3305,
      "step": 51
    },
    {
      "epoch": 0.0016324288264955755,
      "grad_norm": 0.3169202506542206,
      "learning_rate": 0.0004839924670433145,
      "loss": 0.1556,
      "step": 52
    },
    {
      "epoch": 0.0016638216885435674,
      "grad_norm": 1.533530831336975,
      "learning_rate": 0.00048367859384808537,
      "loss": 0.743,
      "step": 53
    },
    {
      "epoch": 0.0016952145505915593,
      "grad_norm": 2.1047446727752686,
      "learning_rate": 0.00048336472065285625,
      "loss": 0.6715,
      "step": 54
    },
    {
      "epoch": 0.001726607412639551,
      "grad_norm": 1.53585684299469,
      "learning_rate": 0.00048305084745762714,
      "loss": 0.4415,
      "step": 55
    },
    {
      "epoch": 0.001758000274687543,
      "grad_norm": 5.724369525909424,
      "learning_rate": 0.00048273697426239796,
      "loss": 0.0882,
      "step": 56
    },
    {
      "epoch": 0.0017893931367355348,
      "grad_norm": 10.491963386535645,
      "learning_rate": 0.00048242310106716885,
      "loss": 0.5713,
      "step": 57
    },
    {
      "epoch": 0.0018207859987835267,
      "grad_norm": 0.9015160202980042,
      "learning_rate": 0.00048210922787193973,
      "loss": 0.2494,
      "step": 58
    },
    {
      "epoch": 0.0018521788608315184,
      "grad_norm": 0.7770019173622131,
      "learning_rate": 0.0004817953546767106,
      "loss": 0.4958,
      "step": 59
    },
    {
      "epoch": 0.0018835717228795103,
      "grad_norm": 0.35057732462882996,
      "learning_rate": 0.00048148148148148144,
      "loss": 0.2069,
      "step": 60
    },
    {
      "epoch": 0.0019149645849275022,
      "grad_norm": 0.3497695028781891,
      "learning_rate": 0.0004811676082862524,
      "loss": 0.3913,
      "step": 61
    },
    {
      "epoch": 0.0019463574469754938,
      "grad_norm": 0.5173444747924805,
      "learning_rate": 0.00048085373509102326,
      "loss": 0.5993,
      "step": 62
    },
    {
      "epoch": 0.001977750309023486,
      "grad_norm": 0.35983842611312866,
      "learning_rate": 0.00048053986189579414,
      "loss": 0.4576,
      "step": 63
    },
    {
      "epoch": 0.0020091431710714774,
      "grad_norm": 0.30752888321876526,
      "learning_rate": 0.00048022598870056497,
      "loss": 0.4199,
      "step": 64
    },
    {
      "epoch": 0.0020405360331194693,
      "grad_norm": 0.29541462659835815,
      "learning_rate": 0.00047991211550533585,
      "loss": 0.0989,
      "step": 65
    },
    {
      "epoch": 0.002071928895167461,
      "grad_norm": 0.27809619903564453,
      "learning_rate": 0.00047959824231010674,
      "loss": 0.0979,
      "step": 66
    },
    {
      "epoch": 0.002103321757215453,
      "grad_norm": 0.45040959119796753,
      "learning_rate": 0.0004792843691148776,
      "loss": 0.5211,
      "step": 67
    },
    {
      "epoch": 0.002134714619263445,
      "grad_norm": 0.8834818005561829,
      "learning_rate": 0.00047897049591964845,
      "loss": 0.5973,
      "step": 68
    },
    {
      "epoch": 0.002166107481311437,
      "grad_norm": 0.20665140450000763,
      "learning_rate": 0.00047865662272441933,
      "loss": 0.2403,
      "step": 69
    },
    {
      "epoch": 0.002197500343359429,
      "grad_norm": 1.0069276094436646,
      "learning_rate": 0.0004783427495291902,
      "loss": 1.0787,
      "step": 70
    },
    {
      "epoch": 0.0022288932054074207,
      "grad_norm": 0.19445323944091797,
      "learning_rate": 0.0004780288763339611,
      "loss": 0.2049,
      "step": 71
    },
    {
      "epoch": 0.002260286067455412,
      "grad_norm": 0.08396635204553604,
      "learning_rate": 0.0004777150031387319,
      "loss": 0.08,
      "step": 72
    },
    {
      "epoch": 0.002291678929503404,
      "grad_norm": 0.9895219802856445,
      "learning_rate": 0.0004774011299435028,
      "loss": 0.5533,
      "step": 73
    },
    {
      "epoch": 0.002323071791551396,
      "grad_norm": 0.386055052280426,
      "learning_rate": 0.0004770872567482737,
      "loss": 0.3467,
      "step": 74
    },
    {
      "epoch": 0.002354464653599388,
      "grad_norm": 0.8307139277458191,
      "learning_rate": 0.0004767733835530446,
      "loss": 0.756,
      "step": 75
    },
    {
      "epoch": 0.0023858575156473797,
      "grad_norm": 0.42867472767829895,
      "learning_rate": 0.0004764595103578154,
      "loss": 0.3306,
      "step": 76
    },
    {
      "epoch": 0.0024172503776953716,
      "grad_norm": 0.3970087170600891,
      "learning_rate": 0.0004761456371625863,
      "loss": 0.3417,
      "step": 77
    },
    {
      "epoch": 0.0024486432397433635,
      "grad_norm": 2.4944279193878174,
      "learning_rate": 0.0004758317639673572,
      "loss": 0.6229,
      "step": 78
    },
    {
      "epoch": 0.002480036101791355,
      "grad_norm": 119.94081115722656,
      "learning_rate": 0.0004755178907721281,
      "loss": 0.7681,
      "step": 79
    },
    {
      "epoch": 0.002511428963839347,
      "grad_norm": 1.3498166799545288,
      "learning_rate": 0.000475204017576899,
      "loss": 0.4844,
      "step": 80
    },
    {
      "epoch": 0.002542821825887339,
      "grad_norm": 0.762312650680542,
      "learning_rate": 0.0004748901443816698,
      "loss": 0.2478,
      "step": 81
    },
    {
      "epoch": 0.0025742146879353307,
      "grad_norm": 14.356493949890137,
      "learning_rate": 0.0004745762711864407,
      "loss": 0.5523,
      "step": 82
    },
    {
      "epoch": 0.0026056075499833226,
      "grad_norm": 2.2848329544067383,
      "learning_rate": 0.0004742623979912116,
      "loss": 0.3522,
      "step": 83
    },
    {
      "epoch": 0.0026370004120313145,
      "grad_norm": 24.273473739624023,
      "learning_rate": 0.00047394852479598246,
      "loss": 0.5768,
      "step": 84
    },
    {
      "epoch": 0.0026683932740793064,
      "grad_norm": 1.672568917274475,
      "learning_rate": 0.0004736346516007533,
      "loss": 0.3381,
      "step": 85
    },
    {
      "epoch": 0.0026997861361272983,
      "grad_norm": 1.146852970123291,
      "learning_rate": 0.0004733207784055242,
      "loss": 0.7524,
      "step": 86
    },
    {
      "epoch": 0.0027311789981752897,
      "grad_norm": 0.6218042373657227,
      "learning_rate": 0.00047300690521029506,
      "loss": 0.5544,
      "step": 87
    },
    {
      "epoch": 0.0027625718602232816,
      "grad_norm": 0.5098215937614441,
      "learning_rate": 0.00047269303201506594,
      "loss": 0.664,
      "step": 88
    },
    {
      "epoch": 0.0027939647222712735,
      "grad_norm": 0.8062493205070496,
      "learning_rate": 0.00047237915881983677,
      "loss": 0.467,
      "step": 89
    },
    {
      "epoch": 0.0028253575843192654,
      "grad_norm": 0.7544141411781311,
      "learning_rate": 0.00047206528562460765,
      "loss": 0.654,
      "step": 90
    },
    {
      "epoch": 0.0028567504463672573,
      "grad_norm": 0.21385176479816437,
      "learning_rate": 0.00047175141242937854,
      "loss": 0.078,
      "step": 91
    },
    {
      "epoch": 0.002888143308415249,
      "grad_norm": 0.7112462520599365,
      "learning_rate": 0.0004714375392341494,
      "loss": 0.3545,
      "step": 92
    },
    {
      "epoch": 0.002919536170463241,
      "grad_norm": 0.5261557102203369,
      "learning_rate": 0.00047112366603892025,
      "loss": 0.5557,
      "step": 93
    },
    {
      "epoch": 0.0029509290325112326,
      "grad_norm": 0.5365926027297974,
      "learning_rate": 0.00047080979284369113,
      "loss": 0.52,
      "step": 94
    },
    {
      "epoch": 0.0029823218945592245,
      "grad_norm": 0.0829518586397171,
      "learning_rate": 0.00047049591964846207,
      "loss": 0.0913,
      "step": 95
    },
    {
      "epoch": 0.0030137147566072164,
      "grad_norm": 0.5581322908401489,
      "learning_rate": 0.00047018204645323295,
      "loss": 0.7444,
      "step": 96
    },
    {
      "epoch": 0.0030451076186552082,
      "grad_norm": 0.4603085219860077,
      "learning_rate": 0.0004698681732580038,
      "loss": 0.3974,
      "step": 97
    },
    {
      "epoch": 0.0030765004807032,
      "grad_norm": 0.6926019191741943,
      "learning_rate": 0.00046955430006277466,
      "loss": 0.5698,
      "step": 98
    },
    {
      "epoch": 0.003107893342751192,
      "grad_norm": 0.44651320576667786,
      "learning_rate": 0.00046924042686754554,
      "loss": 0.3378,
      "step": 99
    },
    {
      "epoch": 0.003139286204799184,
      "grad_norm": 0.4662633240222931,
      "learning_rate": 0.0004689265536723164,
      "loss": 0.5412,
      "step": 100
    },
    {
      "epoch": 0.003170679066847176,
      "grad_norm": 0.2515503168106079,
      "learning_rate": 0.00046861268047708725,
      "loss": 0.1934,
      "step": 101
    },
    {
      "epoch": 0.0032020719288951673,
      "grad_norm": 0.8559925556182861,
      "learning_rate": 0.00046829880728185814,
      "loss": 0.6307,
      "step": 102
    },
    {
      "epoch": 0.003233464790943159,
      "grad_norm": 0.7301199436187744,
      "learning_rate": 0.000467984934086629,
      "loss": 0.5341,
      "step": 103
    },
    {
      "epoch": 0.003264857652991151,
      "grad_norm": 0.3187093734741211,
      "learning_rate": 0.0004676710608913999,
      "loss": 0.2435,
      "step": 104
    },
    {
      "epoch": 0.003296250515039143,
      "grad_norm": 0.4127710461616516,
      "learning_rate": 0.00046735718769617073,
      "loss": 0.3496,
      "step": 105
    },
    {
      "epoch": 0.003327643377087135,
      "grad_norm": 0.9699143171310425,
      "learning_rate": 0.0004670433145009416,
      "loss": 0.5465,
      "step": 106
    },
    {
      "epoch": 0.0033590362391351268,
      "grad_norm": 0.2763116955757141,
      "learning_rate": 0.0004667294413057125,
      "loss": 0.1799,
      "step": 107
    },
    {
      "epoch": 0.0033904291011831187,
      "grad_norm": 0.1660744696855545,
      "learning_rate": 0.0004664155681104834,
      "loss": 0.1032,
      "step": 108
    },
    {
      "epoch": 0.00342182196323111,
      "grad_norm": 0.6037685871124268,
      "learning_rate": 0.0004661016949152542,
      "loss": 0.334,
      "step": 109
    },
    {
      "epoch": 0.003453214825279102,
      "grad_norm": 0.5608342289924622,
      "learning_rate": 0.0004657878217200251,
      "loss": 0.224,
      "step": 110
    },
    {
      "epoch": 0.003484607687327094,
      "grad_norm": 0.362684965133667,
      "learning_rate": 0.000465473948524796,
      "loss": 0.2159,
      "step": 111
    },
    {
      "epoch": 0.003516000549375086,
      "grad_norm": 0.16162040829658508,
      "learning_rate": 0.0004651600753295669,
      "loss": 0.116,
      "step": 112
    },
    {
      "epoch": 0.0035473934114230777,
      "grad_norm": 0.29444101452827454,
      "learning_rate": 0.00046484620213433774,
      "loss": 0.2675,
      "step": 113
    },
    {
      "epoch": 0.0035787862734710696,
      "grad_norm": 0.1416115015745163,
      "learning_rate": 0.0004645323289391086,
      "loss": 0.1625,
      "step": 114
    },
    {
      "epoch": 0.0036101791355190615,
      "grad_norm": 0.6716390252113342,
      "learning_rate": 0.0004642184557438795,
      "loss": 0.6593,
      "step": 115
    },
    {
      "epoch": 0.0036415719975670534,
      "grad_norm": 0.7845706939697266,
      "learning_rate": 0.0004639045825486504,
      "loss": 0.6569,
      "step": 116
    },
    {
      "epoch": 0.003672964859615045,
      "grad_norm": 0.27318504452705383,
      "learning_rate": 0.0004635907093534212,
      "loss": 0.0905,
      "step": 117
    },
    {
      "epoch": 0.0037043577216630368,
      "grad_norm": 0.42192599177360535,
      "learning_rate": 0.0004632768361581921,
      "loss": 0.3925,
      "step": 118
    },
    {
      "epoch": 0.0037357505837110287,
      "grad_norm": 0.5998942255973816,
      "learning_rate": 0.000462962962962963,
      "loss": 0.3985,
      "step": 119
    },
    {
      "epoch": 0.0037671434457590205,
      "grad_norm": 0.4464702904224396,
      "learning_rate": 0.00046264908976773386,
      "loss": 0.4627,
      "step": 120
    },
    {
      "epoch": 0.0037985363078070124,
      "grad_norm": 0.2729499340057373,
      "learning_rate": 0.0004623352165725047,
      "loss": 0.0947,
      "step": 121
    },
    {
      "epoch": 0.0038299291698550043,
      "grad_norm": 0.4267243444919586,
      "learning_rate": 0.0004620213433772756,
      "loss": 0.3889,
      "step": 122
    },
    {
      "epoch": 0.0038613220319029962,
      "grad_norm": 0.2147495150566101,
      "learning_rate": 0.00046170747018204646,
      "loss": 0.0942,
      "step": 123
    },
    {
      "epoch": 0.0038927148939509877,
      "grad_norm": 1.036523699760437,
      "learning_rate": 0.00046139359698681734,
      "loss": 0.8092,
      "step": 124
    },
    {
      "epoch": 0.00392410775599898,
      "grad_norm": 0.15600870549678802,
      "learning_rate": 0.00046107972379158817,
      "loss": 0.1371,
      "step": 125
    },
    {
      "epoch": 0.003955500618046972,
      "grad_norm": 0.12969575822353363,
      "learning_rate": 0.00046076585059635905,
      "loss": 0.0707,
      "step": 126
    },
    {
      "epoch": 0.003986893480094964,
      "grad_norm": 0.11029458791017532,
      "learning_rate": 0.00046045197740112993,
      "loss": 0.0803,
      "step": 127
    },
    {
      "epoch": 0.004018286342142955,
      "grad_norm": 0.32625389099121094,
      "learning_rate": 0.0004601381042059008,
      "loss": 0.2608,
      "step": 128
    },
    {
      "epoch": 0.004049679204190947,
      "grad_norm": 0.3363161087036133,
      "learning_rate": 0.0004598242310106717,
      "loss": 0.208,
      "step": 129
    },
    {
      "epoch": 0.004081072066238939,
      "grad_norm": 0.5546752214431763,
      "learning_rate": 0.0004595103578154426,
      "loss": 0.503,
      "step": 130
    },
    {
      "epoch": 0.0041124649282869305,
      "grad_norm": 1.0024831295013428,
      "learning_rate": 0.00045919648462021347,
      "loss": 0.7194,
      "step": 131
    },
    {
      "epoch": 0.004143857790334922,
      "grad_norm": 0.1920463591814041,
      "learning_rate": 0.00045888261142498435,
      "loss": 0.1736,
      "step": 132
    },
    {
      "epoch": 0.004175250652382914,
      "grad_norm": 0.708552896976471,
      "learning_rate": 0.0004585687382297552,
      "loss": 0.7255,
      "step": 133
    },
    {
      "epoch": 0.004206643514430906,
      "grad_norm": 0.6060677170753479,
      "learning_rate": 0.00045825486503452606,
      "loss": 0.6493,
      "step": 134
    },
    {
      "epoch": 0.004238036376478898,
      "grad_norm": 0.3708290755748749,
      "learning_rate": 0.00045794099183929694,
      "loss": 0.4219,
      "step": 135
    },
    {
      "epoch": 0.00426942923852689,
      "grad_norm": 0.2822354733943939,
      "learning_rate": 0.0004576271186440678,
      "loss": 0.1557,
      "step": 136
    },
    {
      "epoch": 0.004300822100574882,
      "grad_norm": 0.20355430245399475,
      "learning_rate": 0.00045731324544883865,
      "loss": 0.1636,
      "step": 137
    },
    {
      "epoch": 0.004332214962622874,
      "grad_norm": 0.09699676185846329,
      "learning_rate": 0.00045699937225360954,
      "loss": 0.0691,
      "step": 138
    },
    {
      "epoch": 0.004363607824670866,
      "grad_norm": 0.4930858612060547,
      "learning_rate": 0.0004566854990583804,
      "loss": 0.36,
      "step": 139
    },
    {
      "epoch": 0.004395000686718858,
      "grad_norm": 0.1905820518732071,
      "learning_rate": 0.0004563716258631513,
      "loss": 0.2352,
      "step": 140
    },
    {
      "epoch": 0.0044263935487668495,
      "grad_norm": 0.6099149584770203,
      "learning_rate": 0.00045605775266792213,
      "loss": 0.6444,
      "step": 141
    },
    {
      "epoch": 0.004457786410814841,
      "grad_norm": 0.10827162116765976,
      "learning_rate": 0.000455743879472693,
      "loss": 0.1101,
      "step": 142
    },
    {
      "epoch": 0.004489179272862832,
      "grad_norm": 0.09179286658763885,
      "learning_rate": 0.0004554300062774639,
      "loss": 0.0769,
      "step": 143
    },
    {
      "epoch": 0.004520572134910824,
      "grad_norm": 0.27270302176475525,
      "learning_rate": 0.0004551161330822348,
      "loss": 0.255,
      "step": 144
    },
    {
      "epoch": 0.004551964996958816,
      "grad_norm": 0.6075727343559265,
      "learning_rate": 0.0004548022598870056,
      "loss": 0.6836,
      "step": 145
    },
    {
      "epoch": 0.004583357859006808,
      "grad_norm": 0.11618257313966751,
      "learning_rate": 0.00045448838669177654,
      "loss": 0.1008,
      "step": 146
    },
    {
      "epoch": 0.0046147507210548,
      "grad_norm": 0.13267013430595398,
      "learning_rate": 0.0004541745134965474,
      "loss": 0.1266,
      "step": 147
    },
    {
      "epoch": 0.004646143583102792,
      "grad_norm": 0.7619274258613586,
      "learning_rate": 0.0004538606403013183,
      "loss": 0.6558,
      "step": 148
    },
    {
      "epoch": 0.004677536445150784,
      "grad_norm": 0.13794519007205963,
      "learning_rate": 0.00045354676710608914,
      "loss": 0.1781,
      "step": 149
    },
    {
      "epoch": 0.004708929307198776,
      "grad_norm": 0.35890576243400574,
      "learning_rate": 0.00045323289391086,
      "loss": 0.2611,
      "step": 150
    },
    {
      "epoch": 0.004740322169246768,
      "grad_norm": 0.20366284251213074,
      "learning_rate": 0.0004529190207156309,
      "loss": 0.0793,
      "step": 151
    },
    {
      "epoch": 0.0047717150312947595,
      "grad_norm": 0.46556320786476135,
      "learning_rate": 0.0004526051475204018,
      "loss": 0.4489,
      "step": 152
    },
    {
      "epoch": 0.004803107893342751,
      "grad_norm": 0.5947221517562866,
      "learning_rate": 0.00045229127432517267,
      "loss": 0.466,
      "step": 153
    },
    {
      "epoch": 0.004834500755390743,
      "grad_norm": 0.306321918964386,
      "learning_rate": 0.0004519774011299435,
      "loss": 0.1636,
      "step": 154
    },
    {
      "epoch": 0.004865893617438735,
      "grad_norm": 0.8183008432388306,
      "learning_rate": 0.0004516635279347144,
      "loss": 0.5656,
      "step": 155
    },
    {
      "epoch": 0.004897286479486727,
      "grad_norm": 0.692209780216217,
      "learning_rate": 0.00045134965473948526,
      "loss": 0.632,
      "step": 156
    },
    {
      "epoch": 0.004928679341534719,
      "grad_norm": 0.3576177954673767,
      "learning_rate": 0.00045103578154425615,
      "loss": 0.1332,
      "step": 157
    },
    {
      "epoch": 0.00496007220358271,
      "grad_norm": 0.413246750831604,
      "learning_rate": 0.000450721908349027,
      "loss": 0.3214,
      "step": 158
    },
    {
      "epoch": 0.004991465065630702,
      "grad_norm": 0.3453329801559448,
      "learning_rate": 0.00045040803515379786,
      "loss": 0.1421,
      "step": 159
    },
    {
      "epoch": 0.005022857927678694,
      "grad_norm": 0.159720316529274,
      "learning_rate": 0.00045009416195856874,
      "loss": 0.1026,
      "step": 160
    },
    {
      "epoch": 0.005054250789726686,
      "grad_norm": 0.4353010356426239,
      "learning_rate": 0.0004497802887633396,
      "loss": 0.3653,
      "step": 161
    },
    {
      "epoch": 0.005085643651774678,
      "grad_norm": 0.7215173840522766,
      "learning_rate": 0.0004494664155681105,
      "loss": 0.3794,
      "step": 162
    },
    {
      "epoch": 0.0051170365138226695,
      "grad_norm": 0.6596155166625977,
      "learning_rate": 0.0004491525423728814,
      "loss": 0.482,
      "step": 163
    },
    {
      "epoch": 0.005148429375870661,
      "grad_norm": 0.13398224115371704,
      "learning_rate": 0.00044883866917765227,
      "loss": 0.1047,
      "step": 164
    },
    {
      "epoch": 0.005179822237918653,
      "grad_norm": 0.20166881382465363,
      "learning_rate": 0.00044852479598242315,
      "loss": 0.1741,
      "step": 165
    },
    {
      "epoch": 0.005211215099966645,
      "grad_norm": 0.7058696150779724,
      "learning_rate": 0.000448210922787194,
      "loss": 0.4282,
      "step": 166
    },
    {
      "epoch": 0.005242607962014637,
      "grad_norm": 0.1712052822113037,
      "learning_rate": 0.00044789704959196486,
      "loss": 0.2,
      "step": 167
    },
    {
      "epoch": 0.005274000824062629,
      "grad_norm": 0.24053902924060822,
      "learning_rate": 0.00044758317639673575,
      "loss": 0.2104,
      "step": 168
    },
    {
      "epoch": 0.005305393686110621,
      "grad_norm": 0.3100311756134033,
      "learning_rate": 0.00044726930320150663,
      "loss": 0.2414,
      "step": 169
    },
    {
      "epoch": 0.005336786548158613,
      "grad_norm": 0.08260626345872879,
      "learning_rate": 0.00044695543000627746,
      "loss": 0.0887,
      "step": 170
    },
    {
      "epoch": 0.005368179410206605,
      "grad_norm": 0.06906753778457642,
      "learning_rate": 0.00044664155681104834,
      "loss": 0.061,
      "step": 171
    },
    {
      "epoch": 0.0053995722722545965,
      "grad_norm": 0.6832339763641357,
      "learning_rate": 0.0004463276836158192,
      "loss": 0.742,
      "step": 172
    },
    {
      "epoch": 0.0054309651343025876,
      "grad_norm": 0.5821923613548279,
      "learning_rate": 0.0004460138104205901,
      "loss": 0.5469,
      "step": 173
    },
    {
      "epoch": 0.0054623579963505794,
      "grad_norm": 0.30896517634391785,
      "learning_rate": 0.00044569993722536093,
      "loss": 0.2488,
      "step": 174
    },
    {
      "epoch": 0.005493750858398571,
      "grad_norm": 0.3786836564540863,
      "learning_rate": 0.0004453860640301318,
      "loss": 0.3165,
      "step": 175
    },
    {
      "epoch": 0.005525143720446563,
      "grad_norm": 0.08376660943031311,
      "learning_rate": 0.0004450721908349027,
      "loss": 0.0628,
      "step": 176
    },
    {
      "epoch": 0.005556536582494555,
      "grad_norm": 0.39430320262908936,
      "learning_rate": 0.0004447583176396736,
      "loss": 0.3048,
      "step": 177
    },
    {
      "epoch": 0.005587929444542547,
      "grad_norm": 0.4123237729072571,
      "learning_rate": 0.0004444444444444444,
      "loss": 0.3563,
      "step": 178
    },
    {
      "epoch": 0.005619322306590539,
      "grad_norm": 0.3382517099380493,
      "learning_rate": 0.00044413057124921535,
      "loss": 0.3242,
      "step": 179
    },
    {
      "epoch": 0.005650715168638531,
      "grad_norm": 0.1934645175933838,
      "learning_rate": 0.00044381669805398623,
      "loss": 0.1133,
      "step": 180
    },
    {
      "epoch": 0.005682108030686523,
      "grad_norm": 1.2315717935562134,
      "learning_rate": 0.0004435028248587571,
      "loss": 0.6119,
      "step": 181
    },
    {
      "epoch": 0.005713500892734515,
      "grad_norm": 0.20577332377433777,
      "learning_rate": 0.00044318895166352794,
      "loss": 0.0856,
      "step": 182
    },
    {
      "epoch": 0.0057448937547825065,
      "grad_norm": 0.12678678333759308,
      "learning_rate": 0.0004428750784682988,
      "loss": 0.0916,
      "step": 183
    },
    {
      "epoch": 0.005776286616830498,
      "grad_norm": 0.23696044087409973,
      "learning_rate": 0.0004425612052730697,
      "loss": 0.1772,
      "step": 184
    },
    {
      "epoch": 0.00580767947887849,
      "grad_norm": 0.15154629945755005,
      "learning_rate": 0.0004422473320778406,
      "loss": 0.1078,
      "step": 185
    },
    {
      "epoch": 0.005839072340926482,
      "grad_norm": 0.42418065667152405,
      "learning_rate": 0.0004419334588826114,
      "loss": 0.4866,
      "step": 186
    },
    {
      "epoch": 0.005870465202974474,
      "grad_norm": 0.29116398096084595,
      "learning_rate": 0.0004416195856873823,
      "loss": 0.2997,
      "step": 187
    },
    {
      "epoch": 0.005901858065022465,
      "grad_norm": 0.07030633091926575,
      "learning_rate": 0.0004413057124921532,
      "loss": 0.0886,
      "step": 188
    },
    {
      "epoch": 0.005933250927070457,
      "grad_norm": 0.18542173504829407,
      "learning_rate": 0.00044099183929692407,
      "loss": 0.2038,
      "step": 189
    },
    {
      "epoch": 0.005964643789118449,
      "grad_norm": 0.1698259711265564,
      "learning_rate": 0.0004406779661016949,
      "loss": 0.2533,
      "step": 190
    },
    {
      "epoch": 0.005996036651166441,
      "grad_norm": 0.2127656787633896,
      "learning_rate": 0.0004403640929064658,
      "loss": 0.2181,
      "step": 191
    },
    {
      "epoch": 0.006027429513214433,
      "grad_norm": 0.19994334876537323,
      "learning_rate": 0.00044005021971123666,
      "loss": 0.1925,
      "step": 192
    },
    {
      "epoch": 0.006058822375262425,
      "grad_norm": 0.17183515429496765,
      "learning_rate": 0.00043973634651600754,
      "loss": 0.1523,
      "step": 193
    },
    {
      "epoch": 0.0060902152373104165,
      "grad_norm": 0.38329342007637024,
      "learning_rate": 0.00043942247332077837,
      "loss": 0.2697,
      "step": 194
    },
    {
      "epoch": 0.006121608099358408,
      "grad_norm": 0.7442147135734558,
      "learning_rate": 0.00043910860012554926,
      "loss": 0.5352,
      "step": 195
    },
    {
      "epoch": 0.0061530009614064,
      "grad_norm": 0.6009419560432434,
      "learning_rate": 0.0004387947269303202,
      "loss": 0.5719,
      "step": 196
    },
    {
      "epoch": 0.006184393823454392,
      "grad_norm": 0.20884180068969727,
      "learning_rate": 0.0004384808537350911,
      "loss": 0.2364,
      "step": 197
    },
    {
      "epoch": 0.006215786685502384,
      "grad_norm": 0.07495586574077606,
      "learning_rate": 0.0004381669805398619,
      "loss": 0.0846,
      "step": 198
    },
    {
      "epoch": 0.006247179547550376,
      "grad_norm": 0.48184916377067566,
      "learning_rate": 0.0004378531073446328,
      "loss": 0.4098,
      "step": 199
    },
    {
      "epoch": 0.006278572409598368,
      "grad_norm": 1.050857663154602,
      "learning_rate": 0.00043753923414940367,
      "loss": 0.5771,
      "step": 200
    },
    {
      "epoch": 0.00630996527164636,
      "grad_norm": 0.3095160126686096,
      "learning_rate": 0.00043722536095417455,
      "loss": 0.4022,
      "step": 201
    },
    {
      "epoch": 0.006341358133694352,
      "grad_norm": 0.3285010755062103,
      "learning_rate": 0.0004369114877589454,
      "loss": 0.2496,
      "step": 202
    },
    {
      "epoch": 0.006372750995742343,
      "grad_norm": 0.17246532440185547,
      "learning_rate": 0.00043659761456371626,
      "loss": 0.1971,
      "step": 203
    },
    {
      "epoch": 0.006404143857790335,
      "grad_norm": 0.8994677662849426,
      "learning_rate": 0.00043628374136848715,
      "loss": 0.6045,
      "step": 204
    },
    {
      "epoch": 0.0064355367198383265,
      "grad_norm": 0.4659116864204407,
      "learning_rate": 0.00043596986817325803,
      "loss": 0.3702,
      "step": 205
    },
    {
      "epoch": 0.006466929581886318,
      "grad_norm": 0.4711190164089203,
      "learning_rate": 0.00043565599497802886,
      "loss": 0.6637,
      "step": 206
    },
    {
      "epoch": 0.00649832244393431,
      "grad_norm": 0.2232106477022171,
      "learning_rate": 0.00043534212178279974,
      "loss": 0.2248,
      "step": 207
    },
    {
      "epoch": 0.006529715305982302,
      "grad_norm": 0.2913540303707123,
      "learning_rate": 0.0004350282485875706,
      "loss": 0.2479,
      "step": 208
    },
    {
      "epoch": 0.006561108168030294,
      "grad_norm": 0.10530843585729599,
      "learning_rate": 0.0004347143753923415,
      "loss": 0.0715,
      "step": 209
    },
    {
      "epoch": 0.006592501030078286,
      "grad_norm": 0.13930724561214447,
      "learning_rate": 0.00043440050219711233,
      "loss": 0.1345,
      "step": 210
    },
    {
      "epoch": 0.006623893892126278,
      "grad_norm": 0.1898338943719864,
      "learning_rate": 0.0004340866290018832,
      "loss": 0.2369,
      "step": 211
    },
    {
      "epoch": 0.00665528675417427,
      "grad_norm": 0.6115697622299194,
      "learning_rate": 0.0004337727558066541,
      "loss": 0.5014,
      "step": 212
    },
    {
      "epoch": 0.006686679616222262,
      "grad_norm": 0.7597313523292542,
      "learning_rate": 0.00043345888261142504,
      "loss": 0.9477,
      "step": 213
    },
    {
      "epoch": 0.0067180724782702535,
      "grad_norm": 0.4137459993362427,
      "learning_rate": 0.00043314500941619586,
      "loss": 0.641,
      "step": 214
    },
    {
      "epoch": 0.0067494653403182454,
      "grad_norm": 0.28583240509033203,
      "learning_rate": 0.00043283113622096675,
      "loss": 0.2875,
      "step": 215
    },
    {
      "epoch": 0.006780858202366237,
      "grad_norm": 0.37229564785957336,
      "learning_rate": 0.00043251726302573763,
      "loss": 0.2563,
      "step": 216
    },
    {
      "epoch": 0.006812251064414229,
      "grad_norm": 0.3626352846622467,
      "learning_rate": 0.0004322033898305085,
      "loss": 0.3522,
      "step": 217
    },
    {
      "epoch": 0.00684364392646222,
      "grad_norm": 0.24693110585212708,
      "learning_rate": 0.00043188951663527934,
      "loss": 0.2187,
      "step": 218
    },
    {
      "epoch": 0.006875036788510212,
      "grad_norm": 0.5969752073287964,
      "learning_rate": 0.0004315756434400502,
      "loss": 0.4341,
      "step": 219
    },
    {
      "epoch": 0.006906429650558204,
      "grad_norm": 0.13004398345947266,
      "learning_rate": 0.0004312617702448211,
      "loss": 0.0951,
      "step": 220
    },
    {
      "epoch": 0.006937822512606196,
      "grad_norm": 0.34187600016593933,
      "learning_rate": 0.000430947897049592,
      "loss": 0.2431,
      "step": 221
    },
    {
      "epoch": 0.006969215374654188,
      "grad_norm": 0.10195444524288177,
      "learning_rate": 0.0004306340238543628,
      "loss": 0.0987,
      "step": 222
    },
    {
      "epoch": 0.00700060823670218,
      "grad_norm": 0.17440545558929443,
      "learning_rate": 0.0004303201506591337,
      "loss": 0.1255,
      "step": 223
    },
    {
      "epoch": 0.007032001098750172,
      "grad_norm": 0.2901516258716583,
      "learning_rate": 0.0004300062774639046,
      "loss": 0.3662,
      "step": 224
    },
    {
      "epoch": 0.0070633939607981635,
      "grad_norm": 0.13743190467357635,
      "learning_rate": 0.00042969240426867547,
      "loss": 0.1563,
      "step": 225
    },
    {
      "epoch": 0.007094786822846155,
      "grad_norm": 0.12324824184179306,
      "learning_rate": 0.0004293785310734463,
      "loss": 0.138,
      "step": 226
    },
    {
      "epoch": 0.007126179684894147,
      "grad_norm": 0.7292394042015076,
      "learning_rate": 0.0004290646578782172,
      "loss": 0.6259,
      "step": 227
    },
    {
      "epoch": 0.007157572546942139,
      "grad_norm": 0.11175162345170975,
      "learning_rate": 0.00042875078468298806,
      "loss": 0.1159,
      "step": 228
    },
    {
      "epoch": 0.007188965408990131,
      "grad_norm": 0.5174059867858887,
      "learning_rate": 0.00042843691148775894,
      "loss": 0.4237,
      "step": 229
    },
    {
      "epoch": 0.007220358271038123,
      "grad_norm": 0.09735316783189774,
      "learning_rate": 0.0004281230382925299,
      "loss": 0.1251,
      "step": 230
    },
    {
      "epoch": 0.007251751133086115,
      "grad_norm": 0.13511084020137787,
      "learning_rate": 0.0004278091650973007,
      "loss": 0.1571,
      "step": 231
    },
    {
      "epoch": 0.007283143995134107,
      "grad_norm": 0.345018208026886,
      "learning_rate": 0.0004274952919020716,
      "loss": 0.2469,
      "step": 232
    },
    {
      "epoch": 0.007314536857182098,
      "grad_norm": 0.15722493827342987,
      "learning_rate": 0.0004271814187068425,
      "loss": 0.1777,
      "step": 233
    },
    {
      "epoch": 0.00734592971923009,
      "grad_norm": 0.1342659741640091,
      "learning_rate": 0.00042686754551161336,
      "loss": 0.1616,
      "step": 234
    },
    {
      "epoch": 0.007377322581278082,
      "grad_norm": 0.6091296672821045,
      "learning_rate": 0.0004265536723163842,
      "loss": 0.6736,
      "step": 235
    },
    {
      "epoch": 0.0074087154433260735,
      "grad_norm": 0.2699945867061615,
      "learning_rate": 0.00042623979912115507,
      "loss": 0.3886,
      "step": 236
    },
    {
      "epoch": 0.007440108305374065,
      "grad_norm": 0.13952302932739258,
      "learning_rate": 0.00042592592592592595,
      "loss": 0.1427,
      "step": 237
    },
    {
      "epoch": 0.007471501167422057,
      "grad_norm": 0.2061435431241989,
      "learning_rate": 0.00042561205273069683,
      "loss": 0.2128,
      "step": 238
    },
    {
      "epoch": 0.007502894029470049,
      "grad_norm": 0.6011762022972107,
      "learning_rate": 0.00042529817953546766,
      "loss": 0.726,
      "step": 239
    },
    {
      "epoch": 0.007534286891518041,
      "grad_norm": 0.4378836154937744,
      "learning_rate": 0.00042498430634023854,
      "loss": 0.5623,
      "step": 240
    },
    {
      "epoch": 0.007565679753566033,
      "grad_norm": 0.09827189147472382,
      "learning_rate": 0.00042467043314500943,
      "loss": 0.1523,
      "step": 241
    },
    {
      "epoch": 0.007597072615614025,
      "grad_norm": 0.13745637238025665,
      "learning_rate": 0.0004243565599497803,
      "loss": 0.1485,
      "step": 242
    },
    {
      "epoch": 0.007628465477662017,
      "grad_norm": 0.39443010091781616,
      "learning_rate": 0.00042404268675455114,
      "loss": 0.2939,
      "step": 243
    },
    {
      "epoch": 0.007659858339710009,
      "grad_norm": 0.23917275667190552,
      "learning_rate": 0.000423728813559322,
      "loss": 0.3003,
      "step": 244
    },
    {
      "epoch": 0.007691251201758001,
      "grad_norm": 0.1360331028699875,
      "learning_rate": 0.0004234149403640929,
      "loss": 0.0863,
      "step": 245
    },
    {
      "epoch": 0.0077226440638059925,
      "grad_norm": 0.4865764081478119,
      "learning_rate": 0.0004231010671688638,
      "loss": 0.5566,
      "step": 246
    },
    {
      "epoch": 0.007754036925853984,
      "grad_norm": 0.5750142335891724,
      "learning_rate": 0.00042278719397363467,
      "loss": 0.5706,
      "step": 247
    },
    {
      "epoch": 0.007785429787901975,
      "grad_norm": 0.22778449952602386,
      "learning_rate": 0.00042247332077840555,
      "loss": 0.2275,
      "step": 248
    },
    {
      "epoch": 0.007816822649949968,
      "grad_norm": 0.16149292886257172,
      "learning_rate": 0.00042215944758317644,
      "loss": 0.1687,
      "step": 249
    },
    {
      "epoch": 0.00784821551199796,
      "grad_norm": 0.14878077805042267,
      "learning_rate": 0.0004218455743879473,
      "loss": 0.0977,
      "step": 250
    },
    {
      "epoch": 0.007879608374045952,
      "grad_norm": 0.12995260953903198,
      "learning_rate": 0.00042153170119271815,
      "loss": 0.0648,
      "step": 251
    },
    {
      "epoch": 0.007911001236093944,
      "grad_norm": 0.1695975810289383,
      "learning_rate": 0.00042121782799748903,
      "loss": 0.1025,
      "step": 252
    },
    {
      "epoch": 0.007942394098141936,
      "grad_norm": 0.5052897930145264,
      "learning_rate": 0.0004209039548022599,
      "loss": 0.5384,
      "step": 253
    },
    {
      "epoch": 0.007973786960189928,
      "grad_norm": 0.13117747008800507,
      "learning_rate": 0.0004205900816070308,
      "loss": 0.121,
      "step": 254
    },
    {
      "epoch": 0.008005179822237918,
      "grad_norm": 0.09745685756206512,
      "learning_rate": 0.0004202762084118016,
      "loss": 0.0886,
      "step": 255
    },
    {
      "epoch": 0.00803657268428591,
      "grad_norm": 0.3442350924015045,
      "learning_rate": 0.0004199623352165725,
      "loss": 0.532,
      "step": 256
    },
    {
      "epoch": 0.008067965546333902,
      "grad_norm": 0.16567160189151764,
      "learning_rate": 0.0004196484620213434,
      "loss": 0.1376,
      "step": 257
    },
    {
      "epoch": 0.008099358408381893,
      "grad_norm": 0.059566859155893326,
      "learning_rate": 0.00041933458882611427,
      "loss": 0.0763,
      "step": 258
    },
    {
      "epoch": 0.008130751270429885,
      "grad_norm": 0.4386281371116638,
      "learning_rate": 0.0004190207156308851,
      "loss": 0.4708,
      "step": 259
    },
    {
      "epoch": 0.008162144132477877,
      "grad_norm": 0.07012102752923965,
      "learning_rate": 0.000418706842435656,
      "loss": 0.0638,
      "step": 260
    },
    {
      "epoch": 0.00819353699452587,
      "grad_norm": 0.16345606744289398,
      "learning_rate": 0.00041839296924042687,
      "loss": 0.1313,
      "step": 261
    },
    {
      "epoch": 0.008224929856573861,
      "grad_norm": 0.08083750307559967,
      "learning_rate": 0.00041807909604519775,
      "loss": 0.0753,
      "step": 262
    },
    {
      "epoch": 0.008256322718621853,
      "grad_norm": 0.148153617978096,
      "learning_rate": 0.0004177652228499686,
      "loss": 0.1915,
      "step": 263
    },
    {
      "epoch": 0.008287715580669845,
      "grad_norm": 0.858864426612854,
      "learning_rate": 0.0004174513496547395,
      "loss": 0.5661,
      "step": 264
    },
    {
      "epoch": 0.008319108442717837,
      "grad_norm": 0.3431791663169861,
      "learning_rate": 0.0004171374764595104,
      "loss": 0.5547,
      "step": 265
    },
    {
      "epoch": 0.008350501304765829,
      "grad_norm": 0.6945920586585999,
      "learning_rate": 0.0004168236032642813,
      "loss": 0.3599,
      "step": 266
    },
    {
      "epoch": 0.00838189416681382,
      "grad_norm": 0.10839489847421646,
      "learning_rate": 0.0004165097300690521,
      "loss": 0.0937,
      "step": 267
    },
    {
      "epoch": 0.008413287028861812,
      "grad_norm": 0.28981366753578186,
      "learning_rate": 0.000416195856873823,
      "loss": 0.3089,
      "step": 268
    },
    {
      "epoch": 0.008444679890909804,
      "grad_norm": 0.2190171331167221,
      "learning_rate": 0.0004158819836785939,
      "loss": 0.1963,
      "step": 269
    },
    {
      "epoch": 0.008476072752957796,
      "grad_norm": 0.09046245366334915,
      "learning_rate": 0.00041556811048336476,
      "loss": 0.1083,
      "step": 270
    },
    {
      "epoch": 0.008507465615005788,
      "grad_norm": 0.31608304381370544,
      "learning_rate": 0.0004152542372881356,
      "loss": 0.3235,
      "step": 271
    },
    {
      "epoch": 0.00853885847705378,
      "grad_norm": 0.46136388182640076,
      "learning_rate": 0.00041494036409290647,
      "loss": 0.374,
      "step": 272
    },
    {
      "epoch": 0.008570251339101772,
      "grad_norm": 0.9492784738540649,
      "learning_rate": 0.00041462649089767735,
      "loss": 0.6664,
      "step": 273
    },
    {
      "epoch": 0.008601644201149764,
      "grad_norm": 0.366775780916214,
      "learning_rate": 0.00041431261770244823,
      "loss": 0.4484,
      "step": 274
    },
    {
      "epoch": 0.008633037063197756,
      "grad_norm": 0.30082371830940247,
      "learning_rate": 0.00041399874450721906,
      "loss": 0.2442,
      "step": 275
    },
    {
      "epoch": 0.008664429925245748,
      "grad_norm": 0.22125345468521118,
      "learning_rate": 0.00041368487131198994,
      "loss": 0.2265,
      "step": 276
    },
    {
      "epoch": 0.00869582278729374,
      "grad_norm": 0.16708774864673615,
      "learning_rate": 0.0004133709981167608,
      "loss": 0.199,
      "step": 277
    },
    {
      "epoch": 0.008727215649341731,
      "grad_norm": 0.090663842856884,
      "learning_rate": 0.0004130571249215317,
      "loss": 0.0671,
      "step": 278
    },
    {
      "epoch": 0.008758608511389723,
      "grad_norm": 0.14024698734283447,
      "learning_rate": 0.00041274325172630254,
      "loss": 0.1506,
      "step": 279
    },
    {
      "epoch": 0.008790001373437715,
      "grad_norm": 0.4157974421977997,
      "learning_rate": 0.0004124293785310734,
      "loss": 0.4469,
      "step": 280
    },
    {
      "epoch": 0.008821394235485707,
      "grad_norm": 0.8214335441589355,
      "learning_rate": 0.00041211550533584436,
      "loss": 0.7515,
      "step": 281
    },
    {
      "epoch": 0.008852787097533699,
      "grad_norm": 0.14746148884296417,
      "learning_rate": 0.00041180163214061524,
      "loss": 0.1452,
      "step": 282
    },
    {
      "epoch": 0.008884179959581691,
      "grad_norm": 0.3938036561012268,
      "learning_rate": 0.00041148775894538607,
      "loss": 0.265,
      "step": 283
    },
    {
      "epoch": 0.008915572821629683,
      "grad_norm": 0.3587690591812134,
      "learning_rate": 0.00041117388575015695,
      "loss": 0.3861,
      "step": 284
    },
    {
      "epoch": 0.008946965683677673,
      "grad_norm": 0.37818604707717896,
      "learning_rate": 0.00041086001255492783,
      "loss": 0.3778,
      "step": 285
    },
    {
      "epoch": 0.008978358545725665,
      "grad_norm": 0.5733450055122375,
      "learning_rate": 0.0004105461393596987,
      "loss": 0.3763,
      "step": 286
    },
    {
      "epoch": 0.009009751407773657,
      "grad_norm": 0.10780566930770874,
      "learning_rate": 0.00041023226616446955,
      "loss": 0.1008,
      "step": 287
    },
    {
      "epoch": 0.009041144269821649,
      "grad_norm": 0.5866308212280273,
      "learning_rate": 0.00040991839296924043,
      "loss": 0.5841,
      "step": 288
    },
    {
      "epoch": 0.00907253713186964,
      "grad_norm": 0.31997111439704895,
      "learning_rate": 0.0004096045197740113,
      "loss": 0.3167,
      "step": 289
    },
    {
      "epoch": 0.009103929993917632,
      "grad_norm": 0.4030456244945526,
      "learning_rate": 0.0004092906465787822,
      "loss": 0.3932,
      "step": 290
    },
    {
      "epoch": 0.009135322855965624,
      "grad_norm": 0.10402122139930725,
      "learning_rate": 0.000408976773383553,
      "loss": 0.1371,
      "step": 291
    },
    {
      "epoch": 0.009166715718013616,
      "grad_norm": 0.0808812603354454,
      "learning_rate": 0.0004086629001883239,
      "loss": 0.0806,
      "step": 292
    },
    {
      "epoch": 0.009198108580061608,
      "grad_norm": 0.06989949941635132,
      "learning_rate": 0.0004083490269930948,
      "loss": 0.0819,
      "step": 293
    },
    {
      "epoch": 0.0092295014421096,
      "grad_norm": 0.11068540811538696,
      "learning_rate": 0.00040803515379786567,
      "loss": 0.1125,
      "step": 294
    },
    {
      "epoch": 0.009260894304157592,
      "grad_norm": 0.08902270346879959,
      "learning_rate": 0.0004077212806026365,
      "loss": 0.0798,
      "step": 295
    },
    {
      "epoch": 0.009292287166205584,
      "grad_norm": 0.11246879398822784,
      "learning_rate": 0.0004074074074074074,
      "loss": 0.1075,
      "step": 296
    },
    {
      "epoch": 0.009323680028253576,
      "grad_norm": 0.3406447768211365,
      "learning_rate": 0.00040709353421217826,
      "loss": 0.3038,
      "step": 297
    },
    {
      "epoch": 0.009355072890301568,
      "grad_norm": 0.4925673007965088,
      "learning_rate": 0.0004067796610169492,
      "loss": 0.436,
      "step": 298
    },
    {
      "epoch": 0.00938646575234956,
      "grad_norm": 0.14237207174301147,
      "learning_rate": 0.00040646578782172003,
      "loss": 0.1401,
      "step": 299
    },
    {
      "epoch": 0.009417858614397551,
      "grad_norm": 0.10493702441453934,
      "learning_rate": 0.0004061519146264909,
      "loss": 0.1043,
      "step": 300
    },
    {
      "epoch": 0.009449251476445543,
      "grad_norm": 0.1742645800113678,
      "learning_rate": 0.0004058380414312618,
      "loss": 0.1965,
      "step": 301
    },
    {
      "epoch": 0.009480644338493535,
      "grad_norm": 0.5708502531051636,
      "learning_rate": 0.0004055241682360327,
      "loss": 0.4388,
      "step": 302
    },
    {
      "epoch": 0.009512037200541527,
      "grad_norm": 0.3391205072402954,
      "learning_rate": 0.0004052102950408035,
      "loss": 0.3389,
      "step": 303
    },
    {
      "epoch": 0.009543430062589519,
      "grad_norm": 0.3149205446243286,
      "learning_rate": 0.0004048964218455744,
      "loss": 0.3519,
      "step": 304
    },
    {
      "epoch": 0.00957482292463751,
      "grad_norm": 0.39305728673934937,
      "learning_rate": 0.00040458254865034527,
      "loss": 0.3502,
      "step": 305
    },
    {
      "epoch": 0.009606215786685503,
      "grad_norm": 0.7561007738113403,
      "learning_rate": 0.00040426867545511615,
      "loss": 1.034,
      "step": 306
    },
    {
      "epoch": 0.009637608648733495,
      "grad_norm": 0.1134338527917862,
      "learning_rate": 0.00040395480225988704,
      "loss": 0.0837,
      "step": 307
    },
    {
      "epoch": 0.009669001510781487,
      "grad_norm": 0.1480364054441452,
      "learning_rate": 0.00040364092906465787,
      "loss": 0.2634,
      "step": 308
    },
    {
      "epoch": 0.009700394372829478,
      "grad_norm": 0.5152333974838257,
      "learning_rate": 0.00040332705586942875,
      "loss": 0.6904,
      "step": 309
    },
    {
      "epoch": 0.00973178723487747,
      "grad_norm": 0.23424407839775085,
      "learning_rate": 0.00040301318267419963,
      "loss": 0.2931,
      "step": 310
    },
    {
      "epoch": 0.009763180096925462,
      "grad_norm": 0.15584316849708557,
      "learning_rate": 0.0004026993094789705,
      "loss": 0.1789,
      "step": 311
    },
    {
      "epoch": 0.009794572958973454,
      "grad_norm": 0.17075270414352417,
      "learning_rate": 0.00040238543628374134,
      "loss": 0.1297,
      "step": 312
    },
    {
      "epoch": 0.009825965821021446,
      "grad_norm": 0.599952220916748,
      "learning_rate": 0.0004020715630885122,
      "loss": 0.5934,
      "step": 313
    },
    {
      "epoch": 0.009857358683069438,
      "grad_norm": 0.11592965573072433,
      "learning_rate": 0.0004017576898932831,
      "loss": 0.1748,
      "step": 314
    },
    {
      "epoch": 0.009888751545117428,
      "grad_norm": 0.19406600296497345,
      "learning_rate": 0.00040144381669805405,
      "loss": 0.298,
      "step": 315
    },
    {
      "epoch": 0.00992014440716542,
      "grad_norm": 0.47990116477012634,
      "learning_rate": 0.0004011299435028249,
      "loss": 0.3843,
      "step": 316
    },
    {
      "epoch": 0.009951537269213412,
      "grad_norm": 0.12471677362918854,
      "learning_rate": 0.00040081607030759576,
      "loss": 0.1499,
      "step": 317
    },
    {
      "epoch": 0.009982930131261404,
      "grad_norm": 0.08782271295785904,
      "learning_rate": 0.00040050219711236664,
      "loss": 0.1046,
      "step": 318
    },
    {
      "epoch": 0.010014322993309396,
      "grad_norm": 0.21217529475688934,
      "learning_rate": 0.0004001883239171375,
      "loss": 0.2197,
      "step": 319
    },
    {
      "epoch": 0.010045715855357388,
      "grad_norm": 0.2277897745370865,
      "learning_rate": 0.00039987445072190835,
      "loss": 0.2281,
      "step": 320
    },
    {
      "epoch": 0.01007710871740538,
      "grad_norm": 0.2417740821838379,
      "learning_rate": 0.00039956057752667923,
      "loss": 0.2645,
      "step": 321
    },
    {
      "epoch": 0.010108501579453371,
      "grad_norm": 0.2583855986595154,
      "learning_rate": 0.0003992467043314501,
      "loss": 0.3293,
      "step": 322
    },
    {
      "epoch": 0.010139894441501363,
      "grad_norm": 0.4027941823005676,
      "learning_rate": 0.000398932831136221,
      "loss": 0.4066,
      "step": 323
    },
    {
      "epoch": 0.010171287303549355,
      "grad_norm": 0.24893797934055328,
      "learning_rate": 0.00039861895794099183,
      "loss": 0.1969,
      "step": 324
    },
    {
      "epoch": 0.010202680165597347,
      "grad_norm": 0.1169666051864624,
      "learning_rate": 0.0003983050847457627,
      "loss": 0.0788,
      "step": 325
    },
    {
      "epoch": 0.010234073027645339,
      "grad_norm": 0.3906494975090027,
      "learning_rate": 0.0003979912115505336,
      "loss": 0.4704,
      "step": 326
    },
    {
      "epoch": 0.01026546588969333,
      "grad_norm": 0.5414161086082458,
      "learning_rate": 0.0003976773383553045,
      "loss": 0.4063,
      "step": 327
    },
    {
      "epoch": 0.010296858751741323,
      "grad_norm": 1.1639254093170166,
      "learning_rate": 0.0003973634651600753,
      "loss": 0.8853,
      "step": 328
    },
    {
      "epoch": 0.010328251613789315,
      "grad_norm": 0.14771905541419983,
      "learning_rate": 0.0003970495919648462,
      "loss": 0.0655,
      "step": 329
    },
    {
      "epoch": 0.010359644475837307,
      "grad_norm": 0.2193101942539215,
      "learning_rate": 0.00039673571876961707,
      "loss": 0.2908,
      "step": 330
    },
    {
      "epoch": 0.010391037337885298,
      "grad_norm": 0.6950047016143799,
      "learning_rate": 0.00039642184557438795,
      "loss": 0.6595,
      "step": 331
    },
    {
      "epoch": 0.01042243019993329,
      "grad_norm": 0.8517575263977051,
      "learning_rate": 0.00039610797237915883,
      "loss": 0.9694,
      "step": 332
    },
    {
      "epoch": 0.010453823061981282,
      "grad_norm": 0.12538664042949677,
      "learning_rate": 0.0003957940991839297,
      "loss": 0.0898,
      "step": 333
    },
    {
      "epoch": 0.010485215924029274,
      "grad_norm": 0.27100512385368347,
      "learning_rate": 0.0003954802259887006,
      "loss": 0.3037,
      "step": 334
    },
    {
      "epoch": 0.010516608786077266,
      "grad_norm": 0.15106834471225739,
      "learning_rate": 0.0003951663527934715,
      "loss": 0.0927,
      "step": 335
    },
    {
      "epoch": 0.010548001648125258,
      "grad_norm": 0.14369209110736847,
      "learning_rate": 0.0003948524795982423,
      "loss": 0.1662,
      "step": 336
    },
    {
      "epoch": 0.01057939451017325,
      "grad_norm": 0.29189959168434143,
      "learning_rate": 0.0003945386064030132,
      "loss": 0.2687,
      "step": 337
    },
    {
      "epoch": 0.010610787372221242,
      "grad_norm": 0.5646464824676514,
      "learning_rate": 0.0003942247332077841,
      "loss": 0.6332,
      "step": 338
    },
    {
      "epoch": 0.010642180234269234,
      "grad_norm": 0.21042315661907196,
      "learning_rate": 0.00039391086001255496,
      "loss": 0.2167,
      "step": 339
    },
    {
      "epoch": 0.010673573096317225,
      "grad_norm": 0.304057776927948,
      "learning_rate": 0.0003935969868173258,
      "loss": 0.3883,
      "step": 340
    },
    {
      "epoch": 0.010704965958365217,
      "grad_norm": 0.26680988073349,
      "learning_rate": 0.00039328311362209667,
      "loss": 0.3793,
      "step": 341
    },
    {
      "epoch": 0.01073635882041321,
      "grad_norm": 0.12470843642950058,
      "learning_rate": 0.00039296924042686755,
      "loss": 0.1882,
      "step": 342
    },
    {
      "epoch": 0.010767751682461201,
      "grad_norm": 0.09472187608480453,
      "learning_rate": 0.00039265536723163844,
      "loss": 0.0696,
      "step": 343
    },
    {
      "epoch": 0.010799144544509193,
      "grad_norm": 0.14089860022068024,
      "learning_rate": 0.00039234149403640927,
      "loss": 0.2032,
      "step": 344
    },
    {
      "epoch": 0.010830537406557183,
      "grad_norm": 0.4323394000530243,
      "learning_rate": 0.00039202762084118015,
      "loss": 0.3729,
      "step": 345
    },
    {
      "epoch": 0.010861930268605175,
      "grad_norm": 0.16112515330314636,
      "learning_rate": 0.00039171374764595103,
      "loss": 0.1413,
      "step": 346
    },
    {
      "epoch": 0.010893323130653167,
      "grad_norm": 0.4631687104701996,
      "learning_rate": 0.0003913998744507219,
      "loss": 0.7119,
      "step": 347
    },
    {
      "epoch": 0.010924715992701159,
      "grad_norm": 0.13551479578018188,
      "learning_rate": 0.00039108600125549274,
      "loss": 0.1443,
      "step": 348
    },
    {
      "epoch": 0.01095610885474915,
      "grad_norm": 0.16456685960292816,
      "learning_rate": 0.0003907721280602637,
      "loss": 0.182,
      "step": 349
    },
    {
      "epoch": 0.010987501716797143,
      "grad_norm": 0.5428619384765625,
      "learning_rate": 0.00039045825486503456,
      "loss": 0.4821,
      "step": 350
    },
    {
      "epoch": 0.011018894578845135,
      "grad_norm": 0.23991285264492035,
      "learning_rate": 0.00039014438166980544,
      "loss": 0.317,
      "step": 351
    },
    {
      "epoch": 0.011050287440893126,
      "grad_norm": 0.3527587950229645,
      "learning_rate": 0.00038983050847457627,
      "loss": 0.3428,
      "step": 352
    },
    {
      "epoch": 0.011081680302941118,
      "grad_norm": 0.2137792855501175,
      "learning_rate": 0.00038951663527934716,
      "loss": 0.25,
      "step": 353
    },
    {
      "epoch": 0.01111307316498911,
      "grad_norm": 0.1634453982114792,
      "learning_rate": 0.00038920276208411804,
      "loss": 0.1798,
      "step": 354
    },
    {
      "epoch": 0.011144466027037102,
      "grad_norm": 0.6010992527008057,
      "learning_rate": 0.0003888888888888889,
      "loss": 0.7254,
      "step": 355
    },
    {
      "epoch": 0.011175858889085094,
      "grad_norm": 0.19525359570980072,
      "learning_rate": 0.00038857501569365975,
      "loss": 0.2669,
      "step": 356
    },
    {
      "epoch": 0.011207251751133086,
      "grad_norm": 0.22716408967971802,
      "learning_rate": 0.00038826114249843063,
      "loss": 0.2074,
      "step": 357
    },
    {
      "epoch": 0.011238644613181078,
      "grad_norm": 0.16461077332496643,
      "learning_rate": 0.0003879472693032015,
      "loss": 0.1712,
      "step": 358
    },
    {
      "epoch": 0.01127003747522907,
      "grad_norm": 0.10642246901988983,
      "learning_rate": 0.0003876333961079724,
      "loss": 0.1116,
      "step": 359
    },
    {
      "epoch": 0.011301430337277062,
      "grad_norm": 0.2809276580810547,
      "learning_rate": 0.0003873195229127432,
      "loss": 0.3383,
      "step": 360
    },
    {
      "epoch": 0.011332823199325054,
      "grad_norm": 0.12472961843013763,
      "learning_rate": 0.0003870056497175141,
      "loss": 0.1668,
      "step": 361
    },
    {
      "epoch": 0.011364216061373045,
      "grad_norm": 0.30314400792121887,
      "learning_rate": 0.000386691776522285,
      "loss": 0.3826,
      "step": 362
    },
    {
      "epoch": 0.011395608923421037,
      "grad_norm": 0.19785869121551514,
      "learning_rate": 0.0003863779033270559,
      "loss": 0.1468,
      "step": 363
    },
    {
      "epoch": 0.01142700178546903,
      "grad_norm": 0.2342146635055542,
      "learning_rate": 0.0003860640301318267,
      "loss": 0.2427,
      "step": 364
    },
    {
      "epoch": 0.011458394647517021,
      "grad_norm": 0.08489097654819489,
      "learning_rate": 0.0003857501569365976,
      "loss": 0.1269,
      "step": 365
    },
    {
      "epoch": 0.011489787509565013,
      "grad_norm": 0.2625172734260559,
      "learning_rate": 0.0003854362837413685,
      "loss": 0.2872,
      "step": 366
    },
    {
      "epoch": 0.011521180371613005,
      "grad_norm": 0.08170238882303238,
      "learning_rate": 0.0003851224105461394,
      "loss": 0.0936,
      "step": 367
    },
    {
      "epoch": 0.011552573233660997,
      "grad_norm": 0.1924302726984024,
      "learning_rate": 0.00038480853735091023,
      "loss": 0.2537,
      "step": 368
    },
    {
      "epoch": 0.011583966095708989,
      "grad_norm": 0.37206608057022095,
      "learning_rate": 0.0003844946641556811,
      "loss": 0.5084,
      "step": 369
    },
    {
      "epoch": 0.01161535895775698,
      "grad_norm": 0.10552245378494263,
      "learning_rate": 0.000384180790960452,
      "loss": 0.1437,
      "step": 370
    },
    {
      "epoch": 0.011646751819804972,
      "grad_norm": 0.16660639643669128,
      "learning_rate": 0.0003838669177652229,
      "loss": 0.2048,
      "step": 371
    },
    {
      "epoch": 0.011678144681852964,
      "grad_norm": 0.09285039454698563,
      "learning_rate": 0.0003835530445699937,
      "loss": 0.121,
      "step": 372
    },
    {
      "epoch": 0.011709537543900956,
      "grad_norm": 0.4283754825592041,
      "learning_rate": 0.0003832391713747646,
      "loss": 0.4061,
      "step": 373
    },
    {
      "epoch": 0.011740930405948948,
      "grad_norm": 0.09694583714008331,
      "learning_rate": 0.0003829252981795355,
      "loss": 0.0631,
      "step": 374
    },
    {
      "epoch": 0.011772323267996938,
      "grad_norm": 0.07585065066814423,
      "learning_rate": 0.00038261142498430636,
      "loss": 0.0839,
      "step": 375
    },
    {
      "epoch": 0.01180371613004493,
      "grad_norm": 0.13653281331062317,
      "learning_rate": 0.0003822975517890772,
      "loss": 0.1819,
      "step": 376
    },
    {
      "epoch": 0.011835108992092922,
      "grad_norm": 0.1389441192150116,
      "learning_rate": 0.00038198367859384807,
      "loss": 0.1656,
      "step": 377
    },
    {
      "epoch": 0.011866501854140914,
      "grad_norm": 0.12157243490219116,
      "learning_rate": 0.00038166980539861895,
      "loss": 0.0967,
      "step": 378
    },
    {
      "epoch": 0.011897894716188906,
      "grad_norm": 0.12734529376029968,
      "learning_rate": 0.00038135593220338984,
      "loss": 0.1423,
      "step": 379
    },
    {
      "epoch": 0.011929287578236898,
      "grad_norm": 0.48109254240989685,
      "learning_rate": 0.00038104205900816066,
      "loss": 0.5307,
      "step": 380
    },
    {
      "epoch": 0.01196068044028489,
      "grad_norm": 0.06515716016292572,
      "learning_rate": 0.00038072818581293155,
      "loss": 0.0702,
      "step": 381
    },
    {
      "epoch": 0.011992073302332882,
      "grad_norm": 0.18877924978733063,
      "learning_rate": 0.00038041431261770243,
      "loss": 0.2181,
      "step": 382
    },
    {
      "epoch": 0.012023466164380874,
      "grad_norm": 0.3045728802680969,
      "learning_rate": 0.00038010043942247337,
      "loss": 0.2283,
      "step": 383
    },
    {
      "epoch": 0.012054859026428865,
      "grad_norm": 0.23083168268203735,
      "learning_rate": 0.00037978656622724425,
      "loss": 0.3283,
      "step": 384
    },
    {
      "epoch": 0.012086251888476857,
      "grad_norm": 0.46005040407180786,
      "learning_rate": 0.0003794726930320151,
      "loss": 0.326,
      "step": 385
    },
    {
      "epoch": 0.01211764475052485,
      "grad_norm": 0.18880565464496613,
      "learning_rate": 0.00037915881983678596,
      "loss": 0.2348,
      "step": 386
    },
    {
      "epoch": 0.012149037612572841,
      "grad_norm": 0.36152777075767517,
      "learning_rate": 0.00037884494664155684,
      "loss": 0.4609,
      "step": 387
    },
    {
      "epoch": 0.012180430474620833,
      "grad_norm": 0.32133862376213074,
      "learning_rate": 0.0003785310734463277,
      "loss": 0.2632,
      "step": 388
    },
    {
      "epoch": 0.012211823336668825,
      "grad_norm": 0.4059191346168518,
      "learning_rate": 0.00037821720025109855,
      "loss": 0.527,
      "step": 389
    },
    {
      "epoch": 0.012243216198716817,
      "grad_norm": 0.2856064736843109,
      "learning_rate": 0.00037790332705586944,
      "loss": 0.3084,
      "step": 390
    },
    {
      "epoch": 0.012274609060764809,
      "grad_norm": 0.35303544998168945,
      "learning_rate": 0.0003775894538606403,
      "loss": 0.2531,
      "step": 391
    },
    {
      "epoch": 0.0123060019228128,
      "grad_norm": 0.37042683362960815,
      "learning_rate": 0.0003772755806654112,
      "loss": 0.3244,
      "step": 392
    },
    {
      "epoch": 0.012337394784860792,
      "grad_norm": 0.085485078394413,
      "learning_rate": 0.00037696170747018203,
      "loss": 0.0785,
      "step": 393
    },
    {
      "epoch": 0.012368787646908784,
      "grad_norm": 0.45420143008232117,
      "learning_rate": 0.0003766478342749529,
      "loss": 0.6881,
      "step": 394
    },
    {
      "epoch": 0.012400180508956776,
      "grad_norm": 0.30421191453933716,
      "learning_rate": 0.0003763339610797238,
      "loss": 0.2508,
      "step": 395
    },
    {
      "epoch": 0.012431573371004768,
      "grad_norm": 0.3017801344394684,
      "learning_rate": 0.0003760200878844947,
      "loss": 0.2958,
      "step": 396
    },
    {
      "epoch": 0.01246296623305276,
      "grad_norm": 0.2847144603729248,
      "learning_rate": 0.0003757062146892655,
      "loss": 0.3217,
      "step": 397
    },
    {
      "epoch": 0.012494359095100752,
      "grad_norm": 0.28573349118232727,
      "learning_rate": 0.0003753923414940364,
      "loss": 0.333,
      "step": 398
    },
    {
      "epoch": 0.012525751957148744,
      "grad_norm": 0.5232532620429993,
      "learning_rate": 0.0003750784682988073,
      "loss": 0.6782,
      "step": 399
    },
    {
      "epoch": 0.012557144819196736,
      "grad_norm": 0.37670865654945374,
      "learning_rate": 0.0003747645951035782,
      "loss": 0.4393,
      "step": 400
    },
    {
      "epoch": 0.012588537681244728,
      "grad_norm": 0.6171202063560486,
      "learning_rate": 0.00037445072190834904,
      "loss": 0.3473,
      "step": 401
    },
    {
      "epoch": 0.01261993054329272,
      "grad_norm": 0.17356184124946594,
      "learning_rate": 0.0003741368487131199,
      "loss": 0.0805,
      "step": 402
    },
    {
      "epoch": 0.012651323405340711,
      "grad_norm": 0.5425993204116821,
      "learning_rate": 0.0003738229755178908,
      "loss": 0.796,
      "step": 403
    },
    {
      "epoch": 0.012682716267388703,
      "grad_norm": 0.25045129656791687,
      "learning_rate": 0.0003735091023226617,
      "loss": 0.0999,
      "step": 404
    },
    {
      "epoch": 0.012714109129436693,
      "grad_norm": 0.1153755709528923,
      "learning_rate": 0.0003731952291274325,
      "loss": 0.0811,
      "step": 405
    },
    {
      "epoch": 0.012745501991484685,
      "grad_norm": 0.17308388650417328,
      "learning_rate": 0.0003728813559322034,
      "loss": 0.1073,
      "step": 406
    },
    {
      "epoch": 0.012776894853532677,
      "grad_norm": 0.15567506849765778,
      "learning_rate": 0.0003725674827369743,
      "loss": 0.1711,
      "step": 407
    },
    {
      "epoch": 0.01280828771558067,
      "grad_norm": 0.3895443379878998,
      "learning_rate": 0.00037225360954174516,
      "loss": 0.3413,
      "step": 408
    },
    {
      "epoch": 0.012839680577628661,
      "grad_norm": 0.40349942445755005,
      "learning_rate": 0.000371939736346516,
      "loss": 0.5914,
      "step": 409
    },
    {
      "epoch": 0.012871073439676653,
      "grad_norm": 0.24536900222301483,
      "learning_rate": 0.0003716258631512869,
      "loss": 0.321,
      "step": 410
    },
    {
      "epoch": 0.012902466301724645,
      "grad_norm": 0.3071572780609131,
      "learning_rate": 0.00037131198995605776,
      "loss": 0.4537,
      "step": 411
    },
    {
      "epoch": 0.012933859163772637,
      "grad_norm": 0.2573400139808655,
      "learning_rate": 0.00037099811676082864,
      "loss": 0.3812,
      "step": 412
    },
    {
      "epoch": 0.012965252025820629,
      "grad_norm": 0.3048875033855438,
      "learning_rate": 0.00037068424356559947,
      "loss": 0.4702,
      "step": 413
    },
    {
      "epoch": 0.01299664488786862,
      "grad_norm": 0.04255865141749382,
      "learning_rate": 0.00037037037037037035,
      "loss": 0.0578,
      "step": 414
    },
    {
      "epoch": 0.013028037749916612,
      "grad_norm": 0.28975945711135864,
      "learning_rate": 0.00037005649717514123,
      "loss": 0.3316,
      "step": 415
    },
    {
      "epoch": 0.013059430611964604,
      "grad_norm": 0.22970609366893768,
      "learning_rate": 0.0003697426239799121,
      "loss": 0.2285,
      "step": 416
    },
    {
      "epoch": 0.013090823474012596,
      "grad_norm": 0.05889875814318657,
      "learning_rate": 0.000369428750784683,
      "loss": 0.0686,
      "step": 417
    },
    {
      "epoch": 0.013122216336060588,
      "grad_norm": 0.29453399777412415,
      "learning_rate": 0.0003691148775894539,
      "loss": 0.3868,
      "step": 418
    },
    {
      "epoch": 0.01315360919810858,
      "grad_norm": 0.10660131275653839,
      "learning_rate": 0.00036880100439422477,
      "loss": 0.125,
      "step": 419
    },
    {
      "epoch": 0.013185002060156572,
      "grad_norm": 0.5539801120758057,
      "learning_rate": 0.00036848713119899565,
      "loss": 0.4962,
      "step": 420
    },
    {
      "epoch": 0.013216394922204564,
      "grad_norm": 0.13828584551811218,
      "learning_rate": 0.0003681732580037665,
      "loss": 0.1757,
      "step": 421
    },
    {
      "epoch": 0.013247787784252556,
      "grad_norm": 0.39979544281959534,
      "learning_rate": 0.00036785938480853736,
      "loss": 0.5493,
      "step": 422
    },
    {
      "epoch": 0.013279180646300548,
      "grad_norm": 0.2024221569299698,
      "learning_rate": 0.00036754551161330824,
      "loss": 0.2799,
      "step": 423
    },
    {
      "epoch": 0.01331057350834854,
      "grad_norm": 0.21049994230270386,
      "learning_rate": 0.0003672316384180791,
      "loss": 0.3047,
      "step": 424
    },
    {
      "epoch": 0.013341966370396531,
      "grad_norm": 0.2188449501991272,
      "learning_rate": 0.00036691776522284995,
      "loss": 0.2266,
      "step": 425
    },
    {
      "epoch": 0.013373359232444523,
      "grad_norm": 0.14319999516010284,
      "learning_rate": 0.00036660389202762084,
      "loss": 0.2314,
      "step": 426
    },
    {
      "epoch": 0.013404752094492515,
      "grad_norm": 0.11876737326383591,
      "learning_rate": 0.0003662900188323917,
      "loss": 0.1414,
      "step": 427
    },
    {
      "epoch": 0.013436144956540507,
      "grad_norm": 0.29878032207489014,
      "learning_rate": 0.0003659761456371626,
      "loss": 0.3025,
      "step": 428
    },
    {
      "epoch": 0.013467537818588499,
      "grad_norm": 0.08209379017353058,
      "learning_rate": 0.00036566227244193343,
      "loss": 0.1069,
      "step": 429
    },
    {
      "epoch": 0.013498930680636491,
      "grad_norm": 0.08848581463098526,
      "learning_rate": 0.0003653483992467043,
      "loss": 0.1289,
      "step": 430
    },
    {
      "epoch": 0.013530323542684483,
      "grad_norm": 0.17036406695842743,
      "learning_rate": 0.0003650345260514752,
      "loss": 0.2689,
      "step": 431
    },
    {
      "epoch": 0.013561716404732475,
      "grad_norm": 0.11420828849077225,
      "learning_rate": 0.0003647206528562461,
      "loss": 0.1529,
      "step": 432
    },
    {
      "epoch": 0.013593109266780467,
      "grad_norm": 0.10864466428756714,
      "learning_rate": 0.0003644067796610169,
      "loss": 0.1459,
      "step": 433
    },
    {
      "epoch": 0.013624502128828458,
      "grad_norm": 0.11618626117706299,
      "learning_rate": 0.00036409290646578784,
      "loss": 0.1043,
      "step": 434
    },
    {
      "epoch": 0.013655894990876449,
      "grad_norm": 0.3513491153717041,
      "learning_rate": 0.0003637790332705587,
      "loss": 0.3633,
      "step": 435
    },
    {
      "epoch": 0.01368728785292444,
      "grad_norm": 0.3398301303386688,
      "learning_rate": 0.0003634651600753296,
      "loss": 0.4139,
      "step": 436
    },
    {
      "epoch": 0.013718680714972432,
      "grad_norm": 0.38055703043937683,
      "learning_rate": 0.00036315128688010044,
      "loss": 0.5726,
      "step": 437
    },
    {
      "epoch": 0.013750073577020424,
      "grad_norm": 0.10133642703294754,
      "learning_rate": 0.0003628374136848713,
      "loss": 0.1283,
      "step": 438
    },
    {
      "epoch": 0.013781466439068416,
      "grad_norm": 0.5991387963294983,
      "learning_rate": 0.0003625235404896422,
      "loss": 0.6197,
      "step": 439
    },
    {
      "epoch": 0.013812859301116408,
      "grad_norm": 0.2679746448993683,
      "learning_rate": 0.0003622096672944131,
      "loss": 0.3699,
      "step": 440
    },
    {
      "epoch": 0.0138442521631644,
      "grad_norm": 0.35546568036079407,
      "learning_rate": 0.0003618957940991839,
      "loss": 0.4065,
      "step": 441
    },
    {
      "epoch": 0.013875645025212392,
      "grad_norm": 0.06893204152584076,
      "learning_rate": 0.0003615819209039548,
      "loss": 0.0646,
      "step": 442
    },
    {
      "epoch": 0.013907037887260384,
      "grad_norm": 0.08696425706148148,
      "learning_rate": 0.0003612680477087257,
      "loss": 0.0763,
      "step": 443
    },
    {
      "epoch": 0.013938430749308376,
      "grad_norm": 0.5558043718338013,
      "learning_rate": 0.00036095417451349656,
      "loss": 0.6345,
      "step": 444
    },
    {
      "epoch": 0.013969823611356368,
      "grad_norm": 0.4516489803791046,
      "learning_rate": 0.0003606403013182674,
      "loss": 0.5019,
      "step": 445
    },
    {
      "epoch": 0.01400121647340436,
      "grad_norm": 0.2036416381597519,
      "learning_rate": 0.0003603264281230383,
      "loss": 0.2496,
      "step": 446
    },
    {
      "epoch": 0.014032609335452351,
      "grad_norm": 0.07995781302452087,
      "learning_rate": 0.00036001255492780916,
      "loss": 0.0893,
      "step": 447
    },
    {
      "epoch": 0.014064002197500343,
      "grad_norm": 0.5126325488090515,
      "learning_rate": 0.00035969868173258004,
      "loss": 0.5269,
      "step": 448
    },
    {
      "epoch": 0.014095395059548335,
      "grad_norm": 0.20367272198200226,
      "learning_rate": 0.00035938480853735087,
      "loss": 0.1968,
      "step": 449
    },
    {
      "epoch": 0.014126787921596327,
      "grad_norm": 0.41099923849105835,
      "learning_rate": 0.00035907093534212175,
      "loss": 0.6825,
      "step": 450
    },
    {
      "epoch": 0.014158180783644319,
      "grad_norm": 0.3278372585773468,
      "learning_rate": 0.0003587570621468927,
      "loss": 0.4316,
      "step": 451
    },
    {
      "epoch": 0.01418957364569231,
      "grad_norm": 0.37516388297080994,
      "learning_rate": 0.00035844318895166357,
      "loss": 0.5366,
      "step": 452
    },
    {
      "epoch": 0.014220966507740303,
      "grad_norm": 0.09302529692649841,
      "learning_rate": 0.0003581293157564344,
      "loss": 0.1124,
      "step": 453
    },
    {
      "epoch": 0.014252359369788295,
      "grad_norm": 0.10916230082511902,
      "learning_rate": 0.0003578154425612053,
      "loss": 0.1677,
      "step": 454
    },
    {
      "epoch": 0.014283752231836287,
      "grad_norm": 0.23817813396453857,
      "learning_rate": 0.00035750156936597616,
      "loss": 0.348,
      "step": 455
    },
    {
      "epoch": 0.014315145093884278,
      "grad_norm": 0.10098927468061447,
      "learning_rate": 0.00035718769617074705,
      "loss": 0.2082,
      "step": 456
    },
    {
      "epoch": 0.01434653795593227,
      "grad_norm": 0.39620810747146606,
      "learning_rate": 0.00035687382297551793,
      "loss": 0.5995,
      "step": 457
    },
    {
      "epoch": 0.014377930817980262,
      "grad_norm": 0.3474411070346832,
      "learning_rate": 0.00035655994978028876,
      "loss": 0.474,
      "step": 458
    },
    {
      "epoch": 0.014409323680028254,
      "grad_norm": 0.3846759796142578,
      "learning_rate": 0.00035624607658505964,
      "loss": 0.6887,
      "step": 459
    },
    {
      "epoch": 0.014440716542076246,
      "grad_norm": 0.5737337470054626,
      "learning_rate": 0.0003559322033898305,
      "loss": 0.9182,
      "step": 460
    },
    {
      "epoch": 0.014472109404124238,
      "grad_norm": 0.6170313954353333,
      "learning_rate": 0.0003556183301946014,
      "loss": 0.3788,
      "step": 461
    },
    {
      "epoch": 0.01450350226617223,
      "grad_norm": 0.30223560333251953,
      "learning_rate": 0.00035530445699937224,
      "loss": 0.3045,
      "step": 462
    },
    {
      "epoch": 0.014534895128220222,
      "grad_norm": 0.7941040992736816,
      "learning_rate": 0.0003549905838041431,
      "loss": 0.603,
      "step": 463
    },
    {
      "epoch": 0.014566287990268214,
      "grad_norm": 0.24407728016376495,
      "learning_rate": 0.000354676710608914,
      "loss": 0.2344,
      "step": 464
    },
    {
      "epoch": 0.014597680852316204,
      "grad_norm": 0.3062468469142914,
      "learning_rate": 0.0003543628374136849,
      "loss": 0.2921,
      "step": 465
    },
    {
      "epoch": 0.014629073714364196,
      "grad_norm": 0.6725497841835022,
      "learning_rate": 0.0003540489642184557,
      "loss": 0.6887,
      "step": 466
    },
    {
      "epoch": 0.014660466576412188,
      "grad_norm": 0.1278679221868515,
      "learning_rate": 0.0003537350910232266,
      "loss": 0.1586,
      "step": 467
    },
    {
      "epoch": 0.01469185943846018,
      "grad_norm": 0.46825432777404785,
      "learning_rate": 0.00035342121782799753,
      "loss": 0.5694,
      "step": 468
    },
    {
      "epoch": 0.014723252300508171,
      "grad_norm": 0.11341341584920883,
      "learning_rate": 0.0003531073446327684,
      "loss": 0.1459,
      "step": 469
    },
    {
      "epoch": 0.014754645162556163,
      "grad_norm": 0.8981351852416992,
      "learning_rate": 0.00035279347143753924,
      "loss": 0.8805,
      "step": 470
    },
    {
      "epoch": 0.014786038024604155,
      "grad_norm": 0.32232823967933655,
      "learning_rate": 0.0003524795982423101,
      "loss": 0.2767,
      "step": 471
    },
    {
      "epoch": 0.014817430886652147,
      "grad_norm": 0.1150660365819931,
      "learning_rate": 0.000352165725047081,
      "loss": 0.0859,
      "step": 472
    },
    {
      "epoch": 0.014848823748700139,
      "grad_norm": 0.21488453447818756,
      "learning_rate": 0.0003518518518518519,
      "loss": 0.1926,
      "step": 473
    },
    {
      "epoch": 0.01488021661074813,
      "grad_norm": 0.37961435317993164,
      "learning_rate": 0.0003515379786566227,
      "loss": 0.2766,
      "step": 474
    },
    {
      "epoch": 0.014911609472796123,
      "grad_norm": 0.380583792924881,
      "learning_rate": 0.0003512241054613936,
      "loss": 0.4563,
      "step": 475
    },
    {
      "epoch": 0.014943002334844115,
      "grad_norm": 0.4914204776287079,
      "learning_rate": 0.0003509102322661645,
      "loss": 0.3304,
      "step": 476
    },
    {
      "epoch": 0.014974395196892107,
      "grad_norm": 0.32656237483024597,
      "learning_rate": 0.00035059635907093537,
      "loss": 0.4347,
      "step": 477
    },
    {
      "epoch": 0.015005788058940098,
      "grad_norm": 0.2636162042617798,
      "learning_rate": 0.0003502824858757062,
      "loss": 0.2887,
      "step": 478
    },
    {
      "epoch": 0.01503718092098809,
      "grad_norm": 0.1424351930618286,
      "learning_rate": 0.0003499686126804771,
      "loss": 0.1523,
      "step": 479
    },
    {
      "epoch": 0.015068573783036082,
      "grad_norm": 0.501393735408783,
      "learning_rate": 0.00034965473948524796,
      "loss": 0.6391,
      "step": 480
    },
    {
      "epoch": 0.015099966645084074,
      "grad_norm": 0.14167845249176025,
      "learning_rate": 0.00034934086629001884,
      "loss": 0.1963,
      "step": 481
    },
    {
      "epoch": 0.015131359507132066,
      "grad_norm": 0.6582550406455994,
      "learning_rate": 0.0003490269930947897,
      "loss": 0.5559,
      "step": 482
    },
    {
      "epoch": 0.015162752369180058,
      "grad_norm": 0.20320163667201996,
      "learning_rate": 0.00034871311989956056,
      "loss": 0.2054,
      "step": 483
    },
    {
      "epoch": 0.01519414523122805,
      "grad_norm": 0.5033726692199707,
      "learning_rate": 0.0003483992467043315,
      "loss": 0.4683,
      "step": 484
    },
    {
      "epoch": 0.015225538093276042,
      "grad_norm": 0.10508647561073303,
      "learning_rate": 0.0003480853735091024,
      "loss": 0.1037,
      "step": 485
    },
    {
      "epoch": 0.015256930955324034,
      "grad_norm": 0.11877160519361496,
      "learning_rate": 0.0003477715003138732,
      "loss": 0.1451,
      "step": 486
    },
    {
      "epoch": 0.015288323817372025,
      "grad_norm": 0.07227110862731934,
      "learning_rate": 0.0003474576271186441,
      "loss": 0.0919,
      "step": 487
    },
    {
      "epoch": 0.015319716679420017,
      "grad_norm": 0.7295058965682983,
      "learning_rate": 0.00034714375392341497,
      "loss": 0.6426,
      "step": 488
    },
    {
      "epoch": 0.01535110954146801,
      "grad_norm": 0.5602145791053772,
      "learning_rate": 0.00034682988072818585,
      "loss": 0.5813,
      "step": 489
    },
    {
      "epoch": 0.015382502403516001,
      "grad_norm": 0.09065290540456772,
      "learning_rate": 0.0003465160075329567,
      "loss": 0.0805,
      "step": 490
    },
    {
      "epoch": 0.015413895265563993,
      "grad_norm": 0.3218078315258026,
      "learning_rate": 0.00034620213433772756,
      "loss": 0.293,
      "step": 491
    },
    {
      "epoch": 0.015445288127611985,
      "grad_norm": 0.11637149006128311,
      "learning_rate": 0.00034588826114249845,
      "loss": 0.0835,
      "step": 492
    },
    {
      "epoch": 0.015476680989659977,
      "grad_norm": 0.5252712965011597,
      "learning_rate": 0.00034557438794726933,
      "loss": 0.4277,
      "step": 493
    },
    {
      "epoch": 0.015508073851707969,
      "grad_norm": 0.23012366890907288,
      "learning_rate": 0.00034526051475204016,
      "loss": 0.2324,
      "step": 494
    },
    {
      "epoch": 0.015539466713755959,
      "grad_norm": 0.6932777762413025,
      "learning_rate": 0.00034494664155681104,
      "loss": 0.5198,
      "step": 495
    },
    {
      "epoch": 0.01557085957580395,
      "grad_norm": 0.12603572010993958,
      "learning_rate": 0.0003446327683615819,
      "loss": 0.1305,
      "step": 496
    },
    {
      "epoch": 0.015602252437851943,
      "grad_norm": 0.5181828141212463,
      "learning_rate": 0.0003443188951663528,
      "loss": 0.5144,
      "step": 497
    },
    {
      "epoch": 0.015633645299899936,
      "grad_norm": 0.12489528954029083,
      "learning_rate": 0.00034400502197112363,
      "loss": 0.1387,
      "step": 498
    },
    {
      "epoch": 0.015665038161947926,
      "grad_norm": 0.30035561323165894,
      "learning_rate": 0.0003436911487758945,
      "loss": 0.2393,
      "step": 499
    },
    {
      "epoch": 0.01569643102399592,
      "grad_norm": 0.13652117550373077,
      "learning_rate": 0.0003433772755806654,
      "loss": 0.1461,
      "step": 500
    },
    {
      "epoch": 0.01572782388604391,
      "grad_norm": 0.45482879877090454,
      "learning_rate": 0.00034306340238543634,
      "loss": 0.3239,
      "step": 501
    },
    {
      "epoch": 0.015759216748091904,
      "grad_norm": 0.156234472990036,
      "learning_rate": 0.00034274952919020717,
      "loss": 0.0861,
      "step": 502
    },
    {
      "epoch": 0.015790609610139894,
      "grad_norm": 0.156267449259758,
      "learning_rate": 0.00034243565599497805,
      "loss": 0.1302,
      "step": 503
    },
    {
      "epoch": 0.015822002472187888,
      "grad_norm": 0.2710404098033905,
      "learning_rate": 0.00034212178279974893,
      "loss": 0.2139,
      "step": 504
    },
    {
      "epoch": 0.015853395334235878,
      "grad_norm": 0.3569515347480774,
      "learning_rate": 0.0003418079096045198,
      "loss": 0.2594,
      "step": 505
    },
    {
      "epoch": 0.01588478819628387,
      "grad_norm": 0.24971576035022736,
      "learning_rate": 0.00034149403640929064,
      "loss": 0.2752,
      "step": 506
    },
    {
      "epoch": 0.01591618105833186,
      "grad_norm": 0.2896486222743988,
      "learning_rate": 0.0003411801632140615,
      "loss": 0.3581,
      "step": 507
    },
    {
      "epoch": 0.015947573920379855,
      "grad_norm": 0.46070775389671326,
      "learning_rate": 0.0003408662900188324,
      "loss": 0.4388,
      "step": 508
    },
    {
      "epoch": 0.015978966782427845,
      "grad_norm": 0.16328543424606323,
      "learning_rate": 0.0003405524168236033,
      "loss": 0.1122,
      "step": 509
    },
    {
      "epoch": 0.016010359644475836,
      "grad_norm": 0.1315733641386032,
      "learning_rate": 0.0003402385436283741,
      "loss": 0.1225,
      "step": 510
    },
    {
      "epoch": 0.01604175250652383,
      "grad_norm": 0.09833891689777374,
      "learning_rate": 0.000339924670433145,
      "loss": 0.0918,
      "step": 511
    },
    {
      "epoch": 0.01607314536857182,
      "grad_norm": 0.10359932482242584,
      "learning_rate": 0.0003396107972379159,
      "loss": 0.0824,
      "step": 512
    },
    {
      "epoch": 0.016104538230619813,
      "grad_norm": 0.2748242914676666,
      "learning_rate": 0.00033929692404268677,
      "loss": 0.3109,
      "step": 513
    },
    {
      "epoch": 0.016135931092667803,
      "grad_norm": 0.1312568187713623,
      "learning_rate": 0.0003389830508474576,
      "loss": 0.1302,
      "step": 514
    },
    {
      "epoch": 0.016167323954715797,
      "grad_norm": 0.1980227380990982,
      "learning_rate": 0.0003386691776522285,
      "loss": 0.2434,
      "step": 515
    },
    {
      "epoch": 0.016198716816763787,
      "grad_norm": 0.23012137413024902,
      "learning_rate": 0.00033835530445699936,
      "loss": 0.2332,
      "step": 516
    },
    {
      "epoch": 0.01623010967881178,
      "grad_norm": 0.4985153079032898,
      "learning_rate": 0.00033804143126177024,
      "loss": 0.6083,
      "step": 517
    },
    {
      "epoch": 0.01626150254085977,
      "grad_norm": 0.1196935847401619,
      "learning_rate": 0.0003377275580665411,
      "loss": 0.1532,
      "step": 518
    },
    {
      "epoch": 0.016292895402907764,
      "grad_norm": 0.06620965152978897,
      "learning_rate": 0.000337413684871312,
      "loss": 0.0705,
      "step": 519
    },
    {
      "epoch": 0.016324288264955755,
      "grad_norm": 0.26222512125968933,
      "learning_rate": 0.0003370998116760829,
      "loss": 0.3384,
      "step": 520
    },
    {
      "epoch": 0.016355681127003748,
      "grad_norm": 0.12048272788524628,
      "learning_rate": 0.0003367859384808538,
      "loss": 0.1139,
      "step": 521
    },
    {
      "epoch": 0.01638707398905174,
      "grad_norm": 0.06429348886013031,
      "learning_rate": 0.0003364720652856246,
      "loss": 0.0855,
      "step": 522
    },
    {
      "epoch": 0.016418466851099732,
      "grad_norm": 0.3766354024410248,
      "learning_rate": 0.0003361581920903955,
      "loss": 0.4373,
      "step": 523
    },
    {
      "epoch": 0.016449859713147722,
      "grad_norm": 0.0503232441842556,
      "learning_rate": 0.00033584431889516637,
      "loss": 0.0716,
      "step": 524
    },
    {
      "epoch": 0.016481252575195716,
      "grad_norm": 0.32005614042282104,
      "learning_rate": 0.00033553044569993725,
      "loss": 0.3171,
      "step": 525
    },
    {
      "epoch": 0.016512645437243706,
      "grad_norm": 0.3807719945907593,
      "learning_rate": 0.0003352165725047081,
      "loss": 0.4473,
      "step": 526
    },
    {
      "epoch": 0.0165440382992917,
      "grad_norm": 0.34612348675727844,
      "learning_rate": 0.00033490269930947896,
      "loss": 0.2862,
      "step": 527
    },
    {
      "epoch": 0.01657543116133969,
      "grad_norm": 0.48163342475891113,
      "learning_rate": 0.00033458882611424985,
      "loss": 0.5723,
      "step": 528
    },
    {
      "epoch": 0.016606824023387683,
      "grad_norm": 0.40149134397506714,
      "learning_rate": 0.00033427495291902073,
      "loss": 0.7899,
      "step": 529
    },
    {
      "epoch": 0.016638216885435674,
      "grad_norm": 0.18105755746364594,
      "learning_rate": 0.00033396107972379156,
      "loss": 0.2881,
      "step": 530
    },
    {
      "epoch": 0.016669609747483667,
      "grad_norm": 0.29071545600891113,
      "learning_rate": 0.00033364720652856244,
      "loss": 0.3519,
      "step": 531
    },
    {
      "epoch": 0.016701002609531657,
      "grad_norm": 0.7519810795783997,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.6545,
      "step": 532
    },
    {
      "epoch": 0.01673239547157965,
      "grad_norm": 0.11551214009523392,
      "learning_rate": 0.0003330194601381042,
      "loss": 0.0875,
      "step": 533
    },
    {
      "epoch": 0.01676378833362764,
      "grad_norm": 0.14271441102027893,
      "learning_rate": 0.00033270558694287503,
      "loss": 0.197,
      "step": 534
    },
    {
      "epoch": 0.016795181195675635,
      "grad_norm": 0.2955506145954132,
      "learning_rate": 0.00033239171374764597,
      "loss": 0.393,
      "step": 535
    },
    {
      "epoch": 0.016826574057723625,
      "grad_norm": 0.15418682992458344,
      "learning_rate": 0.00033207784055241685,
      "loss": 0.0875,
      "step": 536
    },
    {
      "epoch": 0.01685796691977162,
      "grad_norm": 0.25074735283851624,
      "learning_rate": 0.00033176396735718774,
      "loss": 0.0937,
      "step": 537
    },
    {
      "epoch": 0.01688935978181961,
      "grad_norm": 0.2768358290195465,
      "learning_rate": 0.0003314500941619586,
      "loss": 0.3349,
      "step": 538
    },
    {
      "epoch": 0.016920752643867602,
      "grad_norm": 0.2839558720588684,
      "learning_rate": 0.00033113622096672945,
      "loss": 0.4411,
      "step": 539
    },
    {
      "epoch": 0.016952145505915592,
      "grad_norm": 0.28645098209381104,
      "learning_rate": 0.00033082234777150033,
      "loss": 0.4473,
      "step": 540
    },
    {
      "epoch": 0.016983538367963583,
      "grad_norm": 0.23301611840724945,
      "learning_rate": 0.0003305084745762712,
      "loss": 0.3165,
      "step": 541
    },
    {
      "epoch": 0.017014931230011576,
      "grad_norm": 0.09526162594556808,
      "learning_rate": 0.0003301946013810421,
      "loss": 0.0652,
      "step": 542
    },
    {
      "epoch": 0.017046324092059566,
      "grad_norm": 0.08009558916091919,
      "learning_rate": 0.0003298807281858129,
      "loss": 0.0501,
      "step": 543
    },
    {
      "epoch": 0.01707771695410756,
      "grad_norm": 0.5110572576522827,
      "learning_rate": 0.0003295668549905838,
      "loss": 0.4309,
      "step": 544
    },
    {
      "epoch": 0.01710910981615555,
      "grad_norm": 0.23759347200393677,
      "learning_rate": 0.0003292529817953547,
      "loss": 0.3934,
      "step": 545
    },
    {
      "epoch": 0.017140502678203544,
      "grad_norm": 0.13065044581890106,
      "learning_rate": 0.00032893910860012557,
      "loss": 0.1912,
      "step": 546
    },
    {
      "epoch": 0.017171895540251534,
      "grad_norm": 0.3813381791114807,
      "learning_rate": 0.0003286252354048964,
      "loss": 0.577,
      "step": 547
    },
    {
      "epoch": 0.017203288402299528,
      "grad_norm": 0.3368072509765625,
      "learning_rate": 0.0003283113622096673,
      "loss": 0.5371,
      "step": 548
    },
    {
      "epoch": 0.017234681264347518,
      "grad_norm": 0.0807497650384903,
      "learning_rate": 0.00032799748901443817,
      "loss": 0.0682,
      "step": 549
    },
    {
      "epoch": 0.01726607412639551,
      "grad_norm": 0.33683857321739197,
      "learning_rate": 0.00032768361581920905,
      "loss": 0.4342,
      "step": 550
    },
    {
      "epoch": 0.0172974669884435,
      "grad_norm": 0.3230961561203003,
      "learning_rate": 0.0003273697426239799,
      "loss": 0.3776,
      "step": 551
    },
    {
      "epoch": 0.017328859850491495,
      "grad_norm": 0.15684013068675995,
      "learning_rate": 0.0003270558694287508,
      "loss": 0.1673,
      "step": 552
    },
    {
      "epoch": 0.017360252712539485,
      "grad_norm": 0.39723461866378784,
      "learning_rate": 0.0003267419962335217,
      "loss": 0.4356,
      "step": 553
    },
    {
      "epoch": 0.01739164557458748,
      "grad_norm": 0.6631824970245361,
      "learning_rate": 0.0003264281230382926,
      "loss": 0.2991,
      "step": 554
    },
    {
      "epoch": 0.01742303843663547,
      "grad_norm": 0.7996995449066162,
      "learning_rate": 0.0003261142498430634,
      "loss": 0.6235,
      "step": 555
    },
    {
      "epoch": 0.017454431298683463,
      "grad_norm": 0.3293347656726837,
      "learning_rate": 0.0003258003766478343,
      "loss": 0.3505,
      "step": 556
    },
    {
      "epoch": 0.017485824160731453,
      "grad_norm": 0.14115563035011292,
      "learning_rate": 0.0003254865034526052,
      "loss": 0.177,
      "step": 557
    },
    {
      "epoch": 0.017517217022779447,
      "grad_norm": 0.27997589111328125,
      "learning_rate": 0.00032517263025737606,
      "loss": 0.4098,
      "step": 558
    },
    {
      "epoch": 0.017548609884827437,
      "grad_norm": 0.05524447187781334,
      "learning_rate": 0.0003248587570621469,
      "loss": 0.0718,
      "step": 559
    },
    {
      "epoch": 0.01758000274687543,
      "grad_norm": 0.20514002442359924,
      "learning_rate": 0.00032454488386691777,
      "loss": 0.2495,
      "step": 560
    },
    {
      "epoch": 0.01761139560892342,
      "grad_norm": 0.11077189445495605,
      "learning_rate": 0.00032423101067168865,
      "loss": 0.1137,
      "step": 561
    },
    {
      "epoch": 0.017642788470971414,
      "grad_norm": 0.11379427462816238,
      "learning_rate": 0.00032391713747645953,
      "loss": 0.0925,
      "step": 562
    },
    {
      "epoch": 0.017674181333019404,
      "grad_norm": 0.16876651346683502,
      "learning_rate": 0.00032360326428123036,
      "loss": 0.2667,
      "step": 563
    },
    {
      "epoch": 0.017705574195067398,
      "grad_norm": 0.07549270242452621,
      "learning_rate": 0.00032328939108600124,
      "loss": 0.1067,
      "step": 564
    },
    {
      "epoch": 0.017736967057115388,
      "grad_norm": 0.3262005150318146,
      "learning_rate": 0.00032297551789077213,
      "loss": 0.5528,
      "step": 565
    },
    {
      "epoch": 0.017768359919163382,
      "grad_norm": 0.3017822802066803,
      "learning_rate": 0.000322661644695543,
      "loss": 0.3239,
      "step": 566
    },
    {
      "epoch": 0.017799752781211372,
      "grad_norm": 0.33609938621520996,
      "learning_rate": 0.00032234777150031384,
      "loss": 0.4266,
      "step": 567
    },
    {
      "epoch": 0.017831145643259366,
      "grad_norm": 0.1914227157831192,
      "learning_rate": 0.0003220338983050847,
      "loss": 0.2668,
      "step": 568
    },
    {
      "epoch": 0.017862538505307356,
      "grad_norm": 0.1954031139612198,
      "learning_rate": 0.00032172002510985566,
      "loss": 0.3036,
      "step": 569
    },
    {
      "epoch": 0.017893931367355346,
      "grad_norm": 0.14100885391235352,
      "learning_rate": 0.00032140615191462654,
      "loss": 0.2099,
      "step": 570
    },
    {
      "epoch": 0.01792532422940334,
      "grad_norm": 0.12784424424171448,
      "learning_rate": 0.00032109227871939737,
      "loss": 0.1733,
      "step": 571
    },
    {
      "epoch": 0.01795671709145133,
      "grad_norm": 0.17901305854320526,
      "learning_rate": 0.00032077840552416825,
      "loss": 0.2869,
      "step": 572
    },
    {
      "epoch": 0.017988109953499323,
      "grad_norm": 0.506366491317749,
      "learning_rate": 0.00032046453232893913,
      "loss": 0.5323,
      "step": 573
    },
    {
      "epoch": 0.018019502815547313,
      "grad_norm": 0.09066405892372131,
      "learning_rate": 0.00032015065913371,
      "loss": 0.0964,
      "step": 574
    },
    {
      "epoch": 0.018050895677595307,
      "grad_norm": 0.3219265639781952,
      "learning_rate": 0.00031983678593848085,
      "loss": 0.474,
      "step": 575
    },
    {
      "epoch": 0.018082288539643297,
      "grad_norm": 0.13276948034763336,
      "learning_rate": 0.00031952291274325173,
      "loss": 0.2522,
      "step": 576
    },
    {
      "epoch": 0.01811368140169129,
      "grad_norm": 0.2196033000946045,
      "learning_rate": 0.0003192090395480226,
      "loss": 0.2401,
      "step": 577
    },
    {
      "epoch": 0.01814507426373928,
      "grad_norm": 0.3391559422016144,
      "learning_rate": 0.0003188951663527935,
      "loss": 0.4339,
      "step": 578
    },
    {
      "epoch": 0.018176467125787275,
      "grad_norm": 0.12065351754426956,
      "learning_rate": 0.0003185812931575643,
      "loss": 0.1567,
      "step": 579
    },
    {
      "epoch": 0.018207859987835265,
      "grad_norm": 0.33111655712127686,
      "learning_rate": 0.0003182674199623352,
      "loss": 0.4175,
      "step": 580
    },
    {
      "epoch": 0.01823925284988326,
      "grad_norm": 0.18807706236839294,
      "learning_rate": 0.0003179535467671061,
      "loss": 0.2188,
      "step": 581
    },
    {
      "epoch": 0.01827064571193125,
      "grad_norm": 0.3610953688621521,
      "learning_rate": 0.00031763967357187697,
      "loss": 0.7348,
      "step": 582
    },
    {
      "epoch": 0.018302038573979242,
      "grad_norm": 0.1696382462978363,
      "learning_rate": 0.0003173258003766478,
      "loss": 0.0888,
      "step": 583
    },
    {
      "epoch": 0.018333431436027232,
      "grad_norm": 0.23161087930202484,
      "learning_rate": 0.0003170119271814187,
      "loss": 0.2576,
      "step": 584
    },
    {
      "epoch": 0.018364824298075226,
      "grad_norm": 0.13648997247219086,
      "learning_rate": 0.00031669805398618957,
      "loss": 0.1354,
      "step": 585
    },
    {
      "epoch": 0.018396217160123216,
      "grad_norm": 0.3493953049182892,
      "learning_rate": 0.0003163841807909605,
      "loss": 0.4907,
      "step": 586
    },
    {
      "epoch": 0.01842761002217121,
      "grad_norm": 0.1305798590183258,
      "learning_rate": 0.00031607030759573133,
      "loss": 0.1053,
      "step": 587
    },
    {
      "epoch": 0.0184590028842192,
      "grad_norm": 0.13136382400989532,
      "learning_rate": 0.0003157564344005022,
      "loss": 0.0854,
      "step": 588
    },
    {
      "epoch": 0.018490395746267194,
      "grad_norm": 0.11647126078605652,
      "learning_rate": 0.0003154425612052731,
      "loss": 0.1447,
      "step": 589
    },
    {
      "epoch": 0.018521788608315184,
      "grad_norm": 0.5267751812934875,
      "learning_rate": 0.000315128688010044,
      "loss": 0.4089,
      "step": 590
    },
    {
      "epoch": 0.018553181470363177,
      "grad_norm": 0.25343888998031616,
      "learning_rate": 0.0003148148148148148,
      "loss": 0.3374,
      "step": 591
    },
    {
      "epoch": 0.018584574332411168,
      "grad_norm": 0.13532298803329468,
      "learning_rate": 0.0003145009416195857,
      "loss": 0.1387,
      "step": 592
    },
    {
      "epoch": 0.01861596719445916,
      "grad_norm": 0.27602142095565796,
      "learning_rate": 0.00031418706842435657,
      "loss": 0.2649,
      "step": 593
    },
    {
      "epoch": 0.01864736005650715,
      "grad_norm": 0.422728568315506,
      "learning_rate": 0.00031387319522912746,
      "loss": 0.4576,
      "step": 594
    },
    {
      "epoch": 0.018678752918555145,
      "grad_norm": 0.15226851403713226,
      "learning_rate": 0.0003135593220338983,
      "loss": 0.1488,
      "step": 595
    },
    {
      "epoch": 0.018710145780603135,
      "grad_norm": 0.3494429290294647,
      "learning_rate": 0.00031324544883866917,
      "loss": 0.3017,
      "step": 596
    },
    {
      "epoch": 0.01874153864265113,
      "grad_norm": 0.3713906407356262,
      "learning_rate": 0.00031293157564344005,
      "loss": 0.4215,
      "step": 597
    },
    {
      "epoch": 0.01877293150469912,
      "grad_norm": 0.08609607815742493,
      "learning_rate": 0.00031261770244821093,
      "loss": 0.0988,
      "step": 598
    },
    {
      "epoch": 0.018804324366747113,
      "grad_norm": 0.3099645972251892,
      "learning_rate": 0.00031230382925298176,
      "loss": 0.218,
      "step": 599
    },
    {
      "epoch": 0.018835717228795103,
      "grad_norm": 0.0575394444167614,
      "learning_rate": 0.00031198995605775264,
      "loss": 0.0897,
      "step": 600
    },
    {
      "epoch": 0.018867110090843093,
      "grad_norm": 0.08127318322658539,
      "learning_rate": 0.0003116760828625235,
      "loss": 0.0995,
      "step": 601
    },
    {
      "epoch": 0.018898502952891087,
      "grad_norm": 0.06511104851961136,
      "learning_rate": 0.0003113622096672944,
      "loss": 0.0822,
      "step": 602
    },
    {
      "epoch": 0.018929895814939077,
      "grad_norm": 0.0762634128332138,
      "learning_rate": 0.0003110483364720653,
      "loss": 0.1031,
      "step": 603
    },
    {
      "epoch": 0.01896128867698707,
      "grad_norm": 0.14336542785167694,
      "learning_rate": 0.0003107344632768362,
      "loss": 0.1474,
      "step": 604
    },
    {
      "epoch": 0.01899268153903506,
      "grad_norm": 0.46176812052726746,
      "learning_rate": 0.00031042059008160706,
      "loss": 0.4165,
      "step": 605
    },
    {
      "epoch": 0.019024074401083054,
      "grad_norm": 0.42926737666130066,
      "learning_rate": 0.00031010671688637794,
      "loss": 0.5161,
      "step": 606
    },
    {
      "epoch": 0.019055467263131044,
      "grad_norm": 0.320346862077713,
      "learning_rate": 0.0003097928436911488,
      "loss": 0.2623,
      "step": 607
    },
    {
      "epoch": 0.019086860125179038,
      "grad_norm": 0.5093856453895569,
      "learning_rate": 0.00030947897049591965,
      "loss": 0.7382,
      "step": 608
    },
    {
      "epoch": 0.019118252987227028,
      "grad_norm": 0.07341869920492172,
      "learning_rate": 0.00030916509730069053,
      "loss": 0.0941,
      "step": 609
    },
    {
      "epoch": 0.01914964584927502,
      "grad_norm": 0.39327767491340637,
      "learning_rate": 0.0003088512241054614,
      "loss": 0.4147,
      "step": 610
    },
    {
      "epoch": 0.019181038711323012,
      "grad_norm": 0.09012908488512039,
      "learning_rate": 0.0003085373509102323,
      "loss": 0.1247,
      "step": 611
    },
    {
      "epoch": 0.019212431573371005,
      "grad_norm": 0.3031710386276245,
      "learning_rate": 0.00030822347771500313,
      "loss": 0.2886,
      "step": 612
    },
    {
      "epoch": 0.019243824435418996,
      "grad_norm": 0.0714639276266098,
      "learning_rate": 0.000307909604519774,
      "loss": 0.0869,
      "step": 613
    },
    {
      "epoch": 0.01927521729746699,
      "grad_norm": 0.3652655780315399,
      "learning_rate": 0.0003075957313245449,
      "loss": 0.3733,
      "step": 614
    },
    {
      "epoch": 0.01930661015951498,
      "grad_norm": 0.048923101276159286,
      "learning_rate": 0.0003072818581293158,
      "loss": 0.0561,
      "step": 615
    },
    {
      "epoch": 0.019338003021562973,
      "grad_norm": 0.09051372110843658,
      "learning_rate": 0.0003069679849340866,
      "loss": 0.1047,
      "step": 616
    },
    {
      "epoch": 0.019369395883610963,
      "grad_norm": 0.0753413736820221,
      "learning_rate": 0.0003066541117388575,
      "loss": 0.0844,
      "step": 617
    },
    {
      "epoch": 0.019400788745658957,
      "grad_norm": 0.08823724091053009,
      "learning_rate": 0.00030634023854362837,
      "loss": 0.1071,
      "step": 618
    },
    {
      "epoch": 0.019432181607706947,
      "grad_norm": 0.3592212498188019,
      "learning_rate": 0.00030602636534839925,
      "loss": 0.3237,
      "step": 619
    },
    {
      "epoch": 0.01946357446975494,
      "grad_norm": 0.33116188645362854,
      "learning_rate": 0.00030571249215317014,
      "loss": 0.4184,
      "step": 620
    },
    {
      "epoch": 0.01949496733180293,
      "grad_norm": 0.05832076817750931,
      "learning_rate": 0.000305398618957941,
      "loss": 0.0808,
      "step": 621
    },
    {
      "epoch": 0.019526360193850924,
      "grad_norm": 0.3995286226272583,
      "learning_rate": 0.0003050847457627119,
      "loss": 0.3117,
      "step": 622
    },
    {
      "epoch": 0.019557753055898915,
      "grad_norm": 0.4852924644947052,
      "learning_rate": 0.0003047708725674828,
      "loss": 0.6688,
      "step": 623
    },
    {
      "epoch": 0.019589145917946908,
      "grad_norm": 0.24025338888168335,
      "learning_rate": 0.0003044569993722536,
      "loss": 0.2132,
      "step": 624
    },
    {
      "epoch": 0.0196205387799949,
      "grad_norm": 0.20598800480365753,
      "learning_rate": 0.0003041431261770245,
      "loss": 0.2787,
      "step": 625
    },
    {
      "epoch": 0.019651931642042892,
      "grad_norm": 0.22609825432300568,
      "learning_rate": 0.0003038292529817954,
      "loss": 0.254,
      "step": 626
    },
    {
      "epoch": 0.019683324504090882,
      "grad_norm": 0.32474443316459656,
      "learning_rate": 0.00030351537978656626,
      "loss": 0.3828,
      "step": 627
    },
    {
      "epoch": 0.019714717366138876,
      "grad_norm": 0.28957459330558777,
      "learning_rate": 0.0003032015065913371,
      "loss": 0.3681,
      "step": 628
    },
    {
      "epoch": 0.019746110228186866,
      "grad_norm": 0.20387662947177887,
      "learning_rate": 0.00030288763339610797,
      "loss": 0.2412,
      "step": 629
    },
    {
      "epoch": 0.019777503090234856,
      "grad_norm": 0.2475360780954361,
      "learning_rate": 0.00030257376020087885,
      "loss": 0.2325,
      "step": 630
    },
    {
      "epoch": 0.01980889595228285,
      "grad_norm": 0.13399894535541534,
      "learning_rate": 0.00030225988700564974,
      "loss": 0.1493,
      "step": 631
    },
    {
      "epoch": 0.01984028881433084,
      "grad_norm": 0.16737666726112366,
      "learning_rate": 0.00030194601381042057,
      "loss": 0.1634,
      "step": 632
    },
    {
      "epoch": 0.019871681676378834,
      "grad_norm": 0.4053106904029846,
      "learning_rate": 0.00030163214061519145,
      "loss": 0.5128,
      "step": 633
    },
    {
      "epoch": 0.019903074538426824,
      "grad_norm": 0.1628747135400772,
      "learning_rate": 0.00030131826741996233,
      "loss": 0.2036,
      "step": 634
    },
    {
      "epoch": 0.019934467400474817,
      "grad_norm": 0.1525375247001648,
      "learning_rate": 0.0003010043942247332,
      "loss": 0.1445,
      "step": 635
    },
    {
      "epoch": 0.019965860262522808,
      "grad_norm": 0.3947545886039734,
      "learning_rate": 0.00030069052102950404,
      "loss": 0.504,
      "step": 636
    },
    {
      "epoch": 0.0199972531245708,
      "grad_norm": 0.1276589184999466,
      "learning_rate": 0.000300376647834275,
      "loss": 0.1676,
      "step": 637
    },
    {
      "epoch": 0.02002864598661879,
      "grad_norm": 0.2918011546134949,
      "learning_rate": 0.00030006277463904586,
      "loss": 0.4759,
      "step": 638
    },
    {
      "epoch": 0.020060038848666785,
      "grad_norm": 0.08352089673280716,
      "learning_rate": 0.00029974890144381674,
      "loss": 0.1163,
      "step": 639
    },
    {
      "epoch": 0.020091431710714775,
      "grad_norm": 0.15109919011592865,
      "learning_rate": 0.0002994350282485876,
      "loss": 0.1379,
      "step": 640
    },
    {
      "epoch": 0.02012282457276277,
      "grad_norm": 0.280244916677475,
      "learning_rate": 0.00029912115505335846,
      "loss": 0.3228,
      "step": 641
    },
    {
      "epoch": 0.02015421743481076,
      "grad_norm": 0.3357614278793335,
      "learning_rate": 0.00029880728185812934,
      "loss": 0.4832,
      "step": 642
    },
    {
      "epoch": 0.020185610296858753,
      "grad_norm": 0.2761925458908081,
      "learning_rate": 0.0002984934086629002,
      "loss": 0.2404,
      "step": 643
    },
    {
      "epoch": 0.020217003158906743,
      "grad_norm": 0.44830939173698425,
      "learning_rate": 0.00029817953546767105,
      "loss": 0.5069,
      "step": 644
    },
    {
      "epoch": 0.020248396020954736,
      "grad_norm": 0.1291070282459259,
      "learning_rate": 0.00029786566227244193,
      "loss": 0.1744,
      "step": 645
    },
    {
      "epoch": 0.020279788883002726,
      "grad_norm": 0.3456226885318756,
      "learning_rate": 0.0002975517890772128,
      "loss": 0.3873,
      "step": 646
    },
    {
      "epoch": 0.02031118174505072,
      "grad_norm": 0.24912598729133606,
      "learning_rate": 0.0002972379158819837,
      "loss": 0.3438,
      "step": 647
    },
    {
      "epoch": 0.02034257460709871,
      "grad_norm": 0.5327059030532837,
      "learning_rate": 0.0002969240426867545,
      "loss": 0.6163,
      "step": 648
    },
    {
      "epoch": 0.020373967469146704,
      "grad_norm": 0.5765470862388611,
      "learning_rate": 0.0002966101694915254,
      "loss": 0.5751,
      "step": 649
    },
    {
      "epoch": 0.020405360331194694,
      "grad_norm": 0.2609751224517822,
      "learning_rate": 0.0002962962962962963,
      "loss": 0.3425,
      "step": 650
    },
    {
      "epoch": 0.020436753193242688,
      "grad_norm": 0.3670404851436615,
      "learning_rate": 0.0002959824231010672,
      "loss": 0.5186,
      "step": 651
    },
    {
      "epoch": 0.020468146055290678,
      "grad_norm": 0.08604005724191666,
      "learning_rate": 0.000295668549905838,
      "loss": 0.0921,
      "step": 652
    },
    {
      "epoch": 0.02049953891733867,
      "grad_norm": 0.06647100299596786,
      "learning_rate": 0.0002953546767106089,
      "loss": 0.0674,
      "step": 653
    },
    {
      "epoch": 0.02053093177938666,
      "grad_norm": 0.4153648614883423,
      "learning_rate": 0.0002950408035153798,
      "loss": 0.7727,
      "step": 654
    },
    {
      "epoch": 0.020562324641434655,
      "grad_norm": 0.25095224380493164,
      "learning_rate": 0.0002947269303201507,
      "loss": 0.3127,
      "step": 655
    },
    {
      "epoch": 0.020593717503482645,
      "grad_norm": 0.3695579171180725,
      "learning_rate": 0.00029441305712492153,
      "loss": 0.5128,
      "step": 656
    },
    {
      "epoch": 0.02062511036553064,
      "grad_norm": 0.29747700691223145,
      "learning_rate": 0.0002940991839296924,
      "loss": 0.3254,
      "step": 657
    },
    {
      "epoch": 0.02065650322757863,
      "grad_norm": 0.3189714252948761,
      "learning_rate": 0.0002937853107344633,
      "loss": 0.6401,
      "step": 658
    },
    {
      "epoch": 0.020687896089626623,
      "grad_norm": 0.09620893746614456,
      "learning_rate": 0.0002934714375392342,
      "loss": 0.1341,
      "step": 659
    },
    {
      "epoch": 0.020719288951674613,
      "grad_norm": 0.28182336688041687,
      "learning_rate": 0.000293157564344005,
      "loss": 0.2965,
      "step": 660
    },
    {
      "epoch": 0.020750681813722603,
      "grad_norm": 0.08469105511903763,
      "learning_rate": 0.0002928436911487759,
      "loss": 0.0711,
      "step": 661
    },
    {
      "epoch": 0.020782074675770597,
      "grad_norm": 0.083750419318676,
      "learning_rate": 0.0002925298179535468,
      "loss": 0.0687,
      "step": 662
    },
    {
      "epoch": 0.020813467537818587,
      "grad_norm": 0.2791332006454468,
      "learning_rate": 0.00029221594475831766,
      "loss": 0.2966,
      "step": 663
    },
    {
      "epoch": 0.02084486039986658,
      "grad_norm": 0.4955212473869324,
      "learning_rate": 0.0002919020715630885,
      "loss": 0.5031,
      "step": 664
    },
    {
      "epoch": 0.02087625326191457,
      "grad_norm": 0.4834003448486328,
      "learning_rate": 0.00029158819836785937,
      "loss": 0.4159,
      "step": 665
    },
    {
      "epoch": 0.020907646123962564,
      "grad_norm": 0.3019258677959442,
      "learning_rate": 0.00029127432517263025,
      "loss": 0.3555,
      "step": 666
    },
    {
      "epoch": 0.020939038986010555,
      "grad_norm": 0.1764862835407257,
      "learning_rate": 0.00029096045197740114,
      "loss": 0.1985,
      "step": 667
    },
    {
      "epoch": 0.020970431848058548,
      "grad_norm": 0.08317003399133682,
      "learning_rate": 0.00029064657878217196,
      "loss": 0.0902,
      "step": 668
    },
    {
      "epoch": 0.02100182471010654,
      "grad_norm": 0.15165287256240845,
      "learning_rate": 0.00029033270558694285,
      "loss": 0.2661,
      "step": 669
    },
    {
      "epoch": 0.021033217572154532,
      "grad_norm": 0.35397520661354065,
      "learning_rate": 0.00029001883239171373,
      "loss": 0.2623,
      "step": 670
    },
    {
      "epoch": 0.021064610434202522,
      "grad_norm": 0.19094879925251007,
      "learning_rate": 0.00028970495919648467,
      "loss": 0.1859,
      "step": 671
    },
    {
      "epoch": 0.021096003296250516,
      "grad_norm": 0.5688789486885071,
      "learning_rate": 0.0002893910860012555,
      "loss": 0.7719,
      "step": 672
    },
    {
      "epoch": 0.021127396158298506,
      "grad_norm": 0.2650083303451538,
      "learning_rate": 0.0002890772128060264,
      "loss": 0.2672,
      "step": 673
    },
    {
      "epoch": 0.0211587890203465,
      "grad_norm": 0.07906338572502136,
      "learning_rate": 0.00028876333961079726,
      "loss": 0.0708,
      "step": 674
    },
    {
      "epoch": 0.02119018188239449,
      "grad_norm": 0.3136499226093292,
      "learning_rate": 0.00028844946641556814,
      "loss": 0.5508,
      "step": 675
    },
    {
      "epoch": 0.021221574744442483,
      "grad_norm": 0.052998196333646774,
      "learning_rate": 0.00028813559322033897,
      "loss": 0.0734,
      "step": 676
    },
    {
      "epoch": 0.021252967606490474,
      "grad_norm": 0.11756192147731781,
      "learning_rate": 0.00028782172002510986,
      "loss": 0.1467,
      "step": 677
    },
    {
      "epoch": 0.021284360468538467,
      "grad_norm": 0.09341969341039658,
      "learning_rate": 0.00028750784682988074,
      "loss": 0.1223,
      "step": 678
    },
    {
      "epoch": 0.021315753330586457,
      "grad_norm": 0.40882086753845215,
      "learning_rate": 0.0002871939736346516,
      "loss": 0.3948,
      "step": 679
    },
    {
      "epoch": 0.02134714619263445,
      "grad_norm": 0.12428344786167145,
      "learning_rate": 0.00028688010043942245,
      "loss": 0.1936,
      "step": 680
    },
    {
      "epoch": 0.02137853905468244,
      "grad_norm": 0.059447698295116425,
      "learning_rate": 0.00028656622724419333,
      "loss": 0.0683,
      "step": 681
    },
    {
      "epoch": 0.021409931916730435,
      "grad_norm": 0.10830309987068176,
      "learning_rate": 0.0002862523540489642,
      "loss": 0.1218,
      "step": 682
    },
    {
      "epoch": 0.021441324778778425,
      "grad_norm": 0.14901790022850037,
      "learning_rate": 0.0002859384808537351,
      "loss": 0.1857,
      "step": 683
    },
    {
      "epoch": 0.02147271764082642,
      "grad_norm": 0.10745233297348022,
      "learning_rate": 0.0002856246076585059,
      "loss": 0.1523,
      "step": 684
    },
    {
      "epoch": 0.02150411050287441,
      "grad_norm": 0.1149291917681694,
      "learning_rate": 0.0002853107344632768,
      "loss": 0.1665,
      "step": 685
    },
    {
      "epoch": 0.021535503364922402,
      "grad_norm": 0.23353499174118042,
      "learning_rate": 0.0002849968612680477,
      "loss": 0.2614,
      "step": 686
    },
    {
      "epoch": 0.021566896226970392,
      "grad_norm": 0.3208937346935272,
      "learning_rate": 0.0002846829880728186,
      "loss": 0.3955,
      "step": 687
    },
    {
      "epoch": 0.021598289089018386,
      "grad_norm": 0.3003716468811035,
      "learning_rate": 0.0002843691148775895,
      "loss": 0.5413,
      "step": 688
    },
    {
      "epoch": 0.021629681951066376,
      "grad_norm": 0.07723014056682587,
      "learning_rate": 0.00028405524168236034,
      "loss": 0.1046,
      "step": 689
    },
    {
      "epoch": 0.021661074813114366,
      "grad_norm": 0.11547818779945374,
      "learning_rate": 0.0002837413684871312,
      "loss": 0.1524,
      "step": 690
    },
    {
      "epoch": 0.02169246767516236,
      "grad_norm": 0.09848518669605255,
      "learning_rate": 0.0002834274952919021,
      "loss": 0.144,
      "step": 691
    },
    {
      "epoch": 0.02172386053721035,
      "grad_norm": 0.2205198109149933,
      "learning_rate": 0.000283113622096673,
      "loss": 0.2232,
      "step": 692
    },
    {
      "epoch": 0.021755253399258344,
      "grad_norm": 0.4573250114917755,
      "learning_rate": 0.0002827997489014438,
      "loss": 0.8372,
      "step": 693
    },
    {
      "epoch": 0.021786646261306334,
      "grad_norm": 0.08982353657484055,
      "learning_rate": 0.0002824858757062147,
      "loss": 0.1302,
      "step": 694
    },
    {
      "epoch": 0.021818039123354328,
      "grad_norm": 0.340599924325943,
      "learning_rate": 0.0002821720025109856,
      "loss": 0.4687,
      "step": 695
    },
    {
      "epoch": 0.021849431985402318,
      "grad_norm": 0.3667614459991455,
      "learning_rate": 0.00028185812931575646,
      "loss": 0.7446,
      "step": 696
    },
    {
      "epoch": 0.02188082484745031,
      "grad_norm": 0.20404425263404846,
      "learning_rate": 0.0002815442561205273,
      "loss": 0.1967,
      "step": 697
    },
    {
      "epoch": 0.0219122177094983,
      "grad_norm": 0.08084029704332352,
      "learning_rate": 0.0002812303829252982,
      "loss": 0.1002,
      "step": 698
    },
    {
      "epoch": 0.021943610571546295,
      "grad_norm": 0.09588555991649628,
      "learning_rate": 0.00028091650973006906,
      "loss": 0.0831,
      "step": 699
    },
    {
      "epoch": 0.021975003433594285,
      "grad_norm": 0.09961599111557007,
      "learning_rate": 0.00028060263653483994,
      "loss": 0.0759,
      "step": 700
    },
    {
      "epoch": 0.02200639629564228,
      "grad_norm": 0.129481703042984,
      "learning_rate": 0.00028028876333961077,
      "loss": 0.1835,
      "step": 701
    },
    {
      "epoch": 0.02203778915769027,
      "grad_norm": 0.1398092359304428,
      "learning_rate": 0.00027997489014438165,
      "loss": 0.1762,
      "step": 702
    },
    {
      "epoch": 0.022069182019738263,
      "grad_norm": 0.06006341800093651,
      "learning_rate": 0.00027966101694915254,
      "loss": 0.0704,
      "step": 703
    },
    {
      "epoch": 0.022100574881786253,
      "grad_norm": 0.33067795634269714,
      "learning_rate": 0.0002793471437539234,
      "loss": 0.3558,
      "step": 704
    },
    {
      "epoch": 0.022131967743834247,
      "grad_norm": 0.3823869228363037,
      "learning_rate": 0.0002790332705586943,
      "loss": 0.4379,
      "step": 705
    },
    {
      "epoch": 0.022163360605882237,
      "grad_norm": 0.3589467406272888,
      "learning_rate": 0.0002787193973634652,
      "loss": 0.3164,
      "step": 706
    },
    {
      "epoch": 0.02219475346793023,
      "grad_norm": 0.09830351173877716,
      "learning_rate": 0.00027840552416823607,
      "loss": 0.0879,
      "step": 707
    },
    {
      "epoch": 0.02222614632997822,
      "grad_norm": 0.3349307179450989,
      "learning_rate": 0.00027809165097300695,
      "loss": 0.4635,
      "step": 708
    },
    {
      "epoch": 0.022257539192026214,
      "grad_norm": 0.07818076014518738,
      "learning_rate": 0.0002777777777777778,
      "loss": 0.0996,
      "step": 709
    },
    {
      "epoch": 0.022288932054074204,
      "grad_norm": 0.19239644706249237,
      "learning_rate": 0.00027746390458254866,
      "loss": 0.2585,
      "step": 710
    },
    {
      "epoch": 0.022320324916122198,
      "grad_norm": 0.4144919216632843,
      "learning_rate": 0.00027715003138731954,
      "loss": 0.492,
      "step": 711
    },
    {
      "epoch": 0.022351717778170188,
      "grad_norm": 0.2082359790802002,
      "learning_rate": 0.0002768361581920904,
      "loss": 0.316,
      "step": 712
    },
    {
      "epoch": 0.022383110640218182,
      "grad_norm": 0.11764474213123322,
      "learning_rate": 0.00027652228499686125,
      "loss": 0.1926,
      "step": 713
    },
    {
      "epoch": 0.022414503502266172,
      "grad_norm": 0.09139523655176163,
      "learning_rate": 0.00027620841180163214,
      "loss": 0.1133,
      "step": 714
    },
    {
      "epoch": 0.022445896364314166,
      "grad_norm": 0.1739950031042099,
      "learning_rate": 0.000275894538606403,
      "loss": 0.1706,
      "step": 715
    },
    {
      "epoch": 0.022477289226362156,
      "grad_norm": 0.16951081156730652,
      "learning_rate": 0.0002755806654111739,
      "loss": 0.109,
      "step": 716
    },
    {
      "epoch": 0.02250868208841015,
      "grad_norm": 0.27434101700782776,
      "learning_rate": 0.00027526679221594473,
      "loss": 0.3873,
      "step": 717
    },
    {
      "epoch": 0.02254007495045814,
      "grad_norm": 0.10108233988285065,
      "learning_rate": 0.0002749529190207156,
      "loss": 0.1027,
      "step": 718
    },
    {
      "epoch": 0.022571467812506133,
      "grad_norm": 0.09766082465648651,
      "learning_rate": 0.0002746390458254865,
      "loss": 0.1115,
      "step": 719
    },
    {
      "epoch": 0.022602860674554123,
      "grad_norm": 0.2587259113788605,
      "learning_rate": 0.0002743251726302574,
      "loss": 0.3182,
      "step": 720
    },
    {
      "epoch": 0.022634253536602113,
      "grad_norm": 0.06617457419633865,
      "learning_rate": 0.0002740112994350282,
      "loss": 0.0615,
      "step": 721
    },
    {
      "epoch": 0.022665646398650107,
      "grad_norm": 0.621688187122345,
      "learning_rate": 0.00027369742623979914,
      "loss": 0.7987,
      "step": 722
    },
    {
      "epoch": 0.022697039260698097,
      "grad_norm": 0.2669420540332794,
      "learning_rate": 0.00027338355304457003,
      "loss": 0.281,
      "step": 723
    },
    {
      "epoch": 0.02272843212274609,
      "grad_norm": 0.3650408685207367,
      "learning_rate": 0.0002730696798493409,
      "loss": 0.4266,
      "step": 724
    },
    {
      "epoch": 0.02275982498479408,
      "grad_norm": 0.06699304282665253,
      "learning_rate": 0.00027275580665411174,
      "loss": 0.0756,
      "step": 725
    },
    {
      "epoch": 0.022791217846842075,
      "grad_norm": 0.056618582457304,
      "learning_rate": 0.0002724419334588826,
      "loss": 0.0541,
      "step": 726
    },
    {
      "epoch": 0.022822610708890065,
      "grad_norm": 0.09949510544538498,
      "learning_rate": 0.0002721280602636535,
      "loss": 0.1472,
      "step": 727
    },
    {
      "epoch": 0.02285400357093806,
      "grad_norm": 0.05919935181736946,
      "learning_rate": 0.0002718141870684244,
      "loss": 0.0554,
      "step": 728
    },
    {
      "epoch": 0.02288539643298605,
      "grad_norm": 0.09349964559078217,
      "learning_rate": 0.0002715003138731952,
      "loss": 0.1224,
      "step": 729
    },
    {
      "epoch": 0.022916789295034042,
      "grad_norm": 0.31865766644477844,
      "learning_rate": 0.0002711864406779661,
      "loss": 0.308,
      "step": 730
    },
    {
      "epoch": 0.022948182157082032,
      "grad_norm": 0.5610237121582031,
      "learning_rate": 0.000270872567482737,
      "loss": 0.4516,
      "step": 731
    },
    {
      "epoch": 0.022979575019130026,
      "grad_norm": 0.06336276978254318,
      "learning_rate": 0.00027055869428750786,
      "loss": 0.0548,
      "step": 732
    },
    {
      "epoch": 0.023010967881178016,
      "grad_norm": 0.2619459927082062,
      "learning_rate": 0.0002702448210922787,
      "loss": 0.2832,
      "step": 733
    },
    {
      "epoch": 0.02304236074322601,
      "grad_norm": 0.14248478412628174,
      "learning_rate": 0.0002699309478970496,
      "loss": 0.235,
      "step": 734
    },
    {
      "epoch": 0.023073753605274,
      "grad_norm": 0.48904117941856384,
      "learning_rate": 0.00026961707470182046,
      "loss": 0.6582,
      "step": 735
    },
    {
      "epoch": 0.023105146467321994,
      "grad_norm": 0.5451900959014893,
      "learning_rate": 0.00026930320150659134,
      "loss": 0.3413,
      "step": 736
    },
    {
      "epoch": 0.023136539329369984,
      "grad_norm": 0.6962970495223999,
      "learning_rate": 0.00026898932831136217,
      "loss": 0.8537,
      "step": 737
    },
    {
      "epoch": 0.023167932191417977,
      "grad_norm": 0.049628112465143204,
      "learning_rate": 0.00026867545511613305,
      "loss": 0.075,
      "step": 738
    },
    {
      "epoch": 0.023199325053465968,
      "grad_norm": 0.4547477662563324,
      "learning_rate": 0.000268361581920904,
      "loss": 0.5642,
      "step": 739
    },
    {
      "epoch": 0.02323071791551396,
      "grad_norm": 0.2830231785774231,
      "learning_rate": 0.00026804770872567487,
      "loss": 0.3538,
      "step": 740
    },
    {
      "epoch": 0.02326211077756195,
      "grad_norm": 0.3324791491031647,
      "learning_rate": 0.0002677338355304457,
      "loss": 0.4567,
      "step": 741
    },
    {
      "epoch": 0.023293503639609945,
      "grad_norm": 0.39615583419799805,
      "learning_rate": 0.0002674199623352166,
      "loss": 0.6546,
      "step": 742
    },
    {
      "epoch": 0.023324896501657935,
      "grad_norm": 0.2897731363773346,
      "learning_rate": 0.00026710608913998747,
      "loss": 0.4225,
      "step": 743
    },
    {
      "epoch": 0.02335628936370593,
      "grad_norm": 0.28190040588378906,
      "learning_rate": 0.00026679221594475835,
      "loss": 0.5057,
      "step": 744
    },
    {
      "epoch": 0.02338768222575392,
      "grad_norm": 0.20169925689697266,
      "learning_rate": 0.0002664783427495292,
      "loss": 0.272,
      "step": 745
    },
    {
      "epoch": 0.023419075087801913,
      "grad_norm": 0.36828988790512085,
      "learning_rate": 0.00026616446955430006,
      "loss": 0.6326,
      "step": 746
    },
    {
      "epoch": 0.023450467949849903,
      "grad_norm": 0.46252506971359253,
      "learning_rate": 0.00026585059635907094,
      "loss": 0.6928,
      "step": 747
    },
    {
      "epoch": 0.023481860811897896,
      "grad_norm": 0.12375358492136002,
      "learning_rate": 0.0002655367231638418,
      "loss": 0.1879,
      "step": 748
    },
    {
      "epoch": 0.023513253673945887,
      "grad_norm": 0.07618744671344757,
      "learning_rate": 0.00026522284996861265,
      "loss": 0.0852,
      "step": 749
    },
    {
      "epoch": 0.023544646535993877,
      "grad_norm": 0.25063034892082214,
      "learning_rate": 0.00026490897677338354,
      "loss": 0.2591,
      "step": 750
    },
    {
      "epoch": 0.02357603939804187,
      "grad_norm": 0.0843338891863823,
      "learning_rate": 0.0002645951035781544,
      "loss": 0.0549,
      "step": 751
    },
    {
      "epoch": 0.02360743226008986,
      "grad_norm": 0.42719966173171997,
      "learning_rate": 0.0002642812303829253,
      "loss": 0.4676,
      "step": 752
    },
    {
      "epoch": 0.023638825122137854,
      "grad_norm": 0.14051032066345215,
      "learning_rate": 0.00026396735718769613,
      "loss": 0.1911,
      "step": 753
    },
    {
      "epoch": 0.023670217984185844,
      "grad_norm": 0.21634520590305328,
      "learning_rate": 0.000263653483992467,
      "loss": 0.3747,
      "step": 754
    },
    {
      "epoch": 0.023701610846233838,
      "grad_norm": 0.41834187507629395,
      "learning_rate": 0.0002633396107972379,
      "loss": 0.6728,
      "step": 755
    },
    {
      "epoch": 0.023733003708281828,
      "grad_norm": 0.30516567826271057,
      "learning_rate": 0.00026302573760200883,
      "loss": 0.4611,
      "step": 756
    },
    {
      "epoch": 0.02376439657032982,
      "grad_norm": 0.07709548622369766,
      "learning_rate": 0.0002627118644067797,
      "loss": 0.0663,
      "step": 757
    },
    {
      "epoch": 0.023795789432377812,
      "grad_norm": 0.221368670463562,
      "learning_rate": 0.00026239799121155054,
      "loss": 0.2617,
      "step": 758
    },
    {
      "epoch": 0.023827182294425805,
      "grad_norm": 0.38958367705345154,
      "learning_rate": 0.0002620841180163214,
      "loss": 0.287,
      "step": 759
    },
    {
      "epoch": 0.023858575156473796,
      "grad_norm": 0.19870834052562714,
      "learning_rate": 0.0002617702448210923,
      "loss": 0.3538,
      "step": 760
    },
    {
      "epoch": 0.02388996801852179,
      "grad_norm": 0.07431105524301529,
      "learning_rate": 0.0002614563716258632,
      "loss": 0.0524,
      "step": 761
    },
    {
      "epoch": 0.02392136088056978,
      "grad_norm": 0.08687291294336319,
      "learning_rate": 0.000261142498430634,
      "loss": 0.0957,
      "step": 762
    },
    {
      "epoch": 0.023952753742617773,
      "grad_norm": 0.07744183391332626,
      "learning_rate": 0.0002608286252354049,
      "loss": 0.1323,
      "step": 763
    },
    {
      "epoch": 0.023984146604665763,
      "grad_norm": 0.08190421015024185,
      "learning_rate": 0.0002605147520401758,
      "loss": 0.1059,
      "step": 764
    },
    {
      "epoch": 0.024015539466713757,
      "grad_norm": 0.08961459994316101,
      "learning_rate": 0.00026020087884494667,
      "loss": 0.131,
      "step": 765
    },
    {
      "epoch": 0.024046932328761747,
      "grad_norm": 0.1456974297761917,
      "learning_rate": 0.0002598870056497175,
      "loss": 0.189,
      "step": 766
    },
    {
      "epoch": 0.02407832519080974,
      "grad_norm": 0.24883151054382324,
      "learning_rate": 0.0002595731324544884,
      "loss": 0.4163,
      "step": 767
    },
    {
      "epoch": 0.02410971805285773,
      "grad_norm": 0.11013521999120712,
      "learning_rate": 0.00025925925925925926,
      "loss": 0.1493,
      "step": 768
    },
    {
      "epoch": 0.024141110914905724,
      "grad_norm": 0.1587013602256775,
      "learning_rate": 0.00025894538606403015,
      "loss": 0.1823,
      "step": 769
    },
    {
      "epoch": 0.024172503776953715,
      "grad_norm": 0.09856116026639938,
      "learning_rate": 0.000258631512868801,
      "loss": 0.1213,
      "step": 770
    },
    {
      "epoch": 0.024203896639001708,
      "grad_norm": 0.2889973223209381,
      "learning_rate": 0.00025831763967357186,
      "loss": 0.2958,
      "step": 771
    },
    {
      "epoch": 0.0242352895010497,
      "grad_norm": 0.28747543692588806,
      "learning_rate": 0.00025800376647834274,
      "loss": 0.3757,
      "step": 772
    },
    {
      "epoch": 0.024266682363097692,
      "grad_norm": 0.11937541514635086,
      "learning_rate": 0.0002576898932831137,
      "loss": 0.1316,
      "step": 773
    },
    {
      "epoch": 0.024298075225145682,
      "grad_norm": 0.10841364413499832,
      "learning_rate": 0.0002573760200878845,
      "loss": 0.1625,
      "step": 774
    },
    {
      "epoch": 0.024329468087193676,
      "grad_norm": 0.5649285912513733,
      "learning_rate": 0.0002570621468926554,
      "loss": 0.5117,
      "step": 775
    },
    {
      "epoch": 0.024360860949241666,
      "grad_norm": 0.0825396329164505,
      "learning_rate": 0.00025674827369742627,
      "loss": 0.0723,
      "step": 776
    },
    {
      "epoch": 0.02439225381128966,
      "grad_norm": 0.5864364504814148,
      "learning_rate": 0.00025643440050219715,
      "loss": 0.7983,
      "step": 777
    },
    {
      "epoch": 0.02442364667333765,
      "grad_norm": 0.09757477790117264,
      "learning_rate": 0.000256120527306968,
      "loss": 0.1149,
      "step": 778
    },
    {
      "epoch": 0.024455039535385643,
      "grad_norm": 0.33978062868118286,
      "learning_rate": 0.00025580665411173886,
      "loss": 0.4664,
      "step": 779
    },
    {
      "epoch": 0.024486432397433634,
      "grad_norm": 0.32391515374183655,
      "learning_rate": 0.00025549278091650975,
      "loss": 0.425,
      "step": 780
    },
    {
      "epoch": 0.024517825259481624,
      "grad_norm": 0.10489833354949951,
      "learning_rate": 0.00025517890772128063,
      "loss": 0.076,
      "step": 781
    },
    {
      "epoch": 0.024549218121529617,
      "grad_norm": 0.13731983304023743,
      "learning_rate": 0.00025486503452605146,
      "loss": 0.0882,
      "step": 782
    },
    {
      "epoch": 0.024580610983577608,
      "grad_norm": 0.08625636249780655,
      "learning_rate": 0.00025455116133082234,
      "loss": 0.0849,
      "step": 783
    },
    {
      "epoch": 0.0246120038456256,
      "grad_norm": 0.29514414072036743,
      "learning_rate": 0.0002542372881355932,
      "loss": 0.487,
      "step": 784
    },
    {
      "epoch": 0.02464339670767359,
      "grad_norm": 0.12796847522258759,
      "learning_rate": 0.0002539234149403641,
      "loss": 0.1763,
      "step": 785
    },
    {
      "epoch": 0.024674789569721585,
      "grad_norm": 0.09434947371482849,
      "learning_rate": 0.00025360954174513494,
      "loss": 0.1131,
      "step": 786
    },
    {
      "epoch": 0.024706182431769575,
      "grad_norm": 0.3353421092033386,
      "learning_rate": 0.0002532956685499058,
      "loss": 0.4344,
      "step": 787
    },
    {
      "epoch": 0.02473757529381757,
      "grad_norm": 0.1162199154496193,
      "learning_rate": 0.0002529817953546767,
      "loss": 0.1539,
      "step": 788
    },
    {
      "epoch": 0.02476896815586556,
      "grad_norm": 0.11332719773054123,
      "learning_rate": 0.0002526679221594476,
      "loss": 0.0831,
      "step": 789
    },
    {
      "epoch": 0.024800361017913553,
      "grad_norm": 0.05533642694354057,
      "learning_rate": 0.00025235404896421847,
      "loss": 0.0559,
      "step": 790
    },
    {
      "epoch": 0.024831753879961543,
      "grad_norm": 0.10556615889072418,
      "learning_rate": 0.00025204017576898935,
      "loss": 0.1001,
      "step": 791
    },
    {
      "epoch": 0.024863146742009536,
      "grad_norm": 0.1949370950460434,
      "learning_rate": 0.00025172630257376023,
      "loss": 0.2858,
      "step": 792
    },
    {
      "epoch": 0.024894539604057526,
      "grad_norm": 0.4593365490436554,
      "learning_rate": 0.0002514124293785311,
      "loss": 0.5851,
      "step": 793
    },
    {
      "epoch": 0.02492593246610552,
      "grad_norm": 0.2171715497970581,
      "learning_rate": 0.00025109855618330194,
      "loss": 0.1933,
      "step": 794
    },
    {
      "epoch": 0.02495732532815351,
      "grad_norm": 0.5392907857894897,
      "learning_rate": 0.0002507846829880728,
      "loss": 0.8402,
      "step": 795
    },
    {
      "epoch": 0.024988718190201504,
      "grad_norm": 0.27427127957344055,
      "learning_rate": 0.0002504708097928437,
      "loss": 0.2584,
      "step": 796
    },
    {
      "epoch": 0.025020111052249494,
      "grad_norm": 0.08619420230388641,
      "learning_rate": 0.0002501569365976146,
      "loss": 0.0888,
      "step": 797
    },
    {
      "epoch": 0.025051503914297488,
      "grad_norm": 0.37265628576278687,
      "learning_rate": 0.0002498430634023854,
      "loss": 0.584,
      "step": 798
    },
    {
      "epoch": 0.025082896776345478,
      "grad_norm": 0.14479467272758484,
      "learning_rate": 0.0002495291902071563,
      "loss": 0.1738,
      "step": 799
    },
    {
      "epoch": 0.02511428963839347,
      "grad_norm": 0.45091941952705383,
      "learning_rate": 0.0002492153170119272,
      "loss": 0.7511,
      "step": 800
    },
    {
      "epoch": 0.02514568250044146,
      "grad_norm": 0.1473684012889862,
      "learning_rate": 0.00024890144381669807,
      "loss": 0.2008,
      "step": 801
    },
    {
      "epoch": 0.025177075362489455,
      "grad_norm": 0.21307054162025452,
      "learning_rate": 0.00024858757062146895,
      "loss": 0.3754,
      "step": 802
    },
    {
      "epoch": 0.025208468224537445,
      "grad_norm": 0.27992576360702515,
      "learning_rate": 0.00024827369742623983,
      "loss": 0.255,
      "step": 803
    },
    {
      "epoch": 0.02523986108658544,
      "grad_norm": 0.23514652252197266,
      "learning_rate": 0.00024795982423101066,
      "loss": 0.2332,
      "step": 804
    },
    {
      "epoch": 0.02527125394863343,
      "grad_norm": 0.3605291247367859,
      "learning_rate": 0.00024764595103578154,
      "loss": 0.4985,
      "step": 805
    },
    {
      "epoch": 0.025302646810681423,
      "grad_norm": 0.19886420667171478,
      "learning_rate": 0.00024733207784055243,
      "loss": 0.2369,
      "step": 806
    },
    {
      "epoch": 0.025334039672729413,
      "grad_norm": 0.08114627003669739,
      "learning_rate": 0.0002470182046453233,
      "loss": 0.0751,
      "step": 807
    },
    {
      "epoch": 0.025365432534777407,
      "grad_norm": 0.0854502022266388,
      "learning_rate": 0.00024670433145009414,
      "loss": 0.0856,
      "step": 808
    },
    {
      "epoch": 0.025396825396825397,
      "grad_norm": 0.45318230986595154,
      "learning_rate": 0.000246390458254865,
      "loss": 0.5461,
      "step": 809
    },
    {
      "epoch": 0.025428218258873387,
      "grad_norm": 0.0637756884098053,
      "learning_rate": 0.0002460765850596359,
      "loss": 0.082,
      "step": 810
    },
    {
      "epoch": 0.02545961112092138,
      "grad_norm": 0.30920130014419556,
      "learning_rate": 0.0002457627118644068,
      "loss": 0.3711,
      "step": 811
    },
    {
      "epoch": 0.02549100398296937,
      "grad_norm": 0.06406299769878387,
      "learning_rate": 0.00024544883866917767,
      "loss": 0.0854,
      "step": 812
    },
    {
      "epoch": 0.025522396845017364,
      "grad_norm": 0.12352404743432999,
      "learning_rate": 0.00024513496547394855,
      "loss": 0.1604,
      "step": 813
    },
    {
      "epoch": 0.025553789707065355,
      "grad_norm": 0.06972622871398926,
      "learning_rate": 0.0002448210922787194,
      "loss": 0.0999,
      "step": 814
    },
    {
      "epoch": 0.025585182569113348,
      "grad_norm": 0.46054235100746155,
      "learning_rate": 0.00024450721908349026,
      "loss": 0.4751,
      "step": 815
    },
    {
      "epoch": 0.02561657543116134,
      "grad_norm": 0.14255966246128082,
      "learning_rate": 0.00024419334588826115,
      "loss": 0.1473,
      "step": 816
    },
    {
      "epoch": 0.025647968293209332,
      "grad_norm": 0.2501252293586731,
      "learning_rate": 0.000243879472693032,
      "loss": 0.2499,
      "step": 817
    },
    {
      "epoch": 0.025679361155257322,
      "grad_norm": 0.2651267945766449,
      "learning_rate": 0.00024356559949780288,
      "loss": 0.3408,
      "step": 818
    },
    {
      "epoch": 0.025710754017305316,
      "grad_norm": 0.06343995779752731,
      "learning_rate": 0.00024325172630257377,
      "loss": 0.0518,
      "step": 819
    },
    {
      "epoch": 0.025742146879353306,
      "grad_norm": 0.4030330777168274,
      "learning_rate": 0.00024293785310734465,
      "loss": 0.5872,
      "step": 820
    },
    {
      "epoch": 0.0257735397414013,
      "grad_norm": 0.10984630882740021,
      "learning_rate": 0.0002426239799121155,
      "loss": 0.0695,
      "step": 821
    },
    {
      "epoch": 0.02580493260344929,
      "grad_norm": 0.06860378384590149,
      "learning_rate": 0.0002423101067168864,
      "loss": 0.1028,
      "step": 822
    },
    {
      "epoch": 0.025836325465497283,
      "grad_norm": 0.13535746932029724,
      "learning_rate": 0.00024199623352165724,
      "loss": 0.2018,
      "step": 823
    },
    {
      "epoch": 0.025867718327545274,
      "grad_norm": 0.21163822710514069,
      "learning_rate": 0.00024168236032642813,
      "loss": 0.2458,
      "step": 824
    },
    {
      "epoch": 0.025899111189593267,
      "grad_norm": 0.1223493441939354,
      "learning_rate": 0.00024136848713119898,
      "loss": 0.1333,
      "step": 825
    },
    {
      "epoch": 0.025930504051641257,
      "grad_norm": 0.2008546143770218,
      "learning_rate": 0.00024105461393596987,
      "loss": 0.247,
      "step": 826
    },
    {
      "epoch": 0.02596189691368925,
      "grad_norm": 0.22133687138557434,
      "learning_rate": 0.00024074074074074072,
      "loss": 0.2846,
      "step": 827
    },
    {
      "epoch": 0.02599328977573724,
      "grad_norm": 0.2514079213142395,
      "learning_rate": 0.00024042686754551163,
      "loss": 0.1954,
      "step": 828
    },
    {
      "epoch": 0.026024682637785235,
      "grad_norm": 0.20661427080631256,
      "learning_rate": 0.00024011299435028249,
      "loss": 0.1047,
      "step": 829
    },
    {
      "epoch": 0.026056075499833225,
      "grad_norm": 0.2477283924818039,
      "learning_rate": 0.00023979912115505337,
      "loss": 0.2284,
      "step": 830
    },
    {
      "epoch": 0.02608746836188122,
      "grad_norm": 0.08023873716592789,
      "learning_rate": 0.00023948524795982422,
      "loss": 0.0969,
      "step": 831
    },
    {
      "epoch": 0.02611886122392921,
      "grad_norm": 0.36397820711135864,
      "learning_rate": 0.0002391713747645951,
      "loss": 0.5743,
      "step": 832
    },
    {
      "epoch": 0.026150254085977202,
      "grad_norm": 0.09093908220529556,
      "learning_rate": 0.00023885750156936596,
      "loss": 0.1093,
      "step": 833
    },
    {
      "epoch": 0.026181646948025192,
      "grad_norm": 0.12025578320026398,
      "learning_rate": 0.00023854362837413685,
      "loss": 0.177,
      "step": 834
    },
    {
      "epoch": 0.026213039810073186,
      "grad_norm": 0.14021502435207367,
      "learning_rate": 0.0002382297551789077,
      "loss": 0.1165,
      "step": 835
    },
    {
      "epoch": 0.026244432672121176,
      "grad_norm": 0.13403403759002686,
      "learning_rate": 0.0002379158819836786,
      "loss": 0.1578,
      "step": 836
    },
    {
      "epoch": 0.02627582553416917,
      "grad_norm": 0.07371189445257187,
      "learning_rate": 0.0002376020087884495,
      "loss": 0.1011,
      "step": 837
    },
    {
      "epoch": 0.02630721839621716,
      "grad_norm": 0.07885623723268509,
      "learning_rate": 0.00023728813559322035,
      "loss": 0.0843,
      "step": 838
    },
    {
      "epoch": 0.026338611258265154,
      "grad_norm": 0.393489271402359,
      "learning_rate": 0.00023697426239799123,
      "loss": 0.6129,
      "step": 839
    },
    {
      "epoch": 0.026370004120313144,
      "grad_norm": 0.10747173428535461,
      "learning_rate": 0.0002366603892027621,
      "loss": 0.1701,
      "step": 840
    },
    {
      "epoch": 0.026401396982361134,
      "grad_norm": 0.32802680134773254,
      "learning_rate": 0.00023634651600753297,
      "loss": 0.4214,
      "step": 841
    },
    {
      "epoch": 0.026432789844409128,
      "grad_norm": 0.33967483043670654,
      "learning_rate": 0.00023603264281230383,
      "loss": 0.4014,
      "step": 842
    },
    {
      "epoch": 0.026464182706457118,
      "grad_norm": 0.2307710200548172,
      "learning_rate": 0.0002357187696170747,
      "loss": 0.3442,
      "step": 843
    },
    {
      "epoch": 0.02649557556850511,
      "grad_norm": 0.25153934955596924,
      "learning_rate": 0.00023540489642184556,
      "loss": 0.2904,
      "step": 844
    },
    {
      "epoch": 0.0265269684305531,
      "grad_norm": 0.2539215087890625,
      "learning_rate": 0.00023509102322661647,
      "loss": 0.1876,
      "step": 845
    },
    {
      "epoch": 0.026558361292601095,
      "grad_norm": 0.13980881869792938,
      "learning_rate": 0.00023477715003138733,
      "loss": 0.1612,
      "step": 846
    },
    {
      "epoch": 0.026589754154649085,
      "grad_norm": 0.12423525005578995,
      "learning_rate": 0.0002344632768361582,
      "loss": 0.1919,
      "step": 847
    },
    {
      "epoch": 0.02662114701669708,
      "grad_norm": 0.2664805054664612,
      "learning_rate": 0.00023414940364092907,
      "loss": 0.3464,
      "step": 848
    },
    {
      "epoch": 0.02665253987874507,
      "grad_norm": 0.09875654429197311,
      "learning_rate": 0.00023383553044569995,
      "loss": 0.0898,
      "step": 849
    },
    {
      "epoch": 0.026683932740793063,
      "grad_norm": 0.09389828145503998,
      "learning_rate": 0.0002335216572504708,
      "loss": 0.0501,
      "step": 850
    },
    {
      "epoch": 0.026715325602841053,
      "grad_norm": 0.2937980592250824,
      "learning_rate": 0.0002332077840552417,
      "loss": 0.2883,
      "step": 851
    },
    {
      "epoch": 0.026746718464889047,
      "grad_norm": 0.11712919175624847,
      "learning_rate": 0.00023289391086001255,
      "loss": 0.152,
      "step": 852
    },
    {
      "epoch": 0.026778111326937037,
      "grad_norm": 0.44333088397979736,
      "learning_rate": 0.00023258003766478345,
      "loss": 0.4867,
      "step": 853
    },
    {
      "epoch": 0.02680950418898503,
      "grad_norm": 0.27153557538986206,
      "learning_rate": 0.0002322661644695543,
      "loss": 0.2288,
      "step": 854
    },
    {
      "epoch": 0.02684089705103302,
      "grad_norm": 0.3259910047054291,
      "learning_rate": 0.0002319522912743252,
      "loss": 0.3884,
      "step": 855
    },
    {
      "epoch": 0.026872289913081014,
      "grad_norm": 0.09378993511199951,
      "learning_rate": 0.00023163841807909605,
      "loss": 0.093,
      "step": 856
    },
    {
      "epoch": 0.026903682775129004,
      "grad_norm": 0.10028670728206635,
      "learning_rate": 0.00023132454488386693,
      "loss": 0.1258,
      "step": 857
    },
    {
      "epoch": 0.026935075637176998,
      "grad_norm": 0.20676079392433167,
      "learning_rate": 0.0002310106716886378,
      "loss": 0.2195,
      "step": 858
    },
    {
      "epoch": 0.026966468499224988,
      "grad_norm": 0.41318196058273315,
      "learning_rate": 0.00023069679849340867,
      "loss": 0.5188,
      "step": 859
    },
    {
      "epoch": 0.026997861361272982,
      "grad_norm": 0.42201709747314453,
      "learning_rate": 0.00023038292529817953,
      "loss": 0.4007,
      "step": 860
    },
    {
      "epoch": 0.027029254223320972,
      "grad_norm": 0.20270630717277527,
      "learning_rate": 0.0002300690521029504,
      "loss": 0.2763,
      "step": 861
    },
    {
      "epoch": 0.027060647085368966,
      "grad_norm": 0.1418854147195816,
      "learning_rate": 0.0002297551789077213,
      "loss": 0.2523,
      "step": 862
    },
    {
      "epoch": 0.027092039947416956,
      "grad_norm": 0.24005109071731567,
      "learning_rate": 0.00022944130571249217,
      "loss": 0.2309,
      "step": 863
    },
    {
      "epoch": 0.02712343280946495,
      "grad_norm": 0.07718340307474136,
      "learning_rate": 0.00022912743251726303,
      "loss": 0.0712,
      "step": 864
    },
    {
      "epoch": 0.02715482567151294,
      "grad_norm": 0.08946117758750916,
      "learning_rate": 0.0002288135593220339,
      "loss": 0.0918,
      "step": 865
    },
    {
      "epoch": 0.027186218533560933,
      "grad_norm": 0.07099156826734543,
      "learning_rate": 0.00022849968612680477,
      "loss": 0.0876,
      "step": 866
    },
    {
      "epoch": 0.027217611395608923,
      "grad_norm": 0.11244063824415207,
      "learning_rate": 0.00022818581293157565,
      "loss": 0.1328,
      "step": 867
    },
    {
      "epoch": 0.027249004257656917,
      "grad_norm": 0.16897961497306824,
      "learning_rate": 0.0002278719397363465,
      "loss": 0.2604,
      "step": 868
    },
    {
      "epoch": 0.027280397119704907,
      "grad_norm": 0.08308101445436478,
      "learning_rate": 0.0002275580665411174,
      "loss": 0.1109,
      "step": 869
    },
    {
      "epoch": 0.027311789981752897,
      "grad_norm": 0.3028383255004883,
      "learning_rate": 0.00022724419334588827,
      "loss": 0.4107,
      "step": 870
    },
    {
      "epoch": 0.02734318284380089,
      "grad_norm": 0.085781991481781,
      "learning_rate": 0.00022693032015065915,
      "loss": 0.0926,
      "step": 871
    },
    {
      "epoch": 0.02737457570584888,
      "grad_norm": 0.5036846399307251,
      "learning_rate": 0.00022661644695543,
      "loss": 0.5481,
      "step": 872
    },
    {
      "epoch": 0.027405968567896875,
      "grad_norm": 0.29502394795417786,
      "learning_rate": 0.0002263025737602009,
      "loss": 0.4244,
      "step": 873
    },
    {
      "epoch": 0.027437361429944865,
      "grad_norm": 0.10393897444009781,
      "learning_rate": 0.00022598870056497175,
      "loss": 0.0671,
      "step": 874
    },
    {
      "epoch": 0.02746875429199286,
      "grad_norm": 0.2860790193080902,
      "learning_rate": 0.00022567482736974263,
      "loss": 0.3081,
      "step": 875
    },
    {
      "epoch": 0.02750014715404085,
      "grad_norm": 0.32255202531814575,
      "learning_rate": 0.0002253609541745135,
      "loss": 0.4867,
      "step": 876
    },
    {
      "epoch": 0.027531540016088842,
      "grad_norm": 0.2996067702770233,
      "learning_rate": 0.00022504708097928437,
      "loss": 0.3822,
      "step": 877
    },
    {
      "epoch": 0.027562932878136832,
      "grad_norm": 0.3275805115699768,
      "learning_rate": 0.00022473320778405525,
      "loss": 0.3689,
      "step": 878
    },
    {
      "epoch": 0.027594325740184826,
      "grad_norm": 0.26060670614242554,
      "learning_rate": 0.00022441933458882614,
      "loss": 0.2316,
      "step": 879
    },
    {
      "epoch": 0.027625718602232816,
      "grad_norm": 0.419142484664917,
      "learning_rate": 0.000224105461393597,
      "loss": 0.5926,
      "step": 880
    },
    {
      "epoch": 0.02765711146428081,
      "grad_norm": 0.2322174608707428,
      "learning_rate": 0.00022379158819836787,
      "loss": 0.261,
      "step": 881
    },
    {
      "epoch": 0.0276885043263288,
      "grad_norm": 0.14125293493270874,
      "learning_rate": 0.00022347771500313873,
      "loss": 0.1395,
      "step": 882
    },
    {
      "epoch": 0.027719897188376794,
      "grad_norm": 0.37183648347854614,
      "learning_rate": 0.0002231638418079096,
      "loss": 0.4777,
      "step": 883
    },
    {
      "epoch": 0.027751290050424784,
      "grad_norm": 0.1401320993900299,
      "learning_rate": 0.00022284996861268047,
      "loss": 0.1098,
      "step": 884
    },
    {
      "epoch": 0.027782682912472777,
      "grad_norm": 0.09012982249259949,
      "learning_rate": 0.00022253609541745135,
      "loss": 0.088,
      "step": 885
    },
    {
      "epoch": 0.027814075774520768,
      "grad_norm": 0.10787734389305115,
      "learning_rate": 0.0002222222222222222,
      "loss": 0.0756,
      "step": 886
    },
    {
      "epoch": 0.02784546863656876,
      "grad_norm": 0.42899778485298157,
      "learning_rate": 0.00022190834902699312,
      "loss": 0.614,
      "step": 887
    },
    {
      "epoch": 0.02787686149861675,
      "grad_norm": 0.2673366069793701,
      "learning_rate": 0.00022159447583176397,
      "loss": 0.3232,
      "step": 888
    },
    {
      "epoch": 0.027908254360664745,
      "grad_norm": 0.3644150495529175,
      "learning_rate": 0.00022128060263653485,
      "loss": 0.4403,
      "step": 889
    },
    {
      "epoch": 0.027939647222712735,
      "grad_norm": 0.14859060943126678,
      "learning_rate": 0.0002209667294413057,
      "loss": 0.1255,
      "step": 890
    },
    {
      "epoch": 0.02797104008476073,
      "grad_norm": 0.42928579449653625,
      "learning_rate": 0.0002206528562460766,
      "loss": 0.4193,
      "step": 891
    },
    {
      "epoch": 0.02800243294680872,
      "grad_norm": 0.22845232486724854,
      "learning_rate": 0.00022033898305084745,
      "loss": 0.2549,
      "step": 892
    },
    {
      "epoch": 0.028033825808856713,
      "grad_norm": 0.3029848337173462,
      "learning_rate": 0.00022002510985561833,
      "loss": 0.4791,
      "step": 893
    },
    {
      "epoch": 0.028065218670904703,
      "grad_norm": 0.37560728192329407,
      "learning_rate": 0.00021971123666038919,
      "loss": 0.3261,
      "step": 894
    },
    {
      "epoch": 0.028096611532952696,
      "grad_norm": 0.3930046856403351,
      "learning_rate": 0.0002193973634651601,
      "loss": 0.4735,
      "step": 895
    },
    {
      "epoch": 0.028128004395000687,
      "grad_norm": 0.08039415627717972,
      "learning_rate": 0.00021908349026993095,
      "loss": 0.0667,
      "step": 896
    },
    {
      "epoch": 0.02815939725704868,
      "grad_norm": 0.19832156598567963,
      "learning_rate": 0.00021876961707470183,
      "loss": 0.2594,
      "step": 897
    },
    {
      "epoch": 0.02819079011909667,
      "grad_norm": 0.09096287935972214,
      "learning_rate": 0.0002184557438794727,
      "loss": 0.0905,
      "step": 898
    },
    {
      "epoch": 0.028222182981144664,
      "grad_norm": 0.15370067954063416,
      "learning_rate": 0.00021814187068424357,
      "loss": 0.0984,
      "step": 899
    },
    {
      "epoch": 0.028253575843192654,
      "grad_norm": 0.3239649534225464,
      "learning_rate": 0.00021782799748901443,
      "loss": 0.4058,
      "step": 900
    },
    {
      "epoch": 0.028284968705240644,
      "grad_norm": 0.17472456395626068,
      "learning_rate": 0.0002175141242937853,
      "loss": 0.1905,
      "step": 901
    },
    {
      "epoch": 0.028316361567288638,
      "grad_norm": 0.6562484502792358,
      "learning_rate": 0.00021720025109855617,
      "loss": 0.7548,
      "step": 902
    },
    {
      "epoch": 0.028347754429336628,
      "grad_norm": 0.23882849514484406,
      "learning_rate": 0.00021688637790332705,
      "loss": 0.2155,
      "step": 903
    },
    {
      "epoch": 0.02837914729138462,
      "grad_norm": 0.1001795157790184,
      "learning_rate": 0.00021657250470809793,
      "loss": 0.1192,
      "step": 904
    },
    {
      "epoch": 0.028410540153432612,
      "grad_norm": 0.13629622757434845,
      "learning_rate": 0.00021625863151286882,
      "loss": 0.1448,
      "step": 905
    },
    {
      "epoch": 0.028441933015480605,
      "grad_norm": 0.093422070145607,
      "learning_rate": 0.00021594475831763967,
      "loss": 0.1002,
      "step": 906
    },
    {
      "epoch": 0.028473325877528596,
      "grad_norm": 0.23554512858390808,
      "learning_rate": 0.00021563088512241055,
      "loss": 0.3318,
      "step": 907
    },
    {
      "epoch": 0.02850471873957659,
      "grad_norm": 0.27247393131256104,
      "learning_rate": 0.0002153170119271814,
      "loss": 0.2371,
      "step": 908
    },
    {
      "epoch": 0.02853611160162458,
      "grad_norm": 0.37068861722946167,
      "learning_rate": 0.0002150031387319523,
      "loss": 0.3559,
      "step": 909
    },
    {
      "epoch": 0.028567504463672573,
      "grad_norm": 0.8342985510826111,
      "learning_rate": 0.00021468926553672315,
      "loss": 0.6757,
      "step": 910
    },
    {
      "epoch": 0.028598897325720563,
      "grad_norm": 0.22262008488178253,
      "learning_rate": 0.00021437539234149403,
      "loss": 0.2971,
      "step": 911
    },
    {
      "epoch": 0.028630290187768557,
      "grad_norm": 0.331661194562912,
      "learning_rate": 0.00021406151914626494,
      "loss": 0.4603,
      "step": 912
    },
    {
      "epoch": 0.028661683049816547,
      "grad_norm": 0.3451356887817383,
      "learning_rate": 0.0002137476459510358,
      "loss": 0.4128,
      "step": 913
    },
    {
      "epoch": 0.02869307591186454,
      "grad_norm": 0.4048866033554077,
      "learning_rate": 0.00021343377275580668,
      "loss": 0.5511,
      "step": 914
    },
    {
      "epoch": 0.02872446877391253,
      "grad_norm": 0.13067416846752167,
      "learning_rate": 0.00021311989956057753,
      "loss": 0.1439,
      "step": 915
    },
    {
      "epoch": 0.028755861635960524,
      "grad_norm": 0.2031944841146469,
      "learning_rate": 0.00021280602636534842,
      "loss": 0.2477,
      "step": 916
    },
    {
      "epoch": 0.028787254498008515,
      "grad_norm": 0.14628872275352478,
      "learning_rate": 0.00021249215317011927,
      "loss": 0.0688,
      "step": 917
    },
    {
      "epoch": 0.028818647360056508,
      "grad_norm": 0.06655966490507126,
      "learning_rate": 0.00021217827997489016,
      "loss": 0.09,
      "step": 918
    },
    {
      "epoch": 0.0288500402221045,
      "grad_norm": 0.11788192391395569,
      "learning_rate": 0.000211864406779661,
      "loss": 0.0843,
      "step": 919
    },
    {
      "epoch": 0.028881433084152492,
      "grad_norm": 0.31264355778694153,
      "learning_rate": 0.0002115505335844319,
      "loss": 0.5522,
      "step": 920
    },
    {
      "epoch": 0.028912825946200482,
      "grad_norm": 0.17629659175872803,
      "learning_rate": 0.00021123666038920278,
      "loss": 0.1129,
      "step": 921
    },
    {
      "epoch": 0.028944218808248476,
      "grad_norm": 0.4359934329986572,
      "learning_rate": 0.00021092278719397366,
      "loss": 0.6302,
      "step": 922
    },
    {
      "epoch": 0.028975611670296466,
      "grad_norm": 0.24385783076286316,
      "learning_rate": 0.00021060891399874451,
      "loss": 0.2109,
      "step": 923
    },
    {
      "epoch": 0.02900700453234446,
      "grad_norm": 0.4487699568271637,
      "learning_rate": 0.0002102950408035154,
      "loss": 0.8221,
      "step": 924
    },
    {
      "epoch": 0.02903839739439245,
      "grad_norm": 0.09764041006565094,
      "learning_rate": 0.00020998116760828625,
      "loss": 0.1299,
      "step": 925
    },
    {
      "epoch": 0.029069790256440443,
      "grad_norm": 0.29411202669143677,
      "learning_rate": 0.00020966729441305714,
      "loss": 0.26,
      "step": 926
    },
    {
      "epoch": 0.029101183118488434,
      "grad_norm": 0.40554648637771606,
      "learning_rate": 0.000209353421217828,
      "loss": 0.4965,
      "step": 927
    },
    {
      "epoch": 0.029132575980536427,
      "grad_norm": 0.06874842196702957,
      "learning_rate": 0.00020903954802259887,
      "loss": 0.0609,
      "step": 928
    },
    {
      "epoch": 0.029163968842584417,
      "grad_norm": 0.31574171781539917,
      "learning_rate": 0.00020872567482736976,
      "loss": 0.3347,
      "step": 929
    },
    {
      "epoch": 0.029195361704632408,
      "grad_norm": 0.24118860065937042,
      "learning_rate": 0.00020841180163214064,
      "loss": 0.3609,
      "step": 930
    },
    {
      "epoch": 0.0292267545666804,
      "grad_norm": 0.44203782081604004,
      "learning_rate": 0.0002080979284369115,
      "loss": 0.5318,
      "step": 931
    },
    {
      "epoch": 0.02925814742872839,
      "grad_norm": 0.24028262495994568,
      "learning_rate": 0.00020778405524168238,
      "loss": 0.2474,
      "step": 932
    },
    {
      "epoch": 0.029289540290776385,
      "grad_norm": 0.4428505301475525,
      "learning_rate": 0.00020747018204645323,
      "loss": 0.7766,
      "step": 933
    },
    {
      "epoch": 0.029320933152824375,
      "grad_norm": 0.3075149953365326,
      "learning_rate": 0.00020715630885122412,
      "loss": 0.2961,
      "step": 934
    },
    {
      "epoch": 0.02935232601487237,
      "grad_norm": 0.054681241512298584,
      "learning_rate": 0.00020684243565599497,
      "loss": 0.0703,
      "step": 935
    },
    {
      "epoch": 0.02938371887692036,
      "grad_norm": 0.06705967336893082,
      "learning_rate": 0.00020652856246076585,
      "loss": 0.0945,
      "step": 936
    },
    {
      "epoch": 0.029415111738968353,
      "grad_norm": 0.1251409500837326,
      "learning_rate": 0.0002062146892655367,
      "loss": 0.1236,
      "step": 937
    },
    {
      "epoch": 0.029446504601016343,
      "grad_norm": 0.06025072559714317,
      "learning_rate": 0.00020590081607030762,
      "loss": 0.0679,
      "step": 938
    },
    {
      "epoch": 0.029477897463064336,
      "grad_norm": 0.13389283418655396,
      "learning_rate": 0.00020558694287507848,
      "loss": 0.1481,
      "step": 939
    },
    {
      "epoch": 0.029509290325112326,
      "grad_norm": 0.16505073010921478,
      "learning_rate": 0.00020527306967984936,
      "loss": 0.1576,
      "step": 940
    },
    {
      "epoch": 0.02954068318716032,
      "grad_norm": 0.2446054220199585,
      "learning_rate": 0.00020495919648462021,
      "loss": 0.2402,
      "step": 941
    },
    {
      "epoch": 0.02957207604920831,
      "grad_norm": 0.1485103964805603,
      "learning_rate": 0.0002046453232893911,
      "loss": 0.1667,
      "step": 942
    },
    {
      "epoch": 0.029603468911256304,
      "grad_norm": 0.20440657436847687,
      "learning_rate": 0.00020433145009416195,
      "loss": 0.2285,
      "step": 943
    },
    {
      "epoch": 0.029634861773304294,
      "grad_norm": 0.24901625514030457,
      "learning_rate": 0.00020401757689893284,
      "loss": 0.302,
      "step": 944
    },
    {
      "epoch": 0.029666254635352288,
      "grad_norm": 0.4514497220516205,
      "learning_rate": 0.0002037037037037037,
      "loss": 0.6724,
      "step": 945
    },
    {
      "epoch": 0.029697647497400278,
      "grad_norm": 0.9341163039207458,
      "learning_rate": 0.0002033898305084746,
      "loss": 0.8709,
      "step": 946
    },
    {
      "epoch": 0.02972904035944827,
      "grad_norm": 0.12296976894140244,
      "learning_rate": 0.00020307595731324546,
      "loss": 0.1699,
      "step": 947
    },
    {
      "epoch": 0.02976043322149626,
      "grad_norm": 0.09792088717222214,
      "learning_rate": 0.00020276208411801634,
      "loss": 0.1035,
      "step": 948
    },
    {
      "epoch": 0.029791826083544255,
      "grad_norm": 0.10461267828941345,
      "learning_rate": 0.0002024482109227872,
      "loss": 0.1016,
      "step": 949
    },
    {
      "epoch": 0.029823218945592245,
      "grad_norm": 0.08323752880096436,
      "learning_rate": 0.00020213433772755808,
      "loss": 0.0752,
      "step": 950
    },
    {
      "epoch": 0.02985461180764024,
      "grad_norm": 0.16668984293937683,
      "learning_rate": 0.00020182046453232893,
      "loss": 0.2723,
      "step": 951
    },
    {
      "epoch": 0.02988600466968823,
      "grad_norm": 0.35989320278167725,
      "learning_rate": 0.00020150659133709982,
      "loss": 0.45,
      "step": 952
    },
    {
      "epoch": 0.029917397531736223,
      "grad_norm": 0.2992565631866455,
      "learning_rate": 0.00020119271814187067,
      "loss": 0.409,
      "step": 953
    },
    {
      "epoch": 0.029948790393784213,
      "grad_norm": 0.08462408185005188,
      "learning_rate": 0.00020087884494664155,
      "loss": 0.1358,
      "step": 954
    },
    {
      "epoch": 0.029980183255832207,
      "grad_norm": 0.14199963212013245,
      "learning_rate": 0.00020056497175141244,
      "loss": 0.1345,
      "step": 955
    },
    {
      "epoch": 0.030011576117880197,
      "grad_norm": 0.33704695105552673,
      "learning_rate": 0.00020025109855618332,
      "loss": 0.2621,
      "step": 956
    },
    {
      "epoch": 0.03004296897992819,
      "grad_norm": 0.07728646695613861,
      "learning_rate": 0.00019993722536095418,
      "loss": 0.0659,
      "step": 957
    },
    {
      "epoch": 0.03007436184197618,
      "grad_norm": 0.5249478816986084,
      "learning_rate": 0.00019962335216572506,
      "loss": 0.7538,
      "step": 958
    },
    {
      "epoch": 0.030105754704024174,
      "grad_norm": 0.30125054717063904,
      "learning_rate": 0.00019930947897049591,
      "loss": 0.3524,
      "step": 959
    },
    {
      "epoch": 0.030137147566072164,
      "grad_norm": 0.10915198922157288,
      "learning_rate": 0.0001989956057752668,
      "loss": 0.1306,
      "step": 960
    },
    {
      "epoch": 0.030168540428120155,
      "grad_norm": 0.1926768571138382,
      "learning_rate": 0.00019868173258003765,
      "loss": 0.2502,
      "step": 961
    },
    {
      "epoch": 0.030199933290168148,
      "grad_norm": 0.09666672348976135,
      "learning_rate": 0.00019836785938480853,
      "loss": 0.183,
      "step": 962
    },
    {
      "epoch": 0.03023132615221614,
      "grad_norm": 0.0848868265748024,
      "learning_rate": 0.00019805398618957942,
      "loss": 0.1139,
      "step": 963
    },
    {
      "epoch": 0.030262719014264132,
      "grad_norm": 0.4678348898887634,
      "learning_rate": 0.0001977401129943503,
      "loss": 0.6285,
      "step": 964
    },
    {
      "epoch": 0.030294111876312122,
      "grad_norm": 0.23292718827724457,
      "learning_rate": 0.00019742623979912116,
      "loss": 0.318,
      "step": 965
    },
    {
      "epoch": 0.030325504738360116,
      "grad_norm": 0.3891974985599518,
      "learning_rate": 0.00019711236660389204,
      "loss": 0.3021,
      "step": 966
    },
    {
      "epoch": 0.030356897600408106,
      "grad_norm": 0.14490263164043427,
      "learning_rate": 0.0001967984934086629,
      "loss": 0.1903,
      "step": 967
    },
    {
      "epoch": 0.0303882904624561,
      "grad_norm": 0.19783753156661987,
      "learning_rate": 0.00019648462021343378,
      "loss": 0.3159,
      "step": 968
    },
    {
      "epoch": 0.03041968332450409,
      "grad_norm": 0.04227091744542122,
      "learning_rate": 0.00019617074701820463,
      "loss": 0.0582,
      "step": 969
    },
    {
      "epoch": 0.030451076186552083,
      "grad_norm": 0.17915253341197968,
      "learning_rate": 0.00019585687382297552,
      "loss": 0.2269,
      "step": 970
    },
    {
      "epoch": 0.030482469048600074,
      "grad_norm": 0.18135006725788116,
      "learning_rate": 0.00019554300062774637,
      "loss": 0.2742,
      "step": 971
    },
    {
      "epoch": 0.030513861910648067,
      "grad_norm": 0.7231743931770325,
      "learning_rate": 0.00019522912743251728,
      "loss": 0.525,
      "step": 972
    },
    {
      "epoch": 0.030545254772696057,
      "grad_norm": 0.47684502601623535,
      "learning_rate": 0.00019491525423728814,
      "loss": 0.556,
      "step": 973
    },
    {
      "epoch": 0.03057664763474405,
      "grad_norm": 0.31020718812942505,
      "learning_rate": 0.00019460138104205902,
      "loss": 0.2912,
      "step": 974
    },
    {
      "epoch": 0.03060804049679204,
      "grad_norm": 0.35877037048339844,
      "learning_rate": 0.00019428750784682987,
      "loss": 0.6934,
      "step": 975
    },
    {
      "epoch": 0.030639433358840035,
      "grad_norm": 0.08564818650484085,
      "learning_rate": 0.00019397363465160076,
      "loss": 0.0804,
      "step": 976
    },
    {
      "epoch": 0.030670826220888025,
      "grad_norm": 0.32066914439201355,
      "learning_rate": 0.0001936597614563716,
      "loss": 0.4102,
      "step": 977
    },
    {
      "epoch": 0.03070221908293602,
      "grad_norm": 0.2928641140460968,
      "learning_rate": 0.0001933458882611425,
      "loss": 0.2748,
      "step": 978
    },
    {
      "epoch": 0.03073361194498401,
      "grad_norm": 0.22899611294269562,
      "learning_rate": 0.00019303201506591335,
      "loss": 0.2629,
      "step": 979
    },
    {
      "epoch": 0.030765004807032002,
      "grad_norm": 0.12348884344100952,
      "learning_rate": 0.00019271814187068426,
      "loss": 0.1789,
      "step": 980
    },
    {
      "epoch": 0.030796397669079992,
      "grad_norm": 0.15142247080802917,
      "learning_rate": 0.00019240426867545512,
      "loss": 0.1796,
      "step": 981
    },
    {
      "epoch": 0.030827790531127986,
      "grad_norm": 0.09515071660280228,
      "learning_rate": 0.000192090395480226,
      "loss": 0.0851,
      "step": 982
    },
    {
      "epoch": 0.030859183393175976,
      "grad_norm": 0.3921228349208832,
      "learning_rate": 0.00019177652228499686,
      "loss": 0.4186,
      "step": 983
    },
    {
      "epoch": 0.03089057625522397,
      "grad_norm": 0.23498454689979553,
      "learning_rate": 0.00019146264908976774,
      "loss": 0.1986,
      "step": 984
    },
    {
      "epoch": 0.03092196911727196,
      "grad_norm": 0.09722762554883957,
      "learning_rate": 0.0001911487758945386,
      "loss": 0.0938,
      "step": 985
    },
    {
      "epoch": 0.030953361979319954,
      "grad_norm": 0.20965136587619781,
      "learning_rate": 0.00019083490269930948,
      "loss": 0.1531,
      "step": 986
    },
    {
      "epoch": 0.030984754841367944,
      "grad_norm": 0.3217007517814636,
      "learning_rate": 0.00019052102950408033,
      "loss": 0.5741,
      "step": 987
    },
    {
      "epoch": 0.031016147703415937,
      "grad_norm": 0.4301605224609375,
      "learning_rate": 0.00019020715630885121,
      "loss": 0.5815,
      "step": 988
    },
    {
      "epoch": 0.031047540565463928,
      "grad_norm": 0.2729642391204834,
      "learning_rate": 0.00018989328311362212,
      "loss": 0.4099,
      "step": 989
    },
    {
      "epoch": 0.031078933427511918,
      "grad_norm": 0.07872483134269714,
      "learning_rate": 0.00018957940991839298,
      "loss": 0.1153,
      "step": 990
    },
    {
      "epoch": 0.03111032628955991,
      "grad_norm": 0.09067665785551071,
      "learning_rate": 0.00018926553672316386,
      "loss": 0.0901,
      "step": 991
    },
    {
      "epoch": 0.0311417191516079,
      "grad_norm": 0.115444116294384,
      "learning_rate": 0.00018895166352793472,
      "loss": 0.0992,
      "step": 992
    },
    {
      "epoch": 0.031173112013655895,
      "grad_norm": 0.10129637271165848,
      "learning_rate": 0.0001886377903327056,
      "loss": 0.1582,
      "step": 993
    },
    {
      "epoch": 0.031204504875703885,
      "grad_norm": 0.4121457040309906,
      "learning_rate": 0.00018832391713747646,
      "loss": 0.3326,
      "step": 994
    },
    {
      "epoch": 0.03123589773775188,
      "grad_norm": 0.12342795729637146,
      "learning_rate": 0.00018801004394224734,
      "loss": 0.218,
      "step": 995
    },
    {
      "epoch": 0.03126729059979987,
      "grad_norm": 0.147767573595047,
      "learning_rate": 0.0001876961707470182,
      "loss": 0.2264,
      "step": 996
    },
    {
      "epoch": 0.03129868346184786,
      "grad_norm": 0.10814180225133896,
      "learning_rate": 0.0001873822975517891,
      "loss": 0.1282,
      "step": 997
    },
    {
      "epoch": 0.03133007632389585,
      "grad_norm": 0.11019047349691391,
      "learning_rate": 0.00018706842435655996,
      "loss": 0.1505,
      "step": 998
    },
    {
      "epoch": 0.03136146918594385,
      "grad_norm": 0.2074289321899414,
      "learning_rate": 0.00018675455116133084,
      "loss": 0.2544,
      "step": 999
    },
    {
      "epoch": 0.03139286204799184,
      "grad_norm": 0.30092185735702515,
      "learning_rate": 0.0001864406779661017,
      "loss": 0.276,
      "step": 1000
    },
    {
      "epoch": 0.03142425491003983,
      "grad_norm": 0.19097352027893066,
      "learning_rate": 0.00018612680477087258,
      "loss": 0.317,
      "step": 1001
    },
    {
      "epoch": 0.03145564777208782,
      "grad_norm": 0.14016710221767426,
      "learning_rate": 0.00018581293157564344,
      "loss": 0.1628,
      "step": 1002
    },
    {
      "epoch": 0.031487040634135814,
      "grad_norm": 0.26280608773231506,
      "learning_rate": 0.00018549905838041432,
      "loss": 0.3557,
      "step": 1003
    },
    {
      "epoch": 0.03151843349618381,
      "grad_norm": 0.1457010954618454,
      "learning_rate": 0.00018518518518518518,
      "loss": 0.1545,
      "step": 1004
    },
    {
      "epoch": 0.031549826358231794,
      "grad_norm": 0.37660062313079834,
      "learning_rate": 0.00018487131198995606,
      "loss": 0.3713,
      "step": 1005
    },
    {
      "epoch": 0.03158121922027979,
      "grad_norm": 0.08937312662601471,
      "learning_rate": 0.00018455743879472694,
      "loss": 0.103,
      "step": 1006
    },
    {
      "epoch": 0.03161261208232778,
      "grad_norm": 0.21885192394256592,
      "learning_rate": 0.00018424356559949782,
      "loss": 0.1984,
      "step": 1007
    },
    {
      "epoch": 0.031644004944375775,
      "grad_norm": 0.10306041687726974,
      "learning_rate": 0.00018392969240426868,
      "loss": 0.174,
      "step": 1008
    },
    {
      "epoch": 0.03167539780642376,
      "grad_norm": 0.2953342795372009,
      "learning_rate": 0.00018361581920903956,
      "loss": 0.3444,
      "step": 1009
    },
    {
      "epoch": 0.031706790668471756,
      "grad_norm": 0.12848444283008575,
      "learning_rate": 0.00018330194601381042,
      "loss": 0.1094,
      "step": 1010
    },
    {
      "epoch": 0.03173818353051975,
      "grad_norm": 0.06976522505283356,
      "learning_rate": 0.0001829880728185813,
      "loss": 0.0449,
      "step": 1011
    },
    {
      "epoch": 0.03176957639256774,
      "grad_norm": 0.11616160720586777,
      "learning_rate": 0.00018267419962335216,
      "loss": 0.1435,
      "step": 1012
    },
    {
      "epoch": 0.03180096925461573,
      "grad_norm": 0.32032284140586853,
      "learning_rate": 0.00018236032642812304,
      "loss": 0.5296,
      "step": 1013
    },
    {
      "epoch": 0.03183236211666372,
      "grad_norm": 0.3035564124584198,
      "learning_rate": 0.00018204645323289392,
      "loss": 0.3278,
      "step": 1014
    },
    {
      "epoch": 0.03186375497871172,
      "grad_norm": 0.30980029702186584,
      "learning_rate": 0.0001817325800376648,
      "loss": 0.2718,
      "step": 1015
    },
    {
      "epoch": 0.03189514784075971,
      "grad_norm": 0.09086193144321442,
      "learning_rate": 0.00018141870684243566,
      "loss": 0.0908,
      "step": 1016
    },
    {
      "epoch": 0.0319265407028077,
      "grad_norm": 0.0848059132695198,
      "learning_rate": 0.00018110483364720654,
      "loss": 0.1081,
      "step": 1017
    },
    {
      "epoch": 0.03195793356485569,
      "grad_norm": 0.30385518074035645,
      "learning_rate": 0.0001807909604519774,
      "loss": 0.2326,
      "step": 1018
    },
    {
      "epoch": 0.031989326426903684,
      "grad_norm": 0.08615944534540176,
      "learning_rate": 0.00018047708725674828,
      "loss": 0.0875,
      "step": 1019
    },
    {
      "epoch": 0.03202071928895167,
      "grad_norm": 0.13319052755832672,
      "learning_rate": 0.00018016321406151914,
      "loss": 0.1783,
      "step": 1020
    },
    {
      "epoch": 0.032052112150999665,
      "grad_norm": 0.34805113077163696,
      "learning_rate": 0.00017984934086629002,
      "loss": 0.3103,
      "step": 1021
    },
    {
      "epoch": 0.03208350501304766,
      "grad_norm": 0.13888640701770782,
      "learning_rate": 0.00017953546767106088,
      "loss": 0.0736,
      "step": 1022
    },
    {
      "epoch": 0.03211489787509565,
      "grad_norm": 0.3516913652420044,
      "learning_rate": 0.00017922159447583179,
      "loss": 0.3647,
      "step": 1023
    },
    {
      "epoch": 0.03214629073714364,
      "grad_norm": 0.27191078662872314,
      "learning_rate": 0.00017890772128060264,
      "loss": 0.3118,
      "step": 1024
    },
    {
      "epoch": 0.03217768359919163,
      "grad_norm": 0.5604800581932068,
      "learning_rate": 0.00017859384808537352,
      "loss": 0.6379,
      "step": 1025
    },
    {
      "epoch": 0.032209076461239626,
      "grad_norm": 0.15238584578037262,
      "learning_rate": 0.00017827997489014438,
      "loss": 0.0712,
      "step": 1026
    },
    {
      "epoch": 0.03224046932328762,
      "grad_norm": 0.2683956027030945,
      "learning_rate": 0.00017796610169491526,
      "loss": 0.233,
      "step": 1027
    },
    {
      "epoch": 0.032271862185335606,
      "grad_norm": 0.5358050465583801,
      "learning_rate": 0.00017765222849968612,
      "loss": 0.4051,
      "step": 1028
    },
    {
      "epoch": 0.0323032550473836,
      "grad_norm": 0.10056852549314499,
      "learning_rate": 0.000177338355304457,
      "loss": 0.1006,
      "step": 1029
    },
    {
      "epoch": 0.032334647909431594,
      "grad_norm": 0.12053495645523071,
      "learning_rate": 0.00017702448210922786,
      "loss": 0.0865,
      "step": 1030
    },
    {
      "epoch": 0.03236604077147959,
      "grad_norm": 0.1406395435333252,
      "learning_rate": 0.00017671060891399877,
      "loss": 0.1901,
      "step": 1031
    },
    {
      "epoch": 0.032397433633527574,
      "grad_norm": 0.06829608976840973,
      "learning_rate": 0.00017639673571876962,
      "loss": 0.0475,
      "step": 1032
    },
    {
      "epoch": 0.03242882649557557,
      "grad_norm": 0.341108113527298,
      "learning_rate": 0.0001760828625235405,
      "loss": 0.5378,
      "step": 1033
    },
    {
      "epoch": 0.03246021935762356,
      "grad_norm": 0.25759926438331604,
      "learning_rate": 0.00017576898932831136,
      "loss": 0.3339,
      "step": 1034
    },
    {
      "epoch": 0.032491612219671555,
      "grad_norm": 0.28126999735832214,
      "learning_rate": 0.00017545511613308224,
      "loss": 0.3181,
      "step": 1035
    },
    {
      "epoch": 0.03252300508171954,
      "grad_norm": 0.25521552562713623,
      "learning_rate": 0.0001751412429378531,
      "loss": 0.3163,
      "step": 1036
    },
    {
      "epoch": 0.032554397943767535,
      "grad_norm": 0.3998030424118042,
      "learning_rate": 0.00017482736974262398,
      "loss": 0.3374,
      "step": 1037
    },
    {
      "epoch": 0.03258579080581553,
      "grad_norm": 0.22814807295799255,
      "learning_rate": 0.00017451349654739484,
      "loss": 0.2385,
      "step": 1038
    },
    {
      "epoch": 0.03261718366786352,
      "grad_norm": 0.08432142436504364,
      "learning_rate": 0.00017419962335216575,
      "loss": 0.0765,
      "step": 1039
    },
    {
      "epoch": 0.03264857652991151,
      "grad_norm": 0.08389034867286682,
      "learning_rate": 0.0001738857501569366,
      "loss": 0.1051,
      "step": 1040
    },
    {
      "epoch": 0.0326799693919595,
      "grad_norm": 0.14971691370010376,
      "learning_rate": 0.00017357187696170748,
      "loss": 0.1671,
      "step": 1041
    },
    {
      "epoch": 0.032711362254007496,
      "grad_norm": 0.11282320320606232,
      "learning_rate": 0.00017325800376647834,
      "loss": 0.1039,
      "step": 1042
    },
    {
      "epoch": 0.03274275511605549,
      "grad_norm": 0.3724212646484375,
      "learning_rate": 0.00017294413057124922,
      "loss": 0.5641,
      "step": 1043
    },
    {
      "epoch": 0.03277414797810348,
      "grad_norm": 0.22860237956047058,
      "learning_rate": 0.00017263025737602008,
      "loss": 0.2387,
      "step": 1044
    },
    {
      "epoch": 0.03280554084015147,
      "grad_norm": 0.19529002904891968,
      "learning_rate": 0.00017231638418079096,
      "loss": 0.1706,
      "step": 1045
    },
    {
      "epoch": 0.032836933702199464,
      "grad_norm": 0.21289892494678497,
      "learning_rate": 0.00017200251098556182,
      "loss": 0.2616,
      "step": 1046
    },
    {
      "epoch": 0.03286832656424746,
      "grad_norm": 0.15497776865959167,
      "learning_rate": 0.0001716886377903327,
      "loss": 0.1411,
      "step": 1047
    },
    {
      "epoch": 0.032899719426295444,
      "grad_norm": 0.2936033606529236,
      "learning_rate": 0.00017137476459510358,
      "loss": 0.4502,
      "step": 1048
    },
    {
      "epoch": 0.03293111228834344,
      "grad_norm": 0.1282798945903778,
      "learning_rate": 0.00017106089139987447,
      "loss": 0.1298,
      "step": 1049
    },
    {
      "epoch": 0.03296250515039143,
      "grad_norm": 0.0769648477435112,
      "learning_rate": 0.00017074701820464532,
      "loss": 0.1125,
      "step": 1050
    },
    {
      "epoch": 0.03299389801243942,
      "grad_norm": 0.11535538733005524,
      "learning_rate": 0.0001704331450094162,
      "loss": 0.0749,
      "step": 1051
    },
    {
      "epoch": 0.03302529087448741,
      "grad_norm": 0.13546986877918243,
      "learning_rate": 0.00017011927181418706,
      "loss": 0.194,
      "step": 1052
    },
    {
      "epoch": 0.033056683736535405,
      "grad_norm": 0.11825471371412277,
      "learning_rate": 0.00016980539861895794,
      "loss": 0.1569,
      "step": 1053
    },
    {
      "epoch": 0.0330880765985834,
      "grad_norm": 0.3100811243057251,
      "learning_rate": 0.0001694915254237288,
      "loss": 0.5555,
      "step": 1054
    },
    {
      "epoch": 0.033119469460631386,
      "grad_norm": 0.17552265524864197,
      "learning_rate": 0.00016917765222849968,
      "loss": 0.2151,
      "step": 1055
    },
    {
      "epoch": 0.03315086232267938,
      "grad_norm": 0.08536378294229507,
      "learning_rate": 0.00016886377903327056,
      "loss": 0.0871,
      "step": 1056
    },
    {
      "epoch": 0.03318225518472737,
      "grad_norm": 0.27373459935188293,
      "learning_rate": 0.00016854990583804145,
      "loss": 0.2918,
      "step": 1057
    },
    {
      "epoch": 0.03321364804677537,
      "grad_norm": 0.16635394096374512,
      "learning_rate": 0.0001682360326428123,
      "loss": 0.1554,
      "step": 1058
    },
    {
      "epoch": 0.03324504090882335,
      "grad_norm": 0.3543218970298767,
      "learning_rate": 0.00016792215944758318,
      "loss": 0.4342,
      "step": 1059
    },
    {
      "epoch": 0.03327643377087135,
      "grad_norm": 0.24170057475566864,
      "learning_rate": 0.00016760828625235404,
      "loss": 0.3097,
      "step": 1060
    },
    {
      "epoch": 0.03330782663291934,
      "grad_norm": 0.20996779203414917,
      "learning_rate": 0.00016729441305712492,
      "loss": 0.2563,
      "step": 1061
    },
    {
      "epoch": 0.033339219494967334,
      "grad_norm": 0.08714239299297333,
      "learning_rate": 0.00016698053986189578,
      "loss": 0.1206,
      "step": 1062
    },
    {
      "epoch": 0.03337061235701532,
      "grad_norm": 0.3769887387752533,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.6935,
      "step": 1063
    },
    {
      "epoch": 0.033402005219063315,
      "grad_norm": 0.35243335366249084,
      "learning_rate": 0.00016635279347143752,
      "loss": 0.3283,
      "step": 1064
    },
    {
      "epoch": 0.03343339808111131,
      "grad_norm": 0.100613072514534,
      "learning_rate": 0.00016603892027620843,
      "loss": 0.1153,
      "step": 1065
    },
    {
      "epoch": 0.0334647909431593,
      "grad_norm": 0.2888551950454712,
      "learning_rate": 0.0001657250470809793,
      "loss": 0.3138,
      "step": 1066
    },
    {
      "epoch": 0.03349618380520729,
      "grad_norm": 0.2801920473575592,
      "learning_rate": 0.00016541117388575016,
      "loss": 0.3772,
      "step": 1067
    },
    {
      "epoch": 0.03352757666725528,
      "grad_norm": 0.13722044229507446,
      "learning_rate": 0.00016509730069052105,
      "loss": 0.1296,
      "step": 1068
    },
    {
      "epoch": 0.033558969529303276,
      "grad_norm": 0.3723188042640686,
      "learning_rate": 0.0001647834274952919,
      "loss": 0.4345,
      "step": 1069
    },
    {
      "epoch": 0.03359036239135127,
      "grad_norm": 0.17805638909339905,
      "learning_rate": 0.00016446955430006279,
      "loss": 0.1838,
      "step": 1070
    },
    {
      "epoch": 0.033621755253399256,
      "grad_norm": 0.2680710256099701,
      "learning_rate": 0.00016415568110483364,
      "loss": 0.3764,
      "step": 1071
    },
    {
      "epoch": 0.03365314811544725,
      "grad_norm": 0.23299244046211243,
      "learning_rate": 0.00016384180790960452,
      "loss": 0.18,
      "step": 1072
    },
    {
      "epoch": 0.03368454097749524,
      "grad_norm": 0.12066090106964111,
      "learning_rate": 0.0001635279347143754,
      "loss": 0.0684,
      "step": 1073
    },
    {
      "epoch": 0.03371593383954324,
      "grad_norm": 0.27259576320648193,
      "learning_rate": 0.0001632140615191463,
      "loss": 0.4888,
      "step": 1074
    },
    {
      "epoch": 0.033747326701591224,
      "grad_norm": 0.2930268943309784,
      "learning_rate": 0.00016290018832391715,
      "loss": 0.3751,
      "step": 1075
    },
    {
      "epoch": 0.03377871956363922,
      "grad_norm": 0.27663370966911316,
      "learning_rate": 0.00016258631512868803,
      "loss": 0.3552,
      "step": 1076
    },
    {
      "epoch": 0.03381011242568721,
      "grad_norm": 0.46951448917388916,
      "learning_rate": 0.00016227244193345888,
      "loss": 0.7266,
      "step": 1077
    },
    {
      "epoch": 0.033841505287735205,
      "grad_norm": 0.09022077172994614,
      "learning_rate": 0.00016195856873822977,
      "loss": 0.0925,
      "step": 1078
    },
    {
      "epoch": 0.03387289814978319,
      "grad_norm": 0.4114091992378235,
      "learning_rate": 0.00016164469554300062,
      "loss": 0.5554,
      "step": 1079
    },
    {
      "epoch": 0.033904291011831185,
      "grad_norm": 0.49448102712631226,
      "learning_rate": 0.0001613308223477715,
      "loss": 0.502,
      "step": 1080
    },
    {
      "epoch": 0.03393568387387918,
      "grad_norm": 0.14010080695152283,
      "learning_rate": 0.00016101694915254236,
      "loss": 0.1162,
      "step": 1081
    },
    {
      "epoch": 0.033967076735927165,
      "grad_norm": 0.15220820903778076,
      "learning_rate": 0.00016070307595731327,
      "loss": 0.0955,
      "step": 1082
    },
    {
      "epoch": 0.03399846959797516,
      "grad_norm": 0.69971764087677,
      "learning_rate": 0.00016038920276208413,
      "loss": 0.533,
      "step": 1083
    },
    {
      "epoch": 0.03402986246002315,
      "grad_norm": 0.3157629370689392,
      "learning_rate": 0.000160075329566855,
      "loss": 0.3669,
      "step": 1084
    },
    {
      "epoch": 0.034061255322071146,
      "grad_norm": 0.11602520197629929,
      "learning_rate": 0.00015976145637162586,
      "loss": 0.1257,
      "step": 1085
    },
    {
      "epoch": 0.03409264818411913,
      "grad_norm": 0.21435660123825073,
      "learning_rate": 0.00015944758317639675,
      "loss": 0.2579,
      "step": 1086
    },
    {
      "epoch": 0.034124041046167126,
      "grad_norm": 0.23677675426006317,
      "learning_rate": 0.0001591337099811676,
      "loss": 0.3405,
      "step": 1087
    },
    {
      "epoch": 0.03415543390821512,
      "grad_norm": 0.34279391169548035,
      "learning_rate": 0.00015881983678593849,
      "loss": 0.2838,
      "step": 1088
    },
    {
      "epoch": 0.034186826770263114,
      "grad_norm": 0.3944159746170044,
      "learning_rate": 0.00015850596359070934,
      "loss": 0.3418,
      "step": 1089
    },
    {
      "epoch": 0.0342182196323111,
      "grad_norm": 0.3038857579231262,
      "learning_rate": 0.00015819209039548025,
      "loss": 0.3005,
      "step": 1090
    },
    {
      "epoch": 0.034249612494359094,
      "grad_norm": 0.10102760046720505,
      "learning_rate": 0.0001578782172002511,
      "loss": 0.1021,
      "step": 1091
    },
    {
      "epoch": 0.03428100535640709,
      "grad_norm": 0.31879475712776184,
      "learning_rate": 0.000157564344005022,
      "loss": 0.2202,
      "step": 1092
    },
    {
      "epoch": 0.03431239821845508,
      "grad_norm": 0.1146325021982193,
      "learning_rate": 0.00015725047080979285,
      "loss": 0.0791,
      "step": 1093
    },
    {
      "epoch": 0.03434379108050307,
      "grad_norm": 0.31021177768707275,
      "learning_rate": 0.00015693659761456373,
      "loss": 0.333,
      "step": 1094
    },
    {
      "epoch": 0.03437518394255106,
      "grad_norm": 0.28397026658058167,
      "learning_rate": 0.00015662272441933458,
      "loss": 0.3326,
      "step": 1095
    },
    {
      "epoch": 0.034406576804599055,
      "grad_norm": 0.08539257943630219,
      "learning_rate": 0.00015630885122410547,
      "loss": 0.0857,
      "step": 1096
    },
    {
      "epoch": 0.03443796966664705,
      "grad_norm": 0.474538654088974,
      "learning_rate": 0.00015599497802887632,
      "loss": 0.2815,
      "step": 1097
    },
    {
      "epoch": 0.034469362528695036,
      "grad_norm": 0.10673452168703079,
      "learning_rate": 0.0001556811048336472,
      "loss": 0.1329,
      "step": 1098
    },
    {
      "epoch": 0.03450075539074303,
      "grad_norm": 0.1732555329799652,
      "learning_rate": 0.0001553672316384181,
      "loss": 0.2746,
      "step": 1099
    },
    {
      "epoch": 0.03453214825279102,
      "grad_norm": 0.08339317888021469,
      "learning_rate": 0.00015505335844318897,
      "loss": 0.1003,
      "step": 1100
    },
    {
      "epoch": 0.034563541114839016,
      "grad_norm": 0.11830796301364899,
      "learning_rate": 0.00015473948524795983,
      "loss": 0.1701,
      "step": 1101
    },
    {
      "epoch": 0.034594933976887,
      "grad_norm": 0.050423186272382736,
      "learning_rate": 0.0001544256120527307,
      "loss": 0.0687,
      "step": 1102
    },
    {
      "epoch": 0.034626326838935,
      "grad_norm": 0.2112022489309311,
      "learning_rate": 0.00015411173885750156,
      "loss": 0.2315,
      "step": 1103
    },
    {
      "epoch": 0.03465771970098299,
      "grad_norm": 0.048865072429180145,
      "learning_rate": 0.00015379786566227245,
      "loss": 0.06,
      "step": 1104
    },
    {
      "epoch": 0.034689112563030984,
      "grad_norm": 0.10573423653841019,
      "learning_rate": 0.0001534839924670433,
      "loss": 0.1536,
      "step": 1105
    },
    {
      "epoch": 0.03472050542507897,
      "grad_norm": 0.2732689082622528,
      "learning_rate": 0.00015317011927181419,
      "loss": 0.3332,
      "step": 1106
    },
    {
      "epoch": 0.034751898287126964,
      "grad_norm": 0.33307430148124695,
      "learning_rate": 0.00015285624607658507,
      "loss": 0.4239,
      "step": 1107
    },
    {
      "epoch": 0.03478329114917496,
      "grad_norm": 0.3760175108909607,
      "learning_rate": 0.00015254237288135595,
      "loss": 0.5072,
      "step": 1108
    },
    {
      "epoch": 0.03481468401122295,
      "grad_norm": 0.1297694444656372,
      "learning_rate": 0.0001522284996861268,
      "loss": 0.1611,
      "step": 1109
    },
    {
      "epoch": 0.03484607687327094,
      "grad_norm": 0.2641807794570923,
      "learning_rate": 0.0001519146264908977,
      "loss": 0.2282,
      "step": 1110
    },
    {
      "epoch": 0.03487746973531893,
      "grad_norm": 0.4113103449344635,
      "learning_rate": 0.00015160075329566854,
      "loss": 0.3115,
      "step": 1111
    },
    {
      "epoch": 0.034908862597366926,
      "grad_norm": 0.08123468607664108,
      "learning_rate": 0.00015128688010043943,
      "loss": 0.0909,
      "step": 1112
    },
    {
      "epoch": 0.03494025545941491,
      "grad_norm": 0.38432198762893677,
      "learning_rate": 0.00015097300690521028,
      "loss": 0.3897,
      "step": 1113
    },
    {
      "epoch": 0.034971648321462906,
      "grad_norm": 0.08229175955057144,
      "learning_rate": 0.00015065913370998117,
      "loss": 0.1308,
      "step": 1114
    },
    {
      "epoch": 0.0350030411835109,
      "grad_norm": 0.5038105845451355,
      "learning_rate": 0.00015034526051475202,
      "loss": 0.3354,
      "step": 1115
    },
    {
      "epoch": 0.03503443404555889,
      "grad_norm": 0.0698757991194725,
      "learning_rate": 0.00015003138731952293,
      "loss": 0.0825,
      "step": 1116
    },
    {
      "epoch": 0.03506582690760688,
      "grad_norm": 0.05876362696290016,
      "learning_rate": 0.0001497175141242938,
      "loss": 0.0599,
      "step": 1117
    },
    {
      "epoch": 0.035097219769654874,
      "grad_norm": 0.4909389615058899,
      "learning_rate": 0.00014940364092906467,
      "loss": 0.6262,
      "step": 1118
    },
    {
      "epoch": 0.03512861263170287,
      "grad_norm": 0.15691371262073517,
      "learning_rate": 0.00014908976773383553,
      "loss": 0.187,
      "step": 1119
    },
    {
      "epoch": 0.03516000549375086,
      "grad_norm": 0.32193806767463684,
      "learning_rate": 0.0001487758945386064,
      "loss": 0.3874,
      "step": 1120
    },
    {
      "epoch": 0.03519139835579885,
      "grad_norm": 0.08913450688123703,
      "learning_rate": 0.00014846202134337726,
      "loss": 0.1326,
      "step": 1121
    },
    {
      "epoch": 0.03522279121784684,
      "grad_norm": 0.1655818372964859,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.1958,
      "step": 1122
    },
    {
      "epoch": 0.035254184079894835,
      "grad_norm": 0.4072738289833069,
      "learning_rate": 0.000147834274952919,
      "loss": 0.6792,
      "step": 1123
    },
    {
      "epoch": 0.03528557694194283,
      "grad_norm": 0.508576512336731,
      "learning_rate": 0.0001475204017576899,
      "loss": 0.371,
      "step": 1124
    },
    {
      "epoch": 0.035316969803990815,
      "grad_norm": 0.10429663211107254,
      "learning_rate": 0.00014720652856246077,
      "loss": 0.0742,
      "step": 1125
    },
    {
      "epoch": 0.03534836266603881,
      "grad_norm": 0.15488220751285553,
      "learning_rate": 0.00014689265536723165,
      "loss": 0.1893,
      "step": 1126
    },
    {
      "epoch": 0.0353797555280868,
      "grad_norm": 0.3000931441783905,
      "learning_rate": 0.0001465787821720025,
      "loss": 0.3071,
      "step": 1127
    },
    {
      "epoch": 0.035411148390134796,
      "grad_norm": 0.3326430022716522,
      "learning_rate": 0.0001462649089767734,
      "loss": 0.6121,
      "step": 1128
    },
    {
      "epoch": 0.03544254125218278,
      "grad_norm": 0.27746111154556274,
      "learning_rate": 0.00014595103578154424,
      "loss": 0.338,
      "step": 1129
    },
    {
      "epoch": 0.035473934114230776,
      "grad_norm": 0.23585054278373718,
      "learning_rate": 0.00014563716258631513,
      "loss": 0.381,
      "step": 1130
    },
    {
      "epoch": 0.03550532697627877,
      "grad_norm": 0.07286782562732697,
      "learning_rate": 0.00014532328939108598,
      "loss": 0.067,
      "step": 1131
    },
    {
      "epoch": 0.035536719838326764,
      "grad_norm": 0.09598681330680847,
      "learning_rate": 0.00014500941619585687,
      "loss": 0.0791,
      "step": 1132
    },
    {
      "epoch": 0.03556811270037475,
      "grad_norm": 0.10895892232656479,
      "learning_rate": 0.00014469554300062775,
      "loss": 0.1396,
      "step": 1133
    },
    {
      "epoch": 0.035599505562422744,
      "grad_norm": 0.1668500453233719,
      "learning_rate": 0.00014438166980539863,
      "loss": 0.2508,
      "step": 1134
    },
    {
      "epoch": 0.03563089842447074,
      "grad_norm": 0.0980127677321434,
      "learning_rate": 0.00014406779661016949,
      "loss": 0.091,
      "step": 1135
    },
    {
      "epoch": 0.03566229128651873,
      "grad_norm": 0.29622334241867065,
      "learning_rate": 0.00014375392341494037,
      "loss": 0.3263,
      "step": 1136
    },
    {
      "epoch": 0.03569368414856672,
      "grad_norm": 0.13416215777397156,
      "learning_rate": 0.00014344005021971122,
      "loss": 0.1963,
      "step": 1137
    },
    {
      "epoch": 0.03572507701061471,
      "grad_norm": 0.2155253142118454,
      "learning_rate": 0.0001431261770244821,
      "loss": 0.2754,
      "step": 1138
    },
    {
      "epoch": 0.035756469872662705,
      "grad_norm": 0.21305592358112335,
      "learning_rate": 0.00014281230382925296,
      "loss": 0.2942,
      "step": 1139
    },
    {
      "epoch": 0.03578786273471069,
      "grad_norm": 0.07276559621095657,
      "learning_rate": 0.00014249843063402385,
      "loss": 0.0641,
      "step": 1140
    },
    {
      "epoch": 0.035819255596758685,
      "grad_norm": 0.2549586594104767,
      "learning_rate": 0.00014218455743879476,
      "loss": 0.2749,
      "step": 1141
    },
    {
      "epoch": 0.03585064845880668,
      "grad_norm": 0.11974319070577621,
      "learning_rate": 0.0001418706842435656,
      "loss": 0.1012,
      "step": 1142
    },
    {
      "epoch": 0.03588204132085467,
      "grad_norm": 0.2777045965194702,
      "learning_rate": 0.0001415568110483365,
      "loss": 0.4448,
      "step": 1143
    },
    {
      "epoch": 0.03591343418290266,
      "grad_norm": 0.46411827206611633,
      "learning_rate": 0.00014124293785310735,
      "loss": 0.5028,
      "step": 1144
    },
    {
      "epoch": 0.03594482704495065,
      "grad_norm": 0.2863602340221405,
      "learning_rate": 0.00014092906465787823,
      "loss": 0.3871,
      "step": 1145
    },
    {
      "epoch": 0.03597621990699865,
      "grad_norm": 0.08048483729362488,
      "learning_rate": 0.0001406151914626491,
      "loss": 0.0664,
      "step": 1146
    },
    {
      "epoch": 0.03600761276904664,
      "grad_norm": 0.37317052483558655,
      "learning_rate": 0.00014030131826741997,
      "loss": 0.4447,
      "step": 1147
    },
    {
      "epoch": 0.03603900563109463,
      "grad_norm": 0.09664826840162277,
      "learning_rate": 0.00013998744507219083,
      "loss": 0.1053,
      "step": 1148
    },
    {
      "epoch": 0.03607039849314262,
      "grad_norm": 0.1893807351589203,
      "learning_rate": 0.0001396735718769617,
      "loss": 0.2018,
      "step": 1149
    },
    {
      "epoch": 0.036101791355190614,
      "grad_norm": 0.3937027156352997,
      "learning_rate": 0.0001393596986817326,
      "loss": 0.5911,
      "step": 1150
    },
    {
      "epoch": 0.03613318421723861,
      "grad_norm": 0.5837543606758118,
      "learning_rate": 0.00013904582548650347,
      "loss": 0.7497,
      "step": 1151
    },
    {
      "epoch": 0.036164577079286594,
      "grad_norm": 0.24817009270191193,
      "learning_rate": 0.00013873195229127433,
      "loss": 0.4367,
      "step": 1152
    },
    {
      "epoch": 0.03619596994133459,
      "grad_norm": 0.2663480341434479,
      "learning_rate": 0.0001384180790960452,
      "loss": 0.3604,
      "step": 1153
    },
    {
      "epoch": 0.03622736280338258,
      "grad_norm": 0.28090396523475647,
      "learning_rate": 0.00013810420590081607,
      "loss": 0.4013,
      "step": 1154
    },
    {
      "epoch": 0.036258755665430575,
      "grad_norm": 0.7291832566261292,
      "learning_rate": 0.00013779033270558695,
      "loss": 0.4651,
      "step": 1155
    },
    {
      "epoch": 0.03629014852747856,
      "grad_norm": 0.26280373334884644,
      "learning_rate": 0.0001374764595103578,
      "loss": 0.3658,
      "step": 1156
    },
    {
      "epoch": 0.036321541389526556,
      "grad_norm": 0.3176479637622833,
      "learning_rate": 0.0001371625863151287,
      "loss": 0.5237,
      "step": 1157
    },
    {
      "epoch": 0.03635293425157455,
      "grad_norm": 0.08322864025831223,
      "learning_rate": 0.00013684871311989957,
      "loss": 0.1246,
      "step": 1158
    },
    {
      "epoch": 0.03638432711362254,
      "grad_norm": 0.27368873357772827,
      "learning_rate": 0.00013653483992467046,
      "loss": 0.3549,
      "step": 1159
    },
    {
      "epoch": 0.03641571997567053,
      "grad_norm": 0.11986517906188965,
      "learning_rate": 0.0001362209667294413,
      "loss": 0.1942,
      "step": 1160
    },
    {
      "epoch": 0.03644711283771852,
      "grad_norm": 0.2689218819141388,
      "learning_rate": 0.0001359070935342122,
      "loss": 0.3962,
      "step": 1161
    },
    {
      "epoch": 0.03647850569976652,
      "grad_norm": 0.2528596520423889,
      "learning_rate": 0.00013559322033898305,
      "loss": 0.3415,
      "step": 1162
    },
    {
      "epoch": 0.03650989856181451,
      "grad_norm": 0.11511453986167908,
      "learning_rate": 0.00013527934714375393,
      "loss": 0.1572,
      "step": 1163
    },
    {
      "epoch": 0.0365412914238625,
      "grad_norm": 0.09592452645301819,
      "learning_rate": 0.0001349654739485248,
      "loss": 0.1039,
      "step": 1164
    },
    {
      "epoch": 0.03657268428591049,
      "grad_norm": 0.16228362917900085,
      "learning_rate": 0.00013465160075329567,
      "loss": 0.1931,
      "step": 1165
    },
    {
      "epoch": 0.036604077147958484,
      "grad_norm": 0.08716797828674316,
      "learning_rate": 0.00013433772755806653,
      "loss": 0.0778,
      "step": 1166
    },
    {
      "epoch": 0.03663547001000648,
      "grad_norm": 0.253365695476532,
      "learning_rate": 0.00013402385436283744,
      "loss": 0.271,
      "step": 1167
    },
    {
      "epoch": 0.036666862872054465,
      "grad_norm": 0.586439847946167,
      "learning_rate": 0.0001337099811676083,
      "loss": 0.582,
      "step": 1168
    },
    {
      "epoch": 0.03669825573410246,
      "grad_norm": 0.12081174552440643,
      "learning_rate": 0.00013339610797237917,
      "loss": 0.1486,
      "step": 1169
    },
    {
      "epoch": 0.03672964859615045,
      "grad_norm": 0.2426113337278366,
      "learning_rate": 0.00013308223477715003,
      "loss": 0.3189,
      "step": 1170
    },
    {
      "epoch": 0.03676104145819844,
      "grad_norm": 0.12755243480205536,
      "learning_rate": 0.0001327683615819209,
      "loss": 0.1174,
      "step": 1171
    },
    {
      "epoch": 0.03679243432024643,
      "grad_norm": 0.5045403242111206,
      "learning_rate": 0.00013245448838669177,
      "loss": 0.5984,
      "step": 1172
    },
    {
      "epoch": 0.036823827182294426,
      "grad_norm": 0.1340787559747696,
      "learning_rate": 0.00013214061519146265,
      "loss": 0.0731,
      "step": 1173
    },
    {
      "epoch": 0.03685522004434242,
      "grad_norm": 0.309028297662735,
      "learning_rate": 0.0001318267419962335,
      "loss": 0.5047,
      "step": 1174
    },
    {
      "epoch": 0.036886612906390406,
      "grad_norm": 0.07820959389209747,
      "learning_rate": 0.00013151286880100442,
      "loss": 0.0534,
      "step": 1175
    },
    {
      "epoch": 0.0369180057684384,
      "grad_norm": 0.16858677566051483,
      "learning_rate": 0.00013119899560577527,
      "loss": 0.1386,
      "step": 1176
    },
    {
      "epoch": 0.036949398630486394,
      "grad_norm": 0.2244807928800583,
      "learning_rate": 0.00013088512241054615,
      "loss": 0.1647,
      "step": 1177
    },
    {
      "epoch": 0.03698079149253439,
      "grad_norm": 0.14279691874980927,
      "learning_rate": 0.000130571249215317,
      "loss": 0.117,
      "step": 1178
    },
    {
      "epoch": 0.037012184354582374,
      "grad_norm": 0.38938987255096436,
      "learning_rate": 0.0001302573760200879,
      "loss": 0.5202,
      "step": 1179
    },
    {
      "epoch": 0.03704357721663037,
      "grad_norm": 0.1385461539030075,
      "learning_rate": 0.00012994350282485875,
      "loss": 0.1887,
      "step": 1180
    },
    {
      "epoch": 0.03707497007867836,
      "grad_norm": 0.09434716403484344,
      "learning_rate": 0.00012962962962962963,
      "loss": 0.096,
      "step": 1181
    },
    {
      "epoch": 0.037106362940726355,
      "grad_norm": 0.35685813426971436,
      "learning_rate": 0.0001293157564344005,
      "loss": 0.5912,
      "step": 1182
    },
    {
      "epoch": 0.03713775580277434,
      "grad_norm": 0.24430230259895325,
      "learning_rate": 0.00012900188323917137,
      "loss": 0.3423,
      "step": 1183
    },
    {
      "epoch": 0.037169148664822335,
      "grad_norm": 0.20304566621780396,
      "learning_rate": 0.00012868801004394225,
      "loss": 0.2439,
      "step": 1184
    },
    {
      "epoch": 0.03720054152687033,
      "grad_norm": 0.06843544542789459,
      "learning_rate": 0.00012837413684871314,
      "loss": 0.0646,
      "step": 1185
    },
    {
      "epoch": 0.03723193438891832,
      "grad_norm": 0.5733791589736938,
      "learning_rate": 0.000128060263653484,
      "loss": 0.6902,
      "step": 1186
    },
    {
      "epoch": 0.03726332725096631,
      "grad_norm": 0.08474203199148178,
      "learning_rate": 0.00012774639045825487,
      "loss": 0.1226,
      "step": 1187
    },
    {
      "epoch": 0.0372947201130143,
      "grad_norm": 0.11132374405860901,
      "learning_rate": 0.00012743251726302573,
      "loss": 0.1161,
      "step": 1188
    },
    {
      "epoch": 0.037326112975062296,
      "grad_norm": 0.3015662729740143,
      "learning_rate": 0.0001271186440677966,
      "loss": 0.39,
      "step": 1189
    },
    {
      "epoch": 0.03735750583711029,
      "grad_norm": 0.08932752907276154,
      "learning_rate": 0.00012680477087256747,
      "loss": 0.1045,
      "step": 1190
    },
    {
      "epoch": 0.03738889869915828,
      "grad_norm": 0.07164894044399261,
      "learning_rate": 0.00012649089767733835,
      "loss": 0.0795,
      "step": 1191
    },
    {
      "epoch": 0.03742029156120627,
      "grad_norm": 0.14432862401008606,
      "learning_rate": 0.00012617702448210923,
      "loss": 0.1369,
      "step": 1192
    },
    {
      "epoch": 0.037451684423254264,
      "grad_norm": 0.10225943475961685,
      "learning_rate": 0.00012586315128688012,
      "loss": 0.1106,
      "step": 1193
    },
    {
      "epoch": 0.03748307728530226,
      "grad_norm": 0.1726914495229721,
      "learning_rate": 0.00012554927809165097,
      "loss": 0.1775,
      "step": 1194
    },
    {
      "epoch": 0.037514470147350244,
      "grad_norm": 0.3474576473236084,
      "learning_rate": 0.00012523540489642185,
      "loss": 0.6479,
      "step": 1195
    },
    {
      "epoch": 0.03754586300939824,
      "grad_norm": 0.28157439827919006,
      "learning_rate": 0.0001249215317011927,
      "loss": 0.2814,
      "step": 1196
    },
    {
      "epoch": 0.03757725587144623,
      "grad_norm": 0.12035169452428818,
      "learning_rate": 0.0001246076585059636,
      "loss": 0.1454,
      "step": 1197
    },
    {
      "epoch": 0.037608648733494225,
      "grad_norm": 0.4042400121688843,
      "learning_rate": 0.00012429378531073448,
      "loss": 0.6243,
      "step": 1198
    },
    {
      "epoch": 0.03764004159554221,
      "grad_norm": 0.36350616812705994,
      "learning_rate": 0.00012397991211550533,
      "loss": 0.4202,
      "step": 1199
    },
    {
      "epoch": 0.037671434457590205,
      "grad_norm": 0.10036397725343704,
      "learning_rate": 0.00012366603892027621,
      "loss": 0.1129,
      "step": 1200
    },
    {
      "epoch": 0.0377028273196382,
      "grad_norm": 0.23643098771572113,
      "learning_rate": 0.00012335216572504707,
      "loss": 0.4389,
      "step": 1201
    },
    {
      "epoch": 0.037734220181686186,
      "grad_norm": 0.05694598704576492,
      "learning_rate": 0.00012303829252981795,
      "loss": 0.0675,
      "step": 1202
    },
    {
      "epoch": 0.03776561304373418,
      "grad_norm": 0.10779713839292526,
      "learning_rate": 0.00012272441933458883,
      "loss": 0.1114,
      "step": 1203
    },
    {
      "epoch": 0.03779700590578217,
      "grad_norm": 0.46704110503196716,
      "learning_rate": 0.0001224105461393597,
      "loss": 0.4504,
      "step": 1204
    },
    {
      "epoch": 0.03782839876783017,
      "grad_norm": 0.3693296015262604,
      "learning_rate": 0.00012209667294413057,
      "loss": 0.3305,
      "step": 1205
    },
    {
      "epoch": 0.03785979162987815,
      "grad_norm": 0.36219316720962524,
      "learning_rate": 0.00012178279974890144,
      "loss": 0.6259,
      "step": 1206
    },
    {
      "epoch": 0.03789118449192615,
      "grad_norm": 0.25217047333717346,
      "learning_rate": 0.00012146892655367232,
      "loss": 0.2515,
      "step": 1207
    },
    {
      "epoch": 0.03792257735397414,
      "grad_norm": 0.23947031795978546,
      "learning_rate": 0.0001211550533584432,
      "loss": 0.33,
      "step": 1208
    },
    {
      "epoch": 0.037953970216022134,
      "grad_norm": 0.20637038350105286,
      "learning_rate": 0.00012084118016321406,
      "loss": 0.2039,
      "step": 1209
    },
    {
      "epoch": 0.03798536307807012,
      "grad_norm": 0.10363694280385971,
      "learning_rate": 0.00012052730696798493,
      "loss": 0.1366,
      "step": 1210
    },
    {
      "epoch": 0.038016755940118115,
      "grad_norm": 0.3064219653606415,
      "learning_rate": 0.00012021343377275582,
      "loss": 0.4903,
      "step": 1211
    },
    {
      "epoch": 0.03804814880216611,
      "grad_norm": 0.12425560504198074,
      "learning_rate": 0.00011989956057752668,
      "loss": 0.1512,
      "step": 1212
    },
    {
      "epoch": 0.0380795416642141,
      "grad_norm": 0.10870440304279327,
      "learning_rate": 0.00011958568738229755,
      "loss": 0.1377,
      "step": 1213
    },
    {
      "epoch": 0.03811093452626209,
      "grad_norm": 0.08472544699907303,
      "learning_rate": 0.00011927181418706842,
      "loss": 0.0729,
      "step": 1214
    },
    {
      "epoch": 0.03814232738831008,
      "grad_norm": 0.13591672480106354,
      "learning_rate": 0.0001189579409918393,
      "loss": 0.1823,
      "step": 1215
    },
    {
      "epoch": 0.038173720250358076,
      "grad_norm": 0.39043787121772766,
      "learning_rate": 0.00011864406779661017,
      "loss": 0.4632,
      "step": 1216
    },
    {
      "epoch": 0.03820511311240607,
      "grad_norm": 0.25123655796051025,
      "learning_rate": 0.00011833019460138104,
      "loss": 0.4266,
      "step": 1217
    },
    {
      "epoch": 0.038236505974454056,
      "grad_norm": 0.329378604888916,
      "learning_rate": 0.00011801632140615191,
      "loss": 0.3957,
      "step": 1218
    },
    {
      "epoch": 0.03826789883650205,
      "grad_norm": 0.23841454088687897,
      "learning_rate": 0.00011770244821092278,
      "loss": 0.3335,
      "step": 1219
    },
    {
      "epoch": 0.03829929169855004,
      "grad_norm": 0.2406313121318817,
      "learning_rate": 0.00011738857501569367,
      "loss": 0.3823,
      "step": 1220
    },
    {
      "epoch": 0.03833068456059804,
      "grad_norm": 0.2084786742925644,
      "learning_rate": 0.00011707470182046453,
      "loss": 0.2738,
      "step": 1221
    },
    {
      "epoch": 0.038362077422646024,
      "grad_norm": 0.10271147638559341,
      "learning_rate": 0.0001167608286252354,
      "loss": 0.0886,
      "step": 1222
    },
    {
      "epoch": 0.03839347028469402,
      "grad_norm": 0.2569471597671509,
      "learning_rate": 0.00011644695543000627,
      "loss": 0.4006,
      "step": 1223
    },
    {
      "epoch": 0.03842486314674201,
      "grad_norm": 0.26784634590148926,
      "learning_rate": 0.00011613308223477716,
      "loss": 0.2894,
      "step": 1224
    },
    {
      "epoch": 0.038456256008790005,
      "grad_norm": 0.11403680592775345,
      "learning_rate": 0.00011581920903954802,
      "loss": 0.1817,
      "step": 1225
    },
    {
      "epoch": 0.03848764887083799,
      "grad_norm": 0.27675750851631165,
      "learning_rate": 0.0001155053358443189,
      "loss": 0.3588,
      "step": 1226
    },
    {
      "epoch": 0.038519041732885985,
      "grad_norm": 0.1503564864397049,
      "learning_rate": 0.00011519146264908976,
      "loss": 0.1449,
      "step": 1227
    },
    {
      "epoch": 0.03855043459493398,
      "grad_norm": 0.07079238444566727,
      "learning_rate": 0.00011487758945386065,
      "loss": 0.1007,
      "step": 1228
    },
    {
      "epoch": 0.03858182745698197,
      "grad_norm": 0.20809583365917206,
      "learning_rate": 0.00011456371625863151,
      "loss": 0.1736,
      "step": 1229
    },
    {
      "epoch": 0.03861322031902996,
      "grad_norm": 0.24534714221954346,
      "learning_rate": 0.00011424984306340238,
      "loss": 0.3427,
      "step": 1230
    },
    {
      "epoch": 0.03864461318107795,
      "grad_norm": 0.22296403348445892,
      "learning_rate": 0.00011393596986817325,
      "loss": 0.2829,
      "step": 1231
    },
    {
      "epoch": 0.038676006043125946,
      "grad_norm": 0.30769604444503784,
      "learning_rate": 0.00011362209667294414,
      "loss": 0.533,
      "step": 1232
    },
    {
      "epoch": 0.03870739890517393,
      "grad_norm": 0.07927452027797699,
      "learning_rate": 0.000113308223477715,
      "loss": 0.0591,
      "step": 1233
    },
    {
      "epoch": 0.038738791767221926,
      "grad_norm": 0.14625152945518494,
      "learning_rate": 0.00011299435028248587,
      "loss": 0.2135,
      "step": 1234
    },
    {
      "epoch": 0.03877018462926992,
      "grad_norm": 0.37954625487327576,
      "learning_rate": 0.00011268047708725674,
      "loss": 0.4419,
      "step": 1235
    },
    {
      "epoch": 0.038801577491317914,
      "grad_norm": 0.32869699597358704,
      "learning_rate": 0.00011236660389202763,
      "loss": 0.4054,
      "step": 1236
    },
    {
      "epoch": 0.0388329703533659,
      "grad_norm": 0.2369222640991211,
      "learning_rate": 0.0001120527306967985,
      "loss": 0.3068,
      "step": 1237
    },
    {
      "epoch": 0.038864363215413894,
      "grad_norm": 0.09749305993318558,
      "learning_rate": 0.00011173885750156936,
      "loss": 0.1008,
      "step": 1238
    },
    {
      "epoch": 0.03889575607746189,
      "grad_norm": 0.17811383306980133,
      "learning_rate": 0.00011142498430634023,
      "loss": 0.2311,
      "step": 1239
    },
    {
      "epoch": 0.03892714893950988,
      "grad_norm": 0.3706967830657959,
      "learning_rate": 0.0001111111111111111,
      "loss": 0.5774,
      "step": 1240
    },
    {
      "epoch": 0.03895854180155787,
      "grad_norm": 0.3010409474372864,
      "learning_rate": 0.00011079723791588199,
      "loss": 0.2772,
      "step": 1241
    },
    {
      "epoch": 0.03898993466360586,
      "grad_norm": 0.11523199826478958,
      "learning_rate": 0.00011048336472065285,
      "loss": 0.1278,
      "step": 1242
    },
    {
      "epoch": 0.039021327525653855,
      "grad_norm": 0.1960294395685196,
      "learning_rate": 0.00011016949152542372,
      "loss": 0.2867,
      "step": 1243
    },
    {
      "epoch": 0.03905272038770185,
      "grad_norm": 0.07319676876068115,
      "learning_rate": 0.00010985561833019459,
      "loss": 0.0944,
      "step": 1244
    },
    {
      "epoch": 0.039084113249749836,
      "grad_norm": 0.3752014935016632,
      "learning_rate": 0.00010954174513496548,
      "loss": 0.3711,
      "step": 1245
    },
    {
      "epoch": 0.03911550611179783,
      "grad_norm": 0.3409057557582855,
      "learning_rate": 0.00010922787193973635,
      "loss": 0.4644,
      "step": 1246
    },
    {
      "epoch": 0.03914689897384582,
      "grad_norm": 0.11184347420930862,
      "learning_rate": 0.00010891399874450721,
      "loss": 0.1156,
      "step": 1247
    },
    {
      "epoch": 0.039178291835893816,
      "grad_norm": 0.10731879621744156,
      "learning_rate": 0.00010860012554927808,
      "loss": 0.0871,
      "step": 1248
    },
    {
      "epoch": 0.0392096846979418,
      "grad_norm": 0.10396081954240799,
      "learning_rate": 0.00010828625235404897,
      "loss": 0.1111,
      "step": 1249
    },
    {
      "epoch": 0.0392410775599898,
      "grad_norm": 0.2309432029724121,
      "learning_rate": 0.00010797237915881984,
      "loss": 0.3192,
      "step": 1250
    },
    {
      "epoch": 0.03927247042203779,
      "grad_norm": 0.4167362153530121,
      "learning_rate": 0.0001076585059635907,
      "loss": 0.6708,
      "step": 1251
    },
    {
      "epoch": 0.039303863284085784,
      "grad_norm": 0.2598870098590851,
      "learning_rate": 0.00010734463276836157,
      "loss": 0.2526,
      "step": 1252
    },
    {
      "epoch": 0.03933525614613377,
      "grad_norm": 0.16670574247837067,
      "learning_rate": 0.00010703075957313247,
      "loss": 0.1263,
      "step": 1253
    },
    {
      "epoch": 0.039366649008181764,
      "grad_norm": 0.3411177396774292,
      "learning_rate": 0.00010671688637790334,
      "loss": 0.5499,
      "step": 1254
    },
    {
      "epoch": 0.03939804187022976,
      "grad_norm": 0.1119653657078743,
      "learning_rate": 0.00010640301318267421,
      "loss": 0.1243,
      "step": 1255
    },
    {
      "epoch": 0.03942943473227775,
      "grad_norm": 0.5511452555656433,
      "learning_rate": 0.00010608913998744508,
      "loss": 0.8414,
      "step": 1256
    },
    {
      "epoch": 0.03946082759432574,
      "grad_norm": 0.3050209879875183,
      "learning_rate": 0.00010577526679221595,
      "loss": 0.4544,
      "step": 1257
    },
    {
      "epoch": 0.03949222045637373,
      "grad_norm": 0.5172543525695801,
      "learning_rate": 0.00010546139359698683,
      "loss": 0.4778,
      "step": 1258
    },
    {
      "epoch": 0.039523613318421726,
      "grad_norm": 0.39509665966033936,
      "learning_rate": 0.0001051475204017577,
      "loss": 0.4916,
      "step": 1259
    },
    {
      "epoch": 0.03955500618046971,
      "grad_norm": 0.07488046586513519,
      "learning_rate": 0.00010483364720652857,
      "loss": 0.0727,
      "step": 1260
    },
    {
      "epoch": 0.039586399042517706,
      "grad_norm": 0.06620555371046066,
      "learning_rate": 0.00010451977401129944,
      "loss": 0.0773,
      "step": 1261
    },
    {
      "epoch": 0.0396177919045657,
      "grad_norm": 0.2800889313220978,
      "learning_rate": 0.00010420590081607032,
      "loss": 0.3498,
      "step": 1262
    },
    {
      "epoch": 0.03964918476661369,
      "grad_norm": 0.06773272156715393,
      "learning_rate": 0.00010389202762084119,
      "loss": 0.0716,
      "step": 1263
    },
    {
      "epoch": 0.03968057762866168,
      "grad_norm": 0.15439364314079285,
      "learning_rate": 0.00010357815442561206,
      "loss": 0.135,
      "step": 1264
    },
    {
      "epoch": 0.039711970490709673,
      "grad_norm": 0.42691585421562195,
      "learning_rate": 0.00010326428123038293,
      "loss": 0.1778,
      "step": 1265
    },
    {
      "epoch": 0.03974336335275767,
      "grad_norm": 0.23697854578495026,
      "learning_rate": 0.00010295040803515381,
      "loss": 0.1861,
      "step": 1266
    },
    {
      "epoch": 0.03977475621480566,
      "grad_norm": 0.2608192265033722,
      "learning_rate": 0.00010263653483992468,
      "loss": 0.2894,
      "step": 1267
    },
    {
      "epoch": 0.03980614907685365,
      "grad_norm": 0.47999319434165955,
      "learning_rate": 0.00010232266164469555,
      "loss": 0.2828,
      "step": 1268
    },
    {
      "epoch": 0.03983754193890164,
      "grad_norm": 0.1300325244665146,
      "learning_rate": 0.00010200878844946642,
      "loss": 0.1411,
      "step": 1269
    },
    {
      "epoch": 0.039868934800949635,
      "grad_norm": 0.08859150111675262,
      "learning_rate": 0.0001016949152542373,
      "loss": 0.0823,
      "step": 1270
    },
    {
      "epoch": 0.03990032766299763,
      "grad_norm": 0.2144346833229065,
      "learning_rate": 0.00010138104205900817,
      "loss": 0.2863,
      "step": 1271
    },
    {
      "epoch": 0.039931720525045615,
      "grad_norm": 0.1610899418592453,
      "learning_rate": 0.00010106716886377904,
      "loss": 0.1019,
      "step": 1272
    },
    {
      "epoch": 0.03996311338709361,
      "grad_norm": 0.18322886526584625,
      "learning_rate": 0.00010075329566854991,
      "loss": 0.2509,
      "step": 1273
    },
    {
      "epoch": 0.0399945062491416,
      "grad_norm": 0.19205985963344574,
      "learning_rate": 0.00010043942247332078,
      "loss": 0.2426,
      "step": 1274
    },
    {
      "epoch": 0.040025899111189596,
      "grad_norm": 0.2751668095588684,
      "learning_rate": 0.00010012554927809166,
      "loss": 0.3853,
      "step": 1275
    },
    {
      "epoch": 0.04005729197323758,
      "grad_norm": 0.41940048336982727,
      "learning_rate": 9.981167608286253e-05,
      "loss": 0.6884,
      "step": 1276
    },
    {
      "epoch": 0.040088684835285576,
      "grad_norm": 0.09593581408262253,
      "learning_rate": 9.94978028876334e-05,
      "loss": 0.1066,
      "step": 1277
    },
    {
      "epoch": 0.04012007769733357,
      "grad_norm": 0.3596942126750946,
      "learning_rate": 9.918392969240427e-05,
      "loss": 0.3852,
      "step": 1278
    },
    {
      "epoch": 0.040151470559381564,
      "grad_norm": 0.5561053156852722,
      "learning_rate": 9.887005649717515e-05,
      "loss": 0.53,
      "step": 1279
    },
    {
      "epoch": 0.04018286342142955,
      "grad_norm": 0.35859325528144836,
      "learning_rate": 9.855618330194602e-05,
      "loss": 0.3037,
      "step": 1280
    },
    {
      "epoch": 0.040214256283477544,
      "grad_norm": 0.24956318736076355,
      "learning_rate": 9.824231010671689e-05,
      "loss": 0.3521,
      "step": 1281
    },
    {
      "epoch": 0.04024564914552554,
      "grad_norm": 0.33101433515548706,
      "learning_rate": 9.792843691148776e-05,
      "loss": 0.2289,
      "step": 1282
    },
    {
      "epoch": 0.04027704200757353,
      "grad_norm": 0.4861471652984619,
      "learning_rate": 9.761456371625864e-05,
      "loss": 0.6874,
      "step": 1283
    },
    {
      "epoch": 0.04030843486962152,
      "grad_norm": 0.26448118686676025,
      "learning_rate": 9.730069052102951e-05,
      "loss": 0.502,
      "step": 1284
    },
    {
      "epoch": 0.04033982773166951,
      "grad_norm": 0.09505736827850342,
      "learning_rate": 9.698681732580038e-05,
      "loss": 0.0751,
      "step": 1285
    },
    {
      "epoch": 0.040371220593717505,
      "grad_norm": 0.2746273875236511,
      "learning_rate": 9.667294413057125e-05,
      "loss": 0.3402,
      "step": 1286
    },
    {
      "epoch": 0.0404026134557655,
      "grad_norm": 0.1386137753725052,
      "learning_rate": 9.635907093534213e-05,
      "loss": 0.1162,
      "step": 1287
    },
    {
      "epoch": 0.040434006317813485,
      "grad_norm": 0.08406364172697067,
      "learning_rate": 9.6045197740113e-05,
      "loss": 0.0659,
      "step": 1288
    },
    {
      "epoch": 0.04046539917986148,
      "grad_norm": 0.17980735003948212,
      "learning_rate": 9.573132454488387e-05,
      "loss": 0.2459,
      "step": 1289
    },
    {
      "epoch": 0.04049679204190947,
      "grad_norm": 0.389614075422287,
      "learning_rate": 9.541745134965474e-05,
      "loss": 0.5696,
      "step": 1290
    },
    {
      "epoch": 0.04052818490395746,
      "grad_norm": 0.10212960839271545,
      "learning_rate": 9.510357815442561e-05,
      "loss": 0.1417,
      "step": 1291
    },
    {
      "epoch": 0.04055957776600545,
      "grad_norm": 0.2853474020957947,
      "learning_rate": 9.478970495919649e-05,
      "loss": 0.5073,
      "step": 1292
    },
    {
      "epoch": 0.04059097062805345,
      "grad_norm": 0.4086013436317444,
      "learning_rate": 9.447583176396736e-05,
      "loss": 0.7063,
      "step": 1293
    },
    {
      "epoch": 0.04062236349010144,
      "grad_norm": 0.14519979059696198,
      "learning_rate": 9.416195856873823e-05,
      "loss": 0.2323,
      "step": 1294
    },
    {
      "epoch": 0.04065375635214943,
      "grad_norm": 0.3031429648399353,
      "learning_rate": 9.38480853735091e-05,
      "loss": 0.3714,
      "step": 1295
    },
    {
      "epoch": 0.04068514921419742,
      "grad_norm": 0.08919630199670792,
      "learning_rate": 9.353421217827998e-05,
      "loss": 0.0752,
      "step": 1296
    },
    {
      "epoch": 0.040716542076245414,
      "grad_norm": 0.09504242986440659,
      "learning_rate": 9.322033898305085e-05,
      "loss": 0.1013,
      "step": 1297
    },
    {
      "epoch": 0.04074793493829341,
      "grad_norm": 0.2774941623210907,
      "learning_rate": 9.290646578782172e-05,
      "loss": 0.3163,
      "step": 1298
    },
    {
      "epoch": 0.040779327800341394,
      "grad_norm": 0.6883450746536255,
      "learning_rate": 9.259259259259259e-05,
      "loss": 0.6583,
      "step": 1299
    },
    {
      "epoch": 0.04081072066238939,
      "grad_norm": 0.42238688468933105,
      "learning_rate": 9.227871939736347e-05,
      "loss": 0.7354,
      "step": 1300
    },
    {
      "epoch": 0.04084211352443738,
      "grad_norm": 0.12447468936443329,
      "learning_rate": 9.196484620213434e-05,
      "loss": 0.0927,
      "step": 1301
    },
    {
      "epoch": 0.040873506386485375,
      "grad_norm": 0.11834396421909332,
      "learning_rate": 9.165097300690521e-05,
      "loss": 0.1335,
      "step": 1302
    },
    {
      "epoch": 0.04090489924853336,
      "grad_norm": 0.11284567415714264,
      "learning_rate": 9.133709981167608e-05,
      "loss": 0.1339,
      "step": 1303
    },
    {
      "epoch": 0.040936292110581356,
      "grad_norm": 0.19909648597240448,
      "learning_rate": 9.102322661644696e-05,
      "loss": 0.2739,
      "step": 1304
    },
    {
      "epoch": 0.04096768497262935,
      "grad_norm": 0.11296824365854263,
      "learning_rate": 9.070935342121783e-05,
      "loss": 0.1219,
      "step": 1305
    },
    {
      "epoch": 0.04099907783467734,
      "grad_norm": 0.33497700095176697,
      "learning_rate": 9.03954802259887e-05,
      "loss": 0.2693,
      "step": 1306
    },
    {
      "epoch": 0.04103047069672533,
      "grad_norm": 0.27885687351226807,
      "learning_rate": 9.008160703075957e-05,
      "loss": 0.4229,
      "step": 1307
    },
    {
      "epoch": 0.04106186355877332,
      "grad_norm": 0.3364078998565674,
      "learning_rate": 8.976773383553044e-05,
      "loss": 0.3551,
      "step": 1308
    },
    {
      "epoch": 0.04109325642082132,
      "grad_norm": 0.0790112093091011,
      "learning_rate": 8.945386064030132e-05,
      "loss": 0.0686,
      "step": 1309
    },
    {
      "epoch": 0.04112464928286931,
      "grad_norm": 0.3965797424316406,
      "learning_rate": 8.913998744507219e-05,
      "loss": 0.6652,
      "step": 1310
    },
    {
      "epoch": 0.0411560421449173,
      "grad_norm": 0.1774657666683197,
      "learning_rate": 8.882611424984306e-05,
      "loss": 0.1496,
      "step": 1311
    },
    {
      "epoch": 0.04118743500696529,
      "grad_norm": 0.07116003334522247,
      "learning_rate": 8.851224105461393e-05,
      "loss": 0.0555,
      "step": 1312
    },
    {
      "epoch": 0.041218827869013284,
      "grad_norm": 0.34884676337242126,
      "learning_rate": 8.819836785938481e-05,
      "loss": 0.6297,
      "step": 1313
    },
    {
      "epoch": 0.04125022073106128,
      "grad_norm": 0.12554959952831268,
      "learning_rate": 8.788449466415568e-05,
      "loss": 0.1638,
      "step": 1314
    },
    {
      "epoch": 0.041281613593109265,
      "grad_norm": 0.2887241244316101,
      "learning_rate": 8.757062146892655e-05,
      "loss": 0.2654,
      "step": 1315
    },
    {
      "epoch": 0.04131300645515726,
      "grad_norm": 0.07061459124088287,
      "learning_rate": 8.725674827369742e-05,
      "loss": 0.0662,
      "step": 1316
    },
    {
      "epoch": 0.04134439931720525,
      "grad_norm": 0.22678516805171967,
      "learning_rate": 8.69428750784683e-05,
      "loss": 0.3016,
      "step": 1317
    },
    {
      "epoch": 0.041375792179253246,
      "grad_norm": 0.41627073287963867,
      "learning_rate": 8.662900188323917e-05,
      "loss": 0.6892,
      "step": 1318
    },
    {
      "epoch": 0.04140718504130123,
      "grad_norm": 0.06697969883680344,
      "learning_rate": 8.631512868801004e-05,
      "loss": 0.0672,
      "step": 1319
    },
    {
      "epoch": 0.041438577903349226,
      "grad_norm": 0.1287754476070404,
      "learning_rate": 8.600125549278091e-05,
      "loss": 0.1103,
      "step": 1320
    },
    {
      "epoch": 0.04146997076539722,
      "grad_norm": 0.20332977175712585,
      "learning_rate": 8.568738229755179e-05,
      "loss": 0.2918,
      "step": 1321
    },
    {
      "epoch": 0.041501363627445206,
      "grad_norm": 0.5802950263023376,
      "learning_rate": 8.537350910232266e-05,
      "loss": 0.8631,
      "step": 1322
    },
    {
      "epoch": 0.0415327564894932,
      "grad_norm": 0.275058388710022,
      "learning_rate": 8.505963590709353e-05,
      "loss": 0.3566,
      "step": 1323
    },
    {
      "epoch": 0.041564149351541194,
      "grad_norm": 0.11625806987285614,
      "learning_rate": 8.47457627118644e-05,
      "loss": 0.0933,
      "step": 1324
    },
    {
      "epoch": 0.04159554221358919,
      "grad_norm": 0.05551788583397865,
      "learning_rate": 8.443188951663528e-05,
      "loss": 0.0542,
      "step": 1325
    },
    {
      "epoch": 0.041626935075637174,
      "grad_norm": 0.2713240087032318,
      "learning_rate": 8.411801632140615e-05,
      "loss": 0.26,
      "step": 1326
    },
    {
      "epoch": 0.04165832793768517,
      "grad_norm": 0.2230093628168106,
      "learning_rate": 8.380414312617702e-05,
      "loss": 0.2138,
      "step": 1327
    },
    {
      "epoch": 0.04168972079973316,
      "grad_norm": 0.1539614498615265,
      "learning_rate": 8.349026993094789e-05,
      "loss": 0.1668,
      "step": 1328
    },
    {
      "epoch": 0.041721113661781155,
      "grad_norm": 0.10596676170825958,
      "learning_rate": 8.317639673571876e-05,
      "loss": 0.1313,
      "step": 1329
    },
    {
      "epoch": 0.04175250652382914,
      "grad_norm": 0.27662739157676697,
      "learning_rate": 8.286252354048965e-05,
      "loss": 0.3237,
      "step": 1330
    },
    {
      "epoch": 0.041783899385877135,
      "grad_norm": 0.07542623579502106,
      "learning_rate": 8.254865034526052e-05,
      "loss": 0.0872,
      "step": 1331
    },
    {
      "epoch": 0.04181529224792513,
      "grad_norm": 0.0770922377705574,
      "learning_rate": 8.223477715003139e-05,
      "loss": 0.0805,
      "step": 1332
    },
    {
      "epoch": 0.04184668510997312,
      "grad_norm": 0.19735774397850037,
      "learning_rate": 8.192090395480226e-05,
      "loss": 0.2488,
      "step": 1333
    },
    {
      "epoch": 0.04187807797202111,
      "grad_norm": 0.07277996838092804,
      "learning_rate": 8.160703075957314e-05,
      "loss": 0.0862,
      "step": 1334
    },
    {
      "epoch": 0.0419094708340691,
      "grad_norm": 0.36041122674942017,
      "learning_rate": 8.129315756434401e-05,
      "loss": 0.4412,
      "step": 1335
    },
    {
      "epoch": 0.041940863696117096,
      "grad_norm": 0.24095264077186584,
      "learning_rate": 8.097928436911488e-05,
      "loss": 0.3494,
      "step": 1336
    },
    {
      "epoch": 0.04197225655816509,
      "grad_norm": 0.5110629200935364,
      "learning_rate": 8.066541117388575e-05,
      "loss": 0.6482,
      "step": 1337
    },
    {
      "epoch": 0.04200364942021308,
      "grad_norm": 0.10862692445516586,
      "learning_rate": 8.035153797865664e-05,
      "loss": 0.1279,
      "step": 1338
    },
    {
      "epoch": 0.04203504228226107,
      "grad_norm": 0.3450883626937866,
      "learning_rate": 8.00376647834275e-05,
      "loss": 0.6193,
      "step": 1339
    },
    {
      "epoch": 0.042066435144309064,
      "grad_norm": 0.06295591592788696,
      "learning_rate": 7.972379158819837e-05,
      "loss": 0.0599,
      "step": 1340
    },
    {
      "epoch": 0.04209782800635706,
      "grad_norm": 0.10082505643367767,
      "learning_rate": 7.940991839296924e-05,
      "loss": 0.1317,
      "step": 1341
    },
    {
      "epoch": 0.042129220868405044,
      "grad_norm": 0.3068637251853943,
      "learning_rate": 7.909604519774013e-05,
      "loss": 0.3939,
      "step": 1342
    },
    {
      "epoch": 0.04216061373045304,
      "grad_norm": 0.3640410602092743,
      "learning_rate": 7.8782172002511e-05,
      "loss": 0.5645,
      "step": 1343
    },
    {
      "epoch": 0.04219200659250103,
      "grad_norm": 0.06350849568843842,
      "learning_rate": 7.846829880728186e-05,
      "loss": 0.0582,
      "step": 1344
    },
    {
      "epoch": 0.042223399454549025,
      "grad_norm": 0.19654053449630737,
      "learning_rate": 7.815442561205273e-05,
      "loss": 0.1793,
      "step": 1345
    },
    {
      "epoch": 0.04225479231659701,
      "grad_norm": 0.28737103939056396,
      "learning_rate": 7.78405524168236e-05,
      "loss": 0.3893,
      "step": 1346
    },
    {
      "epoch": 0.042286185178645005,
      "grad_norm": 0.2812514305114746,
      "learning_rate": 7.752667922159448e-05,
      "loss": 0.4132,
      "step": 1347
    },
    {
      "epoch": 0.042317578040693,
      "grad_norm": 0.13107378780841827,
      "learning_rate": 7.721280602636535e-05,
      "loss": 0.0917,
      "step": 1348
    },
    {
      "epoch": 0.042348970902740986,
      "grad_norm": 0.26588720083236694,
      "learning_rate": 7.689893283113622e-05,
      "loss": 0.2633,
      "step": 1349
    },
    {
      "epoch": 0.04238036376478898,
      "grad_norm": 0.052908431738615036,
      "learning_rate": 7.658505963590709e-05,
      "loss": 0.0757,
      "step": 1350
    },
    {
      "epoch": 0.04241175662683697,
      "grad_norm": 0.07273955643177032,
      "learning_rate": 7.627118644067798e-05,
      "loss": 0.0642,
      "step": 1351
    },
    {
      "epoch": 0.04244314948888497,
      "grad_norm": 0.06133454665541649,
      "learning_rate": 7.595731324544884e-05,
      "loss": 0.0608,
      "step": 1352
    },
    {
      "epoch": 0.04247454235093295,
      "grad_norm": 0.10891029983758926,
      "learning_rate": 7.564344005021971e-05,
      "loss": 0.1381,
      "step": 1353
    },
    {
      "epoch": 0.04250593521298095,
      "grad_norm": 0.12197288125753403,
      "learning_rate": 7.532956685499058e-05,
      "loss": 0.1401,
      "step": 1354
    },
    {
      "epoch": 0.04253732807502894,
      "grad_norm": 0.2348615974187851,
      "learning_rate": 7.501569365976147e-05,
      "loss": 0.1414,
      "step": 1355
    },
    {
      "epoch": 0.042568720937076934,
      "grad_norm": 0.4043583869934082,
      "learning_rate": 7.470182046453233e-05,
      "loss": 0.2809,
      "step": 1356
    },
    {
      "epoch": 0.04260011379912492,
      "grad_norm": 0.38378262519836426,
      "learning_rate": 7.43879472693032e-05,
      "loss": 0.7172,
      "step": 1357
    },
    {
      "epoch": 0.042631506661172915,
      "grad_norm": 0.23178139328956604,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.294,
      "step": 1358
    },
    {
      "epoch": 0.04266289952322091,
      "grad_norm": 0.4538063108921051,
      "learning_rate": 7.376020087884496e-05,
      "loss": 0.4771,
      "step": 1359
    },
    {
      "epoch": 0.0426942923852689,
      "grad_norm": 0.17354600131511688,
      "learning_rate": 7.344632768361583e-05,
      "loss": 0.2061,
      "step": 1360
    },
    {
      "epoch": 0.04272568524731689,
      "grad_norm": 0.4045553505420685,
      "learning_rate": 7.31324544883867e-05,
      "loss": 0.4454,
      "step": 1361
    },
    {
      "epoch": 0.04275707810936488,
      "grad_norm": 0.39785927534103394,
      "learning_rate": 7.281858129315756e-05,
      "loss": 0.638,
      "step": 1362
    },
    {
      "epoch": 0.042788470971412876,
      "grad_norm": 0.275574266910553,
      "learning_rate": 7.250470809792843e-05,
      "loss": 0.1955,
      "step": 1363
    },
    {
      "epoch": 0.04281986383346087,
      "grad_norm": 0.04739126190543175,
      "learning_rate": 7.219083490269932e-05,
      "loss": 0.0573,
      "step": 1364
    },
    {
      "epoch": 0.042851256695508856,
      "grad_norm": 0.12322037667036057,
      "learning_rate": 7.187696170747018e-05,
      "loss": 0.1323,
      "step": 1365
    },
    {
      "epoch": 0.04288264955755685,
      "grad_norm": 0.24569347500801086,
      "learning_rate": 7.156308851224105e-05,
      "loss": 0.307,
      "step": 1366
    },
    {
      "epoch": 0.04291404241960484,
      "grad_norm": 0.13547085225582123,
      "learning_rate": 7.124921531701192e-05,
      "loss": 0.1037,
      "step": 1367
    },
    {
      "epoch": 0.04294543528165284,
      "grad_norm": 0.13353018462657928,
      "learning_rate": 7.09353421217828e-05,
      "loss": 0.151,
      "step": 1368
    },
    {
      "epoch": 0.042976828143700824,
      "grad_norm": 0.1352171152830124,
      "learning_rate": 7.062146892655367e-05,
      "loss": 0.1711,
      "step": 1369
    },
    {
      "epoch": 0.04300822100574882,
      "grad_norm": 0.39118048548698425,
      "learning_rate": 7.030759573132454e-05,
      "loss": 0.4881,
      "step": 1370
    },
    {
      "epoch": 0.04303961386779681,
      "grad_norm": 0.07139520347118378,
      "learning_rate": 6.999372253609541e-05,
      "loss": 0.0697,
      "step": 1371
    },
    {
      "epoch": 0.043071006729844805,
      "grad_norm": 0.5201083421707153,
      "learning_rate": 6.96798493408663e-05,
      "loss": 0.4429,
      "step": 1372
    },
    {
      "epoch": 0.04310239959189279,
      "grad_norm": 0.19338275492191315,
      "learning_rate": 6.936597614563717e-05,
      "loss": 0.2009,
      "step": 1373
    },
    {
      "epoch": 0.043133792453940785,
      "grad_norm": 0.08066608011722565,
      "learning_rate": 6.905210295040803e-05,
      "loss": 0.1242,
      "step": 1374
    },
    {
      "epoch": 0.04316518531598878,
      "grad_norm": 0.336534708738327,
      "learning_rate": 6.87382297551789e-05,
      "loss": 0.4624,
      "step": 1375
    },
    {
      "epoch": 0.04319657817803677,
      "grad_norm": 0.13081324100494385,
      "learning_rate": 6.842435655994979e-05,
      "loss": 0.1495,
      "step": 1376
    },
    {
      "epoch": 0.04322797104008476,
      "grad_norm": 0.3627382516860962,
      "learning_rate": 6.811048336472066e-05,
      "loss": 0.6464,
      "step": 1377
    },
    {
      "epoch": 0.04325936390213275,
      "grad_norm": 0.05979263409972191,
      "learning_rate": 6.779661016949152e-05,
      "loss": 0.059,
      "step": 1378
    },
    {
      "epoch": 0.043290756764180746,
      "grad_norm": 0.3974705934524536,
      "learning_rate": 6.74827369742624e-05,
      "loss": 0.3722,
      "step": 1379
    },
    {
      "epoch": 0.04332214962622873,
      "grad_norm": 0.2468811720609665,
      "learning_rate": 6.716886377903326e-05,
      "loss": 0.4205,
      "step": 1380
    },
    {
      "epoch": 0.043353542488276726,
      "grad_norm": 0.10355959832668304,
      "learning_rate": 6.685499058380415e-05,
      "loss": 0.1382,
      "step": 1381
    },
    {
      "epoch": 0.04338493535032472,
      "grad_norm": 0.09968746453523636,
      "learning_rate": 6.654111738857501e-05,
      "loss": 0.1394,
      "step": 1382
    },
    {
      "epoch": 0.043416328212372714,
      "grad_norm": 0.23581528663635254,
      "learning_rate": 6.622724419334588e-05,
      "loss": 0.3418,
      "step": 1383
    },
    {
      "epoch": 0.0434477210744207,
      "grad_norm": 0.22373691201210022,
      "learning_rate": 6.591337099811675e-05,
      "loss": 0.2122,
      "step": 1384
    },
    {
      "epoch": 0.043479113936468694,
      "grad_norm": 0.3264179229736328,
      "learning_rate": 6.559949780288764e-05,
      "loss": 0.1941,
      "step": 1385
    },
    {
      "epoch": 0.04351050679851669,
      "grad_norm": 0.19185228645801544,
      "learning_rate": 6.52856246076585e-05,
      "loss": 0.1667,
      "step": 1386
    },
    {
      "epoch": 0.04354189966056468,
      "grad_norm": 0.24208852648735046,
      "learning_rate": 6.497175141242937e-05,
      "loss": 0.3207,
      "step": 1387
    },
    {
      "epoch": 0.04357329252261267,
      "grad_norm": 0.0837770476937294,
      "learning_rate": 6.465787821720024e-05,
      "loss": 0.0572,
      "step": 1388
    },
    {
      "epoch": 0.04360468538466066,
      "grad_norm": 0.060759130865335464,
      "learning_rate": 6.434400502197113e-05,
      "loss": 0.0746,
      "step": 1389
    },
    {
      "epoch": 0.043636078246708655,
      "grad_norm": 0.09912043809890747,
      "learning_rate": 6.4030131826742e-05,
      "loss": 0.1115,
      "step": 1390
    },
    {
      "epoch": 0.04366747110875665,
      "grad_norm": 0.12418905645608902,
      "learning_rate": 6.371625863151286e-05,
      "loss": 0.1435,
      "step": 1391
    },
    {
      "epoch": 0.043698863970804636,
      "grad_norm": 0.3584256172180176,
      "learning_rate": 6.340238543628373e-05,
      "loss": 0.3022,
      "step": 1392
    },
    {
      "epoch": 0.04373025683285263,
      "grad_norm": 0.11361993104219437,
      "learning_rate": 6.308851224105462e-05,
      "loss": 0.1351,
      "step": 1393
    },
    {
      "epoch": 0.04376164969490062,
      "grad_norm": 0.22702039778232574,
      "learning_rate": 6.277463904582549e-05,
      "loss": 0.2427,
      "step": 1394
    },
    {
      "epoch": 0.043793042556948616,
      "grad_norm": 0.10011862218379974,
      "learning_rate": 6.246076585059635e-05,
      "loss": 0.1065,
      "step": 1395
    },
    {
      "epoch": 0.0438244354189966,
      "grad_norm": 0.20443400740623474,
      "learning_rate": 6.214689265536724e-05,
      "loss": 0.201,
      "step": 1396
    },
    {
      "epoch": 0.0438558282810446,
      "grad_norm": 0.08373086154460907,
      "learning_rate": 6.183301946013811e-05,
      "loss": 0.0969,
      "step": 1397
    },
    {
      "epoch": 0.04388722114309259,
      "grad_norm": 0.5292041301727295,
      "learning_rate": 6.151914626490898e-05,
      "loss": 0.9025,
      "step": 1398
    },
    {
      "epoch": 0.043918614005140584,
      "grad_norm": 0.1381625235080719,
      "learning_rate": 6.120527306967985e-05,
      "loss": 0.1244,
      "step": 1399
    },
    {
      "epoch": 0.04395000686718857,
      "grad_norm": 0.22672361135482788,
      "learning_rate": 6.089139987445072e-05,
      "loss": 0.2598,
      "step": 1400
    },
    {
      "epoch": 0.043981399729236564,
      "grad_norm": 0.4197072386741638,
      "learning_rate": 6.05775266792216e-05,
      "loss": 0.498,
      "step": 1401
    },
    {
      "epoch": 0.04401279259128456,
      "grad_norm": 0.16444271802902222,
      "learning_rate": 6.0263653483992466e-05,
      "loss": 0.1371,
      "step": 1402
    },
    {
      "epoch": 0.04404418545333255,
      "grad_norm": 0.3265739381313324,
      "learning_rate": 5.994978028876334e-05,
      "loss": 0.5415,
      "step": 1403
    },
    {
      "epoch": 0.04407557831538054,
      "grad_norm": 0.06830374896526337,
      "learning_rate": 5.963590709353421e-05,
      "loss": 0.0655,
      "step": 1404
    },
    {
      "epoch": 0.04410697117742853,
      "grad_norm": 0.2539794445037842,
      "learning_rate": 5.932203389830509e-05,
      "loss": 0.3146,
      "step": 1405
    },
    {
      "epoch": 0.044138364039476526,
      "grad_norm": 0.15280663967132568,
      "learning_rate": 5.9008160703075957e-05,
      "loss": 0.2379,
      "step": 1406
    },
    {
      "epoch": 0.04416975690152452,
      "grad_norm": 0.25413209199905396,
      "learning_rate": 5.869428750784683e-05,
      "loss": 0.2722,
      "step": 1407
    },
    {
      "epoch": 0.044201149763572506,
      "grad_norm": 0.11259813606739044,
      "learning_rate": 5.83804143126177e-05,
      "loss": 0.1208,
      "step": 1408
    },
    {
      "epoch": 0.0442325426256205,
      "grad_norm": 0.32928216457366943,
      "learning_rate": 5.806654111738858e-05,
      "loss": 0.3627,
      "step": 1409
    },
    {
      "epoch": 0.04426393548766849,
      "grad_norm": 0.07209143042564392,
      "learning_rate": 5.775266792215945e-05,
      "loss": 0.06,
      "step": 1410
    },
    {
      "epoch": 0.04429532834971648,
      "grad_norm": 0.2953157126903534,
      "learning_rate": 5.743879472693032e-05,
      "loss": 0.2757,
      "step": 1411
    },
    {
      "epoch": 0.044326721211764473,
      "grad_norm": 0.08228366822004318,
      "learning_rate": 5.712492153170119e-05,
      "loss": 0.0713,
      "step": 1412
    },
    {
      "epoch": 0.04435811407381247,
      "grad_norm": 0.33658215403556824,
      "learning_rate": 5.681104833647207e-05,
      "loss": 0.3467,
      "step": 1413
    },
    {
      "epoch": 0.04438950693586046,
      "grad_norm": 0.5156931281089783,
      "learning_rate": 5.649717514124294e-05,
      "loss": 0.5403,
      "step": 1414
    },
    {
      "epoch": 0.04442089979790845,
      "grad_norm": 0.08459263294935226,
      "learning_rate": 5.618330194601381e-05,
      "loss": 0.0919,
      "step": 1415
    },
    {
      "epoch": 0.04445229265995644,
      "grad_norm": 0.07859084755182266,
      "learning_rate": 5.586942875078468e-05,
      "loss": 0.0858,
      "step": 1416
    },
    {
      "epoch": 0.044483685522004435,
      "grad_norm": 0.11477742344141006,
      "learning_rate": 5.555555555555555e-05,
      "loss": 0.1069,
      "step": 1417
    },
    {
      "epoch": 0.04451507838405243,
      "grad_norm": 0.23796188831329346,
      "learning_rate": 5.524168236032643e-05,
      "loss": 0.2806,
      "step": 1418
    },
    {
      "epoch": 0.044546471246100415,
      "grad_norm": 0.16440871357917786,
      "learning_rate": 5.4927809165097297e-05,
      "loss": 0.1671,
      "step": 1419
    },
    {
      "epoch": 0.04457786410814841,
      "grad_norm": 0.10242234170436859,
      "learning_rate": 5.461393596986817e-05,
      "loss": 0.0827,
      "step": 1420
    },
    {
      "epoch": 0.0446092569701964,
      "grad_norm": 0.12281743437051773,
      "learning_rate": 5.430006277463904e-05,
      "loss": 0.1496,
      "step": 1421
    },
    {
      "epoch": 0.044640649832244396,
      "grad_norm": 0.11271890252828598,
      "learning_rate": 5.398618957940992e-05,
      "loss": 0.1249,
      "step": 1422
    },
    {
      "epoch": 0.04467204269429238,
      "grad_norm": 0.20903562009334564,
      "learning_rate": 5.367231638418079e-05,
      "loss": 0.3221,
      "step": 1423
    },
    {
      "epoch": 0.044703435556340376,
      "grad_norm": 0.38438570499420166,
      "learning_rate": 5.335844318895167e-05,
      "loss": 0.5947,
      "step": 1424
    },
    {
      "epoch": 0.04473482841838837,
      "grad_norm": 0.43512162566185,
      "learning_rate": 5.304456999372254e-05,
      "loss": 0.582,
      "step": 1425
    },
    {
      "epoch": 0.044766221280436364,
      "grad_norm": 0.07703383266925812,
      "learning_rate": 5.2730696798493415e-05,
      "loss": 0.0585,
      "step": 1426
    },
    {
      "epoch": 0.04479761414248435,
      "grad_norm": 0.10423515737056732,
      "learning_rate": 5.2416823603264284e-05,
      "loss": 0.1242,
      "step": 1427
    },
    {
      "epoch": 0.044829007004532344,
      "grad_norm": 0.10460028052330017,
      "learning_rate": 5.210295040803516e-05,
      "loss": 0.1461,
      "step": 1428
    },
    {
      "epoch": 0.04486039986658034,
      "grad_norm": 0.07071275264024734,
      "learning_rate": 5.178907721280603e-05,
      "loss": 0.0707,
      "step": 1429
    },
    {
      "epoch": 0.04489179272862833,
      "grad_norm": 0.165457084774971,
      "learning_rate": 5.1475204017576905e-05,
      "loss": 0.1638,
      "step": 1430
    },
    {
      "epoch": 0.04492318559067632,
      "grad_norm": 0.1599893718957901,
      "learning_rate": 5.1161330822347774e-05,
      "loss": 0.2034,
      "step": 1431
    },
    {
      "epoch": 0.04495457845272431,
      "grad_norm": 0.4116949141025543,
      "learning_rate": 5.084745762711865e-05,
      "loss": 0.6432,
      "step": 1432
    },
    {
      "epoch": 0.044985971314772305,
      "grad_norm": 0.24466674029827118,
      "learning_rate": 5.053358443188952e-05,
      "loss": 0.3196,
      "step": 1433
    },
    {
      "epoch": 0.0450173641768203,
      "grad_norm": 0.29562878608703613,
      "learning_rate": 5.021971123666039e-05,
      "loss": 0.33,
      "step": 1434
    },
    {
      "epoch": 0.045048757038868285,
      "grad_norm": 0.06943947076797485,
      "learning_rate": 4.9905838041431265e-05,
      "loss": 0.0806,
      "step": 1435
    },
    {
      "epoch": 0.04508014990091628,
      "grad_norm": 0.06356391310691833,
      "learning_rate": 4.9591964846202134e-05,
      "loss": 0.0629,
      "step": 1436
    },
    {
      "epoch": 0.04511154276296427,
      "grad_norm": 0.17123110592365265,
      "learning_rate": 4.927809165097301e-05,
      "loss": 0.136,
      "step": 1437
    },
    {
      "epoch": 0.045142935625012266,
      "grad_norm": 0.28282397985458374,
      "learning_rate": 4.896421845574388e-05,
      "loss": 0.3515,
      "step": 1438
    },
    {
      "epoch": 0.04517432848706025,
      "grad_norm": 0.3002464473247528,
      "learning_rate": 4.8650345260514755e-05,
      "loss": 0.2996,
      "step": 1439
    },
    {
      "epoch": 0.04520572134910825,
      "grad_norm": 0.37398630380630493,
      "learning_rate": 4.8336472065285624e-05,
      "loss": 0.4197,
      "step": 1440
    },
    {
      "epoch": 0.04523711421115624,
      "grad_norm": 0.15607641637325287,
      "learning_rate": 4.80225988700565e-05,
      "loss": 0.2,
      "step": 1441
    },
    {
      "epoch": 0.04526850707320423,
      "grad_norm": 0.3402237892150879,
      "learning_rate": 4.770872567482737e-05,
      "loss": 0.3728,
      "step": 1442
    },
    {
      "epoch": 0.04529989993525222,
      "grad_norm": 0.2527308762073517,
      "learning_rate": 4.7394852479598245e-05,
      "loss": 0.1336,
      "step": 1443
    },
    {
      "epoch": 0.045331292797300214,
      "grad_norm": 0.22833940386772156,
      "learning_rate": 4.7080979284369114e-05,
      "loss": 0.2567,
      "step": 1444
    },
    {
      "epoch": 0.04536268565934821,
      "grad_norm": 0.35050275921821594,
      "learning_rate": 4.676710608913999e-05,
      "loss": 0.4351,
      "step": 1445
    },
    {
      "epoch": 0.045394078521396194,
      "grad_norm": 0.2514791786670685,
      "learning_rate": 4.645323289391086e-05,
      "loss": 0.3134,
      "step": 1446
    },
    {
      "epoch": 0.04542547138344419,
      "grad_norm": 0.2454981505870819,
      "learning_rate": 4.6139359698681735e-05,
      "loss": 0.2272,
      "step": 1447
    },
    {
      "epoch": 0.04545686424549218,
      "grad_norm": 0.2324926257133484,
      "learning_rate": 4.5825486503452605e-05,
      "loss": 0.3081,
      "step": 1448
    },
    {
      "epoch": 0.045488257107540175,
      "grad_norm": 0.10429312288761139,
      "learning_rate": 4.551161330822348e-05,
      "loss": 0.0614,
      "step": 1449
    },
    {
      "epoch": 0.04551964996958816,
      "grad_norm": 0.10641788691282272,
      "learning_rate": 4.519774011299435e-05,
      "loss": 0.0998,
      "step": 1450
    },
    {
      "epoch": 0.045551042831636156,
      "grad_norm": 0.30944982171058655,
      "learning_rate": 4.488386691776522e-05,
      "loss": 0.4489,
      "step": 1451
    },
    {
      "epoch": 0.04558243569368415,
      "grad_norm": 0.10379412770271301,
      "learning_rate": 4.4569993722536095e-05,
      "loss": 0.0705,
      "step": 1452
    },
    {
      "epoch": 0.04561382855573214,
      "grad_norm": 0.13705269992351532,
      "learning_rate": 4.4256120527306964e-05,
      "loss": 0.2012,
      "step": 1453
    },
    {
      "epoch": 0.04564522141778013,
      "grad_norm": 0.3168848156929016,
      "learning_rate": 4.394224733207784e-05,
      "loss": 0.438,
      "step": 1454
    },
    {
      "epoch": 0.04567661427982812,
      "grad_norm": 0.0743420198559761,
      "learning_rate": 4.362837413684871e-05,
      "loss": 0.0971,
      "step": 1455
    },
    {
      "epoch": 0.04570800714187612,
      "grad_norm": 0.46628496050834656,
      "learning_rate": 4.3314500941619585e-05,
      "loss": 0.5677,
      "step": 1456
    },
    {
      "epoch": 0.04573940000392411,
      "grad_norm": 0.30325931310653687,
      "learning_rate": 4.3000627746390454e-05,
      "loss": 0.2334,
      "step": 1457
    },
    {
      "epoch": 0.0457707928659721,
      "grad_norm": 0.17432521283626556,
      "learning_rate": 4.268675455116133e-05,
      "loss": 0.1715,
      "step": 1458
    },
    {
      "epoch": 0.04580218572802009,
      "grad_norm": 0.11579065024852753,
      "learning_rate": 4.23728813559322e-05,
      "loss": 0.1517,
      "step": 1459
    },
    {
      "epoch": 0.045833578590068084,
      "grad_norm": 0.17502515017986298,
      "learning_rate": 4.2059008160703075e-05,
      "loss": 0.1136,
      "step": 1460
    },
    {
      "epoch": 0.04586497145211608,
      "grad_norm": 0.07854917645454407,
      "learning_rate": 4.1745134965473945e-05,
      "loss": 0.0738,
      "step": 1461
    },
    {
      "epoch": 0.045896364314164065,
      "grad_norm": 0.10443483293056488,
      "learning_rate": 4.143126177024483e-05,
      "loss": 0.106,
      "step": 1462
    },
    {
      "epoch": 0.04592775717621206,
      "grad_norm": 0.41156381368637085,
      "learning_rate": 4.1117388575015697e-05,
      "loss": 0.5046,
      "step": 1463
    },
    {
      "epoch": 0.04595915003826005,
      "grad_norm": 0.13160817325115204,
      "learning_rate": 4.080351537978657e-05,
      "loss": 0.082,
      "step": 1464
    },
    {
      "epoch": 0.045990542900308046,
      "grad_norm": 0.13221222162246704,
      "learning_rate": 4.048964218455744e-05,
      "loss": 0.0718,
      "step": 1465
    },
    {
      "epoch": 0.04602193576235603,
      "grad_norm": 0.2396615892648697,
      "learning_rate": 4.017576898932832e-05,
      "loss": 0.3421,
      "step": 1466
    },
    {
      "epoch": 0.046053328624404026,
      "grad_norm": 0.3246491849422455,
      "learning_rate": 3.986189579409919e-05,
      "loss": 0.4374,
      "step": 1467
    },
    {
      "epoch": 0.04608472148645202,
      "grad_norm": 0.11046640574932098,
      "learning_rate": 3.954802259887006e-05,
      "loss": 0.0965,
      "step": 1468
    },
    {
      "epoch": 0.046116114348500006,
      "grad_norm": 0.14076633751392365,
      "learning_rate": 3.923414940364093e-05,
      "loss": 0.1712,
      "step": 1469
    },
    {
      "epoch": 0.046147507210548,
      "grad_norm": 0.37394076585769653,
      "learning_rate": 3.89202762084118e-05,
      "loss": 0.483,
      "step": 1470
    },
    {
      "epoch": 0.046178900072595994,
      "grad_norm": 0.24094726145267487,
      "learning_rate": 3.860640301318268e-05,
      "loss": 0.3252,
      "step": 1471
    },
    {
      "epoch": 0.04621029293464399,
      "grad_norm": 0.32754090428352356,
      "learning_rate": 3.8292529817953546e-05,
      "loss": 0.2974,
      "step": 1472
    },
    {
      "epoch": 0.046241685796691974,
      "grad_norm": 0.13304965198040009,
      "learning_rate": 3.797865662272442e-05,
      "loss": 0.1654,
      "step": 1473
    },
    {
      "epoch": 0.04627307865873997,
      "grad_norm": 0.1484343707561493,
      "learning_rate": 3.766478342749529e-05,
      "loss": 0.1936,
      "step": 1474
    },
    {
      "epoch": 0.04630447152078796,
      "grad_norm": 0.3018442988395691,
      "learning_rate": 3.735091023226617e-05,
      "loss": 0.3427,
      "step": 1475
    },
    {
      "epoch": 0.046335864382835955,
      "grad_norm": 0.37992745637893677,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.5268,
      "step": 1476
    },
    {
      "epoch": 0.04636725724488394,
      "grad_norm": 0.32424867153167725,
      "learning_rate": 3.672316384180791e-05,
      "loss": 0.6417,
      "step": 1477
    },
    {
      "epoch": 0.046398650106931935,
      "grad_norm": 0.46467724442481995,
      "learning_rate": 3.640929064657878e-05,
      "loss": 0.4478,
      "step": 1478
    },
    {
      "epoch": 0.04643004296897993,
      "grad_norm": 0.11610109359025955,
      "learning_rate": 3.609541745134966e-05,
      "loss": 0.1276,
      "step": 1479
    },
    {
      "epoch": 0.04646143583102792,
      "grad_norm": 0.13600243628025055,
      "learning_rate": 3.578154425612053e-05,
      "loss": 0.1849,
      "step": 1480
    },
    {
      "epoch": 0.04649282869307591,
      "grad_norm": 0.3707219958305359,
      "learning_rate": 3.54676710608914e-05,
      "loss": 0.4579,
      "step": 1481
    },
    {
      "epoch": 0.0465242215551239,
      "grad_norm": 0.25378528237342834,
      "learning_rate": 3.515379786566227e-05,
      "loss": 0.3028,
      "step": 1482
    },
    {
      "epoch": 0.046555614417171896,
      "grad_norm": 0.27540886402130127,
      "learning_rate": 3.483992467043315e-05,
      "loss": 0.3104,
      "step": 1483
    },
    {
      "epoch": 0.04658700727921989,
      "grad_norm": 0.08954446017742157,
      "learning_rate": 3.452605147520402e-05,
      "loss": 0.1308,
      "step": 1484
    },
    {
      "epoch": 0.04661840014126788,
      "grad_norm": 0.47360703349113464,
      "learning_rate": 3.421217827997489e-05,
      "loss": 0.5314,
      "step": 1485
    },
    {
      "epoch": 0.04664979300331587,
      "grad_norm": 0.08192318677902222,
      "learning_rate": 3.389830508474576e-05,
      "loss": 0.0761,
      "step": 1486
    },
    {
      "epoch": 0.046681185865363864,
      "grad_norm": 0.4239566922187805,
      "learning_rate": 3.358443188951663e-05,
      "loss": 0.6042,
      "step": 1487
    },
    {
      "epoch": 0.04671257872741186,
      "grad_norm": 0.07255744189023972,
      "learning_rate": 3.327055869428751e-05,
      "loss": 0.065,
      "step": 1488
    },
    {
      "epoch": 0.046743971589459844,
      "grad_norm": 0.35149118304252625,
      "learning_rate": 3.2956685499058377e-05,
      "loss": 0.3213,
      "step": 1489
    },
    {
      "epoch": 0.04677536445150784,
      "grad_norm": 0.12338512390851974,
      "learning_rate": 3.264281230382925e-05,
      "loss": 0.1625,
      "step": 1490
    },
    {
      "epoch": 0.04680675731355583,
      "grad_norm": 0.07756880670785904,
      "learning_rate": 3.232893910860012e-05,
      "loss": 0.0608,
      "step": 1491
    },
    {
      "epoch": 0.046838150175603825,
      "grad_norm": 0.38294127583503723,
      "learning_rate": 3.2015065913371e-05,
      "loss": 0.5468,
      "step": 1492
    },
    {
      "epoch": 0.04686954303765181,
      "grad_norm": 0.09095576405525208,
      "learning_rate": 3.170119271814187e-05,
      "loss": 0.0892,
      "step": 1493
    },
    {
      "epoch": 0.046900935899699805,
      "grad_norm": 0.10392414033412933,
      "learning_rate": 3.138731952291274e-05,
      "loss": 0.1391,
      "step": 1494
    },
    {
      "epoch": 0.0469323287617478,
      "grad_norm": 0.13705633580684662,
      "learning_rate": 3.107344632768362e-05,
      "loss": 0.1137,
      "step": 1495
    },
    {
      "epoch": 0.04696372162379579,
      "grad_norm": 0.12154527753591537,
      "learning_rate": 3.075957313245449e-05,
      "loss": 0.1577,
      "step": 1496
    },
    {
      "epoch": 0.04699511448584378,
      "grad_norm": 0.3189251124858856,
      "learning_rate": 3.044569993722536e-05,
      "loss": 0.6038,
      "step": 1497
    },
    {
      "epoch": 0.04702650734789177,
      "grad_norm": 0.07441667467355728,
      "learning_rate": 3.0131826741996233e-05,
      "loss": 0.0591,
      "step": 1498
    },
    {
      "epoch": 0.04705790020993977,
      "grad_norm": 0.3080808222293854,
      "learning_rate": 2.9817953546767106e-05,
      "loss": 0.3513,
      "step": 1499
    },
    {
      "epoch": 0.04708929307198775,
      "grad_norm": 0.15544594824314117,
      "learning_rate": 2.9504080351537978e-05,
      "loss": 0.1503,
      "step": 1500
    },
    {
      "epoch": 0.04712068593403575,
      "grad_norm": 0.3551001250743866,
      "learning_rate": 2.919020715630885e-05,
      "loss": 0.3907,
      "step": 1501
    },
    {
      "epoch": 0.04715207879608374,
      "grad_norm": 0.284934937953949,
      "learning_rate": 2.8876333961079723e-05,
      "loss": 0.3028,
      "step": 1502
    },
    {
      "epoch": 0.047183471658131734,
      "grad_norm": 0.14449390769004822,
      "learning_rate": 2.8562460765850596e-05,
      "loss": 0.1362,
      "step": 1503
    },
    {
      "epoch": 0.04721486452017972,
      "grad_norm": 0.40726742148399353,
      "learning_rate": 2.824858757062147e-05,
      "loss": 0.7296,
      "step": 1504
    },
    {
      "epoch": 0.047246257382227715,
      "grad_norm": 0.07689078152179718,
      "learning_rate": 2.793471437539234e-05,
      "loss": 0.0854,
      "step": 1505
    },
    {
      "epoch": 0.04727765024427571,
      "grad_norm": 0.0755484327673912,
      "learning_rate": 2.7620841180163214e-05,
      "loss": 0.0724,
      "step": 1506
    },
    {
      "epoch": 0.0473090431063237,
      "grad_norm": 0.43932104110717773,
      "learning_rate": 2.7306967984934086e-05,
      "loss": 0.3076,
      "step": 1507
    },
    {
      "epoch": 0.04734043596837169,
      "grad_norm": 0.25433972477912903,
      "learning_rate": 2.699309478970496e-05,
      "loss": 0.2404,
      "step": 1508
    },
    {
      "epoch": 0.04737182883041968,
      "grad_norm": 0.34050434827804565,
      "learning_rate": 2.6679221594475835e-05,
      "loss": 0.606,
      "step": 1509
    },
    {
      "epoch": 0.047403221692467676,
      "grad_norm": 0.0723019540309906,
      "learning_rate": 2.6365348399246707e-05,
      "loss": 0.0673,
      "step": 1510
    },
    {
      "epoch": 0.04743461455451567,
      "grad_norm": 0.40371572971343994,
      "learning_rate": 2.605147520401758e-05,
      "loss": 0.4462,
      "step": 1511
    },
    {
      "epoch": 0.047466007416563656,
      "grad_norm": 0.08123960345983505,
      "learning_rate": 2.5737602008788453e-05,
      "loss": 0.0917,
      "step": 1512
    },
    {
      "epoch": 0.04749740027861165,
      "grad_norm": 0.09291692823171616,
      "learning_rate": 2.5423728813559325e-05,
      "loss": 0.1022,
      "step": 1513
    },
    {
      "epoch": 0.04752879314065964,
      "grad_norm": 0.07918497920036316,
      "learning_rate": 2.5109855618330194e-05,
      "loss": 0.0731,
      "step": 1514
    },
    {
      "epoch": 0.04756018600270764,
      "grad_norm": 0.4041264057159424,
      "learning_rate": 2.4795982423101067e-05,
      "loss": 0.5191,
      "step": 1515
    },
    {
      "epoch": 0.047591578864755624,
      "grad_norm": 0.4595590829849243,
      "learning_rate": 2.448210922787194e-05,
      "loss": 0.5113,
      "step": 1516
    },
    {
      "epoch": 0.04762297172680362,
      "grad_norm": 0.48683425784111023,
      "learning_rate": 2.4168236032642812e-05,
      "loss": 0.4395,
      "step": 1517
    },
    {
      "epoch": 0.04765436458885161,
      "grad_norm": 0.30495336651802063,
      "learning_rate": 2.3854362837413685e-05,
      "loss": 0.4404,
      "step": 1518
    },
    {
      "epoch": 0.047685757450899605,
      "grad_norm": 0.463186651468277,
      "learning_rate": 2.3540489642184557e-05,
      "loss": 0.6618,
      "step": 1519
    },
    {
      "epoch": 0.04771715031294759,
      "grad_norm": 0.06883500516414642,
      "learning_rate": 2.322661644695543e-05,
      "loss": 0.0706,
      "step": 1520
    },
    {
      "epoch": 0.047748543174995585,
      "grad_norm": 0.1296130120754242,
      "learning_rate": 2.2912743251726302e-05,
      "loss": 0.1514,
      "step": 1521
    },
    {
      "epoch": 0.04777993603704358,
      "grad_norm": 0.19975200295448303,
      "learning_rate": 2.2598870056497175e-05,
      "loss": 0.1864,
      "step": 1522
    },
    {
      "epoch": 0.04781132889909157,
      "grad_norm": 0.15889500081539154,
      "learning_rate": 2.2284996861268047e-05,
      "loss": 0.1753,
      "step": 1523
    },
    {
      "epoch": 0.04784272176113956,
      "grad_norm": 0.22814977169036865,
      "learning_rate": 2.197112366603892e-05,
      "loss": 0.2868,
      "step": 1524
    },
    {
      "epoch": 0.04787411462318755,
      "grad_norm": 0.1579517424106598,
      "learning_rate": 2.1657250470809793e-05,
      "loss": 0.164,
      "step": 1525
    },
    {
      "epoch": 0.047905507485235546,
      "grad_norm": 0.08244320750236511,
      "learning_rate": 2.1343377275580665e-05,
      "loss": 0.0497,
      "step": 1526
    },
    {
      "epoch": 0.04793690034728354,
      "grad_norm": 0.27075523138046265,
      "learning_rate": 2.1029504080351538e-05,
      "loss": 0.4532,
      "step": 1527
    },
    {
      "epoch": 0.047968293209331526,
      "grad_norm": 0.16613656282424927,
      "learning_rate": 2.0715630885122414e-05,
      "loss": 0.2532,
      "step": 1528
    },
    {
      "epoch": 0.04799968607137952,
      "grad_norm": 0.2894567847251892,
      "learning_rate": 2.0401757689893286e-05,
      "loss": 0.4705,
      "step": 1529
    },
    {
      "epoch": 0.048031078933427514,
      "grad_norm": 0.43599244952201843,
      "learning_rate": 2.008788449466416e-05,
      "loss": 0.6427,
      "step": 1530
    },
    {
      "epoch": 0.0480624717954755,
      "grad_norm": 0.0882846862077713,
      "learning_rate": 1.977401129943503e-05,
      "loss": 0.0596,
      "step": 1531
    },
    {
      "epoch": 0.048093864657523494,
      "grad_norm": 0.2839566171169281,
      "learning_rate": 1.94601381042059e-05,
      "loss": 0.395,
      "step": 1532
    },
    {
      "epoch": 0.04812525751957149,
      "grad_norm": 0.15229956805706024,
      "learning_rate": 1.9146264908976773e-05,
      "loss": 0.1495,
      "step": 1533
    },
    {
      "epoch": 0.04815665038161948,
      "grad_norm": 0.5620424151420593,
      "learning_rate": 1.8832391713747646e-05,
      "loss": 0.6058,
      "step": 1534
    },
    {
      "epoch": 0.04818804324366747,
      "grad_norm": 0.11297392100095749,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 0.1059,
      "step": 1535
    },
    {
      "epoch": 0.04821943610571546,
      "grad_norm": 0.0930059403181076,
      "learning_rate": 1.820464532328939e-05,
      "loss": 0.0725,
      "step": 1536
    },
    {
      "epoch": 0.048250828967763455,
      "grad_norm": 0.24961526691913605,
      "learning_rate": 1.7890772128060263e-05,
      "loss": 0.3189,
      "step": 1537
    },
    {
      "epoch": 0.04828222182981145,
      "grad_norm": 0.4009942412376404,
      "learning_rate": 1.7576898932831136e-05,
      "loss": 0.4017,
      "step": 1538
    },
    {
      "epoch": 0.048313614691859436,
      "grad_norm": 0.0946059301495552,
      "learning_rate": 1.726302573760201e-05,
      "loss": 0.0841,
      "step": 1539
    },
    {
      "epoch": 0.04834500755390743,
      "grad_norm": 0.4339591860771179,
      "learning_rate": 1.694915254237288e-05,
      "loss": 0.5786,
      "step": 1540
    },
    {
      "epoch": 0.04837640041595542,
      "grad_norm": 0.13808882236480713,
      "learning_rate": 1.6635279347143754e-05,
      "loss": 0.1393,
      "step": 1541
    },
    {
      "epoch": 0.048407793278003416,
      "grad_norm": 0.35798120498657227,
      "learning_rate": 1.6321406151914626e-05,
      "loss": 0.559,
      "step": 1542
    },
    {
      "epoch": 0.0484391861400514,
      "grad_norm": 0.3817638158798218,
      "learning_rate": 1.60075329566855e-05,
      "loss": 0.7123,
      "step": 1543
    },
    {
      "epoch": 0.0484705790020994,
      "grad_norm": 0.4155788719654083,
      "learning_rate": 1.569365976145637e-05,
      "loss": 0.5529,
      "step": 1544
    },
    {
      "epoch": 0.04850197186414739,
      "grad_norm": 0.27743348479270935,
      "learning_rate": 1.5379786566227244e-05,
      "loss": 0.2616,
      "step": 1545
    },
    {
      "epoch": 0.048533364726195384,
      "grad_norm": 0.08837872743606567,
      "learning_rate": 1.5065913370998117e-05,
      "loss": 0.1279,
      "step": 1546
    },
    {
      "epoch": 0.04856475758824337,
      "grad_norm": 0.08804803341627121,
      "learning_rate": 1.4752040175768989e-05,
      "loss": 0.091,
      "step": 1547
    },
    {
      "epoch": 0.048596150450291364,
      "grad_norm": 0.07731829583644867,
      "learning_rate": 1.4438166980539862e-05,
      "loss": 0.0922,
      "step": 1548
    },
    {
      "epoch": 0.04862754331233936,
      "grad_norm": 0.09620871394872665,
      "learning_rate": 1.4124293785310734e-05,
      "loss": 0.1016,
      "step": 1549
    },
    {
      "epoch": 0.04865893617438735,
      "grad_norm": 0.302491158246994,
      "learning_rate": 1.3810420590081607e-05,
      "loss": 0.4924,
      "step": 1550
    },
    {
      "epoch": 0.04869032903643534,
      "grad_norm": 0.10472076386213303,
      "learning_rate": 1.349654739485248e-05,
      "loss": 0.0952,
      "step": 1551
    },
    {
      "epoch": 0.04872172189848333,
      "grad_norm": 0.3685464560985565,
      "learning_rate": 1.3182674199623354e-05,
      "loss": 0.6634,
      "step": 1552
    },
    {
      "epoch": 0.048753114760531326,
      "grad_norm": 0.3925362825393677,
      "learning_rate": 1.2868801004394226e-05,
      "loss": 0.5922,
      "step": 1553
    },
    {
      "epoch": 0.04878450762257932,
      "grad_norm": 0.2478383630514145,
      "learning_rate": 1.2554927809165097e-05,
      "loss": 0.3816,
      "step": 1554
    },
    {
      "epoch": 0.048815900484627306,
      "grad_norm": 0.21915799379348755,
      "learning_rate": 1.224105461393597e-05,
      "loss": 0.2429,
      "step": 1555
    },
    {
      "epoch": 0.0488472933466753,
      "grad_norm": 0.10077172517776489,
      "learning_rate": 1.1927181418706842e-05,
      "loss": 0.1261,
      "step": 1556
    },
    {
      "epoch": 0.04887868620872329,
      "grad_norm": 0.08830965310335159,
      "learning_rate": 1.1613308223477715e-05,
      "loss": 0.0886,
      "step": 1557
    },
    {
      "epoch": 0.04891007907077129,
      "grad_norm": 0.06084245443344116,
      "learning_rate": 1.1299435028248587e-05,
      "loss": 0.0535,
      "step": 1558
    },
    {
      "epoch": 0.048941471932819273,
      "grad_norm": 0.3115907907485962,
      "learning_rate": 1.098556183301946e-05,
      "loss": 0.3585,
      "step": 1559
    },
    {
      "epoch": 0.04897286479486727,
      "grad_norm": 0.4764237105846405,
      "learning_rate": 1.0671688637790333e-05,
      "loss": 0.3157,
      "step": 1560
    },
    {
      "epoch": 0.04900425765691526,
      "grad_norm": 0.23479104042053223,
      "learning_rate": 1.0357815442561207e-05,
      "loss": 0.2089,
      "step": 1561
    },
    {
      "epoch": 0.04903565051896325,
      "grad_norm": 0.13916365802288055,
      "learning_rate": 1.004394224733208e-05,
      "loss": 0.1771,
      "step": 1562
    },
    {
      "epoch": 0.04906704338101124,
      "grad_norm": 0.07070709764957428,
      "learning_rate": 9.73006905210295e-06,
      "loss": 0.0833,
      "step": 1563
    },
    {
      "epoch": 0.049098436243059235,
      "grad_norm": 0.3439337909221649,
      "learning_rate": 9.416195856873823e-06,
      "loss": 0.4449,
      "step": 1564
    },
    {
      "epoch": 0.04912982910510723,
      "grad_norm": 0.1579776257276535,
      "learning_rate": 9.102322661644695e-06,
      "loss": 0.1797,
      "step": 1565
    },
    {
      "epoch": 0.049161221967155215,
      "grad_norm": 0.12716151773929596,
      "learning_rate": 8.788449466415568e-06,
      "loss": 0.1326,
      "step": 1566
    },
    {
      "epoch": 0.04919261482920321,
      "grad_norm": 0.32705923914909363,
      "learning_rate": 8.47457627118644e-06,
      "loss": 0.5665,
      "step": 1567
    },
    {
      "epoch": 0.0492240076912512,
      "grad_norm": 0.4227954149246216,
      "learning_rate": 8.160703075957313e-06,
      "loss": 0.2824,
      "step": 1568
    },
    {
      "epoch": 0.049255400553299196,
      "grad_norm": 0.09073784202337265,
      "learning_rate": 7.846829880728186e-06,
      "loss": 0.1011,
      "step": 1569
    },
    {
      "epoch": 0.04928679341534718,
      "grad_norm": 0.414020299911499,
      "learning_rate": 7.532956685499058e-06,
      "loss": 0.4255,
      "step": 1570
    },
    {
      "epoch": 0.049318186277395176,
      "grad_norm": 0.24464820325374603,
      "learning_rate": 7.219083490269931e-06,
      "loss": 0.2055,
      "step": 1571
    },
    {
      "epoch": 0.04934957913944317,
      "grad_norm": 0.2241389900445938,
      "learning_rate": 6.9052102950408034e-06,
      "loss": 0.1999,
      "step": 1572
    },
    {
      "epoch": 0.049380972001491164,
      "grad_norm": 0.5871670246124268,
      "learning_rate": 6.591337099811677e-06,
      "loss": 0.6603,
      "step": 1573
    },
    {
      "epoch": 0.04941236486353915,
      "grad_norm": 0.1936277151107788,
      "learning_rate": 6.2774639045825486e-06,
      "loss": 0.1703,
      "step": 1574
    },
    {
      "epoch": 0.049443757725587144,
      "grad_norm": 0.24912865459918976,
      "learning_rate": 5.963590709353421e-06,
      "loss": 0.3612,
      "step": 1575
    },
    {
      "epoch": 0.04947515058763514,
      "grad_norm": 0.11230044066905975,
      "learning_rate": 5.649717514124294e-06,
      "loss": 0.1308,
      "step": 1576
    },
    {
      "epoch": 0.04950654344968313,
      "grad_norm": 0.23636022210121155,
      "learning_rate": 5.335844318895166e-06,
      "loss": 0.2135,
      "step": 1577
    },
    {
      "epoch": 0.04953793631173112,
      "grad_norm": 0.14196433126926422,
      "learning_rate": 5.02197112366604e-06,
      "loss": 0.1654,
      "step": 1578
    },
    {
      "epoch": 0.04956932917377911,
      "grad_norm": 0.5147472620010376,
      "learning_rate": 4.7080979284369114e-06,
      "loss": 0.4897,
      "step": 1579
    },
    {
      "epoch": 0.049600722035827105,
      "grad_norm": 0.13883697986602783,
      "learning_rate": 4.394224733207784e-06,
      "loss": 0.1019,
      "step": 1580
    },
    {
      "epoch": 0.0496321148978751,
      "grad_norm": 0.11198721081018448,
      "learning_rate": 4.0803515379786566e-06,
      "loss": 0.1528,
      "step": 1581
    },
    {
      "epoch": 0.049663507759923085,
      "grad_norm": 0.2419990599155426,
      "learning_rate": 3.766478342749529e-06,
      "loss": 0.247,
      "step": 1582
    },
    {
      "epoch": 0.04969490062197108,
      "grad_norm": 0.0610719732940197,
      "learning_rate": 3.4526051475204017e-06,
      "loss": 0.0491,
      "step": 1583
    },
    {
      "epoch": 0.04972629348401907,
      "grad_norm": 0.1005571261048317,
      "learning_rate": 3.1387319522912743e-06,
      "loss": 0.1117,
      "step": 1584
    },
    {
      "epoch": 0.049757686346067066,
      "grad_norm": 0.11454711109399796,
      "learning_rate": 2.824858757062147e-06,
      "loss": 0.1563,
      "step": 1585
    },
    {
      "epoch": 0.04978907920811505,
      "grad_norm": 0.07460258156061172,
      "learning_rate": 2.51098556183302e-06,
      "loss": 0.0576,
      "step": 1586
    },
    {
      "epoch": 0.04982047207016305,
      "grad_norm": 0.1130179762840271,
      "learning_rate": 2.197112366603892e-06,
      "loss": 0.099,
      "step": 1587
    },
    {
      "epoch": 0.04985186493221104,
      "grad_norm": 0.31168025732040405,
      "learning_rate": 1.8832391713747646e-06,
      "loss": 0.3886,
      "step": 1588
    },
    {
      "epoch": 0.04988325779425903,
      "grad_norm": 0.07869655638933182,
      "learning_rate": 1.5693659761456371e-06,
      "loss": 0.0815,
      "step": 1589
    },
    {
      "epoch": 0.04991465065630702,
      "grad_norm": 0.22711369395256042,
      "learning_rate": 1.25549278091651e-06,
      "loss": 0.2665,
      "step": 1590
    },
    {
      "epoch": 0.049946043518355014,
      "grad_norm": 0.31388357281684875,
      "learning_rate": 9.416195856873823e-07,
      "loss": 0.4315,
      "step": 1591
    },
    {
      "epoch": 0.04997743638040301,
      "grad_norm": 0.27848368883132935,
      "learning_rate": 6.27746390458255e-07,
      "loss": 0.2772,
      "step": 1592
    },
    {
      "epoch": 0.050008829242450994,
      "grad_norm": 0.13562534749507904,
      "learning_rate": 3.138731952291275e-07,
      "loss": 0.0929,
      "step": 1593
    }
  ],
  "logging_steps": 1,
  "max_steps": 1593,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3471677092832256.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
