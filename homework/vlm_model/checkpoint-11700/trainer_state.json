{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.04999273609817377,
  "eval_steps": 500,
  "global_step": 11700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.272883427194339e-06,
      "grad_norm": 23.990047454833984,
      "learning_rate": 0.0005,
      "loss": 5.9738,
      "step": 1
    },
    {
      "epoch": 8.545766854388679e-06,
      "grad_norm": 28.768457412719727,
      "learning_rate": 0.000499957272261152,
      "loss": 6.8246,
      "step": 2
    },
    {
      "epoch": 1.2818650281583018e-05,
      "grad_norm": 39.573524475097656,
      "learning_rate": 0.0004999145445223039,
      "loss": 6.3932,
      "step": 3
    },
    {
      "epoch": 1.7091533708777357e-05,
      "grad_norm": 52.87775802612305,
      "learning_rate": 0.0004998718167834558,
      "loss": 3.3643,
      "step": 4
    },
    {
      "epoch": 2.1364417135971697e-05,
      "grad_norm": 33.26308059692383,
      "learning_rate": 0.0004998290890446078,
      "loss": 3.906,
      "step": 5
    },
    {
      "epoch": 2.5637300563166036e-05,
      "grad_norm": 68.92984771728516,
      "learning_rate": 0.0004997863613057597,
      "loss": 1.6216,
      "step": 6
    },
    {
      "epoch": 2.9910183990360375e-05,
      "grad_norm": 9.880203247070312,
      "learning_rate": 0.0004997436335669117,
      "loss": 0.7853,
      "step": 7
    },
    {
      "epoch": 3.4183067417554715e-05,
      "grad_norm": 18.457103729248047,
      "learning_rate": 0.0004997009058280635,
      "loss": 1.6772,
      "step": 8
    },
    {
      "epoch": 3.845595084474905e-05,
      "grad_norm": 87.22883605957031,
      "learning_rate": 0.0004996581780892155,
      "loss": 8.0908,
      "step": 9
    },
    {
      "epoch": 4.272883427194339e-05,
      "grad_norm": 35.773929595947266,
      "learning_rate": 0.0004996154503503675,
      "loss": 3.805,
      "step": 10
    },
    {
      "epoch": 4.700171769913773e-05,
      "grad_norm": 19.920673370361328,
      "learning_rate": 0.0004995727226115194,
      "loss": 1.7089,
      "step": 11
    },
    {
      "epoch": 5.127460112633207e-05,
      "grad_norm": 17.866832733154297,
      "learning_rate": 0.0004995299948726714,
      "loss": 5.8961,
      "step": 12
    },
    {
      "epoch": 5.554748455352641e-05,
      "grad_norm": 12.889073371887207,
      "learning_rate": 0.0004994872671338233,
      "loss": 3.7659,
      "step": 13
    },
    {
      "epoch": 5.982036798072075e-05,
      "grad_norm": 6.077735424041748,
      "learning_rate": 0.0004994445393949752,
      "loss": 1.3439,
      "step": 14
    },
    {
      "epoch": 6.409325140791509e-05,
      "grad_norm": 8.332498550415039,
      "learning_rate": 0.0004994018116561272,
      "loss": 4.9111,
      "step": 15
    },
    {
      "epoch": 6.836613483510943e-05,
      "grad_norm": 11.103389739990234,
      "learning_rate": 0.0004993590839172792,
      "loss": 5.1347,
      "step": 16
    },
    {
      "epoch": 7.263901826230377e-05,
      "grad_norm": 18.594257354736328,
      "learning_rate": 0.0004993163561784311,
      "loss": 2.2965,
      "step": 17
    },
    {
      "epoch": 7.69119016894981e-05,
      "grad_norm": 32.02130889892578,
      "learning_rate": 0.000499273628439583,
      "loss": 1.3346,
      "step": 18
    },
    {
      "epoch": 8.118478511669244e-05,
      "grad_norm": 18.847986221313477,
      "learning_rate": 0.0004992309007007349,
      "loss": 1.8135,
      "step": 19
    },
    {
      "epoch": 8.545766854388679e-05,
      "grad_norm": 15.38010311126709,
      "learning_rate": 0.0004991881729618868,
      "loss": 1.2128,
      "step": 20
    },
    {
      "epoch": 8.973055197108113e-05,
      "grad_norm": 32.322418212890625,
      "learning_rate": 0.0004991454452230388,
      "loss": 6.885,
      "step": 21
    },
    {
      "epoch": 9.400343539827546e-05,
      "grad_norm": 5.585812091827393,
      "learning_rate": 0.0004991027174841907,
      "loss": 1.033,
      "step": 22
    },
    {
      "epoch": 9.82763188254698e-05,
      "grad_norm": 26.53032875061035,
      "learning_rate": 0.0004990599897453427,
      "loss": 4.3093,
      "step": 23
    },
    {
      "epoch": 0.00010254920225266414,
      "grad_norm": 9.588038444519043,
      "learning_rate": 0.0004990172620064947,
      "loss": 0.3445,
      "step": 24
    },
    {
      "epoch": 0.00010682208567985849,
      "grad_norm": 11.835704803466797,
      "learning_rate": 0.0004989745342676465,
      "loss": 4.2717,
      "step": 25
    },
    {
      "epoch": 0.00011109496910705282,
      "grad_norm": 13.661279678344727,
      "learning_rate": 0.0004989318065287985,
      "loss": 1.1529,
      "step": 26
    },
    {
      "epoch": 0.00011536785253424716,
      "grad_norm": 54.24889373779297,
      "learning_rate": 0.0004988890787899504,
      "loss": 7.3546,
      "step": 27
    },
    {
      "epoch": 0.0001196407359614415,
      "grad_norm": 13.454455375671387,
      "learning_rate": 0.0004988463510511024,
      "loss": 1.3964,
      "step": 28
    },
    {
      "epoch": 0.00012391361938863583,
      "grad_norm": 22.158960342407227,
      "learning_rate": 0.0004988036233122543,
      "loss": 6.2055,
      "step": 29
    },
    {
      "epoch": 0.00012818650281583017,
      "grad_norm": 10.468451499938965,
      "learning_rate": 0.0004987608955734062,
      "loss": 4.095,
      "step": 30
    },
    {
      "epoch": 0.00013245938624302452,
      "grad_norm": 9.739567756652832,
      "learning_rate": 0.0004987181678345582,
      "loss": 5.8197,
      "step": 31
    },
    {
      "epoch": 0.00013673226967021886,
      "grad_norm": 10.78761100769043,
      "learning_rate": 0.0004986754400957102,
      "loss": 1.4363,
      "step": 32
    },
    {
      "epoch": 0.0001410051530974132,
      "grad_norm": 11.372011184692383,
      "learning_rate": 0.0004986327123568621,
      "loss": 7.0849,
      "step": 33
    },
    {
      "epoch": 0.00014527803652460754,
      "grad_norm": 3.9881396293640137,
      "learning_rate": 0.000498589984618014,
      "loss": 0.9665,
      "step": 34
    },
    {
      "epoch": 0.0001495509199518019,
      "grad_norm": 27.56332015991211,
      "learning_rate": 0.000498547256879166,
      "loss": 5.0698,
      "step": 35
    },
    {
      "epoch": 0.0001538238033789962,
      "grad_norm": 8.091049194335938,
      "learning_rate": 0.0004985045291403179,
      "loss": 4.7831,
      "step": 36
    },
    {
      "epoch": 0.00015809668680619055,
      "grad_norm": 13.709970474243164,
      "learning_rate": 0.0004984618014014699,
      "loss": 4.5401,
      "step": 37
    },
    {
      "epoch": 0.0001623695702333849,
      "grad_norm": 5.049543857574463,
      "learning_rate": 0.0004984190736626217,
      "loss": 3.0624,
      "step": 38
    },
    {
      "epoch": 0.00016664245366057923,
      "grad_norm": 2.5381274223327637,
      "learning_rate": 0.0004983763459237737,
      "loss": 0.928,
      "step": 39
    },
    {
      "epoch": 0.00017091533708777357,
      "grad_norm": 3.113624095916748,
      "learning_rate": 0.0004983336181849257,
      "loss": 1.3433,
      "step": 40
    },
    {
      "epoch": 0.00017518822051496792,
      "grad_norm": 10.5970458984375,
      "learning_rate": 0.0004982908904460776,
      "loss": 1.0791,
      "step": 41
    },
    {
      "epoch": 0.00017946110394216226,
      "grad_norm": 15.586479187011719,
      "learning_rate": 0.0004982481627072296,
      "loss": 6.4365,
      "step": 42
    },
    {
      "epoch": 0.0001837339873693566,
      "grad_norm": 11.523524284362793,
      "learning_rate": 0.0004982054349683815,
      "loss": 6.233,
      "step": 43
    },
    {
      "epoch": 0.00018800687079655092,
      "grad_norm": 10.914887428283691,
      "learning_rate": 0.0004981627072295334,
      "loss": 3.3301,
      "step": 44
    },
    {
      "epoch": 0.00019227975422374526,
      "grad_norm": 5.278870582580566,
      "learning_rate": 0.0004981199794906854,
      "loss": 0.4548,
      "step": 45
    },
    {
      "epoch": 0.0001965526376509396,
      "grad_norm": 10.512524604797363,
      "learning_rate": 0.0004980772517518373,
      "loss": 4.8451,
      "step": 46
    },
    {
      "epoch": 0.00020082552107813395,
      "grad_norm": 3.050786018371582,
      "learning_rate": 0.0004980345240129893,
      "loss": 1.0041,
      "step": 47
    },
    {
      "epoch": 0.0002050984045053283,
      "grad_norm": 1.524299144744873,
      "learning_rate": 0.0004979917962741412,
      "loss": 0.0811,
      "step": 48
    },
    {
      "epoch": 0.00020937128793252263,
      "grad_norm": 2.9409220218658447,
      "learning_rate": 0.0004979490685352931,
      "loss": 0.8687,
      "step": 49
    },
    {
      "epoch": 0.00021364417135971697,
      "grad_norm": 6.465839862823486,
      "learning_rate": 0.0004979063407964451,
      "loss": 1.0438,
      "step": 50
    },
    {
      "epoch": 0.00021791705478691132,
      "grad_norm": 3.229707717895508,
      "learning_rate": 0.000497863613057597,
      "loss": 0.5073,
      "step": 51
    },
    {
      "epoch": 0.00022218993821410563,
      "grad_norm": 13.382052421569824,
      "learning_rate": 0.0004978208853187489,
      "loss": 3.024,
      "step": 52
    },
    {
      "epoch": 0.00022646282164129997,
      "grad_norm": 11.071243286132812,
      "learning_rate": 0.0004977781575799009,
      "loss": 1.1058,
      "step": 53
    },
    {
      "epoch": 0.00023073570506849432,
      "grad_norm": 6.480239391326904,
      "learning_rate": 0.0004977354298410529,
      "loss": 2.1332,
      "step": 54
    },
    {
      "epoch": 0.00023500858849568866,
      "grad_norm": 18.559598922729492,
      "learning_rate": 0.0004976927021022047,
      "loss": 2.7801,
      "step": 55
    },
    {
      "epoch": 0.000239281471922883,
      "grad_norm": 9.887965202331543,
      "learning_rate": 0.0004976499743633567,
      "loss": 4.3596,
      "step": 56
    },
    {
      "epoch": 0.00024355435535007735,
      "grad_norm": 3.1099071502685547,
      "learning_rate": 0.0004976072466245086,
      "loss": 0.1357,
      "step": 57
    },
    {
      "epoch": 0.00024782723877727166,
      "grad_norm": 11.252128601074219,
      "learning_rate": 0.0004975645188856606,
      "loss": 1.075,
      "step": 58
    },
    {
      "epoch": 0.000252100122204466,
      "grad_norm": 45.12053680419922,
      "learning_rate": 0.0004975217911468125,
      "loss": 6.7722,
      "step": 59
    },
    {
      "epoch": 0.00025637300563166035,
      "grad_norm": 20.79254150390625,
      "learning_rate": 0.0004974790634079644,
      "loss": 1.6774,
      "step": 60
    },
    {
      "epoch": 0.0002606458890588547,
      "grad_norm": 42.610538482666016,
      "learning_rate": 0.0004974363356691164,
      "loss": 1.8421,
      "step": 61
    },
    {
      "epoch": 0.00026491877248604903,
      "grad_norm": 3.107830047607422,
      "learning_rate": 0.0004973936079302684,
      "loss": 0.7187,
      "step": 62
    },
    {
      "epoch": 0.0002691916559132434,
      "grad_norm": 3.757321834564209,
      "learning_rate": 0.0004973508801914203,
      "loss": 0.366,
      "step": 63
    },
    {
      "epoch": 0.0002734645393404377,
      "grad_norm": 6.336926460266113,
      "learning_rate": 0.0004973081524525722,
      "loss": 1.6698,
      "step": 64
    },
    {
      "epoch": 0.00027773742276763206,
      "grad_norm": 8.449953079223633,
      "learning_rate": 0.0004972654247137241,
      "loss": 1.4435,
      "step": 65
    },
    {
      "epoch": 0.0002820103061948264,
      "grad_norm": 7.913733959197998,
      "learning_rate": 0.0004972226969748761,
      "loss": 1.1533,
      "step": 66
    },
    {
      "epoch": 0.00028628318962202075,
      "grad_norm": 6.319676876068115,
      "learning_rate": 0.0004971799692360281,
      "loss": 0.7039,
      "step": 67
    },
    {
      "epoch": 0.0002905560730492151,
      "grad_norm": 7.725971698760986,
      "learning_rate": 0.00049713724149718,
      "loss": 1.1649,
      "step": 68
    },
    {
      "epoch": 0.00029482895647640943,
      "grad_norm": 3.585078477859497,
      "learning_rate": 0.0004970945137583319,
      "loss": 0.1615,
      "step": 69
    },
    {
      "epoch": 0.0002991018399036038,
      "grad_norm": 17.19951820373535,
      "learning_rate": 0.0004970517860194839,
      "loss": 4.2098,
      "step": 70
    },
    {
      "epoch": 0.00030337472333079806,
      "grad_norm": 18.18385124206543,
      "learning_rate": 0.0004970090582806358,
      "loss": 2.1922,
      "step": 71
    },
    {
      "epoch": 0.0003076476067579924,
      "grad_norm": 17.497447967529297,
      "learning_rate": 0.0004969663305417878,
      "loss": 3.1291,
      "step": 72
    },
    {
      "epoch": 0.00031192049018518675,
      "grad_norm": 31.867694854736328,
      "learning_rate": 0.0004969236028029396,
      "loss": 3.6903,
      "step": 73
    },
    {
      "epoch": 0.0003161933736123811,
      "grad_norm": 3.845252752304077,
      "learning_rate": 0.0004968808750640916,
      "loss": 0.8097,
      "step": 74
    },
    {
      "epoch": 0.00032046625703957543,
      "grad_norm": 2.3014438152313232,
      "learning_rate": 0.0004968381473252436,
      "loss": 0.092,
      "step": 75
    },
    {
      "epoch": 0.0003247391404667698,
      "grad_norm": 9.98763656616211,
      "learning_rate": 0.0004967954195863955,
      "loss": 4.4991,
      "step": 76
    },
    {
      "epoch": 0.0003290120238939641,
      "grad_norm": 11.941662788391113,
      "learning_rate": 0.0004967526918475475,
      "loss": 2.984,
      "step": 77
    },
    {
      "epoch": 0.00033328490732115846,
      "grad_norm": 0.3887837827205658,
      "learning_rate": 0.0004967099641086994,
      "loss": 0.0239,
      "step": 78
    },
    {
      "epoch": 0.0003375577907483528,
      "grad_norm": 0.2790675163269043,
      "learning_rate": 0.0004966672363698513,
      "loss": 0.0164,
      "step": 79
    },
    {
      "epoch": 0.00034183067417554715,
      "grad_norm": 6.411506175994873,
      "learning_rate": 0.0004966245086310033,
      "loss": 1.3719,
      "step": 80
    },
    {
      "epoch": 0.0003461035576027415,
      "grad_norm": 10.604838371276855,
      "learning_rate": 0.0004965817808921553,
      "loss": 1.7641,
      "step": 81
    },
    {
      "epoch": 0.00035037644102993583,
      "grad_norm": 7.937271595001221,
      "learning_rate": 0.0004965390531533071,
      "loss": 1.5431,
      "step": 82
    },
    {
      "epoch": 0.0003546493244571302,
      "grad_norm": 7.62607479095459,
      "learning_rate": 0.0004964963254144591,
      "loss": 1.895,
      "step": 83
    },
    {
      "epoch": 0.0003589222078843245,
      "grad_norm": 3.203352212905884,
      "learning_rate": 0.000496453597675611,
      "loss": 0.9554,
      "step": 84
    },
    {
      "epoch": 0.00036319509131151886,
      "grad_norm": 5.590572357177734,
      "learning_rate": 0.0004964108699367629,
      "loss": 1.0898,
      "step": 85
    },
    {
      "epoch": 0.0003674679747387132,
      "grad_norm": 21.124691009521484,
      "learning_rate": 0.0004963681421979149,
      "loss": 3.9825,
      "step": 86
    },
    {
      "epoch": 0.0003717408581659075,
      "grad_norm": 8.958699226379395,
      "learning_rate": 0.0004963254144590668,
      "loss": 1.4928,
      "step": 87
    },
    {
      "epoch": 0.00037601374159310183,
      "grad_norm": 3.231574773788452,
      "learning_rate": 0.0004962826867202188,
      "loss": 0.2264,
      "step": 88
    },
    {
      "epoch": 0.0003802866250202962,
      "grad_norm": 5.911774158477783,
      "learning_rate": 0.0004962399589813706,
      "loss": 0.9364,
      "step": 89
    },
    {
      "epoch": 0.0003845595084474905,
      "grad_norm": 1.636456847190857,
      "learning_rate": 0.0004961972312425226,
      "loss": 0.1383,
      "step": 90
    },
    {
      "epoch": 0.00038883239187468486,
      "grad_norm": 6.825779914855957,
      "learning_rate": 0.0004961545035036746,
      "loss": 0.9396,
      "step": 91
    },
    {
      "epoch": 0.0003931052753018792,
      "grad_norm": 2.4672625064849854,
      "learning_rate": 0.0004961117757648265,
      "loss": 0.6344,
      "step": 92
    },
    {
      "epoch": 0.00039737815872907355,
      "grad_norm": 2.3548614978790283,
      "learning_rate": 0.0004960690480259785,
      "loss": 0.5736,
      "step": 93
    },
    {
      "epoch": 0.0004016510421562679,
      "grad_norm": 7.073177814483643,
      "learning_rate": 0.0004960263202871304,
      "loss": 1.3217,
      "step": 94
    },
    {
      "epoch": 0.00040592392558346223,
      "grad_norm": 8.040250778198242,
      "learning_rate": 0.0004959835925482823,
      "loss": 0.9688,
      "step": 95
    },
    {
      "epoch": 0.0004101968090106566,
      "grad_norm": 15.036831855773926,
      "learning_rate": 0.0004959408648094343,
      "loss": 2.0861,
      "step": 96
    },
    {
      "epoch": 0.0004144696924378509,
      "grad_norm": 3.8126413822174072,
      "learning_rate": 0.0004958981370705863,
      "loss": 0.8096,
      "step": 97
    },
    {
      "epoch": 0.00041874257586504526,
      "grad_norm": 8.270788192749023,
      "learning_rate": 0.0004958554093317382,
      "loss": 1.8851,
      "step": 98
    },
    {
      "epoch": 0.0004230154592922396,
      "grad_norm": 6.122739315032959,
      "learning_rate": 0.0004958126815928901,
      "loss": 0.3706,
      "step": 99
    },
    {
      "epoch": 0.00042728834271943395,
      "grad_norm": 2.9924397468566895,
      "learning_rate": 0.000495769953854042,
      "loss": 0.5341,
      "step": 100
    },
    {
      "epoch": 0.0004315612261466283,
      "grad_norm": 2.484274387359619,
      "learning_rate": 0.000495727226115194,
      "loss": 0.5274,
      "step": 101
    },
    {
      "epoch": 0.00043583410957382263,
      "grad_norm": 51.14118957519531,
      "learning_rate": 0.000495684498376346,
      "loss": 4.7739,
      "step": 102
    },
    {
      "epoch": 0.0004401069930010169,
      "grad_norm": 29.78073501586914,
      "learning_rate": 0.0004956417706374978,
      "loss": 0.7386,
      "step": 103
    },
    {
      "epoch": 0.00044437987642821126,
      "grad_norm": 85.8257064819336,
      "learning_rate": 0.0004955990428986498,
      "loss": 5.1617,
      "step": 104
    },
    {
      "epoch": 0.0004486527598554056,
      "grad_norm": 3.9842917919158936,
      "learning_rate": 0.0004955563151598018,
      "loss": 0.1098,
      "step": 105
    },
    {
      "epoch": 0.00045292564328259995,
      "grad_norm": 73.5184097290039,
      "learning_rate": 0.0004955135874209537,
      "loss": 2.9206,
      "step": 106
    },
    {
      "epoch": 0.0004571985267097943,
      "grad_norm": 1.8114815950393677,
      "learning_rate": 0.0004954708596821057,
      "loss": 0.0558,
      "step": 107
    },
    {
      "epoch": 0.00046147141013698863,
      "grad_norm": 9.363116264343262,
      "learning_rate": 0.0004954281319432575,
      "loss": 0.8485,
      "step": 108
    },
    {
      "epoch": 0.000465744293564183,
      "grad_norm": 36.09660339355469,
      "learning_rate": 0.0004953854042044095,
      "loss": 6.4951,
      "step": 109
    },
    {
      "epoch": 0.0004700171769913773,
      "grad_norm": 6.5467000007629395,
      "learning_rate": 0.0004953426764655615,
      "loss": 1.3686,
      "step": 110
    },
    {
      "epoch": 0.00047429006041857166,
      "grad_norm": 2.048732280731201,
      "learning_rate": 0.0004952999487267134,
      "loss": 0.0661,
      "step": 111
    },
    {
      "epoch": 0.000478562943845766,
      "grad_norm": 38.671875,
      "learning_rate": 0.0004952572209878653,
      "loss": 2.1009,
      "step": 112
    },
    {
      "epoch": 0.00048283582727296035,
      "grad_norm": 26.1302433013916,
      "learning_rate": 0.0004952144932490173,
      "loss": 1.4731,
      "step": 113
    },
    {
      "epoch": 0.0004871087107001547,
      "grad_norm": 0.3332644999027252,
      "learning_rate": 0.0004951717655101692,
      "loss": 0.0217,
      "step": 114
    },
    {
      "epoch": 0.000491381594127349,
      "grad_norm": 20.18718147277832,
      "learning_rate": 0.0004951290377713212,
      "loss": 5.3017,
      "step": 115
    },
    {
      "epoch": 0.0004956544775545433,
      "grad_norm": 38.00613021850586,
      "learning_rate": 0.0004950863100324732,
      "loss": 5.666,
      "step": 116
    },
    {
      "epoch": 0.0004999273609817377,
      "grad_norm": 10.932339668273926,
      "learning_rate": 0.000495043582293625,
      "loss": 2.1456,
      "step": 117
    },
    {
      "epoch": 0.000504200244408932,
      "grad_norm": 15.435791015625,
      "learning_rate": 0.000495000854554777,
      "loss": 3.5562,
      "step": 118
    },
    {
      "epoch": 0.0005084731278361264,
      "grad_norm": 21.458112716674805,
      "learning_rate": 0.0004949581268159288,
      "loss": 3.4667,
      "step": 119
    },
    {
      "epoch": 0.0005127460112633207,
      "grad_norm": 12.373517990112305,
      "learning_rate": 0.0004949153990770808,
      "loss": 2.5143,
      "step": 120
    },
    {
      "epoch": 0.0005170188946905151,
      "grad_norm": 18.087635040283203,
      "learning_rate": 0.0004948726713382328,
      "loss": 4.1752,
      "step": 121
    },
    {
      "epoch": 0.0005212917781177094,
      "grad_norm": 9.717264175415039,
      "learning_rate": 0.0004948299435993847,
      "loss": 1.992,
      "step": 122
    },
    {
      "epoch": 0.0005255646615449038,
      "grad_norm": 3.6003952026367188,
      "learning_rate": 0.0004947872158605367,
      "loss": 1.2824,
      "step": 123
    },
    {
      "epoch": 0.0005298375449720981,
      "grad_norm": 32.302452087402344,
      "learning_rate": 0.0004947444881216886,
      "loss": 4.6815,
      "step": 124
    },
    {
      "epoch": 0.0005341104283992925,
      "grad_norm": 8.331195831298828,
      "learning_rate": 0.0004947017603828405,
      "loss": 2.2874,
      "step": 125
    },
    {
      "epoch": 0.0005383833118264867,
      "grad_norm": 2.122187852859497,
      "learning_rate": 0.0004946590326439925,
      "loss": 0.0671,
      "step": 126
    },
    {
      "epoch": 0.000542656195253681,
      "grad_norm": 5.188237190246582,
      "learning_rate": 0.0004946163049051444,
      "loss": 1.0177,
      "step": 127
    },
    {
      "epoch": 0.0005469290786808754,
      "grad_norm": 2.2050957679748535,
      "learning_rate": 0.0004945735771662964,
      "loss": 0.5632,
      "step": 128
    },
    {
      "epoch": 0.0005512019621080697,
      "grad_norm": 13.69688892364502,
      "learning_rate": 0.0004945308494274483,
      "loss": 2.3318,
      "step": 129
    },
    {
      "epoch": 0.0005554748455352641,
      "grad_norm": 0.5412591695785522,
      "learning_rate": 0.0004944881216886002,
      "loss": 0.0424,
      "step": 130
    },
    {
      "epoch": 0.0005597477289624584,
      "grad_norm": 2.541922092437744,
      "learning_rate": 0.0004944453939497522,
      "loss": 1.4025,
      "step": 131
    },
    {
      "epoch": 0.0005640206123896528,
      "grad_norm": 1.6136952638626099,
      "learning_rate": 0.0004944026662109042,
      "loss": 0.3712,
      "step": 132
    },
    {
      "epoch": 0.0005682934958168471,
      "grad_norm": 24.551103591918945,
      "learning_rate": 0.000494359938472056,
      "loss": 3.3867,
      "step": 133
    },
    {
      "epoch": 0.0005725663792440415,
      "grad_norm": 3.34932017326355,
      "learning_rate": 0.000494317210733208,
      "loss": 1.5804,
      "step": 134
    },
    {
      "epoch": 0.0005768392626712358,
      "grad_norm": 5.844152927398682,
      "learning_rate": 0.00049427448299436,
      "loss": 0.9571,
      "step": 135
    },
    {
      "epoch": 0.0005811121460984302,
      "grad_norm": 3.462252140045166,
      "learning_rate": 0.0004942317552555119,
      "loss": 1.1772,
      "step": 136
    },
    {
      "epoch": 0.0005853850295256245,
      "grad_norm": 4.1638336181640625,
      "learning_rate": 0.0004941890275166639,
      "loss": 0.6804,
      "step": 137
    },
    {
      "epoch": 0.0005896579129528189,
      "grad_norm": 8.962660789489746,
      "learning_rate": 0.0004941462997778157,
      "loss": 2.8215,
      "step": 138
    },
    {
      "epoch": 0.0005939307963800132,
      "grad_norm": 3.9573564529418945,
      "learning_rate": 0.0004941035720389677,
      "loss": 1.5655,
      "step": 139
    },
    {
      "epoch": 0.0005982036798072075,
      "grad_norm": 6.067838668823242,
      "learning_rate": 0.0004940608443001197,
      "loss": 1.9688,
      "step": 140
    },
    {
      "epoch": 0.0006024765632344018,
      "grad_norm": 3.7729508876800537,
      "learning_rate": 0.0004940181165612716,
      "loss": 0.4122,
      "step": 141
    },
    {
      "epoch": 0.0006067494466615961,
      "grad_norm": 18.799564361572266,
      "learning_rate": 0.0004939753888224235,
      "loss": 2.6324,
      "step": 142
    },
    {
      "epoch": 0.0006110223300887905,
      "grad_norm": 8.626225471496582,
      "learning_rate": 0.0004939326610835755,
      "loss": 1.4106,
      "step": 143
    },
    {
      "epoch": 0.0006152952135159848,
      "grad_norm": 11.83678150177002,
      "learning_rate": 0.0004938899333447274,
      "loss": 1.6453,
      "step": 144
    },
    {
      "epoch": 0.0006195680969431792,
      "grad_norm": 5.5737714767456055,
      "learning_rate": 0.0004938472056058794,
      "loss": 0.8945,
      "step": 145
    },
    {
      "epoch": 0.0006238409803703735,
      "grad_norm": 19.839929580688477,
      "learning_rate": 0.0004938044778670313,
      "loss": 4.7726,
      "step": 146
    },
    {
      "epoch": 0.0006281138637975679,
      "grad_norm": 2.1503498554229736,
      "learning_rate": 0.0004937617501281832,
      "loss": 0.7969,
      "step": 147
    },
    {
      "epoch": 0.0006323867472247622,
      "grad_norm": 4.427889823913574,
      "learning_rate": 0.0004937190223893352,
      "loss": 2.79,
      "step": 148
    },
    {
      "epoch": 0.0006366596306519566,
      "grad_norm": 1.4427237510681152,
      "learning_rate": 0.000493676294650487,
      "loss": 0.1588,
      "step": 149
    },
    {
      "epoch": 0.0006409325140791509,
      "grad_norm": 2.733243942260742,
      "learning_rate": 0.000493633566911639,
      "loss": 0.7925,
      "step": 150
    },
    {
      "epoch": 0.0006452053975063453,
      "grad_norm": 1.6667823791503906,
      "learning_rate": 0.000493590839172791,
      "loss": 0.4535,
      "step": 151
    },
    {
      "epoch": 0.0006494782809335396,
      "grad_norm": 1.6783933639526367,
      "learning_rate": 0.0004935481114339429,
      "loss": 0.4401,
      "step": 152
    },
    {
      "epoch": 0.000653751164360734,
      "grad_norm": 2.943074941635132,
      "learning_rate": 0.0004935053836950949,
      "loss": 0.4955,
      "step": 153
    },
    {
      "epoch": 0.0006580240477879282,
      "grad_norm": 9.821622848510742,
      "learning_rate": 0.0004934626559562467,
      "loss": 3.017,
      "step": 154
    },
    {
      "epoch": 0.0006622969312151226,
      "grad_norm": 0.3022940754890442,
      "learning_rate": 0.0004934199282173987,
      "loss": 0.0254,
      "step": 155
    },
    {
      "epoch": 0.0006665698146423169,
      "grad_norm": 3.9394114017486572,
      "learning_rate": 0.0004933772004785507,
      "loss": 0.9544,
      "step": 156
    },
    {
      "epoch": 0.0006708426980695113,
      "grad_norm": 12.642718315124512,
      "learning_rate": 0.0004933344727397026,
      "loss": 2.9926,
      "step": 157
    },
    {
      "epoch": 0.0006751155814967056,
      "grad_norm": 5.654076099395752,
      "learning_rate": 0.0004932917450008546,
      "loss": 1.1744,
      "step": 158
    },
    {
      "epoch": 0.0006793884649238999,
      "grad_norm": 0.33727362751960754,
      "learning_rate": 0.0004932490172620065,
      "loss": 0.0282,
      "step": 159
    },
    {
      "epoch": 0.0006836613483510943,
      "grad_norm": 6.364847183227539,
      "learning_rate": 0.0004932062895231584,
      "loss": 1.4727,
      "step": 160
    },
    {
      "epoch": 0.0006879342317782886,
      "grad_norm": 5.701667785644531,
      "learning_rate": 0.0004931635617843104,
      "loss": 1.2199,
      "step": 161
    },
    {
      "epoch": 0.000692207115205483,
      "grad_norm": 4.52414083480835,
      "learning_rate": 0.0004931208340454624,
      "loss": 1.7141,
      "step": 162
    },
    {
      "epoch": 0.0006964799986326773,
      "grad_norm": 7.158315658569336,
      "learning_rate": 0.0004930781063066142,
      "loss": 2.039,
      "step": 163
    },
    {
      "epoch": 0.0007007528820598717,
      "grad_norm": 0.47692349553108215,
      "learning_rate": 0.0004930353785677662,
      "loss": 0.0229,
      "step": 164
    },
    {
      "epoch": 0.000705025765487066,
      "grad_norm": 8.712705612182617,
      "learning_rate": 0.0004929926508289181,
      "loss": 2.7534,
      "step": 165
    },
    {
      "epoch": 0.0007092986489142604,
      "grad_norm": 6.281673908233643,
      "learning_rate": 0.0004929499230900701,
      "loss": 0.9451,
      "step": 166
    },
    {
      "epoch": 0.0007135715323414546,
      "grad_norm": 24.200098037719727,
      "learning_rate": 0.0004929071953512221,
      "loss": 3.9534,
      "step": 167
    },
    {
      "epoch": 0.000717844415768649,
      "grad_norm": 3.9437031745910645,
      "learning_rate": 0.0004928644676123739,
      "loss": 1.3733,
      "step": 168
    },
    {
      "epoch": 0.0007221172991958433,
      "grad_norm": 12.318136215209961,
      "learning_rate": 0.0004928217398735259,
      "loss": 3.3545,
      "step": 169
    },
    {
      "epoch": 0.0007263901826230377,
      "grad_norm": 4.703141212463379,
      "learning_rate": 0.0004927790121346779,
      "loss": 1.0082,
      "step": 170
    },
    {
      "epoch": 0.000730663066050232,
      "grad_norm": 2.1206114292144775,
      "learning_rate": 0.0004927362843958298,
      "loss": 0.3498,
      "step": 171
    },
    {
      "epoch": 0.0007349359494774264,
      "grad_norm": 1.984127402305603,
      "learning_rate": 0.0004926935566569818,
      "loss": 0.8906,
      "step": 172
    },
    {
      "epoch": 0.0007392088329046207,
      "grad_norm": 3.585676431655884,
      "learning_rate": 0.0004926508289181336,
      "loss": 0.7308,
      "step": 173
    },
    {
      "epoch": 0.000743481716331815,
      "grad_norm": 2.774977922439575,
      "learning_rate": 0.0004926081011792856,
      "loss": 1.0453,
      "step": 174
    },
    {
      "epoch": 0.0007477545997590094,
      "grad_norm": 2.226771593093872,
      "learning_rate": 0.0004925653734404376,
      "loss": 0.371,
      "step": 175
    },
    {
      "epoch": 0.0007520274831862037,
      "grad_norm": 3.1876513957977295,
      "learning_rate": 0.0004925226457015895,
      "loss": 1.5743,
      "step": 176
    },
    {
      "epoch": 0.0007563003666133981,
      "grad_norm": 5.625126838684082,
      "learning_rate": 0.0004924799179627414,
      "loss": 1.5821,
      "step": 177
    },
    {
      "epoch": 0.0007605732500405924,
      "grad_norm": 2.3537933826446533,
      "learning_rate": 0.0004924371902238934,
      "loss": 0.3165,
      "step": 178
    },
    {
      "epoch": 0.0007648461334677868,
      "grad_norm": 5.018526077270508,
      "learning_rate": 0.0004923944624850453,
      "loss": 1.7683,
      "step": 179
    },
    {
      "epoch": 0.000769119016894981,
      "grad_norm": 3.1426055431365967,
      "learning_rate": 0.0004923517347461973,
      "loss": 1.5674,
      "step": 180
    },
    {
      "epoch": 0.0007733919003221754,
      "grad_norm": 1.9635168313980103,
      "learning_rate": 0.0004923090070073493,
      "loss": 0.7475,
      "step": 181
    },
    {
      "epoch": 0.0007776647837493697,
      "grad_norm": 1.8505511283874512,
      "learning_rate": 0.0004922662792685011,
      "loss": 0.3363,
      "step": 182
    },
    {
      "epoch": 0.0007819376671765641,
      "grad_norm": 1.9066898822784424,
      "learning_rate": 0.0004922235515296531,
      "loss": 0.6811,
      "step": 183
    },
    {
      "epoch": 0.0007862105506037584,
      "grad_norm": 1.2060762643814087,
      "learning_rate": 0.0004921808237908049,
      "loss": 0.1799,
      "step": 184
    },
    {
      "epoch": 0.0007904834340309528,
      "grad_norm": 1.1316092014312744,
      "learning_rate": 0.0004921380960519569,
      "loss": 0.1542,
      "step": 185
    },
    {
      "epoch": 0.0007947563174581471,
      "grad_norm": 6.566196441650391,
      "learning_rate": 0.0004920953683131089,
      "loss": 1.2054,
      "step": 186
    },
    {
      "epoch": 0.0007990292008853415,
      "grad_norm": 1.7150905132293701,
      "learning_rate": 0.0004920526405742608,
      "loss": 0.6347,
      "step": 187
    },
    {
      "epoch": 0.0008033020843125358,
      "grad_norm": 0.49154675006866455,
      "learning_rate": 0.0004920099128354128,
      "loss": 0.0474,
      "step": 188
    },
    {
      "epoch": 0.0008075749677397302,
      "grad_norm": 4.279599666595459,
      "learning_rate": 0.0004919671850965646,
      "loss": 1.4166,
      "step": 189
    },
    {
      "epoch": 0.0008118478511669245,
      "grad_norm": 0.20660579204559326,
      "learning_rate": 0.0004919244573577166,
      "loss": 0.0165,
      "step": 190
    },
    {
      "epoch": 0.0008161207345941188,
      "grad_norm": 5.875809192657471,
      "learning_rate": 0.0004918817296188686,
      "loss": 2.109,
      "step": 191
    },
    {
      "epoch": 0.0008203936180213132,
      "grad_norm": 14.229101181030273,
      "learning_rate": 0.0004918390018800205,
      "loss": 3.361,
      "step": 192
    },
    {
      "epoch": 0.0008246665014485074,
      "grad_norm": 2.2023112773895264,
      "learning_rate": 0.0004917962741411724,
      "loss": 0.6132,
      "step": 193
    },
    {
      "epoch": 0.0008289393848757018,
      "grad_norm": 4.324063301086426,
      "learning_rate": 0.0004917535464023244,
      "loss": 1.3458,
      "step": 194
    },
    {
      "epoch": 0.0008332122683028961,
      "grad_norm": 12.574213027954102,
      "learning_rate": 0.0004917108186634763,
      "loss": 4.4245,
      "step": 195
    },
    {
      "epoch": 0.0008374851517300905,
      "grad_norm": 6.7904744148254395,
      "learning_rate": 0.0004916680909246283,
      "loss": 1.3869,
      "step": 196
    },
    {
      "epoch": 0.0008417580351572848,
      "grad_norm": 0.06678695231676102,
      "learning_rate": 0.0004916253631857803,
      "loss": 0.0056,
      "step": 197
    },
    {
      "epoch": 0.0008460309185844792,
      "grad_norm": 4.594455718994141,
      "learning_rate": 0.0004915826354469321,
      "loss": 1.1026,
      "step": 198
    },
    {
      "epoch": 0.0008503038020116735,
      "grad_norm": 0.12064807116985321,
      "learning_rate": 0.0004915399077080841,
      "loss": 0.0079,
      "step": 199
    },
    {
      "epoch": 0.0008545766854388679,
      "grad_norm": 10.626187324523926,
      "learning_rate": 0.000491497179969236,
      "loss": 2.3596,
      "step": 200
    },
    {
      "epoch": 0.0008588495688660622,
      "grad_norm": 2.2782070636749268,
      "learning_rate": 0.000491454452230388,
      "loss": 0.4808,
      "step": 201
    },
    {
      "epoch": 0.0008631224522932566,
      "grad_norm": 3.646575927734375,
      "learning_rate": 0.00049141172449154,
      "loss": 1.6991,
      "step": 202
    },
    {
      "epoch": 0.0008673953357204509,
      "grad_norm": 1.8957958221435547,
      "learning_rate": 0.0004913689967526918,
      "loss": 0.4908,
      "step": 203
    },
    {
      "epoch": 0.0008716682191476453,
      "grad_norm": 3.1911141872406006,
      "learning_rate": 0.0004913262690138438,
      "loss": 0.4839,
      "step": 204
    },
    {
      "epoch": 0.0008759411025748396,
      "grad_norm": 4.203405380249023,
      "learning_rate": 0.0004912835412749958,
      "loss": 2.2683,
      "step": 205
    },
    {
      "epoch": 0.0008802139860020338,
      "grad_norm": 7.159430503845215,
      "learning_rate": 0.0004912408135361477,
      "loss": 2.7067,
      "step": 206
    },
    {
      "epoch": 0.0008844868694292282,
      "grad_norm": 2.5270609855651855,
      "learning_rate": 0.0004911980857972996,
      "loss": 0.2302,
      "step": 207
    },
    {
      "epoch": 0.0008887597528564225,
      "grad_norm": 6.755826950073242,
      "learning_rate": 0.0004911553580584516,
      "loss": 1.8623,
      "step": 208
    },
    {
      "epoch": 0.0008930326362836169,
      "grad_norm": 1.643001914024353,
      "learning_rate": 0.0004911126303196035,
      "loss": 0.4477,
      "step": 209
    },
    {
      "epoch": 0.0008973055197108112,
      "grad_norm": 1.536723017692566,
      "learning_rate": 0.0004910699025807555,
      "loss": 0.3825,
      "step": 210
    },
    {
      "epoch": 0.0009015784031380056,
      "grad_norm": 11.632694244384766,
      "learning_rate": 0.0004910271748419074,
      "loss": 5.5373,
      "step": 211
    },
    {
      "epoch": 0.0009058512865651999,
      "grad_norm": 14.02270221710205,
      "learning_rate": 0.0004909844471030593,
      "loss": 1.2718,
      "step": 212
    },
    {
      "epoch": 0.0009101241699923943,
      "grad_norm": 3.1625494956970215,
      "learning_rate": 0.0004909417193642113,
      "loss": 0.7936,
      "step": 213
    },
    {
      "epoch": 0.0009143970534195886,
      "grad_norm": 2.884687662124634,
      "learning_rate": 0.0004908989916253632,
      "loss": 0.8204,
      "step": 214
    },
    {
      "epoch": 0.000918669936846783,
      "grad_norm": 3.531325340270996,
      "learning_rate": 0.0004908562638865151,
      "loss": 0.3355,
      "step": 215
    },
    {
      "epoch": 0.0009229428202739773,
      "grad_norm": 4.8138346672058105,
      "learning_rate": 0.000490813536147667,
      "loss": 1.238,
      "step": 216
    },
    {
      "epoch": 0.0009272157037011717,
      "grad_norm": 14.020336151123047,
      "learning_rate": 0.000490770808408819,
      "loss": 3.8621,
      "step": 217
    },
    {
      "epoch": 0.000931488587128366,
      "grad_norm": 2.4490272998809814,
      "learning_rate": 0.000490728080669971,
      "loss": 0.8574,
      "step": 218
    },
    {
      "epoch": 0.0009357614705555604,
      "grad_norm": 3.008061170578003,
      "learning_rate": 0.0004906853529311228,
      "loss": 0.8618,
      "step": 219
    },
    {
      "epoch": 0.0009400343539827546,
      "grad_norm": 3.7950851917266846,
      "learning_rate": 0.0004906426251922748,
      "loss": 1.7826,
      "step": 220
    },
    {
      "epoch": 0.0009443072374099489,
      "grad_norm": 2.117006301879883,
      "learning_rate": 0.0004905998974534268,
      "loss": 0.8601,
      "step": 221
    },
    {
      "epoch": 0.0009485801208371433,
      "grad_norm": 2.2792110443115234,
      "learning_rate": 0.0004905571697145787,
      "loss": 0.5502,
      "step": 222
    },
    {
      "epoch": 0.0009528530042643376,
      "grad_norm": 1.4239166975021362,
      "learning_rate": 0.0004905144419757306,
      "loss": 0.1623,
      "step": 223
    },
    {
      "epoch": 0.000957125887691532,
      "grad_norm": 18.645475387573242,
      "learning_rate": 0.0004904717142368826,
      "loss": 4.1768,
      "step": 224
    },
    {
      "epoch": 0.0009613987711187263,
      "grad_norm": 5.643209934234619,
      "learning_rate": 0.0004904289864980345,
      "loss": 1.1914,
      "step": 225
    },
    {
      "epoch": 0.0009656716545459207,
      "grad_norm": 5.657517910003662,
      "learning_rate": 0.0004903862587591865,
      "loss": 1.1218,
      "step": 226
    },
    {
      "epoch": 0.000969944537973115,
      "grad_norm": 4.302478790283203,
      "learning_rate": 0.0004903435310203385,
      "loss": 0.786,
      "step": 227
    },
    {
      "epoch": 0.0009742174214003094,
      "grad_norm": 1.6045410633087158,
      "learning_rate": 0.0004903008032814903,
      "loss": 0.456,
      "step": 228
    },
    {
      "epoch": 0.0009784903048275038,
      "grad_norm": 9.48442268371582,
      "learning_rate": 0.0004902580755426423,
      "loss": 1.1043,
      "step": 229
    },
    {
      "epoch": 0.000982763188254698,
      "grad_norm": 2.8090436458587646,
      "learning_rate": 0.0004902153478037942,
      "loss": 0.54,
      "step": 230
    },
    {
      "epoch": 0.0009870360716818924,
      "grad_norm": 1.4745670557022095,
      "learning_rate": 0.0004901726200649462,
      "loss": 0.5507,
      "step": 231
    },
    {
      "epoch": 0.0009913089551090866,
      "grad_norm": 3.408845901489258,
      "learning_rate": 0.0004901298923260982,
      "loss": 1.0805,
      "step": 232
    },
    {
      "epoch": 0.0009955818385362811,
      "grad_norm": 1.478691577911377,
      "learning_rate": 0.00049008716458725,
      "loss": 0.4917,
      "step": 233
    },
    {
      "epoch": 0.0009998547219634754,
      "grad_norm": 89.89872741699219,
      "learning_rate": 0.000490044436848402,
      "loss": 4.3382,
      "step": 234
    },
    {
      "epoch": 0.0010041276053906697,
      "grad_norm": 3.067673921585083,
      "learning_rate": 0.000490001709109554,
      "loss": 0.596,
      "step": 235
    },
    {
      "epoch": 0.001008400488817864,
      "grad_norm": 4.806703090667725,
      "learning_rate": 0.0004899589813707059,
      "loss": 0.6667,
      "step": 236
    },
    {
      "epoch": 0.0010126733722450583,
      "grad_norm": 8.484466552734375,
      "learning_rate": 0.0004899162536318578,
      "loss": 1.8907,
      "step": 237
    },
    {
      "epoch": 0.0010169462556722528,
      "grad_norm": 2.480952739715576,
      "learning_rate": 0.0004898735258930097,
      "loss": 1.0424,
      "step": 238
    },
    {
      "epoch": 0.001021219139099447,
      "grad_norm": 1.7410047054290771,
      "learning_rate": 0.0004898307981541617,
      "loss": 0.224,
      "step": 239
    },
    {
      "epoch": 0.0010254920225266414,
      "grad_norm": 2.198289632797241,
      "learning_rate": 0.0004897880704153137,
      "loss": 0.5387,
      "step": 240
    },
    {
      "epoch": 0.0010297649059538357,
      "grad_norm": 3.8199498653411865,
      "learning_rate": 0.0004897453426764656,
      "loss": 0.5321,
      "step": 241
    },
    {
      "epoch": 0.0010340377893810302,
      "grad_norm": 28.249509811401367,
      "learning_rate": 0.0004897026149376175,
      "loss": 5.9668,
      "step": 242
    },
    {
      "epoch": 0.0010383106728082245,
      "grad_norm": 3.8708839416503906,
      "learning_rate": 0.0004896598871987695,
      "loss": 1.6067,
      "step": 243
    },
    {
      "epoch": 0.0010425835562354188,
      "grad_norm": 2.7179505825042725,
      "learning_rate": 0.0004896171594599214,
      "loss": 0.8055,
      "step": 244
    },
    {
      "epoch": 0.001046856439662613,
      "grad_norm": 2.381455421447754,
      "learning_rate": 0.0004895744317210734,
      "loss": 0.6793,
      "step": 245
    },
    {
      "epoch": 0.0010511293230898076,
      "grad_norm": 1.898340106010437,
      "learning_rate": 0.0004895317039822252,
      "loss": 0.6945,
      "step": 246
    },
    {
      "epoch": 0.0010554022065170018,
      "grad_norm": 3.3887877464294434,
      "learning_rate": 0.0004894889762433772,
      "loss": 0.6835,
      "step": 247
    },
    {
      "epoch": 0.0010596750899441961,
      "grad_norm": 0.9170712828636169,
      "learning_rate": 0.0004894462485045292,
      "loss": 0.1307,
      "step": 248
    },
    {
      "epoch": 0.0010639479733713904,
      "grad_norm": 0.7765247225761414,
      "learning_rate": 0.000489403520765681,
      "loss": 0.1085,
      "step": 249
    },
    {
      "epoch": 0.001068220856798585,
      "grad_norm": 6.284502029418945,
      "learning_rate": 0.000489360793026833,
      "loss": 3.5048,
      "step": 250
    },
    {
      "epoch": 0.0010724937402257792,
      "grad_norm": 17.16358757019043,
      "learning_rate": 0.000489318065287985,
      "loss": 4.1809,
      "step": 251
    },
    {
      "epoch": 0.0010767666236529735,
      "grad_norm": 8.865484237670898,
      "learning_rate": 0.0004892753375491369,
      "loss": 3.4121,
      "step": 252
    },
    {
      "epoch": 0.0010810395070801678,
      "grad_norm": 3.5154545307159424,
      "learning_rate": 0.0004892326098102888,
      "loss": 0.7158,
      "step": 253
    },
    {
      "epoch": 0.001085312390507362,
      "grad_norm": 1.9356759786605835,
      "learning_rate": 0.0004891898820714407,
      "loss": 0.4329,
      "step": 254
    },
    {
      "epoch": 0.0010895852739345566,
      "grad_norm": 17.345869064331055,
      "learning_rate": 0.0004891471543325927,
      "loss": 7.8894,
      "step": 255
    },
    {
      "epoch": 0.0010938581573617509,
      "grad_norm": 3.144996404647827,
      "learning_rate": 0.0004891044265937447,
      "loss": 0.4873,
      "step": 256
    },
    {
      "epoch": 0.0010981310407889452,
      "grad_norm": 3.5145926475524902,
      "learning_rate": 0.0004890616988548966,
      "loss": 1.0963,
      "step": 257
    },
    {
      "epoch": 0.0011024039242161394,
      "grad_norm": 3.824012279510498,
      "learning_rate": 0.0004890189711160485,
      "loss": 1.3248,
      "step": 258
    },
    {
      "epoch": 0.001106676807643334,
      "grad_norm": 2.9546618461608887,
      "learning_rate": 0.0004889762433772005,
      "loss": 0.9468,
      "step": 259
    },
    {
      "epoch": 0.0011109496910705282,
      "grad_norm": 1.6238456964492798,
      "learning_rate": 0.0004889335156383524,
      "loss": 0.4682,
      "step": 260
    },
    {
      "epoch": 0.0011152225744977225,
      "grad_norm": 1.6505365371704102,
      "learning_rate": 0.0004888907878995044,
      "loss": 0.5082,
      "step": 261
    },
    {
      "epoch": 0.0011194954579249168,
      "grad_norm": 3.3619091510772705,
      "learning_rate": 0.0004888480601606564,
      "loss": 0.7146,
      "step": 262
    },
    {
      "epoch": 0.0011237683413521113,
      "grad_norm": 5.417282581329346,
      "learning_rate": 0.0004888053324218082,
      "loss": 2.327,
      "step": 263
    },
    {
      "epoch": 0.0011280412247793056,
      "grad_norm": 1.4538908004760742,
      "learning_rate": 0.0004887626046829602,
      "loss": 0.4552,
      "step": 264
    },
    {
      "epoch": 0.0011323141082065,
      "grad_norm": 20.18021011352539,
      "learning_rate": 0.0004887198769441121,
      "loss": 2.0655,
      "step": 265
    },
    {
      "epoch": 0.0011365869916336942,
      "grad_norm": 2.9928455352783203,
      "learning_rate": 0.0004886771492052641,
      "loss": 0.27,
      "step": 266
    },
    {
      "epoch": 0.0011408598750608887,
      "grad_norm": 1.8727760314941406,
      "learning_rate": 0.000488634421466416,
      "loss": 0.2432,
      "step": 267
    },
    {
      "epoch": 0.001145132758488083,
      "grad_norm": 21.985126495361328,
      "learning_rate": 0.0004885916937275679,
      "loss": 2.6207,
      "step": 268
    },
    {
      "epoch": 0.0011494056419152773,
      "grad_norm": 4.742542266845703,
      "learning_rate": 0.0004885489659887199,
      "loss": 0.7688,
      "step": 269
    },
    {
      "epoch": 0.0011536785253424716,
      "grad_norm": 18.271570205688477,
      "learning_rate": 0.0004885062382498719,
      "loss": 3.9087,
      "step": 270
    },
    {
      "epoch": 0.0011579514087696658,
      "grad_norm": 5.186782360076904,
      "learning_rate": 0.0004884635105110238,
      "loss": 1.2245,
      "step": 271
    },
    {
      "epoch": 0.0011622242921968604,
      "grad_norm": 2.024225950241089,
      "learning_rate": 0.0004884207827721757,
      "loss": 0.5535,
      "step": 272
    },
    {
      "epoch": 0.0011664971756240546,
      "grad_norm": 3.5349600315093994,
      "learning_rate": 0.0004883780550333276,
      "loss": 1.0844,
      "step": 273
    },
    {
      "epoch": 0.001170770059051249,
      "grad_norm": 1.3209772109985352,
      "learning_rate": 0.0004883353272944796,
      "loss": 0.1654,
      "step": 274
    },
    {
      "epoch": 0.0011750429424784432,
      "grad_norm": 12.787331581115723,
      "learning_rate": 0.0004882925995556316,
      "loss": 3.3492,
      "step": 275
    },
    {
      "epoch": 0.0011793158259056377,
      "grad_norm": 0.8658544421195984,
      "learning_rate": 0.0004882498718167835,
      "loss": 0.1203,
      "step": 276
    },
    {
      "epoch": 0.001183588709332832,
      "grad_norm": 3.6789653301239014,
      "learning_rate": 0.00048820714407793545,
      "loss": 0.8424,
      "step": 277
    },
    {
      "epoch": 0.0011878615927600263,
      "grad_norm": 2.050931215286255,
      "learning_rate": 0.00048816441633908736,
      "loss": 0.5132,
      "step": 278
    },
    {
      "epoch": 0.0011921344761872206,
      "grad_norm": 2.3685054779052734,
      "learning_rate": 0.0004881216886002393,
      "loss": 0.4936,
      "step": 279
    },
    {
      "epoch": 0.001196407359614415,
      "grad_norm": 3.683286666870117,
      "learning_rate": 0.00048807896086139124,
      "loss": 0.7555,
      "step": 280
    },
    {
      "epoch": 0.0012006802430416094,
      "grad_norm": 2.6177501678466797,
      "learning_rate": 0.00048803623312254315,
      "loss": 1.0323,
      "step": 281
    },
    {
      "epoch": 0.0012049531264688037,
      "grad_norm": 4.172267436981201,
      "learning_rate": 0.00048799350538369506,
      "loss": 1.6479,
      "step": 282
    },
    {
      "epoch": 0.001209226009895998,
      "grad_norm": 3.097395420074463,
      "learning_rate": 0.000487950777644847,
      "loss": 0.6322,
      "step": 283
    },
    {
      "epoch": 0.0012134988933231922,
      "grad_norm": 1.8272426128387451,
      "learning_rate": 0.000487908049905999,
      "loss": 0.435,
      "step": 284
    },
    {
      "epoch": 0.0012177717767503868,
      "grad_norm": 1.0837067365646362,
      "learning_rate": 0.0004878653221671509,
      "loss": 0.1558,
      "step": 285
    },
    {
      "epoch": 0.001222044660177581,
      "grad_norm": 6.621882915496826,
      "learning_rate": 0.00048782259442830286,
      "loss": 5.019,
      "step": 286
    },
    {
      "epoch": 0.0012263175436047753,
      "grad_norm": 3.757368564605713,
      "learning_rate": 0.0004877798666894548,
      "loss": 1.4813,
      "step": 287
    },
    {
      "epoch": 0.0012305904270319696,
      "grad_norm": 3.7765820026397705,
      "learning_rate": 0.00048773713895060674,
      "loss": 1.3064,
      "step": 288
    },
    {
      "epoch": 0.0012348633104591641,
      "grad_norm": 1.962213397026062,
      "learning_rate": 0.00048769441121175865,
      "loss": 0.449,
      "step": 289
    },
    {
      "epoch": 0.0012391361938863584,
      "grad_norm": 3.3087353706359863,
      "learning_rate": 0.0004876516834729106,
      "loss": 0.8539,
      "step": 290
    },
    {
      "epoch": 0.0012434090773135527,
      "grad_norm": 10.585546493530273,
      "learning_rate": 0.0004876089557340625,
      "loss": 2.8765,
      "step": 291
    },
    {
      "epoch": 0.001247681960740747,
      "grad_norm": 6.105721473693848,
      "learning_rate": 0.0004875662279952145,
      "loss": 2.2182,
      "step": 292
    },
    {
      "epoch": 0.0012519548441679415,
      "grad_norm": 1.341156005859375,
      "learning_rate": 0.00048752350025636646,
      "loss": 0.2226,
      "step": 293
    },
    {
      "epoch": 0.0012562277275951358,
      "grad_norm": 5.232848167419434,
      "learning_rate": 0.00048748077251751837,
      "loss": 1.9813,
      "step": 294
    },
    {
      "epoch": 0.00126050061102233,
      "grad_norm": 4.24910831451416,
      "learning_rate": 0.00048743804477867033,
      "loss": 1.1913,
      "step": 295
    },
    {
      "epoch": 0.0012647734944495244,
      "grad_norm": 1.8067132234573364,
      "learning_rate": 0.00048739531703982224,
      "loss": 0.6815,
      "step": 296
    },
    {
      "epoch": 0.0012690463778767189,
      "grad_norm": 3.572890043258667,
      "learning_rate": 0.0004873525893009742,
      "loss": 0.8151,
      "step": 297
    },
    {
      "epoch": 0.0012733192613039132,
      "grad_norm": 0.9086761474609375,
      "learning_rate": 0.0004873098615621261,
      "loss": 0.1349,
      "step": 298
    },
    {
      "epoch": 0.0012775921447311074,
      "grad_norm": 4.033933639526367,
      "learning_rate": 0.0004872671338232781,
      "loss": 0.8317,
      "step": 299
    },
    {
      "epoch": 0.0012818650281583017,
      "grad_norm": 20.20608901977539,
      "learning_rate": 0.00048722440608443005,
      "loss": 6.0317,
      "step": 300
    },
    {
      "epoch": 0.001286137911585496,
      "grad_norm": 1.6098222732543945,
      "learning_rate": 0.00048718167834558196,
      "loss": 0.5927,
      "step": 301
    },
    {
      "epoch": 0.0012904107950126905,
      "grad_norm": 1.5132620334625244,
      "learning_rate": 0.0004871389506067339,
      "loss": 0.6158,
      "step": 302
    },
    {
      "epoch": 0.0012946836784398848,
      "grad_norm": 12.189225196838379,
      "learning_rate": 0.00048709622286788583,
      "loss": 1.9627,
      "step": 303
    },
    {
      "epoch": 0.001298956561867079,
      "grad_norm": 1.6478105783462524,
      "learning_rate": 0.0004870534951290378,
      "loss": 0.4701,
      "step": 304
    },
    {
      "epoch": 0.0013032294452942734,
      "grad_norm": 2.671395778656006,
      "learning_rate": 0.0004870107673901897,
      "loss": 0.6463,
      "step": 305
    },
    {
      "epoch": 0.001307502328721468,
      "grad_norm": 3.789043426513672,
      "learning_rate": 0.0004869680396513417,
      "loss": 1.012,
      "step": 306
    },
    {
      "epoch": 0.0013117752121486622,
      "grad_norm": 3.0968143939971924,
      "learning_rate": 0.00048692531191249364,
      "loss": 0.488,
      "step": 307
    },
    {
      "epoch": 0.0013160480955758565,
      "grad_norm": 1.7993172407150269,
      "learning_rate": 0.00048688258417364555,
      "loss": 1.1788,
      "step": 308
    },
    {
      "epoch": 0.0013203209790030508,
      "grad_norm": 5.7451324462890625,
      "learning_rate": 0.0004868398564347975,
      "loss": 1.2351,
      "step": 309
    },
    {
      "epoch": 0.0013245938624302453,
      "grad_norm": 14.443236351013184,
      "learning_rate": 0.0004867971286959494,
      "loss": 3.0233,
      "step": 310
    },
    {
      "epoch": 0.0013288667458574396,
      "grad_norm": 1.5615755319595337,
      "learning_rate": 0.0004867544009571014,
      "loss": 0.2383,
      "step": 311
    },
    {
      "epoch": 0.0013331396292846338,
      "grad_norm": 15.414551734924316,
      "learning_rate": 0.0004867116732182533,
      "loss": 2.0354,
      "step": 312
    },
    {
      "epoch": 0.0013374125127118281,
      "grad_norm": 1.5679893493652344,
      "learning_rate": 0.00048666894547940527,
      "loss": 0.2281,
      "step": 313
    },
    {
      "epoch": 0.0013416853961390226,
      "grad_norm": 14.234248161315918,
      "learning_rate": 0.0004866262177405572,
      "loss": 3.2713,
      "step": 314
    },
    {
      "epoch": 0.001345958279566217,
      "grad_norm": 10.121655464172363,
      "learning_rate": 0.0004865834900017091,
      "loss": 1.5377,
      "step": 315
    },
    {
      "epoch": 0.0013502311629934112,
      "grad_norm": 2.5040504932403564,
      "learning_rate": 0.00048654076226286105,
      "loss": 0.445,
      "step": 316
    },
    {
      "epoch": 0.0013545040464206055,
      "grad_norm": 1.906699776649475,
      "learning_rate": 0.00048649803452401296,
      "loss": 1.0234,
      "step": 317
    },
    {
      "epoch": 0.0013587769298477998,
      "grad_norm": 0.9596120715141296,
      "learning_rate": 0.00048645530678516493,
      "loss": 0.1164,
      "step": 318
    },
    {
      "epoch": 0.0013630498132749943,
      "grad_norm": 1.9382787942886353,
      "learning_rate": 0.00048641257904631684,
      "loss": 0.8966,
      "step": 319
    },
    {
      "epoch": 0.0013673226967021886,
      "grad_norm": 4.196670055389404,
      "learning_rate": 0.0004863698513074688,
      "loss": 0.6333,
      "step": 320
    },
    {
      "epoch": 0.0013715955801293829,
      "grad_norm": 23.013423919677734,
      "learning_rate": 0.00048632712356862077,
      "loss": 4.0246,
      "step": 321
    },
    {
      "epoch": 0.0013758684635565772,
      "grad_norm": 26.77738380432129,
      "learning_rate": 0.0004862843958297727,
      "loss": 2.56,
      "step": 322
    },
    {
      "epoch": 0.0013801413469837717,
      "grad_norm": 8.024496078491211,
      "learning_rate": 0.00048624166809092465,
      "loss": 1.0621,
      "step": 323
    },
    {
      "epoch": 0.001384414230410966,
      "grad_norm": 0.5694377422332764,
      "learning_rate": 0.00048619894035207656,
      "loss": 0.0357,
      "step": 324
    },
    {
      "epoch": 0.0013886871138381602,
      "grad_norm": 2.224217176437378,
      "learning_rate": 0.0004861562126132285,
      "loss": 0.7191,
      "step": 325
    },
    {
      "epoch": 0.0013929599972653545,
      "grad_norm": 4.9992899894714355,
      "learning_rate": 0.00048611348487438043,
      "loss": 2.0908,
      "step": 326
    },
    {
      "epoch": 0.001397232880692549,
      "grad_norm": 5.068819046020508,
      "learning_rate": 0.0004860707571355324,
      "loss": 1.7669,
      "step": 327
    },
    {
      "epoch": 0.0014015057641197433,
      "grad_norm": 3.5245680809020996,
      "learning_rate": 0.0004860280293966843,
      "loss": 1.6968,
      "step": 328
    },
    {
      "epoch": 0.0014057786475469376,
      "grad_norm": 18.21407699584961,
      "learning_rate": 0.0004859853016578363,
      "loss": 5.0273,
      "step": 329
    },
    {
      "epoch": 0.001410051530974132,
      "grad_norm": 9.786495208740234,
      "learning_rate": 0.00048594257391898824,
      "loss": 4.301,
      "step": 330
    },
    {
      "epoch": 0.0014143244144013262,
      "grad_norm": 1.843982219696045,
      "learning_rate": 0.00048589984618014015,
      "loss": 0.7984,
      "step": 331
    },
    {
      "epoch": 0.0014185972978285207,
      "grad_norm": 2.513002395629883,
      "learning_rate": 0.0004858571184412921,
      "loss": 0.7509,
      "step": 332
    },
    {
      "epoch": 0.001422870181255715,
      "grad_norm": 1.8667628765106201,
      "learning_rate": 0.000485814390702444,
      "loss": 0.7002,
      "step": 333
    },
    {
      "epoch": 0.0014271430646829093,
      "grad_norm": 11.253815650939941,
      "learning_rate": 0.000485771662963596,
      "loss": 0.6316,
      "step": 334
    },
    {
      "epoch": 0.0014314159481101036,
      "grad_norm": 7.289201736450195,
      "learning_rate": 0.0004857289352247479,
      "loss": 2.0017,
      "step": 335
    },
    {
      "epoch": 0.001435688831537298,
      "grad_norm": 5.071476459503174,
      "learning_rate": 0.00048568620748589987,
      "loss": 1.102,
      "step": 336
    },
    {
      "epoch": 0.0014399617149644924,
      "grad_norm": 3.611497402191162,
      "learning_rate": 0.00048564347974705183,
      "loss": 0.9814,
      "step": 337
    },
    {
      "epoch": 0.0014442345983916866,
      "grad_norm": 9.256308555603027,
      "learning_rate": 0.00048560075200820374,
      "loss": 1.5929,
      "step": 338
    },
    {
      "epoch": 0.001448507481818881,
      "grad_norm": 13.7034330368042,
      "learning_rate": 0.0004855580242693557,
      "loss": 0.9729,
      "step": 339
    },
    {
      "epoch": 0.0014527803652460754,
      "grad_norm": 3.2856364250183105,
      "learning_rate": 0.0004855152965305076,
      "loss": 1.1985,
      "step": 340
    },
    {
      "epoch": 0.0014570532486732697,
      "grad_norm": 1.4305258989334106,
      "learning_rate": 0.0004854725687916596,
      "loss": 0.3731,
      "step": 341
    },
    {
      "epoch": 0.001461326132100464,
      "grad_norm": 1.7472904920578003,
      "learning_rate": 0.0004854298410528115,
      "loss": 0.8183,
      "step": 342
    },
    {
      "epoch": 0.0014655990155276583,
      "grad_norm": 0.9174702763557434,
      "learning_rate": 0.00048538711331396346,
      "loss": 0.1907,
      "step": 343
    },
    {
      "epoch": 0.0014698718989548528,
      "grad_norm": 0.9223214387893677,
      "learning_rate": 0.0004853443855751154,
      "loss": 0.2423,
      "step": 344
    },
    {
      "epoch": 0.001474144782382047,
      "grad_norm": 1.7323429584503174,
      "learning_rate": 0.00048530165783626733,
      "loss": 0.7645,
      "step": 345
    },
    {
      "epoch": 0.0014784176658092414,
      "grad_norm": 3.840155839920044,
      "learning_rate": 0.0004852589300974193,
      "loss": 0.8631,
      "step": 346
    },
    {
      "epoch": 0.0014826905492364357,
      "grad_norm": 1.3693288564682007,
      "learning_rate": 0.00048521620235857115,
      "loss": 0.6849,
      "step": 347
    },
    {
      "epoch": 0.00148696343266363,
      "grad_norm": 19.957260131835938,
      "learning_rate": 0.0004851734746197231,
      "loss": 1.4867,
      "step": 348
    },
    {
      "epoch": 0.0014912363160908245,
      "grad_norm": 1.9180545806884766,
      "learning_rate": 0.00048513074688087503,
      "loss": 0.6468,
      "step": 349
    },
    {
      "epoch": 0.0014955091995180188,
      "grad_norm": 1.549636721611023,
      "learning_rate": 0.000485088019142027,
      "loss": 0.6458,
      "step": 350
    },
    {
      "epoch": 0.001499782082945213,
      "grad_norm": 4.690141201019287,
      "learning_rate": 0.00048504529140317896,
      "loss": 1.7584,
      "step": 351
    },
    {
      "epoch": 0.0015040549663724073,
      "grad_norm": 1.7727558612823486,
      "learning_rate": 0.00048500256366433087,
      "loss": 0.597,
      "step": 352
    },
    {
      "epoch": 0.0015083278497996018,
      "grad_norm": 1.289721131324768,
      "learning_rate": 0.00048495983592548284,
      "loss": 0.3624,
      "step": 353
    },
    {
      "epoch": 0.0015126007332267961,
      "grad_norm": 3.6547858715057373,
      "learning_rate": 0.00048491710818663475,
      "loss": 1.3611,
      "step": 354
    },
    {
      "epoch": 0.0015168736166539904,
      "grad_norm": 2.168990135192871,
      "learning_rate": 0.0004848743804477867,
      "loss": 0.7306,
      "step": 355
    },
    {
      "epoch": 0.0015211465000811847,
      "grad_norm": 4.0740156173706055,
      "learning_rate": 0.0004848316527089386,
      "loss": 1.3344,
      "step": 356
    },
    {
      "epoch": 0.0015254193835083792,
      "grad_norm": 3.650047779083252,
      "learning_rate": 0.0004847889249700906,
      "loss": 1.1719,
      "step": 357
    },
    {
      "epoch": 0.0015296922669355735,
      "grad_norm": 3.8500306606292725,
      "learning_rate": 0.00048474619723124255,
      "loss": 1.9085,
      "step": 358
    },
    {
      "epoch": 0.0015339651503627678,
      "grad_norm": 3.2683846950531006,
      "learning_rate": 0.00048470346949239446,
      "loss": 0.9424,
      "step": 359
    },
    {
      "epoch": 0.001538238033789962,
      "grad_norm": 0.846308171749115,
      "learning_rate": 0.00048466074175354643,
      "loss": 0.2154,
      "step": 360
    },
    {
      "epoch": 0.0015425109172171566,
      "grad_norm": 5.259896755218506,
      "learning_rate": 0.00048461801401469834,
      "loss": 2.6839,
      "step": 361
    },
    {
      "epoch": 0.0015467838006443509,
      "grad_norm": 5.390887260437012,
      "learning_rate": 0.0004845752862758503,
      "loss": 2.2961,
      "step": 362
    },
    {
      "epoch": 0.0015510566840715452,
      "grad_norm": 13.43085765838623,
      "learning_rate": 0.0004845325585370022,
      "loss": 1.8212,
      "step": 363
    },
    {
      "epoch": 0.0015553295674987395,
      "grad_norm": 6.192680358886719,
      "learning_rate": 0.0004844898307981542,
      "loss": 1.6709,
      "step": 364
    },
    {
      "epoch": 0.0015596024509259337,
      "grad_norm": 1.838156819343567,
      "learning_rate": 0.0004844471030593061,
      "loss": 0.4454,
      "step": 365
    },
    {
      "epoch": 0.0015638753343531282,
      "grad_norm": 1.7700064182281494,
      "learning_rate": 0.00048440437532045805,
      "loss": 0.407,
      "step": 366
    },
    {
      "epoch": 0.0015681482177803225,
      "grad_norm": 3.244955062866211,
      "learning_rate": 0.00048436164758161,
      "loss": 1.6101,
      "step": 367
    },
    {
      "epoch": 0.0015724211012075168,
      "grad_norm": 1.3249017000198364,
      "learning_rate": 0.00048431891984276193,
      "loss": 0.5725,
      "step": 368
    },
    {
      "epoch": 0.0015766939846347111,
      "grad_norm": 2.709705114364624,
      "learning_rate": 0.0004842761921039139,
      "loss": 0.5374,
      "step": 369
    },
    {
      "epoch": 0.0015809668680619056,
      "grad_norm": 2.9994513988494873,
      "learning_rate": 0.0004842334643650658,
      "loss": 1.1889,
      "step": 370
    },
    {
      "epoch": 0.0015852397514891,
      "grad_norm": 8.197307586669922,
      "learning_rate": 0.00048419073662621777,
      "loss": 1.6242,
      "step": 371
    },
    {
      "epoch": 0.0015895126349162942,
      "grad_norm": 5.010361194610596,
      "learning_rate": 0.0004841480088873697,
      "loss": 3.4136,
      "step": 372
    },
    {
      "epoch": 0.0015937855183434885,
      "grad_norm": 12.921281814575195,
      "learning_rate": 0.00048410528114852165,
      "loss": 2.2527,
      "step": 373
    },
    {
      "epoch": 0.001598058401770683,
      "grad_norm": 15.110657691955566,
      "learning_rate": 0.0004840625534096736,
      "loss": 2.2546,
      "step": 374
    },
    {
      "epoch": 0.0016023312851978773,
      "grad_norm": 3.1375622749328613,
      "learning_rate": 0.0004840198256708255,
      "loss": 1.9117,
      "step": 375
    },
    {
      "epoch": 0.0016066041686250716,
      "grad_norm": 8.775486946105957,
      "learning_rate": 0.0004839770979319775,
      "loss": 2.5946,
      "step": 376
    },
    {
      "epoch": 0.0016108770520522659,
      "grad_norm": 3.037782907485962,
      "learning_rate": 0.0004839343701931294,
      "loss": 1.3676,
      "step": 377
    },
    {
      "epoch": 0.0016151499354794604,
      "grad_norm": 2.6325666904449463,
      "learning_rate": 0.00048389164245428136,
      "loss": 1.1775,
      "step": 378
    },
    {
      "epoch": 0.0016194228189066546,
      "grad_norm": 1.7350276708602905,
      "learning_rate": 0.0004838489147154333,
      "loss": 0.7318,
      "step": 379
    },
    {
      "epoch": 0.001623695702333849,
      "grad_norm": 1.5920016765594482,
      "learning_rate": 0.0004838061869765852,
      "loss": 0.6937,
      "step": 380
    },
    {
      "epoch": 0.0016279685857610432,
      "grad_norm": 4.090054035186768,
      "learning_rate": 0.00048376345923773715,
      "loss": 1.6846,
      "step": 381
    },
    {
      "epoch": 0.0016322414691882375,
      "grad_norm": 2.903263568878174,
      "learning_rate": 0.00048372073149888906,
      "loss": 0.79,
      "step": 382
    },
    {
      "epoch": 0.001636514352615432,
      "grad_norm": 3.259169101715088,
      "learning_rate": 0.000483678003760041,
      "loss": 1.0337,
      "step": 383
    },
    {
      "epoch": 0.0016407872360426263,
      "grad_norm": 7.963414192199707,
      "learning_rate": 0.00048363527602119294,
      "loss": 1.1705,
      "step": 384
    },
    {
      "epoch": 0.0016450601194698206,
      "grad_norm": 1.9838249683380127,
      "learning_rate": 0.0004835925482823449,
      "loss": 0.3984,
      "step": 385
    },
    {
      "epoch": 0.0016493330028970149,
      "grad_norm": 1.4003095626831055,
      "learning_rate": 0.0004835498205434968,
      "loss": 0.3071,
      "step": 386
    },
    {
      "epoch": 0.0016536058863242094,
      "grad_norm": 2.6469526290893555,
      "learning_rate": 0.0004835070928046488,
      "loss": 0.5722,
      "step": 387
    },
    {
      "epoch": 0.0016578787697514037,
      "grad_norm": 1.069386601448059,
      "learning_rate": 0.00048346436506580074,
      "loss": 0.2048,
      "step": 388
    },
    {
      "epoch": 0.001662151653178598,
      "grad_norm": 2.97770619392395,
      "learning_rate": 0.00048342163732695265,
      "loss": 0.5679,
      "step": 389
    },
    {
      "epoch": 0.0016664245366057923,
      "grad_norm": 4.888091087341309,
      "learning_rate": 0.0004833789095881046,
      "loss": 1.0769,
      "step": 390
    },
    {
      "epoch": 0.0016706974200329868,
      "grad_norm": 4.824728488922119,
      "learning_rate": 0.00048333618184925653,
      "loss": 0.8696,
      "step": 391
    },
    {
      "epoch": 0.001674970303460181,
      "grad_norm": 2.479412794113159,
      "learning_rate": 0.0004832934541104085,
      "loss": 0.3185,
      "step": 392
    },
    {
      "epoch": 0.0016792431868873753,
      "grad_norm": 3.2835497856140137,
      "learning_rate": 0.0004832507263715604,
      "loss": 1.6922,
      "step": 393
    },
    {
      "epoch": 0.0016835160703145696,
      "grad_norm": 46.31972122192383,
      "learning_rate": 0.00048320799863271237,
      "loss": 8.1158,
      "step": 394
    },
    {
      "epoch": 0.001687788953741764,
      "grad_norm": 1.7219983339309692,
      "learning_rate": 0.0004831652708938643,
      "loss": 0.159,
      "step": 395
    },
    {
      "epoch": 0.0016920618371689584,
      "grad_norm": 3.2112743854522705,
      "learning_rate": 0.00048312254315501624,
      "loss": 0.8572,
      "step": 396
    },
    {
      "epoch": 0.0016963347205961527,
      "grad_norm": 4.083677291870117,
      "learning_rate": 0.0004830798154161682,
      "loss": 0.7785,
      "step": 397
    },
    {
      "epoch": 0.001700607604023347,
      "grad_norm": 0.3608090281486511,
      "learning_rate": 0.0004830370876773201,
      "loss": 0.0373,
      "step": 398
    },
    {
      "epoch": 0.0017048804874505413,
      "grad_norm": 8.687615394592285,
      "learning_rate": 0.0004829943599384721,
      "loss": 3.2957,
      "step": 399
    },
    {
      "epoch": 0.0017091533708777358,
      "grad_norm": 4.8464765548706055,
      "learning_rate": 0.000482951632199624,
      "loss": 1.5791,
      "step": 400
    },
    {
      "epoch": 0.00171342625430493,
      "grad_norm": 4.141875743865967,
      "learning_rate": 0.00048290890446077596,
      "loss": 0.9031,
      "step": 401
    },
    {
      "epoch": 0.0017176991377321244,
      "grad_norm": 1.2809234857559204,
      "learning_rate": 0.00048286617672192787,
      "loss": 0.4168,
      "step": 402
    },
    {
      "epoch": 0.0017219720211593187,
      "grad_norm": 2.2066361904144287,
      "learning_rate": 0.00048282344898307984,
      "loss": 0.6415,
      "step": 403
    },
    {
      "epoch": 0.0017262449045865132,
      "grad_norm": 1.886665940284729,
      "learning_rate": 0.0004827807212442318,
      "loss": 0.4742,
      "step": 404
    },
    {
      "epoch": 0.0017305177880137074,
      "grad_norm": 1.2834416627883911,
      "learning_rate": 0.0004827379935053837,
      "loss": 0.3968,
      "step": 405
    },
    {
      "epoch": 0.0017347906714409017,
      "grad_norm": 2.208726406097412,
      "learning_rate": 0.0004826952657665357,
      "loss": 0.7035,
      "step": 406
    },
    {
      "epoch": 0.001739063554868096,
      "grad_norm": 63.991905212402344,
      "learning_rate": 0.0004826525380276876,
      "loss": 7.1909,
      "step": 407
    },
    {
      "epoch": 0.0017433364382952905,
      "grad_norm": 3.233121871948242,
      "learning_rate": 0.00048260981028883955,
      "loss": 1.1353,
      "step": 408
    },
    {
      "epoch": 0.0017476093217224848,
      "grad_norm": 1.2100650072097778,
      "learning_rate": 0.00048256708254999146,
      "loss": 0.2439,
      "step": 409
    },
    {
      "epoch": 0.001751882205149679,
      "grad_norm": 2.889946699142456,
      "learning_rate": 0.00048252435481114343,
      "loss": 0.9946,
      "step": 410
    },
    {
      "epoch": 0.0017561550885768734,
      "grad_norm": 1.829746127128601,
      "learning_rate": 0.0004824816270722954,
      "loss": 0.9223,
      "step": 411
    },
    {
      "epoch": 0.0017604279720040677,
      "grad_norm": 1.0763894319534302,
      "learning_rate": 0.00048243889933344725,
      "loss": 0.2444,
      "step": 412
    },
    {
      "epoch": 0.0017647008554312622,
      "grad_norm": 1.5929566621780396,
      "learning_rate": 0.0004823961715945992,
      "loss": 0.425,
      "step": 413
    },
    {
      "epoch": 0.0017689737388584565,
      "grad_norm": 13.80115032196045,
      "learning_rate": 0.0004823534438557511,
      "loss": 5.6302,
      "step": 414
    },
    {
      "epoch": 0.0017732466222856508,
      "grad_norm": 2.438790798187256,
      "learning_rate": 0.0004823107161169031,
      "loss": 0.7061,
      "step": 415
    },
    {
      "epoch": 0.001777519505712845,
      "grad_norm": 4.186014175415039,
      "learning_rate": 0.000482267988378055,
      "loss": 1.2707,
      "step": 416
    },
    {
      "epoch": 0.0017817923891400396,
      "grad_norm": 0.8821380734443665,
      "learning_rate": 0.00048222526063920697,
      "loss": 0.1913,
      "step": 417
    },
    {
      "epoch": 0.0017860652725672338,
      "grad_norm": 4.050947189331055,
      "learning_rate": 0.00048218253290035893,
      "loss": 1.7916,
      "step": 418
    },
    {
      "epoch": 0.0017903381559944281,
      "grad_norm": 3.8060271739959717,
      "learning_rate": 0.00048213980516151084,
      "loss": 1.0802,
      "step": 419
    },
    {
      "epoch": 0.0017946110394216224,
      "grad_norm": 3.669431686401367,
      "learning_rate": 0.0004820970774226628,
      "loss": 0.8516,
      "step": 420
    },
    {
      "epoch": 0.001798883922848817,
      "grad_norm": 0.6477017998695374,
      "learning_rate": 0.0004820543496838147,
      "loss": 0.1289,
      "step": 421
    },
    {
      "epoch": 0.0018031568062760112,
      "grad_norm": 3.0267131328582764,
      "learning_rate": 0.0004820116219449667,
      "loss": 0.5652,
      "step": 422
    },
    {
      "epoch": 0.0018074296897032055,
      "grad_norm": 2.759444236755371,
      "learning_rate": 0.0004819688942061186,
      "loss": 1.0641,
      "step": 423
    },
    {
      "epoch": 0.0018117025731303998,
      "grad_norm": 4.612236022949219,
      "learning_rate": 0.00048192616646727056,
      "loss": 2.3293,
      "step": 424
    },
    {
      "epoch": 0.0018159754565575943,
      "grad_norm": 0.9812631011009216,
      "learning_rate": 0.0004818834387284225,
      "loss": 0.1817,
      "step": 425
    },
    {
      "epoch": 0.0018202483399847886,
      "grad_norm": 2.398761034011841,
      "learning_rate": 0.00048184071098957443,
      "loss": 0.3782,
      "step": 426
    },
    {
      "epoch": 0.0018245212234119829,
      "grad_norm": 0.7594543695449829,
      "learning_rate": 0.0004817979832507264,
      "loss": 0.0972,
      "step": 427
    },
    {
      "epoch": 0.0018287941068391772,
      "grad_norm": 5.354525089263916,
      "learning_rate": 0.0004817552555118783,
      "loss": 2.8324,
      "step": 428
    },
    {
      "epoch": 0.0018330669902663715,
      "grad_norm": 2.6947083473205566,
      "learning_rate": 0.0004817125277730303,
      "loss": 0.4272,
      "step": 429
    },
    {
      "epoch": 0.001837339873693566,
      "grad_norm": 3.4246561527252197,
      "learning_rate": 0.0004816698000341822,
      "loss": 1.3259,
      "step": 430
    },
    {
      "epoch": 0.0018416127571207603,
      "grad_norm": 2.088681221008301,
      "learning_rate": 0.00048162707229533415,
      "loss": 0.8461,
      "step": 431
    },
    {
      "epoch": 0.0018458856405479545,
      "grad_norm": 4.06411600112915,
      "learning_rate": 0.00048158434455648606,
      "loss": 1.9803,
      "step": 432
    },
    {
      "epoch": 0.0018501585239751488,
      "grad_norm": 23.085840225219727,
      "learning_rate": 0.000481541616817638,
      "loss": 5.3391,
      "step": 433
    },
    {
      "epoch": 0.0018544314074023433,
      "grad_norm": 4.16632080078125,
      "learning_rate": 0.00048149888907879,
      "loss": 1.0413,
      "step": 434
    },
    {
      "epoch": 0.0018587042908295376,
      "grad_norm": 1.039085865020752,
      "learning_rate": 0.0004814561613399419,
      "loss": 0.2063,
      "step": 435
    },
    {
      "epoch": 0.001862977174256732,
      "grad_norm": 1.2710613012313843,
      "learning_rate": 0.00048141343360109387,
      "loss": 0.5293,
      "step": 436
    },
    {
      "epoch": 0.0018672500576839262,
      "grad_norm": 2.1894278526306152,
      "learning_rate": 0.0004813707058622458,
      "loss": 0.8194,
      "step": 437
    },
    {
      "epoch": 0.0018715229411111207,
      "grad_norm": 4.0603837966918945,
      "learning_rate": 0.00048132797812339774,
      "loss": 1.8044,
      "step": 438
    },
    {
      "epoch": 0.001875795824538315,
      "grad_norm": 2.666315793991089,
      "learning_rate": 0.00048128525038454965,
      "loss": 1.0884,
      "step": 439
    },
    {
      "epoch": 0.0018800687079655093,
      "grad_norm": 2.7733213901519775,
      "learning_rate": 0.0004812425226457016,
      "loss": 0.9083,
      "step": 440
    },
    {
      "epoch": 0.0018843415913927036,
      "grad_norm": 2.220841646194458,
      "learning_rate": 0.0004811997949068536,
      "loss": 0.9558,
      "step": 441
    },
    {
      "epoch": 0.0018886144748198979,
      "grad_norm": 0.37739014625549316,
      "learning_rate": 0.0004811570671680055,
      "loss": 0.0542,
      "step": 442
    },
    {
      "epoch": 0.0018928873582470924,
      "grad_norm": 5.26190710067749,
      "learning_rate": 0.00048111433942915746,
      "loss": 2.0887,
      "step": 443
    },
    {
      "epoch": 0.0018971602416742867,
      "grad_norm": 1.754833459854126,
      "learning_rate": 0.00048107161169030937,
      "loss": 0.6622,
      "step": 444
    },
    {
      "epoch": 0.001901433125101481,
      "grad_norm": 2.097801685333252,
      "learning_rate": 0.0004810288839514613,
      "loss": 0.3824,
      "step": 445
    },
    {
      "epoch": 0.0019057060085286752,
      "grad_norm": 3.6376054286956787,
      "learning_rate": 0.0004809861562126132,
      "loss": 1.4037,
      "step": 446
    },
    {
      "epoch": 0.0019099788919558697,
      "grad_norm": 8.114368438720703,
      "learning_rate": 0.00048094342847376516,
      "loss": 1.6139,
      "step": 447
    },
    {
      "epoch": 0.001914251775383064,
      "grad_norm": 2.475759506225586,
      "learning_rate": 0.0004809007007349171,
      "loss": 0.9164,
      "step": 448
    },
    {
      "epoch": 0.0019185246588102583,
      "grad_norm": 5.002713203430176,
      "learning_rate": 0.00048085797299606903,
      "loss": 2.7063,
      "step": 449
    },
    {
      "epoch": 0.0019227975422374526,
      "grad_norm": 3.0321707725524902,
      "learning_rate": 0.000480815245257221,
      "loss": 1.1059,
      "step": 450
    },
    {
      "epoch": 0.001927070425664647,
      "grad_norm": 2.265699625015259,
      "learning_rate": 0.0004807725175183729,
      "loss": 0.7198,
      "step": 451
    },
    {
      "epoch": 0.0019313433090918414,
      "grad_norm": 2.380944013595581,
      "learning_rate": 0.00048072978977952487,
      "loss": 0.5487,
      "step": 452
    },
    {
      "epoch": 0.0019356161925190357,
      "grad_norm": 2.136137008666992,
      "learning_rate": 0.0004806870620406768,
      "loss": 0.3731,
      "step": 453
    },
    {
      "epoch": 0.00193988907594623,
      "grad_norm": 2.6176528930664062,
      "learning_rate": 0.00048064433430182875,
      "loss": 0.7842,
      "step": 454
    },
    {
      "epoch": 0.0019441619593734245,
      "grad_norm": 6.091004371643066,
      "learning_rate": 0.0004806016065629807,
      "loss": 1.3958,
      "step": 455
    },
    {
      "epoch": 0.0019484348428006188,
      "grad_norm": 2.289351224899292,
      "learning_rate": 0.0004805588788241326,
      "loss": 1.0888,
      "step": 456
    },
    {
      "epoch": 0.001952707726227813,
      "grad_norm": 2.5238027572631836,
      "learning_rate": 0.0004805161510852846,
      "loss": 0.8661,
      "step": 457
    },
    {
      "epoch": 0.0019569806096550076,
      "grad_norm": 4.364010810852051,
      "learning_rate": 0.0004804734233464365,
      "loss": 2.4627,
      "step": 458
    },
    {
      "epoch": 0.001961253493082202,
      "grad_norm": 1.8783998489379883,
      "learning_rate": 0.00048043069560758846,
      "loss": 0.9049,
      "step": 459
    },
    {
      "epoch": 0.001965526376509396,
      "grad_norm": 4.053070068359375,
      "learning_rate": 0.0004803879678687404,
      "loss": 1.2257,
      "step": 460
    },
    {
      "epoch": 0.0019697992599365904,
      "grad_norm": 1.8600176572799683,
      "learning_rate": 0.00048034524012989234,
      "loss": 1.0466,
      "step": 461
    },
    {
      "epoch": 0.0019740721433637847,
      "grad_norm": 2.593270778656006,
      "learning_rate": 0.0004803025123910443,
      "loss": 1.0155,
      "step": 462
    },
    {
      "epoch": 0.001978345026790979,
      "grad_norm": 0.8752343058586121,
      "learning_rate": 0.0004802597846521962,
      "loss": 0.2004,
      "step": 463
    },
    {
      "epoch": 0.0019826179102181733,
      "grad_norm": 0.7574807405471802,
      "learning_rate": 0.0004802170569133482,
      "loss": 0.1761,
      "step": 464
    },
    {
      "epoch": 0.0019868907936453676,
      "grad_norm": 2.379781723022461,
      "learning_rate": 0.0004801743291745001,
      "loss": 0.6174,
      "step": 465
    },
    {
      "epoch": 0.0019911636770725623,
      "grad_norm": 0.6275109052658081,
      "learning_rate": 0.00048013160143565206,
      "loss": 0.1381,
      "step": 466
    },
    {
      "epoch": 0.0019954365604997566,
      "grad_norm": 5.596064567565918,
      "learning_rate": 0.00048008887369680397,
      "loss": 4.1607,
      "step": 467
    },
    {
      "epoch": 0.001999709443926951,
      "grad_norm": 0.3885219991207123,
      "learning_rate": 0.00048004614595795593,
      "loss": 0.0798,
      "step": 468
    },
    {
      "epoch": 0.002003982327354145,
      "grad_norm": 2.486656904220581,
      "learning_rate": 0.00048000341821910784,
      "loss": 0.6683,
      "step": 469
    },
    {
      "epoch": 0.0020082552107813395,
      "grad_norm": 2.6685214042663574,
      "learning_rate": 0.0004799606904802598,
      "loss": 1.3776,
      "step": 470
    },
    {
      "epoch": 0.0020125280942085337,
      "grad_norm": 2.4933481216430664,
      "learning_rate": 0.00047991796274141177,
      "loss": 0.6207,
      "step": 471
    },
    {
      "epoch": 0.002016800977635728,
      "grad_norm": 1.9001765251159668,
      "learning_rate": 0.0004798752350025637,
      "loss": 1.1712,
      "step": 472
    },
    {
      "epoch": 0.0020210738610629223,
      "grad_norm": 2.9837818145751953,
      "learning_rate": 0.00047983250726371565,
      "loss": 1.0589,
      "step": 473
    },
    {
      "epoch": 0.0020253467444901166,
      "grad_norm": 0.18134529888629913,
      "learning_rate": 0.00047978977952486756,
      "loss": 0.0292,
      "step": 474
    },
    {
      "epoch": 0.0020296196279173113,
      "grad_norm": 3.0460946559906006,
      "learning_rate": 0.0004797470517860195,
      "loss": 0.898,
      "step": 475
    },
    {
      "epoch": 0.0020338925113445056,
      "grad_norm": 1.3167654275894165,
      "learning_rate": 0.00047970432404717143,
      "loss": 0.5544,
      "step": 476
    },
    {
      "epoch": 0.0020381653947717,
      "grad_norm": 2.870924234390259,
      "learning_rate": 0.0004796615963083234,
      "loss": 0.7747,
      "step": 477
    },
    {
      "epoch": 0.002042438278198894,
      "grad_norm": 2.1151156425476074,
      "learning_rate": 0.0004796188685694753,
      "loss": 0.6562,
      "step": 478
    },
    {
      "epoch": 0.0020467111616260885,
      "grad_norm": 6.449214458465576,
      "learning_rate": 0.0004795761408306272,
      "loss": 3.0524,
      "step": 479
    },
    {
      "epoch": 0.0020509840450532828,
      "grad_norm": 2.1032233238220215,
      "learning_rate": 0.0004795334130917792,
      "loss": 0.6871,
      "step": 480
    },
    {
      "epoch": 0.002055256928480477,
      "grad_norm": 2.7768242359161377,
      "learning_rate": 0.0004794906853529311,
      "loss": 0.569,
      "step": 481
    },
    {
      "epoch": 0.0020595298119076713,
      "grad_norm": 2.170923948287964,
      "learning_rate": 0.00047944795761408306,
      "loss": 0.9783,
      "step": 482
    },
    {
      "epoch": 0.002063802695334866,
      "grad_norm": 1.2422490119934082,
      "learning_rate": 0.00047940522987523497,
      "loss": 0.5127,
      "step": 483
    },
    {
      "epoch": 0.0020680755787620604,
      "grad_norm": 3.3322229385375977,
      "learning_rate": 0.00047936250213638694,
      "loss": 1.0288,
      "step": 484
    },
    {
      "epoch": 0.0020723484621892546,
      "grad_norm": 1.2777750492095947,
      "learning_rate": 0.0004793197743975389,
      "loss": 0.4915,
      "step": 485
    },
    {
      "epoch": 0.002076621345616449,
      "grad_norm": 2.5228147506713867,
      "learning_rate": 0.0004792770466586908,
      "loss": 1.2657,
      "step": 486
    },
    {
      "epoch": 0.0020808942290436432,
      "grad_norm": 1.7037692070007324,
      "learning_rate": 0.0004792343189198428,
      "loss": 0.2594,
      "step": 487
    },
    {
      "epoch": 0.0020851671124708375,
      "grad_norm": 2.5593316555023193,
      "learning_rate": 0.0004791915911809947,
      "loss": 1.2851,
      "step": 488
    },
    {
      "epoch": 0.002089439995898032,
      "grad_norm": 7.165737628936768,
      "learning_rate": 0.00047914886344214665,
      "loss": 1.6205,
      "step": 489
    },
    {
      "epoch": 0.002093712879325226,
      "grad_norm": 2.858746290206909,
      "learning_rate": 0.00047910613570329856,
      "loss": 1.0203,
      "step": 490
    },
    {
      "epoch": 0.0020979857627524204,
      "grad_norm": 5.676458835601807,
      "learning_rate": 0.00047906340796445053,
      "loss": 1.7503,
      "step": 491
    },
    {
      "epoch": 0.002102258646179615,
      "grad_norm": 1.9179158210754395,
      "learning_rate": 0.0004790206802256025,
      "loss": 0.3122,
      "step": 492
    },
    {
      "epoch": 0.0021065315296068094,
      "grad_norm": 3.227630138397217,
      "learning_rate": 0.0004789779524867544,
      "loss": 0.7017,
      "step": 493
    },
    {
      "epoch": 0.0021108044130340037,
      "grad_norm": 2.5568490028381348,
      "learning_rate": 0.00047893522474790637,
      "loss": 0.8549,
      "step": 494
    },
    {
      "epoch": 0.002115077296461198,
      "grad_norm": 1.2072097063064575,
      "learning_rate": 0.0004788924970090583,
      "loss": 0.187,
      "step": 495
    },
    {
      "epoch": 0.0021193501798883923,
      "grad_norm": 2.271268129348755,
      "learning_rate": 0.00047884976927021024,
      "loss": 0.7715,
      "step": 496
    },
    {
      "epoch": 0.0021236230633155865,
      "grad_norm": 5.424625873565674,
      "learning_rate": 0.00047880704153136216,
      "loss": 1.8531,
      "step": 497
    },
    {
      "epoch": 0.002127895946742781,
      "grad_norm": 1.2031924724578857,
      "learning_rate": 0.0004787643137925141,
      "loss": 0.3832,
      "step": 498
    },
    {
      "epoch": 0.002132168830169975,
      "grad_norm": 2.2326793670654297,
      "learning_rate": 0.00047872158605366603,
      "loss": 1.1204,
      "step": 499
    },
    {
      "epoch": 0.00213644171359717,
      "grad_norm": 1.9015820026397705,
      "learning_rate": 0.000478678858314818,
      "loss": 1.3647,
      "step": 500
    },
    {
      "epoch": 0.002140714597024364,
      "grad_norm": 3.055511236190796,
      "learning_rate": 0.00047863613057596996,
      "loss": 1.0479,
      "step": 501
    },
    {
      "epoch": 0.0021449874804515584,
      "grad_norm": 1.1850860118865967,
      "learning_rate": 0.00047859340283712187,
      "loss": 0.4037,
      "step": 502
    },
    {
      "epoch": 0.0021492603638787527,
      "grad_norm": 1.7316209077835083,
      "learning_rate": 0.00047855067509827384,
      "loss": 0.6504,
      "step": 503
    },
    {
      "epoch": 0.002153533247305947,
      "grad_norm": 1.4385108947753906,
      "learning_rate": 0.00047850794735942575,
      "loss": 0.3022,
      "step": 504
    },
    {
      "epoch": 0.0021578061307331413,
      "grad_norm": 3.199958324432373,
      "learning_rate": 0.0004784652196205777,
      "loss": 1.1254,
      "step": 505
    },
    {
      "epoch": 0.0021620790141603356,
      "grad_norm": 2.2315101623535156,
      "learning_rate": 0.0004784224918817296,
      "loss": 0.7772,
      "step": 506
    },
    {
      "epoch": 0.00216635189758753,
      "grad_norm": 1.263959527015686,
      "learning_rate": 0.0004783797641428816,
      "loss": 0.2437,
      "step": 507
    },
    {
      "epoch": 0.002170624781014724,
      "grad_norm": 1.091982364654541,
      "learning_rate": 0.00047833703640403355,
      "loss": 0.2105,
      "step": 508
    },
    {
      "epoch": 0.002174897664441919,
      "grad_norm": 3.389024496078491,
      "learning_rate": 0.00047829430866518546,
      "loss": 0.6609,
      "step": 509
    },
    {
      "epoch": 0.002179170547869113,
      "grad_norm": 3.3297712802886963,
      "learning_rate": 0.00047825158092633743,
      "loss": 0.8983,
      "step": 510
    },
    {
      "epoch": 0.0021834434312963075,
      "grad_norm": 2.057076930999756,
      "learning_rate": 0.0004782088531874893,
      "loss": 1.0979,
      "step": 511
    },
    {
      "epoch": 0.0021877163147235017,
      "grad_norm": 0.6984241008758545,
      "learning_rate": 0.00047816612544864125,
      "loss": 0.1219,
      "step": 512
    },
    {
      "epoch": 0.002191989198150696,
      "grad_norm": 4.891090393066406,
      "learning_rate": 0.00047812339770979316,
      "loss": 3.0827,
      "step": 513
    },
    {
      "epoch": 0.0021962620815778903,
      "grad_norm": 1.6694107055664062,
      "learning_rate": 0.0004780806699709451,
      "loss": 1.182,
      "step": 514
    },
    {
      "epoch": 0.0022005349650050846,
      "grad_norm": 0.4924973249435425,
      "learning_rate": 0.0004780379422320971,
      "loss": 0.077,
      "step": 515
    },
    {
      "epoch": 0.002204807848432279,
      "grad_norm": 1.627929449081421,
      "learning_rate": 0.000477995214493249,
      "loss": 0.6455,
      "step": 516
    },
    {
      "epoch": 0.0022090807318594736,
      "grad_norm": 1.870009422302246,
      "learning_rate": 0.00047795248675440097,
      "loss": 0.9167,
      "step": 517
    },
    {
      "epoch": 0.002213353615286668,
      "grad_norm": 2.325678825378418,
      "learning_rate": 0.0004779097590155529,
      "loss": 0.6054,
      "step": 518
    },
    {
      "epoch": 0.002217626498713862,
      "grad_norm": 2.330095052719116,
      "learning_rate": 0.00047786703127670484,
      "loss": 0.6034,
      "step": 519
    },
    {
      "epoch": 0.0022218993821410565,
      "grad_norm": 1.8284869194030762,
      "learning_rate": 0.00047782430353785675,
      "loss": 0.8324,
      "step": 520
    },
    {
      "epoch": 0.0022261722655682508,
      "grad_norm": 15.243181228637695,
      "learning_rate": 0.0004777815757990087,
      "loss": 2.9763,
      "step": 521
    },
    {
      "epoch": 0.002230445148995445,
      "grad_norm": 0.516385018825531,
      "learning_rate": 0.0004777388480601607,
      "loss": 0.0736,
      "step": 522
    },
    {
      "epoch": 0.0022347180324226393,
      "grad_norm": 1.853158950805664,
      "learning_rate": 0.0004776961203213126,
      "loss": 0.3924,
      "step": 523
    },
    {
      "epoch": 0.0022389909158498336,
      "grad_norm": 0.720072329044342,
      "learning_rate": 0.00047765339258246456,
      "loss": 0.1021,
      "step": 524
    },
    {
      "epoch": 0.002243263799277028,
      "grad_norm": 2.489081859588623,
      "learning_rate": 0.00047761066484361647,
      "loss": 0.7564,
      "step": 525
    },
    {
      "epoch": 0.0022475366827042226,
      "grad_norm": 0.782084047794342,
      "learning_rate": 0.00047756793710476843,
      "loss": 0.1086,
      "step": 526
    },
    {
      "epoch": 0.002251809566131417,
      "grad_norm": 4.372599124908447,
      "learning_rate": 0.00047752520936592035,
      "loss": 0.3649,
      "step": 527
    },
    {
      "epoch": 0.0022560824495586112,
      "grad_norm": 4.213016033172607,
      "learning_rate": 0.0004774824816270723,
      "loss": 2.2054,
      "step": 528
    },
    {
      "epoch": 0.0022603553329858055,
      "grad_norm": 3.7935640811920166,
      "learning_rate": 0.0004774397538882243,
      "loss": 1.7482,
      "step": 529
    },
    {
      "epoch": 0.002264628216413,
      "grad_norm": 2.4461655616760254,
      "learning_rate": 0.0004773970261493762,
      "loss": 0.8975,
      "step": 530
    },
    {
      "epoch": 0.002268901099840194,
      "grad_norm": 2.4856839179992676,
      "learning_rate": 0.00047735429841052815,
      "loss": 0.8242,
      "step": 531
    },
    {
      "epoch": 0.0022731739832673884,
      "grad_norm": 4.023911476135254,
      "learning_rate": 0.00047731157067168006,
      "loss": 2.3005,
      "step": 532
    },
    {
      "epoch": 0.0022774468666945827,
      "grad_norm": 11.669482231140137,
      "learning_rate": 0.000477268842932832,
      "loss": 4.389,
      "step": 533
    },
    {
      "epoch": 0.0022817197501217774,
      "grad_norm": 1.7046526670455933,
      "learning_rate": 0.00047722611519398394,
      "loss": 1.0024,
      "step": 534
    },
    {
      "epoch": 0.0022859926335489717,
      "grad_norm": 7.394173622131348,
      "learning_rate": 0.0004771833874551359,
      "loss": 1.8575,
      "step": 535
    },
    {
      "epoch": 0.002290265516976166,
      "grad_norm": 0.352748304605484,
      "learning_rate": 0.0004771406597162878,
      "loss": 0.0473,
      "step": 536
    },
    {
      "epoch": 0.0022945384004033603,
      "grad_norm": 1.4735617637634277,
      "learning_rate": 0.0004770979319774398,
      "loss": 0.9785,
      "step": 537
    },
    {
      "epoch": 0.0022988112838305545,
      "grad_norm": 1.3483541011810303,
      "learning_rate": 0.00047705520423859174,
      "loss": 0.6322,
      "step": 538
    },
    {
      "epoch": 0.002303084167257749,
      "grad_norm": 3.2251384258270264,
      "learning_rate": 0.00047701247649974365,
      "loss": 0.789,
      "step": 539
    },
    {
      "epoch": 0.002307357050684943,
      "grad_norm": 2.168527603149414,
      "learning_rate": 0.0004769697487608956,
      "loss": 1.0318,
      "step": 540
    },
    {
      "epoch": 0.0023116299341121374,
      "grad_norm": 6.219940662384033,
      "learning_rate": 0.00047692702102204753,
      "loss": 1.1558,
      "step": 541
    },
    {
      "epoch": 0.0023159028175393317,
      "grad_norm": 2.464008092880249,
      "learning_rate": 0.0004768842932831995,
      "loss": 1.5629,
      "step": 542
    },
    {
      "epoch": 0.0023201757009665264,
      "grad_norm": 2.0097129344940186,
      "learning_rate": 0.0004768415655443514,
      "loss": 0.8744,
      "step": 543
    },
    {
      "epoch": 0.0023244485843937207,
      "grad_norm": 1.5345262289047241,
      "learning_rate": 0.0004767988378055033,
      "loss": 0.9219,
      "step": 544
    },
    {
      "epoch": 0.002328721467820915,
      "grad_norm": 3.884730577468872,
      "learning_rate": 0.0004767561100666553,
      "loss": 2.2358,
      "step": 545
    },
    {
      "epoch": 0.0023329943512481093,
      "grad_norm": 2.4202582836151123,
      "learning_rate": 0.0004767133823278072,
      "loss": 0.7241,
      "step": 546
    },
    {
      "epoch": 0.0023372672346753036,
      "grad_norm": 3.5517826080322266,
      "learning_rate": 0.00047667065458895916,
      "loss": 2.0967,
      "step": 547
    },
    {
      "epoch": 0.002341540118102498,
      "grad_norm": 2.9745044708251953,
      "learning_rate": 0.00047662792685011107,
      "loss": 1.2092,
      "step": 548
    },
    {
      "epoch": 0.002345813001529692,
      "grad_norm": 4.0776872634887695,
      "learning_rate": 0.00047658519911126303,
      "loss": 1.8101,
      "step": 549
    },
    {
      "epoch": 0.0023500858849568864,
      "grad_norm": 23.00278663635254,
      "learning_rate": 0.00047654247137241494,
      "loss": 1.5638,
      "step": 550
    },
    {
      "epoch": 0.002354358768384081,
      "grad_norm": 1.287638545036316,
      "learning_rate": 0.0004764997436335669,
      "loss": 0.8296,
      "step": 551
    },
    {
      "epoch": 0.0023586316518112754,
      "grad_norm": 0.7889854907989502,
      "learning_rate": 0.00047645701589471887,
      "loss": 0.1376,
      "step": 552
    },
    {
      "epoch": 0.0023629045352384697,
      "grad_norm": 1.5958459377288818,
      "learning_rate": 0.0004764142881558708,
      "loss": 0.4653,
      "step": 553
    },
    {
      "epoch": 0.002367177418665664,
      "grad_norm": 3.6857612133026123,
      "learning_rate": 0.00047637156041702275,
      "loss": 1.1816,
      "step": 554
    },
    {
      "epoch": 0.0023714503020928583,
      "grad_norm": 2.6132540702819824,
      "learning_rate": 0.00047632883267817466,
      "loss": 1.0793,
      "step": 555
    },
    {
      "epoch": 0.0023757231855200526,
      "grad_norm": 7.326721668243408,
      "learning_rate": 0.0004762861049393266,
      "loss": 1.1364,
      "step": 556
    },
    {
      "epoch": 0.002379996068947247,
      "grad_norm": 0.92293781042099,
      "learning_rate": 0.00047624337720047853,
      "loss": 0.1494,
      "step": 557
    },
    {
      "epoch": 0.002384268952374441,
      "grad_norm": 3.2365550994873047,
      "learning_rate": 0.0004762006494616305,
      "loss": 0.7903,
      "step": 558
    },
    {
      "epoch": 0.0023885418358016355,
      "grad_norm": 1.2814429998397827,
      "learning_rate": 0.00047615792172278246,
      "loss": 0.6667,
      "step": 559
    },
    {
      "epoch": 0.00239281471922883,
      "grad_norm": 10.584473609924316,
      "learning_rate": 0.0004761151939839344,
      "loss": 1.8629,
      "step": 560
    },
    {
      "epoch": 0.0023970876026560245,
      "grad_norm": 1.9700500965118408,
      "learning_rate": 0.00047607246624508634,
      "loss": 0.7248,
      "step": 561
    },
    {
      "epoch": 0.0024013604860832188,
      "grad_norm": 2.449432611465454,
      "learning_rate": 0.00047602973850623825,
      "loss": 1.5125,
      "step": 562
    },
    {
      "epoch": 0.002405633369510413,
      "grad_norm": 6.904421806335449,
      "learning_rate": 0.0004759870107673902,
      "loss": 1.4176,
      "step": 563
    },
    {
      "epoch": 0.0024099062529376073,
      "grad_norm": 1.361240029335022,
      "learning_rate": 0.0004759442830285421,
      "loss": 0.7134,
      "step": 564
    },
    {
      "epoch": 0.0024141791363648016,
      "grad_norm": 2.273308515548706,
      "learning_rate": 0.0004759015552896941,
      "loss": 0.6288,
      "step": 565
    },
    {
      "epoch": 0.002418452019791996,
      "grad_norm": 1.322403073310852,
      "learning_rate": 0.00047585882755084606,
      "loss": 0.3574,
      "step": 566
    },
    {
      "epoch": 0.00242272490321919,
      "grad_norm": 2.7865078449249268,
      "learning_rate": 0.00047581609981199797,
      "loss": 1.4624,
      "step": 567
    },
    {
      "epoch": 0.0024269977866463845,
      "grad_norm": 2.03625226020813,
      "learning_rate": 0.00047577337207314993,
      "loss": 1.1838,
      "step": 568
    },
    {
      "epoch": 0.0024312706700735792,
      "grad_norm": 2.439175844192505,
      "learning_rate": 0.00047573064433430184,
      "loss": 0.7043,
      "step": 569
    },
    {
      "epoch": 0.0024355435535007735,
      "grad_norm": 3.6782119274139404,
      "learning_rate": 0.0004756879165954538,
      "loss": 1.2766,
      "step": 570
    },
    {
      "epoch": 0.002439816436927968,
      "grad_norm": 1.8082842826843262,
      "learning_rate": 0.0004756451888566057,
      "loss": 1.0523,
      "step": 571
    },
    {
      "epoch": 0.002444089320355162,
      "grad_norm": 1.2565861940383911,
      "learning_rate": 0.0004756024611177577,
      "loss": 0.6225,
      "step": 572
    },
    {
      "epoch": 0.0024483622037823564,
      "grad_norm": 3.0314018726348877,
      "learning_rate": 0.0004755597333789096,
      "loss": 0.7254,
      "step": 573
    },
    {
      "epoch": 0.0024526350872095507,
      "grad_norm": 2.290571928024292,
      "learning_rate": 0.00047551700564006156,
      "loss": 0.5287,
      "step": 574
    },
    {
      "epoch": 0.002456907970636745,
      "grad_norm": 1.8000324964523315,
      "learning_rate": 0.0004754742779012135,
      "loss": 0.9906,
      "step": 575
    },
    {
      "epoch": 0.0024611808540639392,
      "grad_norm": 2.8856656551361084,
      "learning_rate": 0.00047543155016236543,
      "loss": 0.4424,
      "step": 576
    },
    {
      "epoch": 0.002465453737491134,
      "grad_norm": 2.249169111251831,
      "learning_rate": 0.00047538882242351735,
      "loss": 0.4982,
      "step": 577
    },
    {
      "epoch": 0.0024697266209183283,
      "grad_norm": 4.422823429107666,
      "learning_rate": 0.00047534609468466926,
      "loss": 1.502,
      "step": 578
    },
    {
      "epoch": 0.0024739995043455225,
      "grad_norm": 6.368921279907227,
      "learning_rate": 0.0004753033669458212,
      "loss": 2.3193,
      "step": 579
    },
    {
      "epoch": 0.002478272387772717,
      "grad_norm": 2.7927350997924805,
      "learning_rate": 0.00047526063920697313,
      "loss": 0.7884,
      "step": 580
    },
    {
      "epoch": 0.002482545271199911,
      "grad_norm": 1.8650026321411133,
      "learning_rate": 0.0004752179114681251,
      "loss": 0.8289,
      "step": 581
    },
    {
      "epoch": 0.0024868181546271054,
      "grad_norm": 1.863092303276062,
      "learning_rate": 0.00047517518372927706,
      "loss": 0.3744,
      "step": 582
    },
    {
      "epoch": 0.0024910910380542997,
      "grad_norm": 2.8033359050750732,
      "learning_rate": 0.00047513245599042897,
      "loss": 1.0843,
      "step": 583
    },
    {
      "epoch": 0.002495363921481494,
      "grad_norm": 8.052555084228516,
      "learning_rate": 0.00047508972825158094,
      "loss": 3.0164,
      "step": 584
    },
    {
      "epoch": 0.0024996368049086883,
      "grad_norm": 1.5419350862503052,
      "learning_rate": 0.00047504700051273285,
      "loss": 0.5661,
      "step": 585
    },
    {
      "epoch": 0.002503909688335883,
      "grad_norm": 6.234055519104004,
      "learning_rate": 0.0004750042727738848,
      "loss": 1.0539,
      "step": 586
    },
    {
      "epoch": 0.0025081825717630773,
      "grad_norm": 5.440796375274658,
      "learning_rate": 0.0004749615450350367,
      "loss": 1.3992,
      "step": 587
    },
    {
      "epoch": 0.0025124554551902716,
      "grad_norm": 7.002929210662842,
      "learning_rate": 0.0004749188172961887,
      "loss": 2.4734,
      "step": 588
    },
    {
      "epoch": 0.002516728338617466,
      "grad_norm": 2.985811948776245,
      "learning_rate": 0.00047487608955734065,
      "loss": 0.6969,
      "step": 589
    },
    {
      "epoch": 0.00252100122204466,
      "grad_norm": 1.2062416076660156,
      "learning_rate": 0.00047483336181849256,
      "loss": 0.2156,
      "step": 590
    },
    {
      "epoch": 0.0025252741054718544,
      "grad_norm": 2.248382568359375,
      "learning_rate": 0.00047479063407964453,
      "loss": 0.7784,
      "step": 591
    },
    {
      "epoch": 0.0025295469888990487,
      "grad_norm": 4.501870155334473,
      "learning_rate": 0.00047474790634079644,
      "loss": 0.9915,
      "step": 592
    },
    {
      "epoch": 0.002533819872326243,
      "grad_norm": 2.4437615871429443,
      "learning_rate": 0.0004747051786019484,
      "loss": 0.5532,
      "step": 593
    },
    {
      "epoch": 0.0025380927557534377,
      "grad_norm": 0.8371524810791016,
      "learning_rate": 0.0004746624508631003,
      "loss": 0.1643,
      "step": 594
    },
    {
      "epoch": 0.002542365639180632,
      "grad_norm": 3.10353946685791,
      "learning_rate": 0.0004746197231242523,
      "loss": 0.7174,
      "step": 595
    },
    {
      "epoch": 0.0025466385226078263,
      "grad_norm": 6.488059997558594,
      "learning_rate": 0.00047457699538540425,
      "loss": 2.5366,
      "step": 596
    },
    {
      "epoch": 0.0025509114060350206,
      "grad_norm": 3.7385475635528564,
      "learning_rate": 0.00047453426764655616,
      "loss": 1.1596,
      "step": 597
    },
    {
      "epoch": 0.002555184289462215,
      "grad_norm": 2.2305116653442383,
      "learning_rate": 0.0004744915399077081,
      "loss": 0.5833,
      "step": 598
    },
    {
      "epoch": 0.002559457172889409,
      "grad_norm": 2.2858967781066895,
      "learning_rate": 0.00047444881216886003,
      "loss": 0.8036,
      "step": 599
    },
    {
      "epoch": 0.0025637300563166035,
      "grad_norm": 1.2919936180114746,
      "learning_rate": 0.000474406084430012,
      "loss": 0.3044,
      "step": 600
    },
    {
      "epoch": 0.0025680029397437978,
      "grad_norm": 6.893960952758789,
      "learning_rate": 0.0004743633566911639,
      "loss": 2.0465,
      "step": 601
    },
    {
      "epoch": 0.002572275823170992,
      "grad_norm": 4.25232458114624,
      "learning_rate": 0.0004743206289523159,
      "loss": 2.0547,
      "step": 602
    },
    {
      "epoch": 0.0025765487065981868,
      "grad_norm": 13.569190979003906,
      "learning_rate": 0.0004742779012134678,
      "loss": 4.1616,
      "step": 603
    },
    {
      "epoch": 0.002580821590025381,
      "grad_norm": 1.6330592632293701,
      "learning_rate": 0.00047423517347461975,
      "loss": 0.5116,
      "step": 604
    },
    {
      "epoch": 0.0025850944734525753,
      "grad_norm": 2.266634941101074,
      "learning_rate": 0.0004741924457357717,
      "loss": 0.4334,
      "step": 605
    },
    {
      "epoch": 0.0025893673568797696,
      "grad_norm": 1.3586455583572388,
      "learning_rate": 0.0004741497179969236,
      "loss": 0.29,
      "step": 606
    },
    {
      "epoch": 0.002593640240306964,
      "grad_norm": 1.9511348009109497,
      "learning_rate": 0.0004741069902580756,
      "loss": 0.2899,
      "step": 607
    },
    {
      "epoch": 0.002597913123734158,
      "grad_norm": 6.061917781829834,
      "learning_rate": 0.0004740642625192275,
      "loss": 1.0531,
      "step": 608
    },
    {
      "epoch": 0.0026021860071613525,
      "grad_norm": 3.0723190307617188,
      "learning_rate": 0.00047402153478037947,
      "loss": 1.0914,
      "step": 609
    },
    {
      "epoch": 0.002606458890588547,
      "grad_norm": 2.926236391067505,
      "learning_rate": 0.0004739788070415313,
      "loss": 0.7518,
      "step": 610
    },
    {
      "epoch": 0.0026107317740157415,
      "grad_norm": 2.151506185531616,
      "learning_rate": 0.0004739360793026833,
      "loss": 0.987,
      "step": 611
    },
    {
      "epoch": 0.002615004657442936,
      "grad_norm": 2.7864303588867188,
      "learning_rate": 0.00047389335156383525,
      "loss": 1.3556,
      "step": 612
    },
    {
      "epoch": 0.00261927754087013,
      "grad_norm": 4.366945266723633,
      "learning_rate": 0.00047385062382498716,
      "loss": 1.8671,
      "step": 613
    },
    {
      "epoch": 0.0026235504242973244,
      "grad_norm": 5.893106937408447,
      "learning_rate": 0.00047380789608613913,
      "loss": 2.2514,
      "step": 614
    },
    {
      "epoch": 0.0026278233077245187,
      "grad_norm": 1.7183427810668945,
      "learning_rate": 0.00047376516834729104,
      "loss": 0.8007,
      "step": 615
    },
    {
      "epoch": 0.002632096191151713,
      "grad_norm": 1.9619277715682983,
      "learning_rate": 0.000473722440608443,
      "loss": 0.7869,
      "step": 616
    },
    {
      "epoch": 0.0026363690745789072,
      "grad_norm": 5.131474494934082,
      "learning_rate": 0.0004736797128695949,
      "loss": 1.8276,
      "step": 617
    },
    {
      "epoch": 0.0026406419580061015,
      "grad_norm": 1.608582854270935,
      "learning_rate": 0.0004736369851307469,
      "loss": 0.4687,
      "step": 618
    },
    {
      "epoch": 0.002644914841433296,
      "grad_norm": 5.2587714195251465,
      "learning_rate": 0.00047359425739189884,
      "loss": 1.579,
      "step": 619
    },
    {
      "epoch": 0.0026491877248604905,
      "grad_norm": 4.790072441101074,
      "learning_rate": 0.00047355152965305075,
      "loss": 1.4921,
      "step": 620
    },
    {
      "epoch": 0.002653460608287685,
      "grad_norm": 3.1109559535980225,
      "learning_rate": 0.0004735088019142027,
      "loss": 0.9763,
      "step": 621
    },
    {
      "epoch": 0.002657733491714879,
      "grad_norm": 3.6147944927215576,
      "learning_rate": 0.00047346607417535463,
      "loss": 0.9672,
      "step": 622
    },
    {
      "epoch": 0.0026620063751420734,
      "grad_norm": 4.206874847412109,
      "learning_rate": 0.0004734233464365066,
      "loss": 1.5464,
      "step": 623
    },
    {
      "epoch": 0.0026662792585692677,
      "grad_norm": 5.247297763824463,
      "learning_rate": 0.0004733806186976585,
      "loss": 1.8413,
      "step": 624
    },
    {
      "epoch": 0.002670552141996462,
      "grad_norm": 1.6016132831573486,
      "learning_rate": 0.00047333789095881047,
      "loss": 0.417,
      "step": 625
    },
    {
      "epoch": 0.0026748250254236563,
      "grad_norm": 3.083479881286621,
      "learning_rate": 0.00047329516321996244,
      "loss": 0.7699,
      "step": 626
    },
    {
      "epoch": 0.0026790979088508506,
      "grad_norm": 1.5226765871047974,
      "learning_rate": 0.00047325243548111435,
      "loss": 0.5284,
      "step": 627
    },
    {
      "epoch": 0.0026833707922780453,
      "grad_norm": 1.9181885719299316,
      "learning_rate": 0.0004732097077422663,
      "loss": 0.815,
      "step": 628
    },
    {
      "epoch": 0.0026876436757052396,
      "grad_norm": 4.4982757568359375,
      "learning_rate": 0.0004731669800034182,
      "loss": 1.8458,
      "step": 629
    },
    {
      "epoch": 0.002691916559132434,
      "grad_norm": 5.074954032897949,
      "learning_rate": 0.0004731242522645702,
      "loss": 2.5787,
      "step": 630
    },
    {
      "epoch": 0.002696189442559628,
      "grad_norm": 4.709406852722168,
      "learning_rate": 0.0004730815245257221,
      "loss": 2.452,
      "step": 631
    },
    {
      "epoch": 0.0027004623259868224,
      "grad_norm": 3.4247827529907227,
      "learning_rate": 0.00047303879678687406,
      "loss": 1.2841,
      "step": 632
    },
    {
      "epoch": 0.0027047352094140167,
      "grad_norm": 3.814159393310547,
      "learning_rate": 0.00047299606904802603,
      "loss": 0.8682,
      "step": 633
    },
    {
      "epoch": 0.002709008092841211,
      "grad_norm": 1.3859479427337646,
      "learning_rate": 0.00047295334130917794,
      "loss": 0.249,
      "step": 634
    },
    {
      "epoch": 0.0027132809762684053,
      "grad_norm": 4.678046703338623,
      "learning_rate": 0.0004729106135703299,
      "loss": 0.7902,
      "step": 635
    },
    {
      "epoch": 0.0027175538596955996,
      "grad_norm": 3.553906202316284,
      "learning_rate": 0.0004728678858314818,
      "loss": 0.5847,
      "step": 636
    },
    {
      "epoch": 0.0027218267431227943,
      "grad_norm": 1.3020830154418945,
      "learning_rate": 0.0004728251580926338,
      "loss": 0.3886,
      "step": 637
    },
    {
      "epoch": 0.0027260996265499886,
      "grad_norm": 3.7925760746002197,
      "learning_rate": 0.0004727824303537857,
      "loss": 0.9901,
      "step": 638
    },
    {
      "epoch": 0.002730372509977183,
      "grad_norm": 2.7186594009399414,
      "learning_rate": 0.00047273970261493765,
      "loss": 0.7762,
      "step": 639
    },
    {
      "epoch": 0.002734645393404377,
      "grad_norm": 8.615888595581055,
      "learning_rate": 0.00047269697487608957,
      "loss": 1.0338,
      "step": 640
    },
    {
      "epoch": 0.0027389182768315715,
      "grad_norm": 1.7914119958877563,
      "learning_rate": 0.00047265424713724153,
      "loss": 0.6055,
      "step": 641
    },
    {
      "epoch": 0.0027431911602587658,
      "grad_norm": 6.303499221801758,
      "learning_rate": 0.00047261151939839344,
      "loss": 2.8582,
      "step": 642
    },
    {
      "epoch": 0.00274746404368596,
      "grad_norm": 0.44178307056427,
      "learning_rate": 0.00047256879165954535,
      "loss": 0.0684,
      "step": 643
    },
    {
      "epoch": 0.0027517369271131543,
      "grad_norm": 4.519567966461182,
      "learning_rate": 0.0004725260639206973,
      "loss": 1.5953,
      "step": 644
    },
    {
      "epoch": 0.002756009810540349,
      "grad_norm": 2.7306129932403564,
      "learning_rate": 0.00047248333618184923,
      "loss": 1.0209,
      "step": 645
    },
    {
      "epoch": 0.0027602826939675433,
      "grad_norm": 6.665770530700684,
      "learning_rate": 0.0004724406084430012,
      "loss": 1.7787,
      "step": 646
    },
    {
      "epoch": 0.0027645555773947376,
      "grad_norm": 3.654181718826294,
      "learning_rate": 0.0004723978807041531,
      "loss": 0.9354,
      "step": 647
    },
    {
      "epoch": 0.002768828460821932,
      "grad_norm": 3.57179594039917,
      "learning_rate": 0.00047235515296530507,
      "loss": 1.7661,
      "step": 648
    },
    {
      "epoch": 0.002773101344249126,
      "grad_norm": 1.6123297214508057,
      "learning_rate": 0.00047231242522645703,
      "loss": 0.7521,
      "step": 649
    },
    {
      "epoch": 0.0027773742276763205,
      "grad_norm": 3.536734104156494,
      "learning_rate": 0.00047226969748760894,
      "loss": 1.1054,
      "step": 650
    },
    {
      "epoch": 0.0027816471111035148,
      "grad_norm": 3.7778677940368652,
      "learning_rate": 0.0004722269697487609,
      "loss": 1.191,
      "step": 651
    },
    {
      "epoch": 0.002785919994530709,
      "grad_norm": 2.9515600204467773,
      "learning_rate": 0.0004721842420099128,
      "loss": 0.6766,
      "step": 652
    },
    {
      "epoch": 0.0027901928779579034,
      "grad_norm": 4.213303565979004,
      "learning_rate": 0.0004721415142710648,
      "loss": 0.9052,
      "step": 653
    },
    {
      "epoch": 0.002794465761385098,
      "grad_norm": 3.0215158462524414,
      "learning_rate": 0.0004720987865322167,
      "loss": 0.8803,
      "step": 654
    },
    {
      "epoch": 0.0027987386448122924,
      "grad_norm": 2.6149284839630127,
      "learning_rate": 0.00047205605879336866,
      "loss": 0.9346,
      "step": 655
    },
    {
      "epoch": 0.0028030115282394867,
      "grad_norm": 2.920032262802124,
      "learning_rate": 0.0004720133310545206,
      "loss": 1.5011,
      "step": 656
    },
    {
      "epoch": 0.002807284411666681,
      "grad_norm": 1.8373827934265137,
      "learning_rate": 0.00047197060331567254,
      "loss": 0.6228,
      "step": 657
    },
    {
      "epoch": 0.0028115572950938752,
      "grad_norm": 3.953110694885254,
      "learning_rate": 0.0004719278755768245,
      "loss": 0.9519,
      "step": 658
    },
    {
      "epoch": 0.0028158301785210695,
      "grad_norm": 4.928561687469482,
      "learning_rate": 0.0004718851478379764,
      "loss": 1.0812,
      "step": 659
    },
    {
      "epoch": 0.002820103061948264,
      "grad_norm": 1.9973841905593872,
      "learning_rate": 0.0004718424200991284,
      "loss": 1.1825,
      "step": 660
    },
    {
      "epoch": 0.002824375945375458,
      "grad_norm": 2.012657880783081,
      "learning_rate": 0.0004717996923602803,
      "loss": 0.8473,
      "step": 661
    },
    {
      "epoch": 0.0028286488288026524,
      "grad_norm": 3.3955554962158203,
      "learning_rate": 0.00047175696462143225,
      "loss": 1.2115,
      "step": 662
    },
    {
      "epoch": 0.002832921712229847,
      "grad_norm": 1.8924641609191895,
      "learning_rate": 0.0004717142368825842,
      "loss": 0.7543,
      "step": 663
    },
    {
      "epoch": 0.0028371945956570414,
      "grad_norm": 0.9125258326530457,
      "learning_rate": 0.00047167150914373613,
      "loss": 0.251,
      "step": 664
    },
    {
      "epoch": 0.0028414674790842357,
      "grad_norm": 1.5186644792556763,
      "learning_rate": 0.0004716287814048881,
      "loss": 0.4356,
      "step": 665
    },
    {
      "epoch": 0.00284574036251143,
      "grad_norm": 0.9787341952323914,
      "learning_rate": 0.00047158605366604,
      "loss": 0.3864,
      "step": 666
    },
    {
      "epoch": 0.0028500132459386243,
      "grad_norm": 1.3280274868011475,
      "learning_rate": 0.00047154332592719197,
      "loss": 0.3674,
      "step": 667
    },
    {
      "epoch": 0.0028542861293658186,
      "grad_norm": 1.5925995111465454,
      "learning_rate": 0.0004715005981883439,
      "loss": 0.7574,
      "step": 668
    },
    {
      "epoch": 0.002858559012793013,
      "grad_norm": 1.2969133853912354,
      "learning_rate": 0.00047145787044949584,
      "loss": 0.39,
      "step": 669
    },
    {
      "epoch": 0.002862831896220207,
      "grad_norm": 1.2359598875045776,
      "learning_rate": 0.0004714151427106478,
      "loss": 0.7155,
      "step": 670
    },
    {
      "epoch": 0.002867104779647402,
      "grad_norm": 1.487403154373169,
      "learning_rate": 0.0004713724149717997,
      "loss": 0.703,
      "step": 671
    },
    {
      "epoch": 0.002871377663074596,
      "grad_norm": 3.7454771995544434,
      "learning_rate": 0.0004713296872329517,
      "loss": 1.7035,
      "step": 672
    },
    {
      "epoch": 0.0028756505465017904,
      "grad_norm": 1.6377028226852417,
      "learning_rate": 0.0004712869594941036,
      "loss": 0.4237,
      "step": 673
    },
    {
      "epoch": 0.0028799234299289847,
      "grad_norm": 2.4762792587280273,
      "learning_rate": 0.00047124423175525556,
      "loss": 0.9387,
      "step": 674
    },
    {
      "epoch": 0.002884196313356179,
      "grad_norm": 16.146135330200195,
      "learning_rate": 0.0004712015040164074,
      "loss": 4.3371,
      "step": 675
    },
    {
      "epoch": 0.0028884691967833733,
      "grad_norm": 1.8783656358718872,
      "learning_rate": 0.0004711587762775594,
      "loss": 0.5175,
      "step": 676
    },
    {
      "epoch": 0.0028927420802105676,
      "grad_norm": 9.1287202835083,
      "learning_rate": 0.0004711160485387113,
      "loss": 1.5648,
      "step": 677
    },
    {
      "epoch": 0.002897014963637762,
      "grad_norm": 1.4799290895462036,
      "learning_rate": 0.00047107332079986326,
      "loss": 0.337,
      "step": 678
    },
    {
      "epoch": 0.002901287847064956,
      "grad_norm": 1.6375863552093506,
      "learning_rate": 0.0004710305930610152,
      "loss": 0.3843,
      "step": 679
    },
    {
      "epoch": 0.002905560730492151,
      "grad_norm": 1.4186211824417114,
      "learning_rate": 0.00047098786532216713,
      "loss": 0.5528,
      "step": 680
    },
    {
      "epoch": 0.002909833613919345,
      "grad_norm": 2.526580810546875,
      "learning_rate": 0.0004709451375833191,
      "loss": 0.9932,
      "step": 681
    },
    {
      "epoch": 0.0029141064973465395,
      "grad_norm": 1.345091462135315,
      "learning_rate": 0.000470902409844471,
      "loss": 0.3078,
      "step": 682
    },
    {
      "epoch": 0.0029183793807737337,
      "grad_norm": 6.014979362487793,
      "learning_rate": 0.000470859682105623,
      "loss": 2.0158,
      "step": 683
    },
    {
      "epoch": 0.002922652264200928,
      "grad_norm": 2.599473476409912,
      "learning_rate": 0.0004708169543667749,
      "loss": 0.6225,
      "step": 684
    },
    {
      "epoch": 0.0029269251476281223,
      "grad_norm": 1.4356870651245117,
      "learning_rate": 0.00047077422662792685,
      "loss": 0.412,
      "step": 685
    },
    {
      "epoch": 0.0029311980310553166,
      "grad_norm": 2.060176134109497,
      "learning_rate": 0.0004707314988890788,
      "loss": 0.6777,
      "step": 686
    },
    {
      "epoch": 0.002935470914482511,
      "grad_norm": 2.5521135330200195,
      "learning_rate": 0.0004706887711502307,
      "loss": 1.098,
      "step": 687
    },
    {
      "epoch": 0.0029397437979097056,
      "grad_norm": 1.1239675283432007,
      "learning_rate": 0.0004706460434113827,
      "loss": 0.233,
      "step": 688
    },
    {
      "epoch": 0.0029440166813369,
      "grad_norm": 3.0771830081939697,
      "learning_rate": 0.0004706033156725346,
      "loss": 0.6931,
      "step": 689
    },
    {
      "epoch": 0.002948289564764094,
      "grad_norm": 2.6469123363494873,
      "learning_rate": 0.00047056058793368657,
      "loss": 0.6051,
      "step": 690
    },
    {
      "epoch": 0.0029525624481912885,
      "grad_norm": 3.24157452583313,
      "learning_rate": 0.0004705178601948385,
      "loss": 1.0608,
      "step": 691
    },
    {
      "epoch": 0.0029568353316184828,
      "grad_norm": 1.0840377807617188,
      "learning_rate": 0.00047047513245599044,
      "loss": 0.2013,
      "step": 692
    },
    {
      "epoch": 0.002961108215045677,
      "grad_norm": 2.4289016723632812,
      "learning_rate": 0.0004704324047171424,
      "loss": 0.8618,
      "step": 693
    },
    {
      "epoch": 0.0029653810984728714,
      "grad_norm": 2.9909422397613525,
      "learning_rate": 0.0004703896769782943,
      "loss": 1.4688,
      "step": 694
    },
    {
      "epoch": 0.0029696539819000656,
      "grad_norm": 11.587393760681152,
      "learning_rate": 0.0004703469492394463,
      "loss": 1.6402,
      "step": 695
    },
    {
      "epoch": 0.00297392686532726,
      "grad_norm": 3.1154046058654785,
      "learning_rate": 0.0004703042215005982,
      "loss": 0.6109,
      "step": 696
    },
    {
      "epoch": 0.0029781997487544547,
      "grad_norm": 4.730388641357422,
      "learning_rate": 0.00047026149376175016,
      "loss": 1.7005,
      "step": 697
    },
    {
      "epoch": 0.002982472632181649,
      "grad_norm": 3.1873602867126465,
      "learning_rate": 0.00047021876602290207,
      "loss": 0.7123,
      "step": 698
    },
    {
      "epoch": 0.0029867455156088432,
      "grad_norm": 1.7347280979156494,
      "learning_rate": 0.00047017603828405403,
      "loss": 0.3819,
      "step": 699
    },
    {
      "epoch": 0.0029910183990360375,
      "grad_norm": 1.234092116355896,
      "learning_rate": 0.000470133310545206,
      "loss": 0.2868,
      "step": 700
    },
    {
      "epoch": 0.002995291282463232,
      "grad_norm": 4.053548336029053,
      "learning_rate": 0.0004700905828063579,
      "loss": 1.4141,
      "step": 701
    },
    {
      "epoch": 0.002999564165890426,
      "grad_norm": 3.824132204055786,
      "learning_rate": 0.0004700478550675099,
      "loss": 1.6957,
      "step": 702
    },
    {
      "epoch": 0.0030038370493176204,
      "grad_norm": 3.127758741378784,
      "learning_rate": 0.0004700051273286618,
      "loss": 0.6631,
      "step": 703
    },
    {
      "epoch": 0.0030081099327448147,
      "grad_norm": 1.8678312301635742,
      "learning_rate": 0.00046996239958981375,
      "loss": 0.4023,
      "step": 704
    },
    {
      "epoch": 0.0030123828161720094,
      "grad_norm": 2.420121192932129,
      "learning_rate": 0.00046991967185096566,
      "loss": 0.5312,
      "step": 705
    },
    {
      "epoch": 0.0030166556995992037,
      "grad_norm": 2.6359353065490723,
      "learning_rate": 0.0004698769441121176,
      "loss": 0.8438,
      "step": 706
    },
    {
      "epoch": 0.003020928583026398,
      "grad_norm": 2.0826454162597656,
      "learning_rate": 0.00046983421637326954,
      "loss": 0.4329,
      "step": 707
    },
    {
      "epoch": 0.0030252014664535923,
      "grad_norm": 1.3619153499603271,
      "learning_rate": 0.00046979148863442145,
      "loss": 0.3334,
      "step": 708
    },
    {
      "epoch": 0.0030294743498807866,
      "grad_norm": 1.3705674409866333,
      "learning_rate": 0.0004697487608955734,
      "loss": 0.2952,
      "step": 709
    },
    {
      "epoch": 0.003033747233307981,
      "grad_norm": 1.8581418991088867,
      "learning_rate": 0.0004697060331567253,
      "loss": 0.4081,
      "step": 710
    },
    {
      "epoch": 0.003038020116735175,
      "grad_norm": 1.2383904457092285,
      "learning_rate": 0.0004696633054178773,
      "loss": 0.2795,
      "step": 711
    },
    {
      "epoch": 0.0030422930001623694,
      "grad_norm": 2.027446746826172,
      "learning_rate": 0.0004696205776790292,
      "loss": 0.5875,
      "step": 712
    },
    {
      "epoch": 0.0030465658835895637,
      "grad_norm": 3.5468766689300537,
      "learning_rate": 0.00046957784994018116,
      "loss": 1.6448,
      "step": 713
    },
    {
      "epoch": 0.0030508387670167584,
      "grad_norm": 3.392510414123535,
      "learning_rate": 0.0004695351222013331,
      "loss": 1.1258,
      "step": 714
    },
    {
      "epoch": 0.0030551116504439527,
      "grad_norm": 3.060289144515991,
      "learning_rate": 0.00046949239446248504,
      "loss": 1.2469,
      "step": 715
    },
    {
      "epoch": 0.003059384533871147,
      "grad_norm": 3.594270944595337,
      "learning_rate": 0.000469449666723637,
      "loss": 1.4073,
      "step": 716
    },
    {
      "epoch": 0.0030636574172983413,
      "grad_norm": 3.3930299282073975,
      "learning_rate": 0.0004694069389847889,
      "loss": 1.6546,
      "step": 717
    },
    {
      "epoch": 0.0030679303007255356,
      "grad_norm": 14.900784492492676,
      "learning_rate": 0.0004693642112459409,
      "loss": 5.1759,
      "step": 718
    },
    {
      "epoch": 0.00307220318415273,
      "grad_norm": 1.713470220565796,
      "learning_rate": 0.0004693214835070928,
      "loss": 0.5309,
      "step": 719
    },
    {
      "epoch": 0.003076476067579924,
      "grad_norm": 3.028456926345825,
      "learning_rate": 0.00046927875576824476,
      "loss": 1.5549,
      "step": 720
    },
    {
      "epoch": 0.0030807489510071184,
      "grad_norm": 1.3334660530090332,
      "learning_rate": 0.00046923602802939667,
      "loss": 0.2571,
      "step": 721
    },
    {
      "epoch": 0.003085021834434313,
      "grad_norm": 1.7087115049362183,
      "learning_rate": 0.00046919330029054863,
      "loss": 0.3415,
      "step": 722
    },
    {
      "epoch": 0.0030892947178615075,
      "grad_norm": 1.2540833950042725,
      "learning_rate": 0.0004691505725517006,
      "loss": 0.2984,
      "step": 723
    },
    {
      "epoch": 0.0030935676012887017,
      "grad_norm": 2.332371473312378,
      "learning_rate": 0.0004691078448128525,
      "loss": 0.6748,
      "step": 724
    },
    {
      "epoch": 0.003097840484715896,
      "grad_norm": 2.0690462589263916,
      "learning_rate": 0.00046906511707400447,
      "loss": 0.6325,
      "step": 725
    },
    {
      "epoch": 0.0031021133681430903,
      "grad_norm": 1.9076207876205444,
      "learning_rate": 0.0004690223893351564,
      "loss": 0.8637,
      "step": 726
    },
    {
      "epoch": 0.0031063862515702846,
      "grad_norm": 2.2100963592529297,
      "learning_rate": 0.00046897966159630835,
      "loss": 0.8325,
      "step": 727
    },
    {
      "epoch": 0.003110659134997479,
      "grad_norm": 1.3092399835586548,
      "learning_rate": 0.00046893693385746026,
      "loss": 0.4549,
      "step": 728
    },
    {
      "epoch": 0.003114932018424673,
      "grad_norm": 2.6584722995758057,
      "learning_rate": 0.0004688942061186122,
      "loss": 1.0051,
      "step": 729
    },
    {
      "epoch": 0.0031192049018518675,
      "grad_norm": 2.5731475353240967,
      "learning_rate": 0.0004688514783797642,
      "loss": 1.1022,
      "step": 730
    },
    {
      "epoch": 0.003123477785279062,
      "grad_norm": 9.67020320892334,
      "learning_rate": 0.0004688087506409161,
      "loss": 1.0515,
      "step": 731
    },
    {
      "epoch": 0.0031277506687062565,
      "grad_norm": 0.8356909155845642,
      "learning_rate": 0.00046876602290206806,
      "loss": 0.202,
      "step": 732
    },
    {
      "epoch": 0.0031320235521334508,
      "grad_norm": 1.7943158149719238,
      "learning_rate": 0.00046872329516322,
      "loss": 0.9115,
      "step": 733
    },
    {
      "epoch": 0.003136296435560645,
      "grad_norm": 2.206733226776123,
      "learning_rate": 0.00046868056742437194,
      "loss": 0.7837,
      "step": 734
    },
    {
      "epoch": 0.0031405693189878394,
      "grad_norm": 3.6349897384643555,
      "learning_rate": 0.00046863783968552385,
      "loss": 1.7527,
      "step": 735
    },
    {
      "epoch": 0.0031448422024150336,
      "grad_norm": 2.8206427097320557,
      "learning_rate": 0.0004685951119466758,
      "loss": 1.1403,
      "step": 736
    },
    {
      "epoch": 0.003149115085842228,
      "grad_norm": 1.9723477363586426,
      "learning_rate": 0.0004685523842078278,
      "loss": 0.5056,
      "step": 737
    },
    {
      "epoch": 0.0031533879692694222,
      "grad_norm": 0.7133227586746216,
      "learning_rate": 0.0004685096564689797,
      "loss": 0.1957,
      "step": 738
    },
    {
      "epoch": 0.003157660852696617,
      "grad_norm": 3.350486993789673,
      "learning_rate": 0.00046846692873013166,
      "loss": 1.069,
      "step": 739
    },
    {
      "epoch": 0.0031619337361238112,
      "grad_norm": 1.9437320232391357,
      "learning_rate": 0.00046842420099128357,
      "loss": 0.4857,
      "step": 740
    },
    {
      "epoch": 0.0031662066195510055,
      "grad_norm": 7.72560453414917,
      "learning_rate": 0.0004683814732524355,
      "loss": 1.9407,
      "step": 741
    },
    {
      "epoch": 0.0031704795029782,
      "grad_norm": 3.4620938301086426,
      "learning_rate": 0.0004683387455135874,
      "loss": 1.29,
      "step": 742
    },
    {
      "epoch": 0.003174752386405394,
      "grad_norm": 0.8614152669906616,
      "learning_rate": 0.00046829601777473935,
      "loss": 0.1999,
      "step": 743
    },
    {
      "epoch": 0.0031790252698325884,
      "grad_norm": 3.6645452976226807,
      "learning_rate": 0.00046825329003589126,
      "loss": 1.9205,
      "step": 744
    },
    {
      "epoch": 0.0031832981532597827,
      "grad_norm": 2.0591938495635986,
      "learning_rate": 0.00046821056229704323,
      "loss": 0.7585,
      "step": 745
    },
    {
      "epoch": 0.003187571036686977,
      "grad_norm": 1.6749273538589478,
      "learning_rate": 0.0004681678345581952,
      "loss": 0.34,
      "step": 746
    },
    {
      "epoch": 0.0031918439201141712,
      "grad_norm": 1.5538630485534668,
      "learning_rate": 0.0004681251068193471,
      "loss": 0.3068,
      "step": 747
    },
    {
      "epoch": 0.003196116803541366,
      "grad_norm": 1.574137568473816,
      "learning_rate": 0.00046808237908049907,
      "loss": 0.5308,
      "step": 748
    },
    {
      "epoch": 0.0032003896869685603,
      "grad_norm": 3.398513078689575,
      "learning_rate": 0.000468039651341651,
      "loss": 0.8873,
      "step": 749
    },
    {
      "epoch": 0.0032046625703957545,
      "grad_norm": 2.1348841190338135,
      "learning_rate": 0.00046799692360280294,
      "loss": 1.0815,
      "step": 750
    },
    {
      "epoch": 0.003208935453822949,
      "grad_norm": 2.0361168384552,
      "learning_rate": 0.00046795419586395486,
      "loss": 0.6427,
      "step": 751
    },
    {
      "epoch": 0.003213208337250143,
      "grad_norm": 2.218606472015381,
      "learning_rate": 0.0004679114681251068,
      "loss": 0.8189,
      "step": 752
    },
    {
      "epoch": 0.0032174812206773374,
      "grad_norm": 3.8779828548431396,
      "learning_rate": 0.0004678687403862588,
      "loss": 1.3444,
      "step": 753
    },
    {
      "epoch": 0.0032217541041045317,
      "grad_norm": 3.792792320251465,
      "learning_rate": 0.0004678260126474107,
      "loss": 1.2721,
      "step": 754
    },
    {
      "epoch": 0.003226026987531726,
      "grad_norm": 4.895042419433594,
      "learning_rate": 0.00046778328490856266,
      "loss": 2.5523,
      "step": 755
    },
    {
      "epoch": 0.0032302998709589207,
      "grad_norm": 1.120619297027588,
      "learning_rate": 0.00046774055716971457,
      "loss": 0.413,
      "step": 756
    },
    {
      "epoch": 0.003234572754386115,
      "grad_norm": 1.9605094194412231,
      "learning_rate": 0.00046769782943086654,
      "loss": 0.467,
      "step": 757
    },
    {
      "epoch": 0.0032388456378133093,
      "grad_norm": 1.7916004657745361,
      "learning_rate": 0.00046765510169201845,
      "loss": 0.54,
      "step": 758
    },
    {
      "epoch": 0.0032431185212405036,
      "grad_norm": 2.572300672531128,
      "learning_rate": 0.0004676123739531704,
      "loss": 1.1505,
      "step": 759
    },
    {
      "epoch": 0.003247391404667698,
      "grad_norm": 1.099341869354248,
      "learning_rate": 0.0004675696462143224,
      "loss": 0.3296,
      "step": 760
    },
    {
      "epoch": 0.003251664288094892,
      "grad_norm": 2.1074044704437256,
      "learning_rate": 0.0004675269184754743,
      "loss": 0.5037,
      "step": 761
    },
    {
      "epoch": 0.0032559371715220864,
      "grad_norm": 1.9216755628585815,
      "learning_rate": 0.00046748419073662625,
      "loss": 0.4716,
      "step": 762
    },
    {
      "epoch": 0.0032602100549492807,
      "grad_norm": 1.7451330423355103,
      "learning_rate": 0.00046744146299777816,
      "loss": 0.8715,
      "step": 763
    },
    {
      "epoch": 0.003264482938376475,
      "grad_norm": 1.9290691614151,
      "learning_rate": 0.00046739873525893013,
      "loss": 0.7098,
      "step": 764
    },
    {
      "epoch": 0.0032687558218036697,
      "grad_norm": 3.8349318504333496,
      "learning_rate": 0.00046735600752008204,
      "loss": 1.2529,
      "step": 765
    },
    {
      "epoch": 0.003273028705230864,
      "grad_norm": 4.509474754333496,
      "learning_rate": 0.000467313279781234,
      "loss": 1.6197,
      "step": 766
    },
    {
      "epoch": 0.0032773015886580583,
      "grad_norm": 1.274946928024292,
      "learning_rate": 0.00046727055204238597,
      "loss": 0.2631,
      "step": 767
    },
    {
      "epoch": 0.0032815744720852526,
      "grad_norm": 4.486721038818359,
      "learning_rate": 0.0004672278243035379,
      "loss": 1.5682,
      "step": 768
    },
    {
      "epoch": 0.003285847355512447,
      "grad_norm": 1.6170353889465332,
      "learning_rate": 0.00046718509656468985,
      "loss": 0.5155,
      "step": 769
    },
    {
      "epoch": 0.003290120238939641,
      "grad_norm": 2.012418508529663,
      "learning_rate": 0.00046714236882584176,
      "loss": 0.7889,
      "step": 770
    },
    {
      "epoch": 0.0032943931223668355,
      "grad_norm": 2.059091806411743,
      "learning_rate": 0.0004670996410869937,
      "loss": 1.0686,
      "step": 771
    },
    {
      "epoch": 0.0032986660057940298,
      "grad_norm": 2.9298861026763916,
      "learning_rate": 0.00046705691334814563,
      "loss": 0.6234,
      "step": 772
    },
    {
      "epoch": 0.003302938889221224,
      "grad_norm": 1.7488964796066284,
      "learning_rate": 0.0004670141856092976,
      "loss": 0.8019,
      "step": 773
    },
    {
      "epoch": 0.0033072117726484188,
      "grad_norm": 1.8388022184371948,
      "learning_rate": 0.00046697145787044945,
      "loss": 0.3163,
      "step": 774
    },
    {
      "epoch": 0.003311484656075613,
      "grad_norm": 2.086913585662842,
      "learning_rate": 0.0004669287301316014,
      "loss": 0.751,
      "step": 775
    },
    {
      "epoch": 0.0033157575395028074,
      "grad_norm": 0.9960649609565735,
      "learning_rate": 0.0004668860023927534,
      "loss": 0.1473,
      "step": 776
    },
    {
      "epoch": 0.0033200304229300016,
      "grad_norm": 1.814436912536621,
      "learning_rate": 0.0004668432746539053,
      "loss": 0.5382,
      "step": 777
    },
    {
      "epoch": 0.003324303306357196,
      "grad_norm": 0.9910032153129578,
      "learning_rate": 0.00046680054691505726,
      "loss": 0.1329,
      "step": 778
    },
    {
      "epoch": 0.0033285761897843902,
      "grad_norm": 0.8506331443786621,
      "learning_rate": 0.00046675781917620917,
      "loss": 0.1072,
      "step": 779
    },
    {
      "epoch": 0.0033328490732115845,
      "grad_norm": 4.401627540588379,
      "learning_rate": 0.00046671509143736113,
      "loss": 1.042,
      "step": 780
    },
    {
      "epoch": 0.003337121956638779,
      "grad_norm": 3.11877179145813,
      "learning_rate": 0.00046667236369851304,
      "loss": 0.9308,
      "step": 781
    },
    {
      "epoch": 0.0033413948400659735,
      "grad_norm": 0.3220471739768982,
      "learning_rate": 0.000466629635959665,
      "loss": 0.0396,
      "step": 782
    },
    {
      "epoch": 0.003345667723493168,
      "grad_norm": 2.0888798236846924,
      "learning_rate": 0.000466586908220817,
      "loss": 0.807,
      "step": 783
    },
    {
      "epoch": 0.003349940606920362,
      "grad_norm": 0.2653317451477051,
      "learning_rate": 0.0004665441804819689,
      "loss": 0.0297,
      "step": 784
    },
    {
      "epoch": 0.0033542134903475564,
      "grad_norm": 6.089791774749756,
      "learning_rate": 0.00046650145274312085,
      "loss": 1.5145,
      "step": 785
    },
    {
      "epoch": 0.0033584863737747507,
      "grad_norm": 3.8655285835266113,
      "learning_rate": 0.00046645872500427276,
      "loss": 0.9869,
      "step": 786
    },
    {
      "epoch": 0.003362759257201945,
      "grad_norm": 2.240027904510498,
      "learning_rate": 0.0004664159972654247,
      "loss": 0.9936,
      "step": 787
    },
    {
      "epoch": 0.0033670321406291392,
      "grad_norm": 5.186272621154785,
      "learning_rate": 0.00046637326952657664,
      "loss": 0.7662,
      "step": 788
    },
    {
      "epoch": 0.0033713050240563335,
      "grad_norm": 2.4581501483917236,
      "learning_rate": 0.0004663305417877286,
      "loss": 0.6468,
      "step": 789
    },
    {
      "epoch": 0.003375577907483528,
      "grad_norm": 2.2193150520324707,
      "learning_rate": 0.00046628781404888057,
      "loss": 0.6079,
      "step": 790
    },
    {
      "epoch": 0.0033798507909107225,
      "grad_norm": 3.9768271446228027,
      "learning_rate": 0.0004662450863100325,
      "loss": 1.0964,
      "step": 791
    },
    {
      "epoch": 0.003384123674337917,
      "grad_norm": 2.3001317977905273,
      "learning_rate": 0.00046620235857118444,
      "loss": 0.938,
      "step": 792
    },
    {
      "epoch": 0.003388396557765111,
      "grad_norm": 4.370162010192871,
      "learning_rate": 0.00046615963083233635,
      "loss": 1.3191,
      "step": 793
    },
    {
      "epoch": 0.0033926694411923054,
      "grad_norm": 3.392328977584839,
      "learning_rate": 0.0004661169030934883,
      "loss": 1.0375,
      "step": 794
    },
    {
      "epoch": 0.0033969423246194997,
      "grad_norm": 4.190517425537109,
      "learning_rate": 0.00046607417535464023,
      "loss": 1.2261,
      "step": 795
    },
    {
      "epoch": 0.003401215208046694,
      "grad_norm": 2.2604458332061768,
      "learning_rate": 0.0004660314476157922,
      "loss": 0.7216,
      "step": 796
    },
    {
      "epoch": 0.0034054880914738883,
      "grad_norm": 0.09453839063644409,
      "learning_rate": 0.00046598871987694416,
      "loss": 0.0092,
      "step": 797
    },
    {
      "epoch": 0.0034097609749010826,
      "grad_norm": 4.5671563148498535,
      "learning_rate": 0.00046594599213809607,
      "loss": 1.0706,
      "step": 798
    },
    {
      "epoch": 0.0034140338583282773,
      "grad_norm": 2.9269931316375732,
      "learning_rate": 0.00046590326439924803,
      "loss": 0.9174,
      "step": 799
    },
    {
      "epoch": 0.0034183067417554716,
      "grad_norm": 2.0591509342193604,
      "learning_rate": 0.00046586053666039995,
      "loss": 0.5626,
      "step": 800
    },
    {
      "epoch": 0.003422579625182666,
      "grad_norm": 6.347719192504883,
      "learning_rate": 0.0004658178089215519,
      "loss": 1.7359,
      "step": 801
    },
    {
      "epoch": 0.00342685250860986,
      "grad_norm": 3.2193634510040283,
      "learning_rate": 0.0004657750811827038,
      "loss": 0.8978,
      "step": 802
    },
    {
      "epoch": 0.0034311253920370544,
      "grad_norm": 4.167491436004639,
      "learning_rate": 0.0004657323534438558,
      "loss": 1.4232,
      "step": 803
    },
    {
      "epoch": 0.0034353982754642487,
      "grad_norm": 2.1887292861938477,
      "learning_rate": 0.00046568962570500775,
      "loss": 0.6672,
      "step": 804
    },
    {
      "epoch": 0.003439671158891443,
      "grad_norm": 2.7517645359039307,
      "learning_rate": 0.00046564689796615966,
      "loss": 0.8689,
      "step": 805
    },
    {
      "epoch": 0.0034439440423186373,
      "grad_norm": 3.1206932067871094,
      "learning_rate": 0.0004656041702273116,
      "loss": 1.0422,
      "step": 806
    },
    {
      "epoch": 0.0034482169257458316,
      "grad_norm": 2.161574602127075,
      "learning_rate": 0.0004655614424884635,
      "loss": 0.4691,
      "step": 807
    },
    {
      "epoch": 0.0034524898091730263,
      "grad_norm": 3.1575987339019775,
      "learning_rate": 0.00046551871474961545,
      "loss": 1.7625,
      "step": 808
    },
    {
      "epoch": 0.0034567626926002206,
      "grad_norm": 1.5160928964614868,
      "learning_rate": 0.00046547598701076736,
      "loss": 0.3195,
      "step": 809
    },
    {
      "epoch": 0.003461035576027415,
      "grad_norm": 2.1258280277252197,
      "learning_rate": 0.0004654332592719193,
      "loss": 0.8546,
      "step": 810
    },
    {
      "epoch": 0.003465308459454609,
      "grad_norm": 1.7584840059280396,
      "learning_rate": 0.00046539053153307123,
      "loss": 0.343,
      "step": 811
    },
    {
      "epoch": 0.0034695813428818035,
      "grad_norm": 2.7912116050720215,
      "learning_rate": 0.0004653478037942232,
      "loss": 0.9159,
      "step": 812
    },
    {
      "epoch": 0.0034738542263089978,
      "grad_norm": 2.5656981468200684,
      "learning_rate": 0.00046530507605537516,
      "loss": 1.2215,
      "step": 813
    },
    {
      "epoch": 0.003478127109736192,
      "grad_norm": 1.7128143310546875,
      "learning_rate": 0.0004652623483165271,
      "loss": 0.9426,
      "step": 814
    },
    {
      "epoch": 0.0034823999931633863,
      "grad_norm": 2.362046718597412,
      "learning_rate": 0.00046521962057767904,
      "loss": 0.7633,
      "step": 815
    },
    {
      "epoch": 0.003486672876590581,
      "grad_norm": 0.8510921597480774,
      "learning_rate": 0.00046517689283883095,
      "loss": 0.1947,
      "step": 816
    },
    {
      "epoch": 0.0034909457600177753,
      "grad_norm": 2.5749614238739014,
      "learning_rate": 0.0004651341650999829,
      "loss": 0.8651,
      "step": 817
    },
    {
      "epoch": 0.0034952186434449696,
      "grad_norm": 0.6770228147506714,
      "learning_rate": 0.0004650914373611348,
      "loss": 0.1524,
      "step": 818
    },
    {
      "epoch": 0.003499491526872164,
      "grad_norm": 2.3697755336761475,
      "learning_rate": 0.0004650487096222868,
      "loss": 0.7514,
      "step": 819
    },
    {
      "epoch": 0.003503764410299358,
      "grad_norm": 1.890560507774353,
      "learning_rate": 0.00046500598188343876,
      "loss": 0.6859,
      "step": 820
    },
    {
      "epoch": 0.0035080372937265525,
      "grad_norm": 0.43309950828552246,
      "learning_rate": 0.00046496325414459067,
      "loss": 0.0966,
      "step": 821
    },
    {
      "epoch": 0.003512310177153747,
      "grad_norm": 1.3846714496612549,
      "learning_rate": 0.00046492052640574263,
      "loss": 0.4856,
      "step": 822
    },
    {
      "epoch": 0.003516583060580941,
      "grad_norm": 1.6962436437606812,
      "learning_rate": 0.00046487779866689454,
      "loss": 0.7808,
      "step": 823
    },
    {
      "epoch": 0.0035208559440081354,
      "grad_norm": 6.626699447631836,
      "learning_rate": 0.0004648350709280465,
      "loss": 1.4927,
      "step": 824
    },
    {
      "epoch": 0.00352512882743533,
      "grad_norm": 4.794219970703125,
      "learning_rate": 0.0004647923431891984,
      "loss": 1.3298,
      "step": 825
    },
    {
      "epoch": 0.0035294017108625244,
      "grad_norm": 3.5517427921295166,
      "learning_rate": 0.0004647496154503504,
      "loss": 1.3879,
      "step": 826
    },
    {
      "epoch": 0.0035336745942897187,
      "grad_norm": 3.373805046081543,
      "learning_rate": 0.00046470688771150235,
      "loss": 0.8166,
      "step": 827
    },
    {
      "epoch": 0.003537947477716913,
      "grad_norm": 1.5093801021575928,
      "learning_rate": 0.00046466415997265426,
      "loss": 0.8458,
      "step": 828
    },
    {
      "epoch": 0.0035422203611441072,
      "grad_norm": 2.1102728843688965,
      "learning_rate": 0.0004646214322338062,
      "loss": 0.6842,
      "step": 829
    },
    {
      "epoch": 0.0035464932445713015,
      "grad_norm": 2.6712043285369873,
      "learning_rate": 0.00046457870449495813,
      "loss": 1.483,
      "step": 830
    },
    {
      "epoch": 0.003550766127998496,
      "grad_norm": 1.3807562589645386,
      "learning_rate": 0.0004645359767561101,
      "loss": 0.7989,
      "step": 831
    },
    {
      "epoch": 0.00355503901142569,
      "grad_norm": 3.306802749633789,
      "learning_rate": 0.000464493249017262,
      "loss": 0.6674,
      "step": 832
    },
    {
      "epoch": 0.003559311894852885,
      "grad_norm": 3.680792808532715,
      "learning_rate": 0.000464450521278414,
      "loss": 1.1699,
      "step": 833
    },
    {
      "epoch": 0.003563584778280079,
      "grad_norm": 4.555114269256592,
      "learning_rate": 0.00046440779353956594,
      "loss": 1.4774,
      "step": 834
    },
    {
      "epoch": 0.0035678576617072734,
      "grad_norm": 0.3331601023674011,
      "learning_rate": 0.00046436506580071785,
      "loss": 0.062,
      "step": 835
    },
    {
      "epoch": 0.0035721305451344677,
      "grad_norm": 3.3213281631469727,
      "learning_rate": 0.0004643223380618698,
      "loss": 0.8862,
      "step": 836
    },
    {
      "epoch": 0.003576403428561662,
      "grad_norm": 1.2022126913070679,
      "learning_rate": 0.0004642796103230217,
      "loss": 0.5156,
      "step": 837
    },
    {
      "epoch": 0.0035806763119888563,
      "grad_norm": 4.871639728546143,
      "learning_rate": 0.0004642368825841737,
      "loss": 1.6431,
      "step": 838
    },
    {
      "epoch": 0.0035849491954160506,
      "grad_norm": 3.3717517852783203,
      "learning_rate": 0.0004641941548453256,
      "loss": 0.9791,
      "step": 839
    },
    {
      "epoch": 0.003589222078843245,
      "grad_norm": 3.9632208347320557,
      "learning_rate": 0.0004641514271064775,
      "loss": 1.5068,
      "step": 840
    },
    {
      "epoch": 0.003593494962270439,
      "grad_norm": 1.7242761850357056,
      "learning_rate": 0.0004641086993676294,
      "loss": 0.5341,
      "step": 841
    },
    {
      "epoch": 0.003597767845697634,
      "grad_norm": 2.5075201988220215,
      "learning_rate": 0.0004640659716287814,
      "loss": 0.9188,
      "step": 842
    },
    {
      "epoch": 0.003602040729124828,
      "grad_norm": 2.0710198879241943,
      "learning_rate": 0.00046402324388993335,
      "loss": 0.4432,
      "step": 843
    },
    {
      "epoch": 0.0036063136125520224,
      "grad_norm": 1.1043959856033325,
      "learning_rate": 0.00046398051615108526,
      "loss": 0.4431,
      "step": 844
    },
    {
      "epoch": 0.0036105864959792167,
      "grad_norm": 3.6996426582336426,
      "learning_rate": 0.00046393778841223723,
      "loss": 2.1242,
      "step": 845
    },
    {
      "epoch": 0.003614859379406411,
      "grad_norm": 3.259577751159668,
      "learning_rate": 0.00046389506067338914,
      "loss": 1.2124,
      "step": 846
    },
    {
      "epoch": 0.0036191322628336053,
      "grad_norm": 3.7792489528656006,
      "learning_rate": 0.0004638523329345411,
      "loss": 1.4041,
      "step": 847
    },
    {
      "epoch": 0.0036234051462607996,
      "grad_norm": 4.067273139953613,
      "learning_rate": 0.000463809605195693,
      "loss": 1.6826,
      "step": 848
    },
    {
      "epoch": 0.003627678029687994,
      "grad_norm": 1.0973845720291138,
      "learning_rate": 0.000463766877456845,
      "loss": 0.365,
      "step": 849
    },
    {
      "epoch": 0.0036319509131151886,
      "grad_norm": 2.110828161239624,
      "learning_rate": 0.00046372414971799695,
      "loss": 0.4091,
      "step": 850
    },
    {
      "epoch": 0.003636223796542383,
      "grad_norm": 1.1146210432052612,
      "learning_rate": 0.00046368142197914886,
      "loss": 0.178,
      "step": 851
    },
    {
      "epoch": 0.003640496679969577,
      "grad_norm": 3.709524631500244,
      "learning_rate": 0.0004636386942403008,
      "loss": 1.61,
      "step": 852
    },
    {
      "epoch": 0.0036447695633967715,
      "grad_norm": 2.5581326484680176,
      "learning_rate": 0.00046359596650145273,
      "loss": 0.4831,
      "step": 853
    },
    {
      "epoch": 0.0036490424468239658,
      "grad_norm": 1.7707984447479248,
      "learning_rate": 0.0004635532387626047,
      "loss": 0.6229,
      "step": 854
    },
    {
      "epoch": 0.00365331533025116,
      "grad_norm": 4.06162691116333,
      "learning_rate": 0.0004635105110237566,
      "loss": 1.287,
      "step": 855
    },
    {
      "epoch": 0.0036575882136783543,
      "grad_norm": 23.580936431884766,
      "learning_rate": 0.00046346778328490857,
      "loss": 2.7509,
      "step": 856
    },
    {
      "epoch": 0.0036618610971055486,
      "grad_norm": 1.060931921005249,
      "learning_rate": 0.00046342505554606054,
      "loss": 0.1171,
      "step": 857
    },
    {
      "epoch": 0.003666133980532743,
      "grad_norm": 3.902395248413086,
      "learning_rate": 0.00046338232780721245,
      "loss": 0.6143,
      "step": 858
    },
    {
      "epoch": 0.0036704068639599376,
      "grad_norm": 2.1041505336761475,
      "learning_rate": 0.0004633396000683644,
      "loss": 1.0928,
      "step": 859
    },
    {
      "epoch": 0.003674679747387132,
      "grad_norm": 4.469504356384277,
      "learning_rate": 0.0004632968723295163,
      "loss": 1.2324,
      "step": 860
    },
    {
      "epoch": 0.003678952630814326,
      "grad_norm": 0.9940828084945679,
      "learning_rate": 0.0004632541445906683,
      "loss": 0.1964,
      "step": 861
    },
    {
      "epoch": 0.0036832255142415205,
      "grad_norm": 3.0577025413513184,
      "learning_rate": 0.0004632114168518202,
      "loss": 0.9373,
      "step": 862
    },
    {
      "epoch": 0.003687498397668715,
      "grad_norm": 2.8435580730438232,
      "learning_rate": 0.00046316868911297216,
      "loss": 0.6844,
      "step": 863
    },
    {
      "epoch": 0.003691771281095909,
      "grad_norm": 1.9963189363479614,
      "learning_rate": 0.00046312596137412413,
      "loss": 0.7544,
      "step": 864
    },
    {
      "epoch": 0.0036960441645231034,
      "grad_norm": 3.1311824321746826,
      "learning_rate": 0.00046308323363527604,
      "loss": 1.5696,
      "step": 865
    },
    {
      "epoch": 0.0037003170479502977,
      "grad_norm": 3.406268835067749,
      "learning_rate": 0.000463040505896428,
      "loss": 1.2727,
      "step": 866
    },
    {
      "epoch": 0.003704589931377492,
      "grad_norm": 0.6554659008979797,
      "learning_rate": 0.0004629977781575799,
      "loss": 0.0947,
      "step": 867
    },
    {
      "epoch": 0.0037088628148046867,
      "grad_norm": 3.1337437629699707,
      "learning_rate": 0.0004629550504187319,
      "loss": 1.0576,
      "step": 868
    },
    {
      "epoch": 0.003713135698231881,
      "grad_norm": 3.397437334060669,
      "learning_rate": 0.0004629123226798838,
      "loss": 1.3527,
      "step": 869
    },
    {
      "epoch": 0.0037174085816590752,
      "grad_norm": 2.921591281890869,
      "learning_rate": 0.00046286959494103576,
      "loss": 0.8121,
      "step": 870
    },
    {
      "epoch": 0.0037216814650862695,
      "grad_norm": 3.1334316730499268,
      "learning_rate": 0.0004628268672021877,
      "loss": 0.6019,
      "step": 871
    },
    {
      "epoch": 0.003725954348513464,
      "grad_norm": 3.4619009494781494,
      "learning_rate": 0.00046278413946333963,
      "loss": 0.431,
      "step": 872
    },
    {
      "epoch": 0.003730227231940658,
      "grad_norm": 4.5764875411987305,
      "learning_rate": 0.00046274141172449154,
      "loss": 0.8313,
      "step": 873
    },
    {
      "epoch": 0.0037345001153678524,
      "grad_norm": 3.125767469406128,
      "learning_rate": 0.00046269868398564345,
      "loss": 0.3717,
      "step": 874
    },
    {
      "epoch": 0.0037387729987950467,
      "grad_norm": 2.8130128383636475,
      "learning_rate": 0.0004626559562467954,
      "loss": 0.7358,
      "step": 875
    },
    {
      "epoch": 0.0037430458822222414,
      "grad_norm": 1.6554526090621948,
      "learning_rate": 0.00046261322850794733,
      "loss": 0.6382,
      "step": 876
    },
    {
      "epoch": 0.0037473187656494357,
      "grad_norm": 2.8319573402404785,
      "learning_rate": 0.0004625705007690993,
      "loss": 0.5343,
      "step": 877
    },
    {
      "epoch": 0.00375159164907663,
      "grad_norm": 4.897272109985352,
      "learning_rate": 0.0004625277730302512,
      "loss": 2.0284,
      "step": 878
    },
    {
      "epoch": 0.0037558645325038243,
      "grad_norm": 0.7744057774543762,
      "learning_rate": 0.00046248504529140317,
      "loss": 0.1199,
      "step": 879
    },
    {
      "epoch": 0.0037601374159310186,
      "grad_norm": 4.069736480712891,
      "learning_rate": 0.00046244231755255514,
      "loss": 1.1517,
      "step": 880
    },
    {
      "epoch": 0.003764410299358213,
      "grad_norm": 2.349520444869995,
      "learning_rate": 0.00046239958981370705,
      "loss": 1.1708,
      "step": 881
    },
    {
      "epoch": 0.003768683182785407,
      "grad_norm": 3.2727861404418945,
      "learning_rate": 0.000462356862074859,
      "loss": 1.1507,
      "step": 882
    },
    {
      "epoch": 0.0037729560662126014,
      "grad_norm": 4.404483318328857,
      "learning_rate": 0.0004623141343360109,
      "loss": 0.929,
      "step": 883
    },
    {
      "epoch": 0.0037772289496397957,
      "grad_norm": 2.575486183166504,
      "learning_rate": 0.0004622714065971629,
      "loss": 0.4144,
      "step": 884
    },
    {
      "epoch": 0.0037815018330669904,
      "grad_norm": 33.1515007019043,
      "learning_rate": 0.0004622286788583148,
      "loss": 1.6165,
      "step": 885
    },
    {
      "epoch": 0.0037857747164941847,
      "grad_norm": 3.5293564796447754,
      "learning_rate": 0.00046218595111946676,
      "loss": 0.9178,
      "step": 886
    },
    {
      "epoch": 0.003790047599921379,
      "grad_norm": 3.187065362930298,
      "learning_rate": 0.00046214322338061873,
      "loss": 1.0002,
      "step": 887
    },
    {
      "epoch": 0.0037943204833485733,
      "grad_norm": 4.501162528991699,
      "learning_rate": 0.00046210049564177064,
      "loss": 1.3511,
      "step": 888
    },
    {
      "epoch": 0.0037985933667757676,
      "grad_norm": 4.047275543212891,
      "learning_rate": 0.0004620577679029226,
      "loss": 1.4651,
      "step": 889
    },
    {
      "epoch": 0.003802866250202962,
      "grad_norm": 2.1388556957244873,
      "learning_rate": 0.0004620150401640745,
      "loss": 0.6764,
      "step": 890
    },
    {
      "epoch": 0.003807139133630156,
      "grad_norm": 2.8196253776550293,
      "learning_rate": 0.0004619723124252265,
      "loss": 0.7947,
      "step": 891
    },
    {
      "epoch": 0.0038114120170573505,
      "grad_norm": 1.7425605058670044,
      "learning_rate": 0.0004619295846863784,
      "loss": 0.6139,
      "step": 892
    },
    {
      "epoch": 0.003815684900484545,
      "grad_norm": 2.6743264198303223,
      "learning_rate": 0.00046188685694753035,
      "loss": 0.5704,
      "step": 893
    },
    {
      "epoch": 0.0038199577839117395,
      "grad_norm": 3.102741241455078,
      "learning_rate": 0.0004618441292086823,
      "loss": 1.2927,
      "step": 894
    },
    {
      "epoch": 0.0038242306673389338,
      "grad_norm": 1.825021743774414,
      "learning_rate": 0.00046180140146983423,
      "loss": 0.5273,
      "step": 895
    },
    {
      "epoch": 0.003828503550766128,
      "grad_norm": 1.6151528358459473,
      "learning_rate": 0.0004617586737309862,
      "loss": 0.4359,
      "step": 896
    },
    {
      "epoch": 0.0038327764341933223,
      "grad_norm": 1.7097922563552856,
      "learning_rate": 0.0004617159459921381,
      "loss": 0.3621,
      "step": 897
    },
    {
      "epoch": 0.0038370493176205166,
      "grad_norm": 1.808911681175232,
      "learning_rate": 0.00046167321825329007,
      "loss": 0.3476,
      "step": 898
    },
    {
      "epoch": 0.003841322201047711,
      "grad_norm": 1.9336212873458862,
      "learning_rate": 0.000461630490514442,
      "loss": 0.7026,
      "step": 899
    },
    {
      "epoch": 0.003845595084474905,
      "grad_norm": 2.7257754802703857,
      "learning_rate": 0.00046158776277559395,
      "loss": 1.606,
      "step": 900
    },
    {
      "epoch": 0.0038498679679020995,
      "grad_norm": 3.236288547515869,
      "learning_rate": 0.0004615450350367459,
      "loss": 0.8171,
      "step": 901
    },
    {
      "epoch": 0.003854140851329294,
      "grad_norm": 1.7994816303253174,
      "learning_rate": 0.0004615023072978978,
      "loss": 0.4673,
      "step": 902
    },
    {
      "epoch": 0.0038584137347564885,
      "grad_norm": 2.946525812149048,
      "learning_rate": 0.0004614595795590498,
      "loss": 1.0094,
      "step": 903
    },
    {
      "epoch": 0.003862686618183683,
      "grad_norm": 2.888504981994629,
      "learning_rate": 0.0004614168518202017,
      "loss": 0.7105,
      "step": 904
    },
    {
      "epoch": 0.003866959501610877,
      "grad_norm": 2.740718126296997,
      "learning_rate": 0.0004613741240813536,
      "loss": 0.7191,
      "step": 905
    },
    {
      "epoch": 0.0038712323850380714,
      "grad_norm": 1.3704051971435547,
      "learning_rate": 0.0004613313963425055,
      "loss": 0.3044,
      "step": 906
    },
    {
      "epoch": 0.0038755052684652657,
      "grad_norm": 1.905984878540039,
      "learning_rate": 0.0004612886686036575,
      "loss": 0.4495,
      "step": 907
    },
    {
      "epoch": 0.00387977815189246,
      "grad_norm": 1.8070203065872192,
      "learning_rate": 0.0004612459408648094,
      "loss": 0.9426,
      "step": 908
    },
    {
      "epoch": 0.0038840510353196542,
      "grad_norm": 1.527459740638733,
      "learning_rate": 0.00046120321312596136,
      "loss": 0.277,
      "step": 909
    },
    {
      "epoch": 0.003888323918746849,
      "grad_norm": 1.768392562866211,
      "learning_rate": 0.0004611604853871133,
      "loss": 0.4105,
      "step": 910
    },
    {
      "epoch": 0.0038925968021740432,
      "grad_norm": 2.033612012863159,
      "learning_rate": 0.00046111775764826524,
      "loss": 0.4489,
      "step": 911
    },
    {
      "epoch": 0.0038968696856012375,
      "grad_norm": 1.941099762916565,
      "learning_rate": 0.0004610750299094172,
      "loss": 0.6169,
      "step": 912
    },
    {
      "epoch": 0.003901142569028432,
      "grad_norm": 5.961338043212891,
      "learning_rate": 0.0004610323021705691,
      "loss": 2.3975,
      "step": 913
    },
    {
      "epoch": 0.003905415452455626,
      "grad_norm": 1.250181794166565,
      "learning_rate": 0.0004609895744317211,
      "loss": 0.2724,
      "step": 914
    },
    {
      "epoch": 0.00390968833588282,
      "grad_norm": 2.058755397796631,
      "learning_rate": 0.000460946846692873,
      "loss": 0.5553,
      "step": 915
    },
    {
      "epoch": 0.003913961219310015,
      "grad_norm": 1.074455976486206,
      "learning_rate": 0.00046090411895402495,
      "loss": 0.2144,
      "step": 916
    },
    {
      "epoch": 0.003918234102737209,
      "grad_norm": 7.0759501457214355,
      "learning_rate": 0.0004608613912151769,
      "loss": 1.6675,
      "step": 917
    },
    {
      "epoch": 0.003922506986164404,
      "grad_norm": 4.405402660369873,
      "learning_rate": 0.00046081866347632883,
      "loss": 1.9179,
      "step": 918
    },
    {
      "epoch": 0.0039267798695915975,
      "grad_norm": 2.9719178676605225,
      "learning_rate": 0.0004607759357374808,
      "loss": 1.5138,
      "step": 919
    },
    {
      "epoch": 0.003931052753018792,
      "grad_norm": 2.9762325286865234,
      "learning_rate": 0.0004607332079986327,
      "loss": 1.0663,
      "step": 920
    },
    {
      "epoch": 0.003935325636445986,
      "grad_norm": 4.013986587524414,
      "learning_rate": 0.00046069048025978467,
      "loss": 1.0463,
      "step": 921
    },
    {
      "epoch": 0.003939598519873181,
      "grad_norm": 2.8951516151428223,
      "learning_rate": 0.0004606477525209366,
      "loss": 1.4555,
      "step": 922
    },
    {
      "epoch": 0.003943871403300376,
      "grad_norm": 1.9937684535980225,
      "learning_rate": 0.00046060502478208854,
      "loss": 0.9591,
      "step": 923
    },
    {
      "epoch": 0.003948144286727569,
      "grad_norm": 1.7073066234588623,
      "learning_rate": 0.0004605622970432405,
      "loss": 0.6011,
      "step": 924
    },
    {
      "epoch": 0.003952417170154764,
      "grad_norm": 1.6916909217834473,
      "learning_rate": 0.0004605195693043924,
      "loss": 0.8453,
      "step": 925
    },
    {
      "epoch": 0.003956690053581958,
      "grad_norm": 2.05037784576416,
      "learning_rate": 0.0004604768415655444,
      "loss": 0.5074,
      "step": 926
    },
    {
      "epoch": 0.003960962937009153,
      "grad_norm": 1.3948997259140015,
      "learning_rate": 0.0004604341138266963,
      "loss": 0.7426,
      "step": 927
    },
    {
      "epoch": 0.003965235820436347,
      "grad_norm": 6.363650321960449,
      "learning_rate": 0.00046039138608784826,
      "loss": 1.7939,
      "step": 928
    },
    {
      "epoch": 0.003969508703863541,
      "grad_norm": 3.4815022945404053,
      "learning_rate": 0.00046034865834900017,
      "loss": 1.1777,
      "step": 929
    },
    {
      "epoch": 0.003973781587290735,
      "grad_norm": 2.9372928142547607,
      "learning_rate": 0.00046030593061015214,
      "loss": 0.9903,
      "step": 930
    },
    {
      "epoch": 0.00397805447071793,
      "grad_norm": 8.951374053955078,
      "learning_rate": 0.0004602632028713041,
      "loss": 2.1142,
      "step": 931
    },
    {
      "epoch": 0.003982327354145125,
      "grad_norm": 1.3942389488220215,
      "learning_rate": 0.000460220475132456,
      "loss": 0.6837,
      "step": 932
    },
    {
      "epoch": 0.0039866002375723185,
      "grad_norm": 4.613259792327881,
      "learning_rate": 0.000460177747393608,
      "loss": 1.3186,
      "step": 933
    },
    {
      "epoch": 0.003990873120999513,
      "grad_norm": 1.8971587419509888,
      "learning_rate": 0.0004601350196547599,
      "loss": 0.5227,
      "step": 934
    },
    {
      "epoch": 0.003995146004426707,
      "grad_norm": 6.236545085906982,
      "learning_rate": 0.00046009229191591185,
      "loss": 1.4528,
      "step": 935
    },
    {
      "epoch": 0.003999418887853902,
      "grad_norm": 0.7588641047477722,
      "learning_rate": 0.00046004956417706376,
      "loss": 0.1239,
      "step": 936
    },
    {
      "epoch": 0.004003691771281096,
      "grad_norm": 2.763415813446045,
      "learning_rate": 0.00046000683643821573,
      "loss": 0.8578,
      "step": 937
    },
    {
      "epoch": 0.00400796465470829,
      "grad_norm": 1.2574058771133423,
      "learning_rate": 0.00045996410869936764,
      "loss": 0.4356,
      "step": 938
    },
    {
      "epoch": 0.004012237538135484,
      "grad_norm": 1.3295925855636597,
      "learning_rate": 0.00045992138096051955,
      "loss": 0.6767,
      "step": 939
    },
    {
      "epoch": 0.004016510421562679,
      "grad_norm": 6.204926013946533,
      "learning_rate": 0.0004598786532216715,
      "loss": 0.9252,
      "step": 940
    },
    {
      "epoch": 0.004020783304989874,
      "grad_norm": 3.0045361518859863,
      "learning_rate": 0.0004598359254828234,
      "loss": 1.2776,
      "step": 941
    },
    {
      "epoch": 0.0040250561884170675,
      "grad_norm": 4.439476490020752,
      "learning_rate": 0.0004597931977439754,
      "loss": 1.8188,
      "step": 942
    },
    {
      "epoch": 0.004029329071844262,
      "grad_norm": 2.9668710231781006,
      "learning_rate": 0.0004597504700051273,
      "loss": 1.1823,
      "step": 943
    },
    {
      "epoch": 0.004033601955271456,
      "grad_norm": 2.0011019706726074,
      "learning_rate": 0.00045970774226627927,
      "loss": 0.6346,
      "step": 944
    },
    {
      "epoch": 0.004037874838698651,
      "grad_norm": 3.45979642868042,
      "learning_rate": 0.0004596650145274312,
      "loss": 0.8012,
      "step": 945
    },
    {
      "epoch": 0.004042147722125845,
      "grad_norm": 2.079603433609009,
      "learning_rate": 0.00045962228678858314,
      "loss": 0.5586,
      "step": 946
    },
    {
      "epoch": 0.004046420605553039,
      "grad_norm": 1.1968145370483398,
      "learning_rate": 0.0004595795590497351,
      "loss": 0.3115,
      "step": 947
    },
    {
      "epoch": 0.004050693488980233,
      "grad_norm": 0.8278767466545105,
      "learning_rate": 0.000459536831310887,
      "loss": 0.5126,
      "step": 948
    },
    {
      "epoch": 0.004054966372407428,
      "grad_norm": 1.6515083312988281,
      "learning_rate": 0.000459494103572039,
      "loss": 0.5743,
      "step": 949
    },
    {
      "epoch": 0.004059239255834623,
      "grad_norm": 2.209054708480835,
      "learning_rate": 0.0004594513758331909,
      "loss": 1.2428,
      "step": 950
    },
    {
      "epoch": 0.0040635121392618165,
      "grad_norm": 2.170175075531006,
      "learning_rate": 0.00045940864809434286,
      "loss": 1.1603,
      "step": 951
    },
    {
      "epoch": 0.004067785022689011,
      "grad_norm": 5.8775739669799805,
      "learning_rate": 0.00045936592035549477,
      "loss": 0.6937,
      "step": 952
    },
    {
      "epoch": 0.004072057906116205,
      "grad_norm": 2.915773391723633,
      "learning_rate": 0.00045932319261664673,
      "loss": 1.2566,
      "step": 953
    },
    {
      "epoch": 0.0040763307895434,
      "grad_norm": 1.6638102531433105,
      "learning_rate": 0.0004592804648777987,
      "loss": 0.3685,
      "step": 954
    },
    {
      "epoch": 0.004080603672970594,
      "grad_norm": 2.3009698390960693,
      "learning_rate": 0.0004592377371389506,
      "loss": 0.5964,
      "step": 955
    },
    {
      "epoch": 0.004084876556397788,
      "grad_norm": 4.147054195404053,
      "learning_rate": 0.0004591950094001026,
      "loss": 1.7371,
      "step": 956
    },
    {
      "epoch": 0.004089149439824983,
      "grad_norm": 1.5214087963104248,
      "learning_rate": 0.0004591522816612545,
      "loss": 0.3369,
      "step": 957
    },
    {
      "epoch": 0.004093422323252177,
      "grad_norm": 4.1181321144104,
      "learning_rate": 0.00045910955392240645,
      "loss": 1.1526,
      "step": 958
    },
    {
      "epoch": 0.004097695206679372,
      "grad_norm": 1.6993688344955444,
      "learning_rate": 0.00045906682618355836,
      "loss": 0.5302,
      "step": 959
    },
    {
      "epoch": 0.0041019680901065655,
      "grad_norm": 3.112178087234497,
      "learning_rate": 0.0004590240984447103,
      "loss": 1.2525,
      "step": 960
    },
    {
      "epoch": 0.00410624097353376,
      "grad_norm": 0.924262285232544,
      "learning_rate": 0.0004589813707058623,
      "loss": 0.4989,
      "step": 961
    },
    {
      "epoch": 0.004110513856960954,
      "grad_norm": 1.1173590421676636,
      "learning_rate": 0.0004589386429670142,
      "loss": 0.2013,
      "step": 962
    },
    {
      "epoch": 0.004114786740388149,
      "grad_norm": 1.034967064857483,
      "learning_rate": 0.00045889591522816617,
      "loss": 0.1931,
      "step": 963
    },
    {
      "epoch": 0.004119059623815343,
      "grad_norm": 1.287554383277893,
      "learning_rate": 0.0004588531874893181,
      "loss": 0.8113,
      "step": 964
    },
    {
      "epoch": 0.004123332507242537,
      "grad_norm": 5.258865833282471,
      "learning_rate": 0.00045881045975047004,
      "loss": 1.5937,
      "step": 965
    },
    {
      "epoch": 0.004127605390669732,
      "grad_norm": 6.281607627868652,
      "learning_rate": 0.00045876773201162195,
      "loss": 1.0846,
      "step": 966
    },
    {
      "epoch": 0.004131878274096926,
      "grad_norm": 4.316941261291504,
      "learning_rate": 0.0004587250042727739,
      "loss": 1.5214,
      "step": 967
    },
    {
      "epoch": 0.004136151157524121,
      "grad_norm": 4.968129634857178,
      "learning_rate": 0.0004586822765339259,
      "loss": 1.3165,
      "step": 968
    },
    {
      "epoch": 0.004140424040951315,
      "grad_norm": 4.186277866363525,
      "learning_rate": 0.0004586395487950778,
      "loss": 1.3474,
      "step": 969
    },
    {
      "epoch": 0.004144696924378509,
      "grad_norm": 2.7726681232452393,
      "learning_rate": 0.00045859682105622976,
      "loss": 1.1206,
      "step": 970
    },
    {
      "epoch": 0.004148969807805703,
      "grad_norm": 1.1086913347244263,
      "learning_rate": 0.0004585540933173816,
      "loss": 0.6704,
      "step": 971
    },
    {
      "epoch": 0.004153242691232898,
      "grad_norm": 1.9069221019744873,
      "learning_rate": 0.0004585113655785336,
      "loss": 0.46,
      "step": 972
    },
    {
      "epoch": 0.004157515574660092,
      "grad_norm": 3.1908178329467773,
      "learning_rate": 0.0004584686378396855,
      "loss": 1.2248,
      "step": 973
    },
    {
      "epoch": 0.0041617884580872865,
      "grad_norm": 1.5938715934753418,
      "learning_rate": 0.00045842591010083745,
      "loss": 0.5098,
      "step": 974
    },
    {
      "epoch": 0.004166061341514481,
      "grad_norm": 2.9983010292053223,
      "learning_rate": 0.00045838318236198937,
      "loss": 1.1319,
      "step": 975
    },
    {
      "epoch": 0.004170334224941675,
      "grad_norm": 3.90657377243042,
      "learning_rate": 0.00045834045462314133,
      "loss": 0.7209,
      "step": 976
    },
    {
      "epoch": 0.00417460710836887,
      "grad_norm": 3.978498935699463,
      "learning_rate": 0.0004582977268842933,
      "loss": 1.098,
      "step": 977
    },
    {
      "epoch": 0.004178879991796064,
      "grad_norm": 4.411987781524658,
      "learning_rate": 0.0004582549991454452,
      "loss": 1.3244,
      "step": 978
    },
    {
      "epoch": 0.004183152875223258,
      "grad_norm": 2.8642778396606445,
      "learning_rate": 0.00045821227140659717,
      "loss": 0.8089,
      "step": 979
    },
    {
      "epoch": 0.004187425758650452,
      "grad_norm": 2.179623603820801,
      "learning_rate": 0.0004581695436677491,
      "loss": 0.6546,
      "step": 980
    },
    {
      "epoch": 0.004191698642077647,
      "grad_norm": 1.0124623775482178,
      "learning_rate": 0.00045812681592890105,
      "loss": 0.4277,
      "step": 981
    },
    {
      "epoch": 0.004195971525504841,
      "grad_norm": 0.9706234931945801,
      "learning_rate": 0.00045808408819005296,
      "loss": 0.3693,
      "step": 982
    },
    {
      "epoch": 0.0042002444089320355,
      "grad_norm": 2.9485068321228027,
      "learning_rate": 0.0004580413604512049,
      "loss": 0.898,
      "step": 983
    },
    {
      "epoch": 0.00420451729235923,
      "grad_norm": 1.6143112182617188,
      "learning_rate": 0.0004579986327123569,
      "loss": 0.4278,
      "step": 984
    },
    {
      "epoch": 0.004208790175786424,
      "grad_norm": 3.046811819076538,
      "learning_rate": 0.0004579559049735088,
      "loss": 2.2604,
      "step": 985
    },
    {
      "epoch": 0.004213063059213619,
      "grad_norm": 2.879465103149414,
      "learning_rate": 0.00045791317723466076,
      "loss": 0.8288,
      "step": 986
    },
    {
      "epoch": 0.004217335942640813,
      "grad_norm": 4.870797634124756,
      "learning_rate": 0.0004578704494958127,
      "loss": 1.7092,
      "step": 987
    },
    {
      "epoch": 0.004221608826068007,
      "grad_norm": 3.335710048675537,
      "learning_rate": 0.00045782772175696464,
      "loss": 0.9247,
      "step": 988
    },
    {
      "epoch": 0.004225881709495201,
      "grad_norm": 0.8851406574249268,
      "learning_rate": 0.00045778499401811655,
      "loss": 0.1973,
      "step": 989
    },
    {
      "epoch": 0.004230154592922396,
      "grad_norm": 1.775046467781067,
      "learning_rate": 0.0004577422662792685,
      "loss": 0.5066,
      "step": 990
    },
    {
      "epoch": 0.004234427476349591,
      "grad_norm": 0.7663598656654358,
      "learning_rate": 0.0004576995385404205,
      "loss": 0.1405,
      "step": 991
    },
    {
      "epoch": 0.0042387003597767845,
      "grad_norm": 4.53016471862793,
      "learning_rate": 0.0004576568108015724,
      "loss": 1.3261,
      "step": 992
    },
    {
      "epoch": 0.004242973243203979,
      "grad_norm": 0.5577570199966431,
      "learning_rate": 0.00045761408306272436,
      "loss": 0.0801,
      "step": 993
    },
    {
      "epoch": 0.004247246126631173,
      "grad_norm": 3.7300729751586914,
      "learning_rate": 0.00045757135532387627,
      "loss": 1.6103,
      "step": 994
    },
    {
      "epoch": 0.004251519010058368,
      "grad_norm": 1.3294199705123901,
      "learning_rate": 0.00045752862758502823,
      "loss": 0.4332,
      "step": 995
    },
    {
      "epoch": 0.004255791893485562,
      "grad_norm": 1.7651716470718384,
      "learning_rate": 0.00045748589984618014,
      "loss": 0.4504,
      "step": 996
    },
    {
      "epoch": 0.004260064776912756,
      "grad_norm": 1.6101412773132324,
      "learning_rate": 0.0004574431721073321,
      "loss": 0.4219,
      "step": 997
    },
    {
      "epoch": 0.00426433766033995,
      "grad_norm": 1.9830620288848877,
      "learning_rate": 0.00045740044436848407,
      "loss": 0.61,
      "step": 998
    },
    {
      "epoch": 0.004268610543767145,
      "grad_norm": 3.5597262382507324,
      "learning_rate": 0.000457357716629636,
      "loss": 1.3252,
      "step": 999
    },
    {
      "epoch": 0.00427288342719434,
      "grad_norm": 3.0293829441070557,
      "learning_rate": 0.00045731498889078795,
      "loss": 0.6959,
      "step": 1000
    },
    {
      "epoch": 0.0042771563106215335,
      "grad_norm": 9.716403007507324,
      "learning_rate": 0.00045727226115193986,
      "loss": 4.5029,
      "step": 1001
    },
    {
      "epoch": 0.004281429194048728,
      "grad_norm": 0.03967377170920372,
      "learning_rate": 0.0004572295334130918,
      "loss": 0.0035,
      "step": 1002
    },
    {
      "epoch": 0.004285702077475922,
      "grad_norm": 0.04876069352030754,
      "learning_rate": 0.00045718680567424373,
      "loss": 0.0043,
      "step": 1003
    },
    {
      "epoch": 0.004289974960903117,
      "grad_norm": 1.5154974460601807,
      "learning_rate": 0.00045714407793539564,
      "loss": 0.3341,
      "step": 1004
    },
    {
      "epoch": 0.004294247844330311,
      "grad_norm": 1.3363803625106812,
      "learning_rate": 0.0004571013501965476,
      "loss": 0.4946,
      "step": 1005
    },
    {
      "epoch": 0.004298520727757505,
      "grad_norm": 8.330889701843262,
      "learning_rate": 0.0004570586224576995,
      "loss": 4.1308,
      "step": 1006
    },
    {
      "epoch": 0.004302793611184699,
      "grad_norm": 2.9019765853881836,
      "learning_rate": 0.0004570158947188515,
      "loss": 1.0886,
      "step": 1007
    },
    {
      "epoch": 0.004307066494611894,
      "grad_norm": 1.365973711013794,
      "learning_rate": 0.0004569731669800034,
      "loss": 0.3168,
      "step": 1008
    },
    {
      "epoch": 0.004311339378039089,
      "grad_norm": 1.2017180919647217,
      "learning_rate": 0.00045693043924115536,
      "loss": 0.2749,
      "step": 1009
    },
    {
      "epoch": 0.004315612261466283,
      "grad_norm": 2.479938507080078,
      "learning_rate": 0.00045688771150230727,
      "loss": 0.7241,
      "step": 1010
    },
    {
      "epoch": 0.004319885144893477,
      "grad_norm": 0.17434121668338776,
      "learning_rate": 0.00045684498376345924,
      "loss": 0.0197,
      "step": 1011
    },
    {
      "epoch": 0.004324158028320671,
      "grad_norm": 2.034882068634033,
      "learning_rate": 0.00045680225602461115,
      "loss": 0.2564,
      "step": 1012
    },
    {
      "epoch": 0.004328430911747866,
      "grad_norm": 0.21861642599105835,
      "learning_rate": 0.0004567595282857631,
      "loss": 0.0259,
      "step": 1013
    },
    {
      "epoch": 0.00433270379517506,
      "grad_norm": 1.074479341506958,
      "learning_rate": 0.0004567168005469151,
      "loss": 0.3681,
      "step": 1014
    },
    {
      "epoch": 0.0043369766786022544,
      "grad_norm": 4.739996433258057,
      "learning_rate": 0.000456674072808067,
      "loss": 2.2793,
      "step": 1015
    },
    {
      "epoch": 0.004341249562029448,
      "grad_norm": 4.550075531005859,
      "learning_rate": 0.00045663134506921895,
      "loss": 2.28,
      "step": 1016
    },
    {
      "epoch": 0.004345522445456643,
      "grad_norm": 4.283854961395264,
      "learning_rate": 0.00045658861733037086,
      "loss": 1.2852,
      "step": 1017
    },
    {
      "epoch": 0.004349795328883838,
      "grad_norm": 0.7159547209739685,
      "learning_rate": 0.00045654588959152283,
      "loss": 0.0564,
      "step": 1018
    },
    {
      "epoch": 0.004354068212311032,
      "grad_norm": 6.066198348999023,
      "learning_rate": 0.00045650316185267474,
      "loss": 1.9128,
      "step": 1019
    },
    {
      "epoch": 0.004358341095738226,
      "grad_norm": 2.5832314491271973,
      "learning_rate": 0.0004564604341138267,
      "loss": 1.0964,
      "step": 1020
    },
    {
      "epoch": 0.00436261397916542,
      "grad_norm": 2.6890158653259277,
      "learning_rate": 0.00045641770637497867,
      "loss": 1.0487,
      "step": 1021
    },
    {
      "epoch": 0.004366886862592615,
      "grad_norm": 3.0067901611328125,
      "learning_rate": 0.0004563749786361306,
      "loss": 1.0187,
      "step": 1022
    },
    {
      "epoch": 0.004371159746019809,
      "grad_norm": 0.6212704181671143,
      "learning_rate": 0.00045633225089728254,
      "loss": 0.1069,
      "step": 1023
    },
    {
      "epoch": 0.0043754326294470035,
      "grad_norm": 2.651362180709839,
      "learning_rate": 0.00045628952315843446,
      "loss": 0.9121,
      "step": 1024
    },
    {
      "epoch": 0.004379705512874197,
      "grad_norm": 23.08475112915039,
      "learning_rate": 0.0004562467954195864,
      "loss": 3.7714,
      "step": 1025
    },
    {
      "epoch": 0.004383978396301392,
      "grad_norm": 5.988715171813965,
      "learning_rate": 0.00045620406768073833,
      "loss": 1.941,
      "step": 1026
    },
    {
      "epoch": 0.004388251279728587,
      "grad_norm": 6.112349033355713,
      "learning_rate": 0.0004561613399418903,
      "loss": 2.4734,
      "step": 1027
    },
    {
      "epoch": 0.004392524163155781,
      "grad_norm": 0.568579375743866,
      "learning_rate": 0.00045611861220304226,
      "loss": 0.1538,
      "step": 1028
    },
    {
      "epoch": 0.004396797046582975,
      "grad_norm": 2.8449299335479736,
      "learning_rate": 0.00045607588446419417,
      "loss": 0.7237,
      "step": 1029
    },
    {
      "epoch": 0.004401069930010169,
      "grad_norm": 5.351871013641357,
      "learning_rate": 0.00045603315672534614,
      "loss": 2.0781,
      "step": 1030
    },
    {
      "epoch": 0.004405342813437364,
      "grad_norm": 0.8550828695297241,
      "learning_rate": 0.00045599042898649805,
      "loss": 0.2717,
      "step": 1031
    },
    {
      "epoch": 0.004409615696864558,
      "grad_norm": 0.8112425804138184,
      "learning_rate": 0.00045594770124765,
      "loss": 0.2529,
      "step": 1032
    },
    {
      "epoch": 0.0044138885802917525,
      "grad_norm": 0.6001638174057007,
      "learning_rate": 0.0004559049735088019,
      "loss": 0.1694,
      "step": 1033
    },
    {
      "epoch": 0.004418161463718947,
      "grad_norm": 2.588385820388794,
      "learning_rate": 0.0004558622457699539,
      "loss": 1.195,
      "step": 1034
    },
    {
      "epoch": 0.004422434347146141,
      "grad_norm": 2.187683582305908,
      "learning_rate": 0.00045581951803110585,
      "loss": 0.6334,
      "step": 1035
    },
    {
      "epoch": 0.004426707230573336,
      "grad_norm": 0.6725043058395386,
      "learning_rate": 0.00045577679029225776,
      "loss": 0.202,
      "step": 1036
    },
    {
      "epoch": 0.00443098011400053,
      "grad_norm": 3.4803414344787598,
      "learning_rate": 0.0004557340625534097,
      "loss": 1.1511,
      "step": 1037
    },
    {
      "epoch": 0.004435252997427724,
      "grad_norm": 3.7497994899749756,
      "learning_rate": 0.0004556913348145616,
      "loss": 1.5569,
      "step": 1038
    },
    {
      "epoch": 0.004439525880854918,
      "grad_norm": 2.5968527793884277,
      "learning_rate": 0.00045564860707571355,
      "loss": 0.7499,
      "step": 1039
    },
    {
      "epoch": 0.004443798764282113,
      "grad_norm": 7.393900394439697,
      "learning_rate": 0.00045560587933686546,
      "loss": 1.3508,
      "step": 1040
    },
    {
      "epoch": 0.004448071647709307,
      "grad_norm": 3.4203712940216064,
      "learning_rate": 0.0004555631515980174,
      "loss": 1.5127,
      "step": 1041
    },
    {
      "epoch": 0.0044523445311365015,
      "grad_norm": 4.886909008026123,
      "learning_rate": 0.0004555204238591694,
      "loss": 4.3808,
      "step": 1042
    },
    {
      "epoch": 0.004456617414563696,
      "grad_norm": 2.9089746475219727,
      "learning_rate": 0.0004554776961203213,
      "loss": 1.0822,
      "step": 1043
    },
    {
      "epoch": 0.00446089029799089,
      "grad_norm": 2.7663536071777344,
      "learning_rate": 0.00045543496838147327,
      "loss": 1.0063,
      "step": 1044
    },
    {
      "epoch": 0.004465163181418085,
      "grad_norm": 2.2588024139404297,
      "learning_rate": 0.0004553922406426252,
      "loss": 0.5746,
      "step": 1045
    },
    {
      "epoch": 0.004469436064845279,
      "grad_norm": 2.3628523349761963,
      "learning_rate": 0.00045534951290377714,
      "loss": 0.9017,
      "step": 1046
    },
    {
      "epoch": 0.004473708948272473,
      "grad_norm": 1.190991997718811,
      "learning_rate": 0.00045530678516492905,
      "loss": 0.3093,
      "step": 1047
    },
    {
      "epoch": 0.004477981831699667,
      "grad_norm": 2.3506710529327393,
      "learning_rate": 0.000455264057426081,
      "loss": 0.9829,
      "step": 1048
    },
    {
      "epoch": 0.004482254715126862,
      "grad_norm": 1.984787940979004,
      "learning_rate": 0.00045522132968723293,
      "loss": 0.3786,
      "step": 1049
    },
    {
      "epoch": 0.004486527598554056,
      "grad_norm": 1.3415207862854004,
      "learning_rate": 0.0004551786019483849,
      "loss": 0.3529,
      "step": 1050
    },
    {
      "epoch": 0.004490800481981251,
      "grad_norm": 3.3931779861450195,
      "learning_rate": 0.00045513587420953686,
      "loss": 1.3149,
      "step": 1051
    },
    {
      "epoch": 0.004495073365408445,
      "grad_norm": 1.8646209239959717,
      "learning_rate": 0.00045509314647068877,
      "loss": 0.411,
      "step": 1052
    },
    {
      "epoch": 0.004499346248835639,
      "grad_norm": 3.0032906532287598,
      "learning_rate": 0.00045505041873184073,
      "loss": 1.2916,
      "step": 1053
    },
    {
      "epoch": 0.004503619132262834,
      "grad_norm": 2.4998857975006104,
      "learning_rate": 0.00045500769099299264,
      "loss": 1.0708,
      "step": 1054
    },
    {
      "epoch": 0.004507892015690028,
      "grad_norm": 1.5322993993759155,
      "learning_rate": 0.0004549649632541446,
      "loss": 0.5399,
      "step": 1055
    },
    {
      "epoch": 0.0045121648991172224,
      "grad_norm": 2.6799333095550537,
      "learning_rate": 0.0004549222355152965,
      "loss": 0.8575,
      "step": 1056
    },
    {
      "epoch": 0.004516437782544416,
      "grad_norm": 1.9430946111679077,
      "learning_rate": 0.0004548795077764485,
      "loss": 0.6855,
      "step": 1057
    },
    {
      "epoch": 0.004520710665971611,
      "grad_norm": 1.9927314519882202,
      "learning_rate": 0.00045483678003760045,
      "loss": 0.6523,
      "step": 1058
    },
    {
      "epoch": 0.004524983549398805,
      "grad_norm": 3.227579355239868,
      "learning_rate": 0.00045479405229875236,
      "loss": 2.1875,
      "step": 1059
    },
    {
      "epoch": 0.004529256432826,
      "grad_norm": 1.850173830986023,
      "learning_rate": 0.0004547513245599043,
      "loss": 0.8288,
      "step": 1060
    },
    {
      "epoch": 0.004533529316253194,
      "grad_norm": 3.46026873588562,
      "learning_rate": 0.00045470859682105624,
      "loss": 0.8678,
      "step": 1061
    },
    {
      "epoch": 0.004537802199680388,
      "grad_norm": 3.9946184158325195,
      "learning_rate": 0.0004546658690822082,
      "loss": 1.7756,
      "step": 1062
    },
    {
      "epoch": 0.004542075083107583,
      "grad_norm": 3.046112060546875,
      "learning_rate": 0.0004546231413433601,
      "loss": 0.7927,
      "step": 1063
    },
    {
      "epoch": 0.004546347966534777,
      "grad_norm": 5.895371437072754,
      "learning_rate": 0.0004545804136045121,
      "loss": 1.6244,
      "step": 1064
    },
    {
      "epoch": 0.0045506208499619715,
      "grad_norm": 1.6194227933883667,
      "learning_rate": 0.00045453768586566404,
      "loss": 0.2921,
      "step": 1065
    },
    {
      "epoch": 0.004554893733389165,
      "grad_norm": 2.072051763534546,
      "learning_rate": 0.00045449495812681595,
      "loss": 0.6683,
      "step": 1066
    },
    {
      "epoch": 0.00455916661681636,
      "grad_norm": 3.881636142730713,
      "learning_rate": 0.0004544522303879679,
      "loss": 1.3656,
      "step": 1067
    },
    {
      "epoch": 0.004563439500243555,
      "grad_norm": 1.4569740295410156,
      "learning_rate": 0.00045440950264911983,
      "loss": 0.4063,
      "step": 1068
    },
    {
      "epoch": 0.004567712383670749,
      "grad_norm": 1.7068243026733398,
      "learning_rate": 0.0004543667749102718,
      "loss": 0.524,
      "step": 1069
    },
    {
      "epoch": 0.004571985267097943,
      "grad_norm": 2.76334810256958,
      "learning_rate": 0.00045432404717142365,
      "loss": 0.3299,
      "step": 1070
    },
    {
      "epoch": 0.004576258150525137,
      "grad_norm": 1.4635306596755981,
      "learning_rate": 0.0004542813194325756,
      "loss": 0.8414,
      "step": 1071
    },
    {
      "epoch": 0.004580531033952332,
      "grad_norm": 3.014538049697876,
      "learning_rate": 0.0004542385916937276,
      "loss": 0.7665,
      "step": 1072
    },
    {
      "epoch": 0.004584803917379526,
      "grad_norm": 3.7629640102386475,
      "learning_rate": 0.0004541958639548795,
      "loss": 0.9642,
      "step": 1073
    },
    {
      "epoch": 0.0045890768008067205,
      "grad_norm": 1.3152323961257935,
      "learning_rate": 0.00045415313621603146,
      "loss": 0.3504,
      "step": 1074
    },
    {
      "epoch": 0.004593349684233914,
      "grad_norm": 3.1416659355163574,
      "learning_rate": 0.00045411040847718337,
      "loss": 2.1992,
      "step": 1075
    },
    {
      "epoch": 0.004597622567661109,
      "grad_norm": 3.3997678756713867,
      "learning_rate": 0.00045406768073833533,
      "loss": 0.653,
      "step": 1076
    },
    {
      "epoch": 0.004601895451088304,
      "grad_norm": 0.7968749403953552,
      "learning_rate": 0.00045402495299948724,
      "loss": 0.2054,
      "step": 1077
    },
    {
      "epoch": 0.004606168334515498,
      "grad_norm": 3.4464497566223145,
      "learning_rate": 0.0004539822252606392,
      "loss": 0.6268,
      "step": 1078
    },
    {
      "epoch": 0.004610441217942692,
      "grad_norm": 2.5558085441589355,
      "learning_rate": 0.0004539394975217911,
      "loss": 0.9064,
      "step": 1079
    },
    {
      "epoch": 0.004614714101369886,
      "grad_norm": 6.809138298034668,
      "learning_rate": 0.0004538967697829431,
      "loss": 1.9142,
      "step": 1080
    },
    {
      "epoch": 0.004618986984797081,
      "grad_norm": 3.5831077098846436,
      "learning_rate": 0.00045385404204409505,
      "loss": 0.7466,
      "step": 1081
    },
    {
      "epoch": 0.004623259868224275,
      "grad_norm": 4.249333381652832,
      "learning_rate": 0.00045381131430524696,
      "loss": 1.321,
      "step": 1082
    },
    {
      "epoch": 0.0046275327516514695,
      "grad_norm": 3.597282886505127,
      "learning_rate": 0.0004537685865663989,
      "loss": 1.2404,
      "step": 1083
    },
    {
      "epoch": 0.004631805635078663,
      "grad_norm": 1.9812140464782715,
      "learning_rate": 0.00045372585882755083,
      "loss": 0.3819,
      "step": 1084
    },
    {
      "epoch": 0.004636078518505858,
      "grad_norm": 4.740042686462402,
      "learning_rate": 0.0004536831310887028,
      "loss": 0.991,
      "step": 1085
    },
    {
      "epoch": 0.004640351401933053,
      "grad_norm": 1.4253188371658325,
      "learning_rate": 0.0004536404033498547,
      "loss": 0.6925,
      "step": 1086
    },
    {
      "epoch": 0.004644624285360247,
      "grad_norm": 1.311727523803711,
      "learning_rate": 0.0004535976756110067,
      "loss": 0.2755,
      "step": 1087
    },
    {
      "epoch": 0.004648897168787441,
      "grad_norm": 1.9468011856079102,
      "learning_rate": 0.00045355494787215864,
      "loss": 0.3254,
      "step": 1088
    },
    {
      "epoch": 0.004653170052214635,
      "grad_norm": 2.2621943950653076,
      "learning_rate": 0.00045351222013331055,
      "loss": 0.3871,
      "step": 1089
    },
    {
      "epoch": 0.00465744293564183,
      "grad_norm": 2.8150439262390137,
      "learning_rate": 0.0004534694923944625,
      "loss": 0.862,
      "step": 1090
    },
    {
      "epoch": 0.004661715819069024,
      "grad_norm": 4.9661149978637695,
      "learning_rate": 0.0004534267646556144,
      "loss": 0.9419,
      "step": 1091
    },
    {
      "epoch": 0.004665988702496219,
      "grad_norm": 3.7349865436553955,
      "learning_rate": 0.0004533840369167664,
      "loss": 1.1929,
      "step": 1092
    },
    {
      "epoch": 0.004670261585923412,
      "grad_norm": 2.4674603939056396,
      "learning_rate": 0.0004533413091779183,
      "loss": 0.7296,
      "step": 1093
    },
    {
      "epoch": 0.004674534469350607,
      "grad_norm": 3.804717779159546,
      "learning_rate": 0.00045329858143907027,
      "loss": 2.1137,
      "step": 1094
    },
    {
      "epoch": 0.004678807352777802,
      "grad_norm": 2.71807599067688,
      "learning_rate": 0.00045325585370022223,
      "loss": 1.0478,
      "step": 1095
    },
    {
      "epoch": 0.004683080236204996,
      "grad_norm": 2.002983808517456,
      "learning_rate": 0.00045321312596137414,
      "loss": 0.508,
      "step": 1096
    },
    {
      "epoch": 0.0046873531196321904,
      "grad_norm": 2.231231689453125,
      "learning_rate": 0.0004531703982225261,
      "loss": 1.1332,
      "step": 1097
    },
    {
      "epoch": 0.004691626003059384,
      "grad_norm": 1.7230849266052246,
      "learning_rate": 0.000453127670483678,
      "loss": 0.4559,
      "step": 1098
    },
    {
      "epoch": 0.004695898886486579,
      "grad_norm": 1.7720268964767456,
      "learning_rate": 0.00045308494274483,
      "loss": 0.3693,
      "step": 1099
    },
    {
      "epoch": 0.004700171769913773,
      "grad_norm": 1.539787769317627,
      "learning_rate": 0.0004530422150059819,
      "loss": 0.6471,
      "step": 1100
    },
    {
      "epoch": 0.004704444653340968,
      "grad_norm": 1.7469995021820068,
      "learning_rate": 0.00045299948726713386,
      "loss": 0.4563,
      "step": 1101
    },
    {
      "epoch": 0.004708717536768162,
      "grad_norm": 0.9879215359687805,
      "learning_rate": 0.0004529567595282858,
      "loss": 0.2212,
      "step": 1102
    },
    {
      "epoch": 0.004712990420195356,
      "grad_norm": 2.792339324951172,
      "learning_rate": 0.0004529140317894377,
      "loss": 0.8931,
      "step": 1103
    },
    {
      "epoch": 0.004717263303622551,
      "grad_norm": 1.6417468786239624,
      "learning_rate": 0.00045287130405058965,
      "loss": 0.6317,
      "step": 1104
    },
    {
      "epoch": 0.004721536187049745,
      "grad_norm": 3.0400571823120117,
      "learning_rate": 0.00045282857631174156,
      "loss": 1.0912,
      "step": 1105
    },
    {
      "epoch": 0.0047258090704769395,
      "grad_norm": 1.668020486831665,
      "learning_rate": 0.0004527858485728935,
      "loss": 0.3619,
      "step": 1106
    },
    {
      "epoch": 0.004730081953904133,
      "grad_norm": 2.440019369125366,
      "learning_rate": 0.00045274312083404543,
      "loss": 0.5207,
      "step": 1107
    },
    {
      "epoch": 0.004734354837331328,
      "grad_norm": 1.6858806610107422,
      "learning_rate": 0.0004527003930951974,
      "loss": 0.3702,
      "step": 1108
    },
    {
      "epoch": 0.004738627720758522,
      "grad_norm": 1.9320499897003174,
      "learning_rate": 0.00045265766535634936,
      "loss": 0.4201,
      "step": 1109
    },
    {
      "epoch": 0.004742900604185717,
      "grad_norm": 4.474892616271973,
      "learning_rate": 0.00045261493761750127,
      "loss": 1.6355,
      "step": 1110
    },
    {
      "epoch": 0.004747173487612911,
      "grad_norm": 4.596563816070557,
      "learning_rate": 0.00045257220987865324,
      "loss": 1.4577,
      "step": 1111
    },
    {
      "epoch": 0.004751446371040105,
      "grad_norm": 1.2986061573028564,
      "learning_rate": 0.00045252948213980515,
      "loss": 0.2455,
      "step": 1112
    },
    {
      "epoch": 0.0047557192544673,
      "grad_norm": 2.2783255577087402,
      "learning_rate": 0.0004524867544009571,
      "loss": 0.6172,
      "step": 1113
    },
    {
      "epoch": 0.004759992137894494,
      "grad_norm": 4.186773777008057,
      "learning_rate": 0.000452444026662109,
      "loss": 1.676,
      "step": 1114
    },
    {
      "epoch": 0.0047642650213216885,
      "grad_norm": 2.5565922260284424,
      "learning_rate": 0.000452401298923261,
      "loss": 0.7758,
      "step": 1115
    },
    {
      "epoch": 0.004768537904748882,
      "grad_norm": 2.6941492557525635,
      "learning_rate": 0.0004523585711844129,
      "loss": 0.7082,
      "step": 1116
    },
    {
      "epoch": 0.004772810788176077,
      "grad_norm": 2.46138596534729,
      "learning_rate": 0.00045231584344556486,
      "loss": 1.007,
      "step": 1117
    },
    {
      "epoch": 0.004777083671603271,
      "grad_norm": 3.003821611404419,
      "learning_rate": 0.00045227311570671683,
      "loss": 0.9762,
      "step": 1118
    },
    {
      "epoch": 0.004781356555030466,
      "grad_norm": 1.3472870588302612,
      "learning_rate": 0.00045223038796786874,
      "loss": 0.4078,
      "step": 1119
    },
    {
      "epoch": 0.00478562943845766,
      "grad_norm": 1.9832743406295776,
      "learning_rate": 0.0004521876602290207,
      "loss": 0.2546,
      "step": 1120
    },
    {
      "epoch": 0.004789902321884854,
      "grad_norm": 2.2853214740753174,
      "learning_rate": 0.0004521449324901726,
      "loss": 0.5553,
      "step": 1121
    },
    {
      "epoch": 0.004794175205312049,
      "grad_norm": 3.45784330368042,
      "learning_rate": 0.0004521022047513246,
      "loss": 1.1088,
      "step": 1122
    },
    {
      "epoch": 0.004798448088739243,
      "grad_norm": 2.5798282623291016,
      "learning_rate": 0.0004520594770124765,
      "loss": 0.6239,
      "step": 1123
    },
    {
      "epoch": 0.0048027209721664375,
      "grad_norm": 1.1062695980072021,
      "learning_rate": 0.00045201674927362846,
      "loss": 0.3078,
      "step": 1124
    },
    {
      "epoch": 0.004806993855593631,
      "grad_norm": 1.7038395404815674,
      "learning_rate": 0.0004519740215347804,
      "loss": 0.6664,
      "step": 1125
    },
    {
      "epoch": 0.004811266739020826,
      "grad_norm": 2.266608715057373,
      "learning_rate": 0.00045193129379593233,
      "loss": 0.5228,
      "step": 1126
    },
    {
      "epoch": 0.00481553962244802,
      "grad_norm": 1.8217601776123047,
      "learning_rate": 0.0004518885660570843,
      "loss": 0.472,
      "step": 1127
    },
    {
      "epoch": 0.004819812505875215,
      "grad_norm": 3.745394706726074,
      "learning_rate": 0.0004518458383182362,
      "loss": 1.3897,
      "step": 1128
    },
    {
      "epoch": 0.004824085389302409,
      "grad_norm": 2.527205467224121,
      "learning_rate": 0.00045180311057938817,
      "loss": 0.7508,
      "step": 1129
    },
    {
      "epoch": 0.004828358272729603,
      "grad_norm": 6.089282512664795,
      "learning_rate": 0.0004517603828405401,
      "loss": 2.1882,
      "step": 1130
    },
    {
      "epoch": 0.004832631156156798,
      "grad_norm": 1.184661626815796,
      "learning_rate": 0.00045171765510169205,
      "loss": 0.2298,
      "step": 1131
    },
    {
      "epoch": 0.004836904039583992,
      "grad_norm": 3.084059238433838,
      "learning_rate": 0.000451674927362844,
      "loss": 0.8486,
      "step": 1132
    },
    {
      "epoch": 0.0048411769230111866,
      "grad_norm": 1.5144739151000977,
      "learning_rate": 0.0004516321996239959,
      "loss": 0.428,
      "step": 1133
    },
    {
      "epoch": 0.00484544980643838,
      "grad_norm": 1.7664976119995117,
      "learning_rate": 0.0004515894718851479,
      "loss": 0.5162,
      "step": 1134
    },
    {
      "epoch": 0.004849722689865575,
      "grad_norm": 2.5709121227264404,
      "learning_rate": 0.0004515467441462998,
      "loss": 0.6107,
      "step": 1135
    },
    {
      "epoch": 0.004853995573292769,
      "grad_norm": 4.529119968414307,
      "learning_rate": 0.0004515040164074517,
      "loss": 1.1509,
      "step": 1136
    },
    {
      "epoch": 0.004858268456719964,
      "grad_norm": 2.2233409881591797,
      "learning_rate": 0.0004514612886686036,
      "loss": 1.2326,
      "step": 1137
    },
    {
      "epoch": 0.0048625413401471584,
      "grad_norm": 2.8111958503723145,
      "learning_rate": 0.0004514185609297556,
      "loss": 0.9076,
      "step": 1138
    },
    {
      "epoch": 0.004866814223574352,
      "grad_norm": 1.7050154209136963,
      "learning_rate": 0.00045137583319090755,
      "loss": 0.3502,
      "step": 1139
    },
    {
      "epoch": 0.004871087107001547,
      "grad_norm": 1.7551653385162354,
      "learning_rate": 0.00045133310545205946,
      "loss": 0.4526,
      "step": 1140
    },
    {
      "epoch": 0.004875359990428741,
      "grad_norm": 5.483144760131836,
      "learning_rate": 0.0004512903777132114,
      "loss": 1.9252,
      "step": 1141
    },
    {
      "epoch": 0.004879632873855936,
      "grad_norm": 2.7476987838745117,
      "learning_rate": 0.00045124764997436334,
      "loss": 0.7845,
      "step": 1142
    },
    {
      "epoch": 0.0048839057572831295,
      "grad_norm": 2.3617775440216064,
      "learning_rate": 0.0004512049222355153,
      "loss": 0.3488,
      "step": 1143
    },
    {
      "epoch": 0.004888178640710324,
      "grad_norm": 2.27017879486084,
      "learning_rate": 0.0004511621944966672,
      "loss": 0.4581,
      "step": 1144
    },
    {
      "epoch": 0.004892451524137519,
      "grad_norm": 1.9630478620529175,
      "learning_rate": 0.0004511194667578192,
      "loss": 0.553,
      "step": 1145
    },
    {
      "epoch": 0.004896724407564713,
      "grad_norm": 2.216017961502075,
      "learning_rate": 0.00045107673901897114,
      "loss": 1.1039,
      "step": 1146
    },
    {
      "epoch": 0.0049009972909919075,
      "grad_norm": 3.5677073001861572,
      "learning_rate": 0.00045103401128012305,
      "loss": 2.2303,
      "step": 1147
    },
    {
      "epoch": 0.004905270174419101,
      "grad_norm": 3.195765733718872,
      "learning_rate": 0.000450991283541275,
      "loss": 0.7088,
      "step": 1148
    },
    {
      "epoch": 0.004909543057846296,
      "grad_norm": 3.607729911804199,
      "learning_rate": 0.00045094855580242693,
      "loss": 1.1531,
      "step": 1149
    },
    {
      "epoch": 0.00491381594127349,
      "grad_norm": 1.0046004056930542,
      "learning_rate": 0.0004509058280635789,
      "loss": 0.298,
      "step": 1150
    },
    {
      "epoch": 0.004918088824700685,
      "grad_norm": 4.142384052276611,
      "learning_rate": 0.0004508631003247308,
      "loss": 1.6016,
      "step": 1151
    },
    {
      "epoch": 0.0049223617081278785,
      "grad_norm": 0.7293552160263062,
      "learning_rate": 0.00045082037258588277,
      "loss": 0.1653,
      "step": 1152
    },
    {
      "epoch": 0.004926634591555073,
      "grad_norm": 3.9481852054595947,
      "learning_rate": 0.0004507776448470347,
      "loss": 1.4554,
      "step": 1153
    },
    {
      "epoch": 0.004930907474982268,
      "grad_norm": 0.9161955714225769,
      "learning_rate": 0.00045073491710818665,
      "loss": 0.2781,
      "step": 1154
    },
    {
      "epoch": 0.004935180358409462,
      "grad_norm": 2.5866100788116455,
      "learning_rate": 0.0004506921893693386,
      "loss": 0.6725,
      "step": 1155
    },
    {
      "epoch": 0.0049394532418366565,
      "grad_norm": 3.318791627883911,
      "learning_rate": 0.0004506494616304905,
      "loss": 1.0693,
      "step": 1156
    },
    {
      "epoch": 0.00494372612526385,
      "grad_norm": 3.186067819595337,
      "learning_rate": 0.0004506067338916425,
      "loss": 0.7087,
      "step": 1157
    },
    {
      "epoch": 0.004947999008691045,
      "grad_norm": 1.8211171627044678,
      "learning_rate": 0.0004505640061527944,
      "loss": 0.5302,
      "step": 1158
    },
    {
      "epoch": 0.004952271892118239,
      "grad_norm": 0.8507198691368103,
      "learning_rate": 0.00045052127841394636,
      "loss": 0.2415,
      "step": 1159
    },
    {
      "epoch": 0.004956544775545434,
      "grad_norm": 3.298290252685547,
      "learning_rate": 0.0004504785506750983,
      "loss": 0.8653,
      "step": 1160
    },
    {
      "epoch": 0.0049608176589726275,
      "grad_norm": 4.0848388671875,
      "learning_rate": 0.00045043582293625024,
      "loss": 1.5274,
      "step": 1161
    },
    {
      "epoch": 0.004965090542399822,
      "grad_norm": 2.8356919288635254,
      "learning_rate": 0.0004503930951974022,
      "loss": 1.1804,
      "step": 1162
    },
    {
      "epoch": 0.004969363425827017,
      "grad_norm": 3.2475132942199707,
      "learning_rate": 0.0004503503674585541,
      "loss": 0.8285,
      "step": 1163
    },
    {
      "epoch": 0.004973636309254211,
      "grad_norm": 4.363229274749756,
      "learning_rate": 0.0004503076397197061,
      "loss": 1.1315,
      "step": 1164
    },
    {
      "epoch": 0.0049779091926814055,
      "grad_norm": 2.919980525970459,
      "learning_rate": 0.000450264911980858,
      "loss": 0.6253,
      "step": 1165
    },
    {
      "epoch": 0.004982182076108599,
      "grad_norm": 2.7227394580841064,
      "learning_rate": 0.00045022218424200995,
      "loss": 0.9705,
      "step": 1166
    },
    {
      "epoch": 0.004986454959535794,
      "grad_norm": 1.4186660051345825,
      "learning_rate": 0.00045017945650316187,
      "loss": 0.5033,
      "step": 1167
    },
    {
      "epoch": 0.004990727842962988,
      "grad_norm": 2.6946513652801514,
      "learning_rate": 0.0004501367287643138,
      "loss": 0.6355,
      "step": 1168
    },
    {
      "epoch": 0.004995000726390183,
      "grad_norm": 4.9690680503845215,
      "learning_rate": 0.00045009400102546574,
      "loss": 1.8505,
      "step": 1169
    },
    {
      "epoch": 0.0049992736098173765,
      "grad_norm": 3.0677123069763184,
      "learning_rate": 0.00045005127328661765,
      "loss": 1.6908,
      "step": 1170
    },
    {
      "epoch": 0.005003546493244571,
      "grad_norm": 4.369708061218262,
      "learning_rate": 0.0004500085455477696,
      "loss": 0.9071,
      "step": 1171
    },
    {
      "epoch": 0.005007819376671766,
      "grad_norm": 1.9077873229980469,
      "learning_rate": 0.00044996581780892153,
      "loss": 0.3689,
      "step": 1172
    },
    {
      "epoch": 0.00501209226009896,
      "grad_norm": 1.7236926555633545,
      "learning_rate": 0.0004499230900700735,
      "loss": 0.322,
      "step": 1173
    },
    {
      "epoch": 0.0050163651435261546,
      "grad_norm": 5.057679653167725,
      "learning_rate": 0.0004498803623312254,
      "loss": 4.2926,
      "step": 1174
    },
    {
      "epoch": 0.005020638026953348,
      "grad_norm": 2.881971836090088,
      "learning_rate": 0.00044983763459237737,
      "loss": 0.6979,
      "step": 1175
    },
    {
      "epoch": 0.005024910910380543,
      "grad_norm": 2.100538492202759,
      "learning_rate": 0.00044979490685352933,
      "loss": 0.9701,
      "step": 1176
    },
    {
      "epoch": 0.005029183793807737,
      "grad_norm": 3.243647336959839,
      "learning_rate": 0.00044975217911468124,
      "loss": 0.6529,
      "step": 1177
    },
    {
      "epoch": 0.005033456677234932,
      "grad_norm": 3.2577412128448486,
      "learning_rate": 0.0004497094513758332,
      "loss": 0.8348,
      "step": 1178
    },
    {
      "epoch": 0.0050377295606621264,
      "grad_norm": 2.9670462608337402,
      "learning_rate": 0.0004496667236369851,
      "loss": 0.9058,
      "step": 1179
    },
    {
      "epoch": 0.00504200244408932,
      "grad_norm": 2.6268391609191895,
      "learning_rate": 0.0004496239958981371,
      "loss": 0.6024,
      "step": 1180
    },
    {
      "epoch": 0.005046275327516515,
      "grad_norm": 3.028294563293457,
      "learning_rate": 0.000449581268159289,
      "loss": 1.5125,
      "step": 1181
    },
    {
      "epoch": 0.005050548210943709,
      "grad_norm": 3.1768264770507812,
      "learning_rate": 0.00044953854042044096,
      "loss": 0.843,
      "step": 1182
    },
    {
      "epoch": 0.005054821094370904,
      "grad_norm": 7.447973728179932,
      "learning_rate": 0.00044949581268159287,
      "loss": 2.5408,
      "step": 1183
    },
    {
      "epoch": 0.0050590939777980974,
      "grad_norm": 2.386699676513672,
      "learning_rate": 0.00044945308494274484,
      "loss": 0.7139,
      "step": 1184
    },
    {
      "epoch": 0.005063366861225292,
      "grad_norm": 2.8022382259368896,
      "learning_rate": 0.0004494103572038968,
      "loss": 0.6995,
      "step": 1185
    },
    {
      "epoch": 0.005067639744652486,
      "grad_norm": 0.7165153622627258,
      "learning_rate": 0.0004493676294650487,
      "loss": 0.1433,
      "step": 1186
    },
    {
      "epoch": 0.005071912628079681,
      "grad_norm": 2.2030653953552246,
      "learning_rate": 0.0004493249017262007,
      "loss": 0.5083,
      "step": 1187
    },
    {
      "epoch": 0.0050761855115068755,
      "grad_norm": 2.417678117752075,
      "learning_rate": 0.0004492821739873526,
      "loss": 0.7232,
      "step": 1188
    },
    {
      "epoch": 0.005080458394934069,
      "grad_norm": 2.1874477863311768,
      "learning_rate": 0.00044923944624850455,
      "loss": 0.8858,
      "step": 1189
    },
    {
      "epoch": 0.005084731278361264,
      "grad_norm": 3.394163131713867,
      "learning_rate": 0.00044919671850965646,
      "loss": 1.2325,
      "step": 1190
    },
    {
      "epoch": 0.005089004161788458,
      "grad_norm": 2.8838605880737305,
      "learning_rate": 0.00044915399077080843,
      "loss": 1.0983,
      "step": 1191
    },
    {
      "epoch": 0.005093277045215653,
      "grad_norm": 3.059088706970215,
      "learning_rate": 0.0004491112630319604,
      "loss": 0.8938,
      "step": 1192
    },
    {
      "epoch": 0.0050975499286428465,
      "grad_norm": 2.307922840118408,
      "learning_rate": 0.0004490685352931123,
      "loss": 1.033,
      "step": 1193
    },
    {
      "epoch": 0.005101822812070041,
      "grad_norm": 2.181708812713623,
      "learning_rate": 0.00044902580755426427,
      "loss": 0.6235,
      "step": 1194
    },
    {
      "epoch": 0.005106095695497235,
      "grad_norm": 2.265742063522339,
      "learning_rate": 0.0004489830798154162,
      "loss": 0.98,
      "step": 1195
    },
    {
      "epoch": 0.00511036857892443,
      "grad_norm": 2.448291778564453,
      "learning_rate": 0.00044894035207656814,
      "loss": 1.1435,
      "step": 1196
    },
    {
      "epoch": 0.0051146414623516245,
      "grad_norm": 1.349344253540039,
      "learning_rate": 0.00044889762433772005,
      "loss": 0.2526,
      "step": 1197
    },
    {
      "epoch": 0.005118914345778818,
      "grad_norm": 1.6562422513961792,
      "learning_rate": 0.000448854896598872,
      "loss": 0.5892,
      "step": 1198
    },
    {
      "epoch": 0.005123187229206013,
      "grad_norm": 4.522043228149414,
      "learning_rate": 0.000448812168860024,
      "loss": 1.3642,
      "step": 1199
    },
    {
      "epoch": 0.005127460112633207,
      "grad_norm": 2.334782600402832,
      "learning_rate": 0.0004487694411211759,
      "loss": 1.0839,
      "step": 1200
    },
    {
      "epoch": 0.005131732996060402,
      "grad_norm": 1.7254949808120728,
      "learning_rate": 0.0004487267133823278,
      "loss": 0.5069,
      "step": 1201
    },
    {
      "epoch": 0.0051360058794875955,
      "grad_norm": 3.165618896484375,
      "learning_rate": 0.0004486839856434797,
      "loss": 0.8175,
      "step": 1202
    },
    {
      "epoch": 0.00514027876291479,
      "grad_norm": 2.809908628463745,
      "learning_rate": 0.0004486412579046317,
      "loss": 0.9008,
      "step": 1203
    },
    {
      "epoch": 0.005144551646341984,
      "grad_norm": 2.715449094772339,
      "learning_rate": 0.0004485985301657836,
      "loss": 0.733,
      "step": 1204
    },
    {
      "epoch": 0.005148824529769179,
      "grad_norm": 2.7794547080993652,
      "learning_rate": 0.00044855580242693556,
      "loss": 0.9995,
      "step": 1205
    },
    {
      "epoch": 0.0051530974131963735,
      "grad_norm": 1.3255664110183716,
      "learning_rate": 0.0004485130746880875,
      "loss": 0.2688,
      "step": 1206
    },
    {
      "epoch": 0.005157370296623567,
      "grad_norm": 1.521425724029541,
      "learning_rate": 0.00044847034694923943,
      "loss": 0.4868,
      "step": 1207
    },
    {
      "epoch": 0.005161643180050762,
      "grad_norm": 1.655775785446167,
      "learning_rate": 0.0004484276192103914,
      "loss": 0.4567,
      "step": 1208
    },
    {
      "epoch": 0.005165916063477956,
      "grad_norm": 1.3484171628952026,
      "learning_rate": 0.0004483848914715433,
      "loss": 0.3257,
      "step": 1209
    },
    {
      "epoch": 0.005170188946905151,
      "grad_norm": 1.312937617301941,
      "learning_rate": 0.0004483421637326953,
      "loss": 0.3253,
      "step": 1210
    },
    {
      "epoch": 0.0051744618303323445,
      "grad_norm": 2.6438562870025635,
      "learning_rate": 0.0004482994359938472,
      "loss": 0.7034,
      "step": 1211
    },
    {
      "epoch": 0.005178734713759539,
      "grad_norm": 2.699000358581543,
      "learning_rate": 0.00044825670825499915,
      "loss": 0.6003,
      "step": 1212
    },
    {
      "epoch": 0.005183007597186734,
      "grad_norm": 4.921844959259033,
      "learning_rate": 0.0004482139805161511,
      "loss": 1.158,
      "step": 1213
    },
    {
      "epoch": 0.005187280480613928,
      "grad_norm": 1.066305160522461,
      "learning_rate": 0.000448171252777303,
      "loss": 0.2686,
      "step": 1214
    },
    {
      "epoch": 0.0051915533640411226,
      "grad_norm": 2.456904888153076,
      "learning_rate": 0.000448128525038455,
      "loss": 0.6521,
      "step": 1215
    },
    {
      "epoch": 0.005195826247468316,
      "grad_norm": 1.9260802268981934,
      "learning_rate": 0.0004480857972996069,
      "loss": 0.4637,
      "step": 1216
    },
    {
      "epoch": 0.005200099130895511,
      "grad_norm": 2.2700259685516357,
      "learning_rate": 0.00044804306956075887,
      "loss": 0.642,
      "step": 1217
    },
    {
      "epoch": 0.005204372014322705,
      "grad_norm": 2.024592638015747,
      "learning_rate": 0.0004480003418219108,
      "loss": 0.478,
      "step": 1218
    },
    {
      "epoch": 0.0052086448977499,
      "grad_norm": 4.3060808181762695,
      "learning_rate": 0.00044795761408306274,
      "loss": 1.2881,
      "step": 1219
    },
    {
      "epoch": 0.005212917781177094,
      "grad_norm": 3.1094913482666016,
      "learning_rate": 0.00044791488634421465,
      "loss": 0.9971,
      "step": 1220
    },
    {
      "epoch": 0.005217190664604288,
      "grad_norm": 1.8280794620513916,
      "learning_rate": 0.0004478721586053666,
      "loss": 0.4105,
      "step": 1221
    },
    {
      "epoch": 0.005221463548031483,
      "grad_norm": 3.06430721282959,
      "learning_rate": 0.0004478294308665186,
      "loss": 0.8908,
      "step": 1222
    },
    {
      "epoch": 0.005225736431458677,
      "grad_norm": 2.2805471420288086,
      "learning_rate": 0.0004477867031276705,
      "loss": 0.701,
      "step": 1223
    },
    {
      "epoch": 0.005230009314885872,
      "grad_norm": 3.8389811515808105,
      "learning_rate": 0.00044774397538882246,
      "loss": 0.9989,
      "step": 1224
    },
    {
      "epoch": 0.0052342821983130654,
      "grad_norm": 4.634206295013428,
      "learning_rate": 0.00044770124764997437,
      "loss": 1.5764,
      "step": 1225
    },
    {
      "epoch": 0.00523855508174026,
      "grad_norm": 1.2374260425567627,
      "learning_rate": 0.00044765851991112633,
      "loss": 0.2469,
      "step": 1226
    },
    {
      "epoch": 0.005242827965167454,
      "grad_norm": 2.301809787750244,
      "learning_rate": 0.00044761579217227824,
      "loss": 0.8074,
      "step": 1227
    },
    {
      "epoch": 0.005247100848594649,
      "grad_norm": 3.1299779415130615,
      "learning_rate": 0.0004475730644334302,
      "loss": 0.8876,
      "step": 1228
    },
    {
      "epoch": 0.005251373732021843,
      "grad_norm": 1.2325420379638672,
      "learning_rate": 0.0004475303366945822,
      "loss": 0.2165,
      "step": 1229
    },
    {
      "epoch": 0.005255646615449037,
      "grad_norm": 1.825047492980957,
      "learning_rate": 0.0004474876089557341,
      "loss": 0.6616,
      "step": 1230
    },
    {
      "epoch": 0.005259919498876232,
      "grad_norm": 4.3322834968566895,
      "learning_rate": 0.00044744488121688605,
      "loss": 1.7554,
      "step": 1231
    },
    {
      "epoch": 0.005264192382303426,
      "grad_norm": 2.0122551918029785,
      "learning_rate": 0.00044740215347803796,
      "loss": 0.6464,
      "step": 1232
    },
    {
      "epoch": 0.005268465265730621,
      "grad_norm": 2.868863821029663,
      "learning_rate": 0.0004473594257391899,
      "loss": 1.0428,
      "step": 1233
    },
    {
      "epoch": 0.0052727381491578145,
      "grad_norm": 2.058549642562866,
      "learning_rate": 0.0004473166980003418,
      "loss": 0.5333,
      "step": 1234
    },
    {
      "epoch": 0.005277011032585009,
      "grad_norm": 1.3701791763305664,
      "learning_rate": 0.00044727397026149375,
      "loss": 0.5214,
      "step": 1235
    },
    {
      "epoch": 0.005281283916012203,
      "grad_norm": 0.8446633815765381,
      "learning_rate": 0.0004472312425226457,
      "loss": 0.1628,
      "step": 1236
    },
    {
      "epoch": 0.005285556799439398,
      "grad_norm": 2.0443716049194336,
      "learning_rate": 0.0004471885147837976,
      "loss": 0.7118,
      "step": 1237
    },
    {
      "epoch": 0.005289829682866592,
      "grad_norm": 2.13236927986145,
      "learning_rate": 0.0004471457870449496,
      "loss": 0.6629,
      "step": 1238
    },
    {
      "epoch": 0.005294102566293786,
      "grad_norm": 1.0224368572235107,
      "learning_rate": 0.0004471030593061015,
      "loss": 0.3873,
      "step": 1239
    },
    {
      "epoch": 0.005298375449720981,
      "grad_norm": 0.6342720985412598,
      "learning_rate": 0.00044706033156725346,
      "loss": 0.1506,
      "step": 1240
    },
    {
      "epoch": 0.005302648333148175,
      "grad_norm": 0.7945282459259033,
      "learning_rate": 0.0004470176038284054,
      "loss": 0.2786,
      "step": 1241
    },
    {
      "epoch": 0.00530692121657537,
      "grad_norm": 2.8503808975219727,
      "learning_rate": 0.00044697487608955734,
      "loss": 1.8217,
      "step": 1242
    },
    {
      "epoch": 0.0053111941000025635,
      "grad_norm": 4.280829906463623,
      "learning_rate": 0.0004469321483507093,
      "loss": 1.8206,
      "step": 1243
    },
    {
      "epoch": 0.005315466983429758,
      "grad_norm": 1.8385246992111206,
      "learning_rate": 0.0004468894206118612,
      "loss": 1.0322,
      "step": 1244
    },
    {
      "epoch": 0.005319739866856952,
      "grad_norm": 2.5754575729370117,
      "learning_rate": 0.0004468466928730132,
      "loss": 0.5912,
      "step": 1245
    },
    {
      "epoch": 0.005324012750284147,
      "grad_norm": 5.089457035064697,
      "learning_rate": 0.0004468039651341651,
      "loss": 1.7311,
      "step": 1246
    },
    {
      "epoch": 0.005328285633711341,
      "grad_norm": 0.8683953285217285,
      "learning_rate": 0.00044676123739531705,
      "loss": 0.2583,
      "step": 1247
    },
    {
      "epoch": 0.005332558517138535,
      "grad_norm": 1.9322668313980103,
      "learning_rate": 0.00044671850965646897,
      "loss": 0.5704,
      "step": 1248
    },
    {
      "epoch": 0.00533683140056573,
      "grad_norm": 1.8610364198684692,
      "learning_rate": 0.00044667578191762093,
      "loss": 0.9199,
      "step": 1249
    },
    {
      "epoch": 0.005341104283992924,
      "grad_norm": 0.5271862745285034,
      "learning_rate": 0.00044663305417877284,
      "loss": 0.1286,
      "step": 1250
    },
    {
      "epoch": 0.005345377167420119,
      "grad_norm": 3.1454825401306152,
      "learning_rate": 0.0004465903264399248,
      "loss": 1.1482,
      "step": 1251
    },
    {
      "epoch": 0.0053496500508473125,
      "grad_norm": 0.8462303876876831,
      "learning_rate": 0.00044654759870107677,
      "loss": 0.2534,
      "step": 1252
    },
    {
      "epoch": 0.005353922934274507,
      "grad_norm": 2.549380302429199,
      "learning_rate": 0.0004465048709622287,
      "loss": 0.4262,
      "step": 1253
    },
    {
      "epoch": 0.005358195817701701,
      "grad_norm": 3.086268901824951,
      "learning_rate": 0.00044646214322338065,
      "loss": 1.0085,
      "step": 1254
    },
    {
      "epoch": 0.005362468701128896,
      "grad_norm": 2.9228243827819824,
      "learning_rate": 0.00044641941548453256,
      "loss": 0.7209,
      "step": 1255
    },
    {
      "epoch": 0.0053667415845560906,
      "grad_norm": 3.935148239135742,
      "learning_rate": 0.0004463766877456845,
      "loss": 0.8576,
      "step": 1256
    },
    {
      "epoch": 0.005371014467983284,
      "grad_norm": 5.1680378913879395,
      "learning_rate": 0.00044633396000683643,
      "loss": 0.7799,
      "step": 1257
    },
    {
      "epoch": 0.005375287351410479,
      "grad_norm": 1.9220072031021118,
      "learning_rate": 0.0004462912322679884,
      "loss": 0.6415,
      "step": 1258
    },
    {
      "epoch": 0.005379560234837673,
      "grad_norm": 2.2877066135406494,
      "learning_rate": 0.00044624850452914036,
      "loss": 0.2651,
      "step": 1259
    },
    {
      "epoch": 0.005383833118264868,
      "grad_norm": 2.609328508377075,
      "learning_rate": 0.0004462057767902923,
      "loss": 0.8161,
      "step": 1260
    },
    {
      "epoch": 0.005388106001692062,
      "grad_norm": 0.9052590727806091,
      "learning_rate": 0.00044616304905144424,
      "loss": 0.2435,
      "step": 1261
    },
    {
      "epoch": 0.005392378885119256,
      "grad_norm": 0.9046390056610107,
      "learning_rate": 0.00044612032131259615,
      "loss": 0.2434,
      "step": 1262
    },
    {
      "epoch": 0.00539665176854645,
      "grad_norm": 2.0533206462860107,
      "learning_rate": 0.0004460775935737481,
      "loss": 0.6621,
      "step": 1263
    },
    {
      "epoch": 0.005400924651973645,
      "grad_norm": 6.286439895629883,
      "learning_rate": 0.0004460348658349,
      "loss": 2.0732,
      "step": 1264
    },
    {
      "epoch": 0.00540519753540084,
      "grad_norm": 0.7558610439300537,
      "learning_rate": 0.000445992138096052,
      "loss": 0.1978,
      "step": 1265
    },
    {
      "epoch": 0.0054094704188280334,
      "grad_norm": 2.6861112117767334,
      "learning_rate": 0.00044594941035720396,
      "loss": 0.8229,
      "step": 1266
    },
    {
      "epoch": 0.005413743302255228,
      "grad_norm": 2.5815563201904297,
      "learning_rate": 0.0004459066826183558,
      "loss": 0.7757,
      "step": 1267
    },
    {
      "epoch": 0.005418016185682422,
      "grad_norm": 0.772483766078949,
      "learning_rate": 0.0004458639548795078,
      "loss": 0.2977,
      "step": 1268
    },
    {
      "epoch": 0.005422289069109617,
      "grad_norm": 4.326590061187744,
      "learning_rate": 0.0004458212271406597,
      "loss": 1.0566,
      "step": 1269
    },
    {
      "epoch": 0.005426561952536811,
      "grad_norm": 2.5497944355010986,
      "learning_rate": 0.00044577849940181165,
      "loss": 0.713,
      "step": 1270
    },
    {
      "epoch": 0.005430834835964005,
      "grad_norm": 5.768808364868164,
      "learning_rate": 0.00044573577166296356,
      "loss": 1.4625,
      "step": 1271
    },
    {
      "epoch": 0.005435107719391199,
      "grad_norm": 2.539376735687256,
      "learning_rate": 0.00044569304392411553,
      "loss": 0.7353,
      "step": 1272
    },
    {
      "epoch": 0.005439380602818394,
      "grad_norm": 4.362766265869141,
      "learning_rate": 0.0004456503161852675,
      "loss": 1.5727,
      "step": 1273
    },
    {
      "epoch": 0.005443653486245589,
      "grad_norm": 1.999411940574646,
      "learning_rate": 0.0004456075884464194,
      "loss": 0.5537,
      "step": 1274
    },
    {
      "epoch": 0.0054479263696727825,
      "grad_norm": 2.9115099906921387,
      "learning_rate": 0.00044556486070757137,
      "loss": 0.9051,
      "step": 1275
    },
    {
      "epoch": 0.005452199253099977,
      "grad_norm": 2.8229293823242188,
      "learning_rate": 0.0004455221329687233,
      "loss": 0.6355,
      "step": 1276
    },
    {
      "epoch": 0.005456472136527171,
      "grad_norm": 5.3874359130859375,
      "learning_rate": 0.00044547940522987524,
      "loss": 1.3582,
      "step": 1277
    },
    {
      "epoch": 0.005460745019954366,
      "grad_norm": 2.634176015853882,
      "learning_rate": 0.00044543667749102716,
      "loss": 0.889,
      "step": 1278
    },
    {
      "epoch": 0.00546501790338156,
      "grad_norm": 1.5844483375549316,
      "learning_rate": 0.0004453939497521791,
      "loss": 0.4525,
      "step": 1279
    },
    {
      "epoch": 0.005469290786808754,
      "grad_norm": 1.2582565546035767,
      "learning_rate": 0.0004453512220133311,
      "loss": 0.3776,
      "step": 1280
    },
    {
      "epoch": 0.005473563670235948,
      "grad_norm": 1.5530527830123901,
      "learning_rate": 0.000445308494274483,
      "loss": 0.4335,
      "step": 1281
    },
    {
      "epoch": 0.005477836553663143,
      "grad_norm": 1.4890841245651245,
      "learning_rate": 0.00044526576653563496,
      "loss": 0.3986,
      "step": 1282
    },
    {
      "epoch": 0.005482109437090338,
      "grad_norm": 3.0359208583831787,
      "learning_rate": 0.00044522303879678687,
      "loss": 0.9828,
      "step": 1283
    },
    {
      "epoch": 0.0054863823205175315,
      "grad_norm": 0.8491348028182983,
      "learning_rate": 0.00044518031105793884,
      "loss": 0.2614,
      "step": 1284
    },
    {
      "epoch": 0.005490655203944726,
      "grad_norm": 2.9675848484039307,
      "learning_rate": 0.00044513758331909075,
      "loss": 1.093,
      "step": 1285
    },
    {
      "epoch": 0.00549492808737192,
      "grad_norm": 2.599419593811035,
      "learning_rate": 0.0004450948555802427,
      "loss": 0.938,
      "step": 1286
    },
    {
      "epoch": 0.005499200970799115,
      "grad_norm": 2.9866580963134766,
      "learning_rate": 0.0004450521278413946,
      "loss": 1.1602,
      "step": 1287
    },
    {
      "epoch": 0.005503473854226309,
      "grad_norm": 6.043055057525635,
      "learning_rate": 0.0004450094001025466,
      "loss": 1.7347,
      "step": 1288
    },
    {
      "epoch": 0.005507746737653503,
      "grad_norm": 0.6887959241867065,
      "learning_rate": 0.00044496667236369855,
      "loss": 0.2035,
      "step": 1289
    },
    {
      "epoch": 0.005512019621080698,
      "grad_norm": 2.6386849880218506,
      "learning_rate": 0.00044492394462485046,
      "loss": 0.9742,
      "step": 1290
    },
    {
      "epoch": 0.005516292504507892,
      "grad_norm": 1.6891562938690186,
      "learning_rate": 0.00044488121688600243,
      "loss": 0.7257,
      "step": 1291
    },
    {
      "epoch": 0.005520565387935087,
      "grad_norm": 1.0555411577224731,
      "learning_rate": 0.00044483848914715434,
      "loss": 0.1878,
      "step": 1292
    },
    {
      "epoch": 0.0055248382713622805,
      "grad_norm": 0.6626130938529968,
      "learning_rate": 0.0004447957614083063,
      "loss": 0.2323,
      "step": 1293
    },
    {
      "epoch": 0.005529111154789475,
      "grad_norm": 0.6816650032997131,
      "learning_rate": 0.0004447530336694582,
      "loss": 0.1733,
      "step": 1294
    },
    {
      "epoch": 0.005533384038216669,
      "grad_norm": 3.0213429927825928,
      "learning_rate": 0.0004447103059306102,
      "loss": 1.0665,
      "step": 1295
    },
    {
      "epoch": 0.005537656921643864,
      "grad_norm": 2.99578857421875,
      "learning_rate": 0.00044466757819176214,
      "loss": 1.4437,
      "step": 1296
    },
    {
      "epoch": 0.005541929805071058,
      "grad_norm": 3.4462673664093018,
      "learning_rate": 0.00044462485045291406,
      "loss": 1.1819,
      "step": 1297
    },
    {
      "epoch": 0.005546202688498252,
      "grad_norm": 0.5439305901527405,
      "learning_rate": 0.000444582122714066,
      "loss": 0.1388,
      "step": 1298
    },
    {
      "epoch": 0.005550475571925447,
      "grad_norm": 3.3196232318878174,
      "learning_rate": 0.00044453939497521793,
      "loss": 1.1365,
      "step": 1299
    },
    {
      "epoch": 0.005554748455352641,
      "grad_norm": 2.9397146701812744,
      "learning_rate": 0.00044449666723636984,
      "loss": 1.2051,
      "step": 1300
    },
    {
      "epoch": 0.005559021338779836,
      "grad_norm": 0.9902400970458984,
      "learning_rate": 0.00044445393949752175,
      "loss": 0.162,
      "step": 1301
    },
    {
      "epoch": 0.0055632942222070296,
      "grad_norm": 5.485885143280029,
      "learning_rate": 0.0004444112117586737,
      "loss": 1.8735,
      "step": 1302
    },
    {
      "epoch": 0.005567567105634224,
      "grad_norm": 3.698171377182007,
      "learning_rate": 0.0004443684840198257,
      "loss": 1.1155,
      "step": 1303
    },
    {
      "epoch": 0.005571839989061418,
      "grad_norm": 3.9577765464782715,
      "learning_rate": 0.0004443257562809776,
      "loss": 1.681,
      "step": 1304
    },
    {
      "epoch": 0.005576112872488613,
      "grad_norm": 0.4461858868598938,
      "learning_rate": 0.00044428302854212956,
      "loss": 0.1044,
      "step": 1305
    },
    {
      "epoch": 0.005580385755915807,
      "grad_norm": 0.9456823468208313,
      "learning_rate": 0.00044424030080328147,
      "loss": 0.314,
      "step": 1306
    },
    {
      "epoch": 0.0055846586393430014,
      "grad_norm": 3.5627963542938232,
      "learning_rate": 0.00044419757306443343,
      "loss": 1.2513,
      "step": 1307
    },
    {
      "epoch": 0.005588931522770196,
      "grad_norm": 4.095888137817383,
      "learning_rate": 0.00044415484532558534,
      "loss": 1.3296,
      "step": 1308
    },
    {
      "epoch": 0.00559320440619739,
      "grad_norm": 3.9602713584899902,
      "learning_rate": 0.0004441121175867373,
      "loss": 1.2093,
      "step": 1309
    },
    {
      "epoch": 0.005597477289624585,
      "grad_norm": 0.925613284111023,
      "learning_rate": 0.0004440693898478893,
      "loss": 0.3311,
      "step": 1310
    },
    {
      "epoch": 0.005601750173051779,
      "grad_norm": 0.7655343413352966,
      "learning_rate": 0.0004440266621090412,
      "loss": 0.2617,
      "step": 1311
    },
    {
      "epoch": 0.005606023056478973,
      "grad_norm": 3.2486023902893066,
      "learning_rate": 0.00044398393437019315,
      "loss": 0.9425,
      "step": 1312
    },
    {
      "epoch": 0.005610295939906167,
      "grad_norm": 0.552647590637207,
      "learning_rate": 0.00044394120663134506,
      "loss": 0.1925,
      "step": 1313
    },
    {
      "epoch": 0.005614568823333362,
      "grad_norm": 1.5677686929702759,
      "learning_rate": 0.000443898478892497,
      "loss": 0.5875,
      "step": 1314
    },
    {
      "epoch": 0.005618841706760556,
      "grad_norm": 2.4318740367889404,
      "learning_rate": 0.00044385575115364894,
      "loss": 1.3055,
      "step": 1315
    },
    {
      "epoch": 0.0056231145901877505,
      "grad_norm": 2.7037527561187744,
      "learning_rate": 0.0004438130234148009,
      "loss": 0.9382,
      "step": 1316
    },
    {
      "epoch": 0.005627387473614945,
      "grad_norm": 3.4800379276275635,
      "learning_rate": 0.00044377029567595287,
      "loss": 1.4595,
      "step": 1317
    },
    {
      "epoch": 0.005631660357042139,
      "grad_norm": 4.260764122009277,
      "learning_rate": 0.0004437275679371048,
      "loss": 3.6913,
      "step": 1318
    },
    {
      "epoch": 0.005635933240469334,
      "grad_norm": 32.70828628540039,
      "learning_rate": 0.00044368484019825674,
      "loss": 6.4403,
      "step": 1319
    },
    {
      "epoch": 0.005640206123896528,
      "grad_norm": 2.0055036544799805,
      "learning_rate": 0.00044364211245940865,
      "loss": 1.1809,
      "step": 1320
    },
    {
      "epoch": 0.005644479007323722,
      "grad_norm": 1.4786968231201172,
      "learning_rate": 0.0004435993847205606,
      "loss": 0.5729,
      "step": 1321
    },
    {
      "epoch": 0.005648751890750916,
      "grad_norm": 2.7331106662750244,
      "learning_rate": 0.00044355665698171253,
      "loss": 0.8072,
      "step": 1322
    },
    {
      "epoch": 0.005653024774178111,
      "grad_norm": 2.20361590385437,
      "learning_rate": 0.0004435139292428645,
      "loss": 0.6392,
      "step": 1323
    },
    {
      "epoch": 0.005657297657605305,
      "grad_norm": 3.667071580886841,
      "learning_rate": 0.0004434712015040164,
      "loss": 1.3437,
      "step": 1324
    },
    {
      "epoch": 0.0056615705410324995,
      "grad_norm": 0.3422843813896179,
      "learning_rate": 0.00044342847376516837,
      "loss": 0.068,
      "step": 1325
    },
    {
      "epoch": 0.005665843424459694,
      "grad_norm": 1.527109146118164,
      "learning_rate": 0.00044338574602632033,
      "loss": 0.6321,
      "step": 1326
    },
    {
      "epoch": 0.005670116307886888,
      "grad_norm": 1.5034353733062744,
      "learning_rate": 0.00044334301828747224,
      "loss": 0.6019,
      "step": 1327
    },
    {
      "epoch": 0.005674389191314083,
      "grad_norm": 1.3808714151382446,
      "learning_rate": 0.0004433002905486242,
      "loss": 0.3643,
      "step": 1328
    },
    {
      "epoch": 0.005678662074741277,
      "grad_norm": 1.677017331123352,
      "learning_rate": 0.0004432575628097761,
      "loss": 0.9251,
      "step": 1329
    },
    {
      "epoch": 0.005682934958168471,
      "grad_norm": 1.9665664434432983,
      "learning_rate": 0.0004432148350709281,
      "loss": 0.5211,
      "step": 1330
    },
    {
      "epoch": 0.005687207841595665,
      "grad_norm": 2.871609687805176,
      "learning_rate": 0.00044317210733208,
      "loss": 1.3843,
      "step": 1331
    },
    {
      "epoch": 0.00569148072502286,
      "grad_norm": 1.4695851802825928,
      "learning_rate": 0.00044312937959323196,
      "loss": 0.4241,
      "step": 1332
    },
    {
      "epoch": 0.005695753608450055,
      "grad_norm": 1.6476112604141235,
      "learning_rate": 0.00044308665185438387,
      "loss": 0.7767,
      "step": 1333
    },
    {
      "epoch": 0.0057000264918772485,
      "grad_norm": 2.0380921363830566,
      "learning_rate": 0.0004430439241155358,
      "loss": 0.8265,
      "step": 1334
    },
    {
      "epoch": 0.005704299375304443,
      "grad_norm": 0.7579970359802246,
      "learning_rate": 0.00044300119637668775,
      "loss": 0.1198,
      "step": 1335
    },
    {
      "epoch": 0.005708572258731637,
      "grad_norm": 2.608231782913208,
      "learning_rate": 0.00044295846863783966,
      "loss": 0.851,
      "step": 1336
    },
    {
      "epoch": 0.005712845142158832,
      "grad_norm": 2.0213441848754883,
      "learning_rate": 0.0004429157408989916,
      "loss": 0.8106,
      "step": 1337
    },
    {
      "epoch": 0.005717118025586026,
      "grad_norm": 2.248987913131714,
      "learning_rate": 0.00044287301316014353,
      "loss": 0.9345,
      "step": 1338
    },
    {
      "epoch": 0.00572139090901322,
      "grad_norm": 1.6810412406921387,
      "learning_rate": 0.0004428302854212955,
      "loss": 0.6578,
      "step": 1339
    },
    {
      "epoch": 0.005725663792440414,
      "grad_norm": 2.2880496978759766,
      "learning_rate": 0.00044278755768244746,
      "loss": 0.5101,
      "step": 1340
    },
    {
      "epoch": 0.005729936675867609,
      "grad_norm": 2.819523334503174,
      "learning_rate": 0.0004427448299435994,
      "loss": 0.977,
      "step": 1341
    },
    {
      "epoch": 0.005734209559294804,
      "grad_norm": 5.520930767059326,
      "learning_rate": 0.00044270210220475134,
      "loss": 1.9519,
      "step": 1342
    },
    {
      "epoch": 0.0057384824427219976,
      "grad_norm": 1.9780161380767822,
      "learning_rate": 0.00044265937446590325,
      "loss": 0.7473,
      "step": 1343
    },
    {
      "epoch": 0.005742755326149192,
      "grad_norm": 2.8089945316314697,
      "learning_rate": 0.0004426166467270552,
      "loss": 0.7784,
      "step": 1344
    },
    {
      "epoch": 0.005747028209576386,
      "grad_norm": 2.5854580402374268,
      "learning_rate": 0.0004425739189882071,
      "loss": 0.9551,
      "step": 1345
    },
    {
      "epoch": 0.005751301093003581,
      "grad_norm": 2.015665292739868,
      "learning_rate": 0.0004425311912493591,
      "loss": 0.6443,
      "step": 1346
    },
    {
      "epoch": 0.005755573976430775,
      "grad_norm": 2.491027593612671,
      "learning_rate": 0.00044248846351051106,
      "loss": 0.8767,
      "step": 1347
    },
    {
      "epoch": 0.0057598468598579694,
      "grad_norm": 2.0020947456359863,
      "learning_rate": 0.00044244573577166297,
      "loss": 0.6443,
      "step": 1348
    },
    {
      "epoch": 0.005764119743285163,
      "grad_norm": 3.88808012008667,
      "learning_rate": 0.00044240300803281493,
      "loss": 1.0774,
      "step": 1349
    },
    {
      "epoch": 0.005768392626712358,
      "grad_norm": 1.9205659627914429,
      "learning_rate": 0.00044236028029396684,
      "loss": 0.6833,
      "step": 1350
    },
    {
      "epoch": 0.005772665510139553,
      "grad_norm": 2.2541604042053223,
      "learning_rate": 0.0004423175525551188,
      "loss": 0.6561,
      "step": 1351
    },
    {
      "epoch": 0.005776938393566747,
      "grad_norm": 1.8270293474197388,
      "learning_rate": 0.0004422748248162707,
      "loss": 0.5934,
      "step": 1352
    },
    {
      "epoch": 0.005781211276993941,
      "grad_norm": 1.4960103034973145,
      "learning_rate": 0.0004422320970774227,
      "loss": 0.3672,
      "step": 1353
    },
    {
      "epoch": 0.005785484160421135,
      "grad_norm": 1.6696337461471558,
      "learning_rate": 0.0004421893693385746,
      "loss": 0.533,
      "step": 1354
    },
    {
      "epoch": 0.00578975704384833,
      "grad_norm": 1.8703736066818237,
      "learning_rate": 0.00044214664159972656,
      "loss": 0.6395,
      "step": 1355
    },
    {
      "epoch": 0.005794029927275524,
      "grad_norm": 3.6815576553344727,
      "learning_rate": 0.0004421039138608785,
      "loss": 0.8866,
      "step": 1356
    },
    {
      "epoch": 0.0057983028107027185,
      "grad_norm": 5.187755584716797,
      "learning_rate": 0.00044206118612203043,
      "loss": 2.5193,
      "step": 1357
    },
    {
      "epoch": 0.005802575694129912,
      "grad_norm": 2.4012959003448486,
      "learning_rate": 0.0004420184583831824,
      "loss": 0.7309,
      "step": 1358
    },
    {
      "epoch": 0.005806848577557107,
      "grad_norm": 1.40771484375,
      "learning_rate": 0.0004419757306443343,
      "loss": 0.4141,
      "step": 1359
    },
    {
      "epoch": 0.005811121460984302,
      "grad_norm": 1.8303486108779907,
      "learning_rate": 0.0004419330029054863,
      "loss": 0.8698,
      "step": 1360
    },
    {
      "epoch": 0.005815394344411496,
      "grad_norm": 3.291226625442505,
      "learning_rate": 0.0004418902751666382,
      "loss": 0.7667,
      "step": 1361
    },
    {
      "epoch": 0.00581966722783869,
      "grad_norm": 1.798979640007019,
      "learning_rate": 0.00044184754742779015,
      "loss": 0.6211,
      "step": 1362
    },
    {
      "epoch": 0.005823940111265884,
      "grad_norm": 1.0668689012527466,
      "learning_rate": 0.0004418048196889421,
      "loss": 0.1611,
      "step": 1363
    },
    {
      "epoch": 0.005828212994693079,
      "grad_norm": 1.7547744512557983,
      "learning_rate": 0.000441762091950094,
      "loss": 0.5793,
      "step": 1364
    },
    {
      "epoch": 0.005832485878120273,
      "grad_norm": 2.129481792449951,
      "learning_rate": 0.000441719364211246,
      "loss": 0.6123,
      "step": 1365
    },
    {
      "epoch": 0.0058367587615474675,
      "grad_norm": 3.4486448764801025,
      "learning_rate": 0.00044167663647239785,
      "loss": 0.7864,
      "step": 1366
    },
    {
      "epoch": 0.005841031644974662,
      "grad_norm": 4.829709529876709,
      "learning_rate": 0.0004416339087335498,
      "loss": 1.2081,
      "step": 1367
    },
    {
      "epoch": 0.005845304528401856,
      "grad_norm": 4.400956153869629,
      "learning_rate": 0.0004415911809947017,
      "loss": 1.6216,
      "step": 1368
    },
    {
      "epoch": 0.005849577411829051,
      "grad_norm": 1.7272971868515015,
      "learning_rate": 0.0004415484532558537,
      "loss": 0.6444,
      "step": 1369
    },
    {
      "epoch": 0.005853850295256245,
      "grad_norm": 1.522713303565979,
      "learning_rate": 0.00044150572551700565,
      "loss": 0.4966,
      "step": 1370
    },
    {
      "epoch": 0.005858123178683439,
      "grad_norm": 1.580744743347168,
      "learning_rate": 0.00044146299777815756,
      "loss": 0.5208,
      "step": 1371
    },
    {
      "epoch": 0.005862396062110633,
      "grad_norm": 0.97267085313797,
      "learning_rate": 0.00044142027003930953,
      "loss": 0.1361,
      "step": 1372
    },
    {
      "epoch": 0.005866668945537828,
      "grad_norm": 2.6217355728149414,
      "learning_rate": 0.00044137754230046144,
      "loss": 0.693,
      "step": 1373
    },
    {
      "epoch": 0.005870941828965022,
      "grad_norm": 3.312239408493042,
      "learning_rate": 0.0004413348145616134,
      "loss": 1.1515,
      "step": 1374
    },
    {
      "epoch": 0.0058752147123922165,
      "grad_norm": 1.8518850803375244,
      "learning_rate": 0.0004412920868227653,
      "loss": 0.4846,
      "step": 1375
    },
    {
      "epoch": 0.005879487595819411,
      "grad_norm": 1.7046293020248413,
      "learning_rate": 0.0004412493590839173,
      "loss": 0.4944,
      "step": 1376
    },
    {
      "epoch": 0.005883760479246605,
      "grad_norm": 2.3615312576293945,
      "learning_rate": 0.00044120663134506925,
      "loss": 0.8903,
      "step": 1377
    },
    {
      "epoch": 0.0058880333626738,
      "grad_norm": 2.8035311698913574,
      "learning_rate": 0.00044116390360622116,
      "loss": 0.7833,
      "step": 1378
    },
    {
      "epoch": 0.005892306246100994,
      "grad_norm": 2.970860004425049,
      "learning_rate": 0.0004411211758673731,
      "loss": 1.0296,
      "step": 1379
    },
    {
      "epoch": 0.005896579129528188,
      "grad_norm": 2.526984453201294,
      "learning_rate": 0.00044107844812852503,
      "loss": 0.8861,
      "step": 1380
    },
    {
      "epoch": 0.005900852012955382,
      "grad_norm": 2.2024247646331787,
      "learning_rate": 0.000441035720389677,
      "loss": 0.7388,
      "step": 1381
    },
    {
      "epoch": 0.005905124896382577,
      "grad_norm": 1.4929766654968262,
      "learning_rate": 0.0004409929926508289,
      "loss": 0.4473,
      "step": 1382
    },
    {
      "epoch": 0.005909397779809771,
      "grad_norm": 1.8152003288269043,
      "learning_rate": 0.00044095026491198087,
      "loss": 0.5205,
      "step": 1383
    },
    {
      "epoch": 0.0059136706632369656,
      "grad_norm": 3.401543617248535,
      "learning_rate": 0.00044090753717313284,
      "loss": 0.8078,
      "step": 1384
    },
    {
      "epoch": 0.00591794354666416,
      "grad_norm": 4.962221622467041,
      "learning_rate": 0.00044086480943428475,
      "loss": 2.1349,
      "step": 1385
    },
    {
      "epoch": 0.005922216430091354,
      "grad_norm": 2.897507429122925,
      "learning_rate": 0.0004408220816954367,
      "loss": 1.5695,
      "step": 1386
    },
    {
      "epoch": 0.005926489313518549,
      "grad_norm": 1.2270832061767578,
      "learning_rate": 0.0004407793539565886,
      "loss": 0.1925,
      "step": 1387
    },
    {
      "epoch": 0.005930762196945743,
      "grad_norm": 0.8738479018211365,
      "learning_rate": 0.0004407366262177406,
      "loss": 0.1406,
      "step": 1388
    },
    {
      "epoch": 0.0059350350803729374,
      "grad_norm": 1.3347350358963013,
      "learning_rate": 0.0004406938984788925,
      "loss": 0.3007,
      "step": 1389
    },
    {
      "epoch": 0.005939307963800131,
      "grad_norm": 3.734663248062134,
      "learning_rate": 0.00044065117074004446,
      "loss": 0.9788,
      "step": 1390
    },
    {
      "epoch": 0.005943580847227326,
      "grad_norm": 4.606037139892578,
      "learning_rate": 0.0004406084430011964,
      "loss": 0.8193,
      "step": 1391
    },
    {
      "epoch": 0.00594785373065452,
      "grad_norm": 2.702256917953491,
      "learning_rate": 0.00044056571526234834,
      "loss": 0.667,
      "step": 1392
    },
    {
      "epoch": 0.005952126614081715,
      "grad_norm": 1.2466728687286377,
      "learning_rate": 0.0004405229875235003,
      "loss": 0.2913,
      "step": 1393
    },
    {
      "epoch": 0.005956399497508909,
      "grad_norm": 3.235935926437378,
      "learning_rate": 0.0004404802597846522,
      "loss": 1.1012,
      "step": 1394
    },
    {
      "epoch": 0.005960672380936103,
      "grad_norm": 2.0694937705993652,
      "learning_rate": 0.0004404375320458042,
      "loss": 0.5605,
      "step": 1395
    },
    {
      "epoch": 0.005964945264363298,
      "grad_norm": 2.176781415939331,
      "learning_rate": 0.0004403948043069561,
      "loss": 1.1809,
      "step": 1396
    },
    {
      "epoch": 0.005969218147790492,
      "grad_norm": 0.837253212928772,
      "learning_rate": 0.00044035207656810806,
      "loss": 0.14,
      "step": 1397
    },
    {
      "epoch": 0.0059734910312176865,
      "grad_norm": 1.654181957244873,
      "learning_rate": 0.00044030934882925997,
      "loss": 0.4774,
      "step": 1398
    },
    {
      "epoch": 0.00597776391464488,
      "grad_norm": 2.6533265113830566,
      "learning_rate": 0.0004402666210904119,
      "loss": 0.6821,
      "step": 1399
    },
    {
      "epoch": 0.005982036798072075,
      "grad_norm": 1.0498945713043213,
      "learning_rate": 0.00044022389335156384,
      "loss": 0.2414,
      "step": 1400
    },
    {
      "epoch": 0.00598630968149927,
      "grad_norm": 0.9611456990242004,
      "learning_rate": 0.00044018116561271575,
      "loss": 0.1625,
      "step": 1401
    },
    {
      "epoch": 0.005990582564926464,
      "grad_norm": 2.5753774642944336,
      "learning_rate": 0.0004401384378738677,
      "loss": 1.3019,
      "step": 1402
    },
    {
      "epoch": 0.005994855448353658,
      "grad_norm": 0.9291893839836121,
      "learning_rate": 0.00044009571013501963,
      "loss": 0.1547,
      "step": 1403
    },
    {
      "epoch": 0.005999128331780852,
      "grad_norm": 5.26912784576416,
      "learning_rate": 0.0004400529823961716,
      "loss": 1.5957,
      "step": 1404
    },
    {
      "epoch": 0.006003401215208047,
      "grad_norm": 2.997464895248413,
      "learning_rate": 0.0004400102546573235,
      "loss": 0.793,
      "step": 1405
    },
    {
      "epoch": 0.006007674098635241,
      "grad_norm": 2.723775863647461,
      "learning_rate": 0.00043996752691847547,
      "loss": 0.6677,
      "step": 1406
    },
    {
      "epoch": 0.0060119469820624355,
      "grad_norm": 1.6303144693374634,
      "learning_rate": 0.00043992479917962743,
      "loss": 0.5445,
      "step": 1407
    },
    {
      "epoch": 0.006016219865489629,
      "grad_norm": 3.2453675270080566,
      "learning_rate": 0.00043988207144077935,
      "loss": 0.7357,
      "step": 1408
    },
    {
      "epoch": 0.006020492748916824,
      "grad_norm": 1.5379719734191895,
      "learning_rate": 0.0004398393437019313,
      "loss": 0.5333,
      "step": 1409
    },
    {
      "epoch": 0.006024765632344019,
      "grad_norm": 4.275866985321045,
      "learning_rate": 0.0004397966159630832,
      "loss": 1.7648,
      "step": 1410
    },
    {
      "epoch": 0.006029038515771213,
      "grad_norm": 3.3808865547180176,
      "learning_rate": 0.0004397538882242352,
      "loss": 1.1273,
      "step": 1411
    },
    {
      "epoch": 0.006033311399198407,
      "grad_norm": 1.7378731966018677,
      "learning_rate": 0.0004397111604853871,
      "loss": 0.4805,
      "step": 1412
    },
    {
      "epoch": 0.006037584282625601,
      "grad_norm": 1.425855278968811,
      "learning_rate": 0.00043966843274653906,
      "loss": 0.6152,
      "step": 1413
    },
    {
      "epoch": 0.006041857166052796,
      "grad_norm": 1.718808650970459,
      "learning_rate": 0.000439625705007691,
      "loss": 0.9776,
      "step": 1414
    },
    {
      "epoch": 0.00604613004947999,
      "grad_norm": 1.9229835271835327,
      "learning_rate": 0.00043958297726884294,
      "loss": 0.4115,
      "step": 1415
    },
    {
      "epoch": 0.0060504029329071845,
      "grad_norm": 1.5215480327606201,
      "learning_rate": 0.0004395402495299949,
      "loss": 0.5638,
      "step": 1416
    },
    {
      "epoch": 0.006054675816334378,
      "grad_norm": 2.119717597961426,
      "learning_rate": 0.0004394975217911468,
      "loss": 0.8955,
      "step": 1417
    },
    {
      "epoch": 0.006058948699761573,
      "grad_norm": 1.1963988542556763,
      "learning_rate": 0.0004394547940522988,
      "loss": 0.4695,
      "step": 1418
    },
    {
      "epoch": 0.006063221583188768,
      "grad_norm": 2.793245553970337,
      "learning_rate": 0.0004394120663134507,
      "loss": 1.386,
      "step": 1419
    },
    {
      "epoch": 0.006067494466615962,
      "grad_norm": 1.1790391206741333,
      "learning_rate": 0.00043936933857460265,
      "loss": 0.4464,
      "step": 1420
    },
    {
      "epoch": 0.006071767350043156,
      "grad_norm": 4.500704288482666,
      "learning_rate": 0.0004393266108357546,
      "loss": 0.8656,
      "step": 1421
    },
    {
      "epoch": 0.00607604023347035,
      "grad_norm": 1.5510376691818237,
      "learning_rate": 0.00043928388309690653,
      "loss": 0.5581,
      "step": 1422
    },
    {
      "epoch": 0.006080313116897545,
      "grad_norm": 4.393588066101074,
      "learning_rate": 0.0004392411553580585,
      "loss": 0.7394,
      "step": 1423
    },
    {
      "epoch": 0.006084586000324739,
      "grad_norm": 2.0227344036102295,
      "learning_rate": 0.0004391984276192104,
      "loss": 0.7888,
      "step": 1424
    },
    {
      "epoch": 0.0060888588837519336,
      "grad_norm": 3.3210790157318115,
      "learning_rate": 0.00043915569988036237,
      "loss": 1.2089,
      "step": 1425
    },
    {
      "epoch": 0.006093131767179127,
      "grad_norm": 1.8086084127426147,
      "learning_rate": 0.0004391129721415143,
      "loss": 0.367,
      "step": 1426
    },
    {
      "epoch": 0.006097404650606322,
      "grad_norm": 1.589837670326233,
      "learning_rate": 0.00043907024440266625,
      "loss": 0.4554,
      "step": 1427
    },
    {
      "epoch": 0.006101677534033517,
      "grad_norm": 1.0767319202423096,
      "learning_rate": 0.00043902751666381816,
      "loss": 0.3027,
      "step": 1428
    },
    {
      "epoch": 0.006105950417460711,
      "grad_norm": 1.387076735496521,
      "learning_rate": 0.0004389847889249701,
      "loss": 0.2804,
      "step": 1429
    },
    {
      "epoch": 0.006110223300887905,
      "grad_norm": 2.0775492191314697,
      "learning_rate": 0.0004389420611861221,
      "loss": 0.5191,
      "step": 1430
    },
    {
      "epoch": 0.006114496184315099,
      "grad_norm": 1.1283687353134155,
      "learning_rate": 0.00043889933344727394,
      "loss": 0.2825,
      "step": 1431
    },
    {
      "epoch": 0.006118769067742294,
      "grad_norm": 3.6957099437713623,
      "learning_rate": 0.0004388566057084259,
      "loss": 1.8168,
      "step": 1432
    },
    {
      "epoch": 0.006123041951169488,
      "grad_norm": 2.175692319869995,
      "learning_rate": 0.0004388138779695778,
      "loss": 0.5106,
      "step": 1433
    },
    {
      "epoch": 0.006127314834596683,
      "grad_norm": 3.4707963466644287,
      "learning_rate": 0.0004387711502307298,
      "loss": 0.82,
      "step": 1434
    },
    {
      "epoch": 0.0061315877180238764,
      "grad_norm": 3.5630366802215576,
      "learning_rate": 0.0004387284224918817,
      "loss": 1.6463,
      "step": 1435
    },
    {
      "epoch": 0.006135860601451071,
      "grad_norm": 2.1239285469055176,
      "learning_rate": 0.00043868569475303366,
      "loss": 1.0147,
      "step": 1436
    },
    {
      "epoch": 0.006140133484878266,
      "grad_norm": 4.494523525238037,
      "learning_rate": 0.0004386429670141856,
      "loss": 1.3217,
      "step": 1437
    },
    {
      "epoch": 0.00614440636830546,
      "grad_norm": 0.8162967562675476,
      "learning_rate": 0.00043860023927533754,
      "loss": 0.163,
      "step": 1438
    },
    {
      "epoch": 0.0061486792517326545,
      "grad_norm": 2.8523168563842773,
      "learning_rate": 0.0004385575115364895,
      "loss": 1.0099,
      "step": 1439
    },
    {
      "epoch": 0.006152952135159848,
      "grad_norm": 3.18994402885437,
      "learning_rate": 0.0004385147837976414,
      "loss": 1.6125,
      "step": 1440
    },
    {
      "epoch": 0.006157225018587043,
      "grad_norm": 2.749884605407715,
      "learning_rate": 0.0004384720560587934,
      "loss": 0.8885,
      "step": 1441
    },
    {
      "epoch": 0.006161497902014237,
      "grad_norm": 2.1239068508148193,
      "learning_rate": 0.0004384293283199453,
      "loss": 0.9635,
      "step": 1442
    },
    {
      "epoch": 0.006165770785441432,
      "grad_norm": 1.3870054483413696,
      "learning_rate": 0.00043838660058109725,
      "loss": 0.357,
      "step": 1443
    },
    {
      "epoch": 0.006170043668868626,
      "grad_norm": 1.0972249507904053,
      "learning_rate": 0.0004383438728422492,
      "loss": 0.2528,
      "step": 1444
    },
    {
      "epoch": 0.00617431655229582,
      "grad_norm": 2.177633762359619,
      "learning_rate": 0.00043830114510340113,
      "loss": 0.8199,
      "step": 1445
    },
    {
      "epoch": 0.006178589435723015,
      "grad_norm": 1.3491770029067993,
      "learning_rate": 0.0004382584173645531,
      "loss": 0.3291,
      "step": 1446
    },
    {
      "epoch": 0.006182862319150209,
      "grad_norm": 2.7105419635772705,
      "learning_rate": 0.000438215689625705,
      "loss": 0.7461,
      "step": 1447
    },
    {
      "epoch": 0.0061871352025774035,
      "grad_norm": 0.7138968110084534,
      "learning_rate": 0.00043817296188685697,
      "loss": 0.1484,
      "step": 1448
    },
    {
      "epoch": 0.006191408086004597,
      "grad_norm": 0.9214909076690674,
      "learning_rate": 0.0004381302341480089,
      "loss": 0.1983,
      "step": 1449
    },
    {
      "epoch": 0.006195680969431792,
      "grad_norm": 2.4260828495025635,
      "learning_rate": 0.00043808750640916084,
      "loss": 0.8046,
      "step": 1450
    },
    {
      "epoch": 0.006199953852858986,
      "grad_norm": 3.8591761589050293,
      "learning_rate": 0.0004380447786703128,
      "loss": 1.7168,
      "step": 1451
    },
    {
      "epoch": 0.006204226736286181,
      "grad_norm": 1.2137974500656128,
      "learning_rate": 0.0004380020509314647,
      "loss": 0.4164,
      "step": 1452
    },
    {
      "epoch": 0.006208499619713375,
      "grad_norm": 3.955214262008667,
      "learning_rate": 0.0004379593231926167,
      "loss": 1.8258,
      "step": 1453
    },
    {
      "epoch": 0.006212772503140569,
      "grad_norm": 2.60359787940979,
      "learning_rate": 0.0004379165954537686,
      "loss": 0.5787,
      "step": 1454
    },
    {
      "epoch": 0.006217045386567764,
      "grad_norm": 1.9405267238616943,
      "learning_rate": 0.00043787386771492056,
      "loss": 0.7721,
      "step": 1455
    },
    {
      "epoch": 0.006221318269994958,
      "grad_norm": 0.6161345839500427,
      "learning_rate": 0.00043783113997607247,
      "loss": 0.1152,
      "step": 1456
    },
    {
      "epoch": 0.0062255911534221525,
      "grad_norm": 2.7478318214416504,
      "learning_rate": 0.00043778841223722444,
      "loss": 0.9919,
      "step": 1457
    },
    {
      "epoch": 0.006229864036849346,
      "grad_norm": 0.6744232773780823,
      "learning_rate": 0.00043774568449837635,
      "loss": 0.1337,
      "step": 1458
    },
    {
      "epoch": 0.006234136920276541,
      "grad_norm": 2.378220796585083,
      "learning_rate": 0.0004377029567595283,
      "loss": 1.3683,
      "step": 1459
    },
    {
      "epoch": 0.006238409803703735,
      "grad_norm": 2.144331216812134,
      "learning_rate": 0.0004376602290206803,
      "loss": 0.5656,
      "step": 1460
    },
    {
      "epoch": 0.00624268268713093,
      "grad_norm": 3.1280431747436523,
      "learning_rate": 0.0004376175012818322,
      "loss": 1.0149,
      "step": 1461
    },
    {
      "epoch": 0.006246955570558124,
      "grad_norm": 1.138665795326233,
      "learning_rate": 0.00043757477354298415,
      "loss": 0.4388,
      "step": 1462
    },
    {
      "epoch": 0.006251228453985318,
      "grad_norm": 2.0130422115325928,
      "learning_rate": 0.00043753204580413606,
      "loss": 0.706,
      "step": 1463
    },
    {
      "epoch": 0.006255501337412513,
      "grad_norm": 1.061835765838623,
      "learning_rate": 0.000437489318065288,
      "loss": 0.4081,
      "step": 1464
    },
    {
      "epoch": 0.006259774220839707,
      "grad_norm": 1.6852428913116455,
      "learning_rate": 0.0004374465903264399,
      "loss": 0.7027,
      "step": 1465
    },
    {
      "epoch": 0.0062640471042669016,
      "grad_norm": 1.6682296991348267,
      "learning_rate": 0.00043740386258759185,
      "loss": 0.5703,
      "step": 1466
    },
    {
      "epoch": 0.006268319987694095,
      "grad_norm": 1.0281649827957153,
      "learning_rate": 0.0004373611348487438,
      "loss": 0.2075,
      "step": 1467
    },
    {
      "epoch": 0.00627259287112129,
      "grad_norm": 0.9808552861213684,
      "learning_rate": 0.0004373184071098957,
      "loss": 0.3676,
      "step": 1468
    },
    {
      "epoch": 0.006276865754548484,
      "grad_norm": 1.8412706851959229,
      "learning_rate": 0.0004372756793710477,
      "loss": 1.018,
      "step": 1469
    },
    {
      "epoch": 0.006281138637975679,
      "grad_norm": 2.314939022064209,
      "learning_rate": 0.0004372329516321996,
      "loss": 1.359,
      "step": 1470
    },
    {
      "epoch": 0.006285411521402873,
      "grad_norm": 0.8623435497283936,
      "learning_rate": 0.00043719022389335157,
      "loss": 0.1993,
      "step": 1471
    },
    {
      "epoch": 0.006289684404830067,
      "grad_norm": 1.5526996850967407,
      "learning_rate": 0.0004371474961545035,
      "loss": 0.4435,
      "step": 1472
    },
    {
      "epoch": 0.006293957288257262,
      "grad_norm": 2.315068244934082,
      "learning_rate": 0.00043710476841565544,
      "loss": 1.3241,
      "step": 1473
    },
    {
      "epoch": 0.006298230171684456,
      "grad_norm": 4.087622165679932,
      "learning_rate": 0.0004370620406768074,
      "loss": 1.8869,
      "step": 1474
    },
    {
      "epoch": 0.006302503055111651,
      "grad_norm": 1.76982581615448,
      "learning_rate": 0.0004370193129379593,
      "loss": 0.5142,
      "step": 1475
    },
    {
      "epoch": 0.0063067759385388444,
      "grad_norm": 2.1987218856811523,
      "learning_rate": 0.0004369765851991113,
      "loss": 0.6333,
      "step": 1476
    },
    {
      "epoch": 0.006311048821966039,
      "grad_norm": 2.2543156147003174,
      "learning_rate": 0.0004369338574602632,
      "loss": 0.7008,
      "step": 1477
    },
    {
      "epoch": 0.006315321705393234,
      "grad_norm": 2.2681713104248047,
      "learning_rate": 0.00043689112972141516,
      "loss": 0.6333,
      "step": 1478
    },
    {
      "epoch": 0.006319594588820428,
      "grad_norm": 1.4804136753082275,
      "learning_rate": 0.00043684840198256707,
      "loss": 0.4225,
      "step": 1479
    },
    {
      "epoch": 0.0063238674722476225,
      "grad_norm": 4.534912586212158,
      "learning_rate": 0.00043680567424371903,
      "loss": 1.1437,
      "step": 1480
    },
    {
      "epoch": 0.006328140355674816,
      "grad_norm": 0.9457059502601624,
      "learning_rate": 0.000436762946504871,
      "loss": 0.3666,
      "step": 1481
    },
    {
      "epoch": 0.006332413239102011,
      "grad_norm": 4.9779253005981445,
      "learning_rate": 0.0004367202187660229,
      "loss": 1.979,
      "step": 1482
    },
    {
      "epoch": 0.006336686122529205,
      "grad_norm": 4.208252906799316,
      "learning_rate": 0.0004366774910271749,
      "loss": 1.0792,
      "step": 1483
    },
    {
      "epoch": 0.0063409590059564,
      "grad_norm": 1.6433061361312866,
      "learning_rate": 0.0004366347632883268,
      "loss": 0.7889,
      "step": 1484
    },
    {
      "epoch": 0.0063452318893835935,
      "grad_norm": 2.898968458175659,
      "learning_rate": 0.00043659203554947875,
      "loss": 1.2449,
      "step": 1485
    },
    {
      "epoch": 0.006349504772810788,
      "grad_norm": 2.5035243034362793,
      "learning_rate": 0.00043654930781063066,
      "loss": 0.9151,
      "step": 1486
    },
    {
      "epoch": 0.006353777656237983,
      "grad_norm": 5.458076477050781,
      "learning_rate": 0.0004365065800717826,
      "loss": 1.8579,
      "step": 1487
    },
    {
      "epoch": 0.006358050539665177,
      "grad_norm": 1.862686038017273,
      "learning_rate": 0.0004364638523329346,
      "loss": 0.4563,
      "step": 1488
    },
    {
      "epoch": 0.0063623234230923715,
      "grad_norm": 0.9823665022850037,
      "learning_rate": 0.0004364211245940865,
      "loss": 0.3577,
      "step": 1489
    },
    {
      "epoch": 0.006366596306519565,
      "grad_norm": 2.399142265319824,
      "learning_rate": 0.00043637839685523847,
      "loss": 0.9635,
      "step": 1490
    },
    {
      "epoch": 0.00637086918994676,
      "grad_norm": 1.8326069116592407,
      "learning_rate": 0.0004363356691163904,
      "loss": 1.1461,
      "step": 1491
    },
    {
      "epoch": 0.006375142073373954,
      "grad_norm": 4.196060657501221,
      "learning_rate": 0.00043629294137754234,
      "loss": 1.8906,
      "step": 1492
    },
    {
      "epoch": 0.006379414956801149,
      "grad_norm": 0.9806463122367859,
      "learning_rate": 0.00043625021363869425,
      "loss": 0.1868,
      "step": 1493
    },
    {
      "epoch": 0.0063836878402283425,
      "grad_norm": 2.780311346054077,
      "learning_rate": 0.0004362074858998462,
      "loss": 0.8487,
      "step": 1494
    },
    {
      "epoch": 0.006387960723655537,
      "grad_norm": 1.8919659852981567,
      "learning_rate": 0.00043616475816099813,
      "loss": 0.4931,
      "step": 1495
    },
    {
      "epoch": 0.006392233607082732,
      "grad_norm": 2.03952693939209,
      "learning_rate": 0.0004361220304221501,
      "loss": 1.1428,
      "step": 1496
    },
    {
      "epoch": 0.006396506490509926,
      "grad_norm": 1.8375803232192993,
      "learning_rate": 0.000436079302683302,
      "loss": 1.0803,
      "step": 1497
    },
    {
      "epoch": 0.0064007793739371205,
      "grad_norm": 2.961426258087158,
      "learning_rate": 0.0004360365749444539,
      "loss": 0.9382,
      "step": 1498
    },
    {
      "epoch": 0.006405052257364314,
      "grad_norm": 2.8014519214630127,
      "learning_rate": 0.0004359938472056059,
      "loss": 0.857,
      "step": 1499
    },
    {
      "epoch": 0.006409325140791509,
      "grad_norm": 0.8112683892250061,
      "learning_rate": 0.0004359511194667578,
      "loss": 0.1287,
      "step": 1500
    },
    {
      "epoch": 0.006413598024218703,
      "grad_norm": 1.0290045738220215,
      "learning_rate": 0.00043590839172790975,
      "loss": 0.364,
      "step": 1501
    },
    {
      "epoch": 0.006417870907645898,
      "grad_norm": 3.8226773738861084,
      "learning_rate": 0.00043586566398906167,
      "loss": 1.7992,
      "step": 1502
    },
    {
      "epoch": 0.0064221437910730915,
      "grad_norm": 2.3297348022460938,
      "learning_rate": 0.00043582293625021363,
      "loss": 0.6633,
      "step": 1503
    },
    {
      "epoch": 0.006426416674500286,
      "grad_norm": 1.5299139022827148,
      "learning_rate": 0.0004357802085113656,
      "loss": 0.2791,
      "step": 1504
    },
    {
      "epoch": 0.006430689557927481,
      "grad_norm": 1.0242420434951782,
      "learning_rate": 0.0004357374807725175,
      "loss": 0.1965,
      "step": 1505
    },
    {
      "epoch": 0.006434962441354675,
      "grad_norm": 3.924363374710083,
      "learning_rate": 0.00043569475303366947,
      "loss": 1.142,
      "step": 1506
    },
    {
      "epoch": 0.0064392353247818696,
      "grad_norm": 2.573221445083618,
      "learning_rate": 0.0004356520252948214,
      "loss": 0.7813,
      "step": 1507
    },
    {
      "epoch": 0.006443508208209063,
      "grad_norm": 0.8074896931648254,
      "learning_rate": 0.00043560929755597335,
      "loss": 0.1504,
      "step": 1508
    },
    {
      "epoch": 0.006447781091636258,
      "grad_norm": 2.128805160522461,
      "learning_rate": 0.00043556656981712526,
      "loss": 1.0629,
      "step": 1509
    },
    {
      "epoch": 0.006452053975063452,
      "grad_norm": 1.8669289350509644,
      "learning_rate": 0.0004355238420782772,
      "loss": 0.8777,
      "step": 1510
    },
    {
      "epoch": 0.006456326858490647,
      "grad_norm": 1.9576739072799683,
      "learning_rate": 0.0004354811143394292,
      "loss": 0.5671,
      "step": 1511
    },
    {
      "epoch": 0.006460599741917841,
      "grad_norm": 1.7830932140350342,
      "learning_rate": 0.0004354383866005811,
      "loss": 0.4431,
      "step": 1512
    },
    {
      "epoch": 0.006464872625345035,
      "grad_norm": 1.3248251676559448,
      "learning_rate": 0.00043539565886173306,
      "loss": 0.2362,
      "step": 1513
    },
    {
      "epoch": 0.00646914550877223,
      "grad_norm": 1.7746065855026245,
      "learning_rate": 0.000435352931122885,
      "loss": 0.8477,
      "step": 1514
    },
    {
      "epoch": 0.006473418392199424,
      "grad_norm": 1.9185824394226074,
      "learning_rate": 0.00043531020338403694,
      "loss": 0.911,
      "step": 1515
    },
    {
      "epoch": 0.006477691275626619,
      "grad_norm": 1.4287086725234985,
      "learning_rate": 0.00043526747564518885,
      "loss": 0.3215,
      "step": 1516
    },
    {
      "epoch": 0.0064819641590538124,
      "grad_norm": 1.2457960844039917,
      "learning_rate": 0.0004352247479063408,
      "loss": 0.52,
      "step": 1517
    },
    {
      "epoch": 0.006486237042481007,
      "grad_norm": 3.6051454544067383,
      "learning_rate": 0.0004351820201674928,
      "loss": 1.1309,
      "step": 1518
    },
    {
      "epoch": 0.006490509925908201,
      "grad_norm": 1.974936604499817,
      "learning_rate": 0.0004351392924286447,
      "loss": 0.8456,
      "step": 1519
    },
    {
      "epoch": 0.006494782809335396,
      "grad_norm": 1.9675649404525757,
      "learning_rate": 0.00043509656468979666,
      "loss": 0.5728,
      "step": 1520
    },
    {
      "epoch": 0.0064990556927625905,
      "grad_norm": 3.215278387069702,
      "learning_rate": 0.00043505383695094857,
      "loss": 1.1206,
      "step": 1521
    },
    {
      "epoch": 0.006503328576189784,
      "grad_norm": 1.656429648399353,
      "learning_rate": 0.00043501110921210053,
      "loss": 0.7686,
      "step": 1522
    },
    {
      "epoch": 0.006507601459616979,
      "grad_norm": 0.7362638711929321,
      "learning_rate": 0.00043496838147325244,
      "loss": 0.1374,
      "step": 1523
    },
    {
      "epoch": 0.006511874343044173,
      "grad_norm": 2.22749400138855,
      "learning_rate": 0.0004349256537344044,
      "loss": 0.9505,
      "step": 1524
    },
    {
      "epoch": 0.006516147226471368,
      "grad_norm": 4.818511009216309,
      "learning_rate": 0.00043488292599555637,
      "loss": 1.0885,
      "step": 1525
    },
    {
      "epoch": 0.0065204201098985615,
      "grad_norm": 1.4074692726135254,
      "learning_rate": 0.0004348401982567083,
      "loss": 0.4151,
      "step": 1526
    },
    {
      "epoch": 0.006524692993325756,
      "grad_norm": 0.45693543553352356,
      "learning_rate": 0.00043479747051786025,
      "loss": 0.0735,
      "step": 1527
    },
    {
      "epoch": 0.00652896587675295,
      "grad_norm": 0.42510077357292175,
      "learning_rate": 0.00043475474277901216,
      "loss": 0.0691,
      "step": 1528
    },
    {
      "epoch": 0.006533238760180145,
      "grad_norm": 2.2735376358032227,
      "learning_rate": 0.0004347120150401641,
      "loss": 0.8785,
      "step": 1529
    },
    {
      "epoch": 0.0065375116436073395,
      "grad_norm": 0.3348176181316376,
      "learning_rate": 0.000434669287301316,
      "loss": 0.0429,
      "step": 1530
    },
    {
      "epoch": 0.006541784527034533,
      "grad_norm": 1.6157512664794922,
      "learning_rate": 0.00043462655956246794,
      "loss": 0.67,
      "step": 1531
    },
    {
      "epoch": 0.006546057410461728,
      "grad_norm": 3.974339246749878,
      "learning_rate": 0.00043458383182361985,
      "loss": 1.4386,
      "step": 1532
    },
    {
      "epoch": 0.006550330293888922,
      "grad_norm": 2.917827606201172,
      "learning_rate": 0.0004345411040847718,
      "loss": 1.1248,
      "step": 1533
    },
    {
      "epoch": 0.006554603177316117,
      "grad_norm": 3.9097180366516113,
      "learning_rate": 0.0004344983763459238,
      "loss": 1.6473,
      "step": 1534
    },
    {
      "epoch": 0.0065588760607433105,
      "grad_norm": 2.140314817428589,
      "learning_rate": 0.0004344556486070757,
      "loss": 0.7455,
      "step": 1535
    },
    {
      "epoch": 0.006563148944170505,
      "grad_norm": 2.163827419281006,
      "learning_rate": 0.00043441292086822766,
      "loss": 0.7589,
      "step": 1536
    },
    {
      "epoch": 0.006567421827597699,
      "grad_norm": 2.2268285751342773,
      "learning_rate": 0.00043437019312937957,
      "loss": 0.5939,
      "step": 1537
    },
    {
      "epoch": 0.006571694711024894,
      "grad_norm": 0.2775498330593109,
      "learning_rate": 0.00043432746539053154,
      "loss": 0.0351,
      "step": 1538
    },
    {
      "epoch": 0.0065759675944520885,
      "grad_norm": 3.6303837299346924,
      "learning_rate": 0.00043428473765168345,
      "loss": 0.9316,
      "step": 1539
    },
    {
      "epoch": 0.006580240477879282,
      "grad_norm": 1.9034615755081177,
      "learning_rate": 0.0004342420099128354,
      "loss": 0.4876,
      "step": 1540
    },
    {
      "epoch": 0.006584513361306477,
      "grad_norm": 0.28796035051345825,
      "learning_rate": 0.0004341992821739874,
      "loss": 0.0357,
      "step": 1541
    },
    {
      "epoch": 0.006588786244733671,
      "grad_norm": 2.1886143684387207,
      "learning_rate": 0.0004341565544351393,
      "loss": 0.5934,
      "step": 1542
    },
    {
      "epoch": 0.006593059128160866,
      "grad_norm": 3.357438564300537,
      "learning_rate": 0.00043411382669629125,
      "loss": 1.3702,
      "step": 1543
    },
    {
      "epoch": 0.0065973320115880595,
      "grad_norm": 2.185986042022705,
      "learning_rate": 0.00043407109895744316,
      "loss": 0.7015,
      "step": 1544
    },
    {
      "epoch": 0.006601604895015254,
      "grad_norm": 4.976556777954102,
      "learning_rate": 0.00043402837121859513,
      "loss": 1.1198,
      "step": 1545
    },
    {
      "epoch": 0.006605877778442448,
      "grad_norm": 4.876164436340332,
      "learning_rate": 0.00043398564347974704,
      "loss": 1.5446,
      "step": 1546
    },
    {
      "epoch": 0.006610150661869643,
      "grad_norm": 2.801506757736206,
      "learning_rate": 0.000433942915740899,
      "loss": 0.6801,
      "step": 1547
    },
    {
      "epoch": 0.0066144235452968375,
      "grad_norm": 20.568490982055664,
      "learning_rate": 0.00043390018800205097,
      "loss": 5.9602,
      "step": 1548
    },
    {
      "epoch": 0.006618696428724031,
      "grad_norm": 0.6302043199539185,
      "learning_rate": 0.0004338574602632029,
      "loss": 0.1344,
      "step": 1549
    },
    {
      "epoch": 0.006622969312151226,
      "grad_norm": 2.2338318824768066,
      "learning_rate": 0.00043381473252435484,
      "loss": 0.4821,
      "step": 1550
    },
    {
      "epoch": 0.00662724219557842,
      "grad_norm": 0.530457079410553,
      "learning_rate": 0.00043377200478550676,
      "loss": 0.1111,
      "step": 1551
    },
    {
      "epoch": 0.006631515079005615,
      "grad_norm": 1.4647717475891113,
      "learning_rate": 0.0004337292770466587,
      "loss": 0.3303,
      "step": 1552
    },
    {
      "epoch": 0.0066357879624328086,
      "grad_norm": 3.499058961868286,
      "learning_rate": 0.00043368654930781063,
      "loss": 1.0759,
      "step": 1553
    },
    {
      "epoch": 0.006640060845860003,
      "grad_norm": 1.221374750137329,
      "learning_rate": 0.0004336438215689626,
      "loss": 0.3399,
      "step": 1554
    },
    {
      "epoch": 0.006644333729287198,
      "grad_norm": 2.3333470821380615,
      "learning_rate": 0.00043360109383011456,
      "loss": 0.6457,
      "step": 1555
    },
    {
      "epoch": 0.006648606612714392,
      "grad_norm": 2.3144853115081787,
      "learning_rate": 0.00043355836609126647,
      "loss": 0.4949,
      "step": 1556
    },
    {
      "epoch": 0.006652879496141587,
      "grad_norm": 2.4450490474700928,
      "learning_rate": 0.00043351563835241844,
      "loss": 0.6991,
      "step": 1557
    },
    {
      "epoch": 0.0066571523795687804,
      "grad_norm": 5.003791332244873,
      "learning_rate": 0.00043347291061357035,
      "loss": 1.237,
      "step": 1558
    },
    {
      "epoch": 0.006661425262995975,
      "grad_norm": 0.9216151833534241,
      "learning_rate": 0.0004334301828747223,
      "loss": 0.2553,
      "step": 1559
    },
    {
      "epoch": 0.006665698146423169,
      "grad_norm": 2.7929093837738037,
      "learning_rate": 0.0004333874551358742,
      "loss": 0.891,
      "step": 1560
    },
    {
      "epoch": 0.006669971029850364,
      "grad_norm": 2.2093889713287354,
      "learning_rate": 0.0004333447273970262,
      "loss": 0.4945,
      "step": 1561
    },
    {
      "epoch": 0.006674243913277558,
      "grad_norm": 2.5830159187316895,
      "learning_rate": 0.0004333019996581781,
      "loss": 0.7241,
      "step": 1562
    },
    {
      "epoch": 0.006678516796704752,
      "grad_norm": 4.615422248840332,
      "learning_rate": 0.00043325927191933,
      "loss": 2.0199,
      "step": 1563
    },
    {
      "epoch": 0.006682789680131947,
      "grad_norm": 4.562439441680908,
      "learning_rate": 0.000433216544180482,
      "loss": 1.2073,
      "step": 1564
    },
    {
      "epoch": 0.006687062563559141,
      "grad_norm": 3.5145819187164307,
      "learning_rate": 0.0004331738164416339,
      "loss": 1.0924,
      "step": 1565
    },
    {
      "epoch": 0.006691335446986336,
      "grad_norm": 3.6275970935821533,
      "learning_rate": 0.00043313108870278585,
      "loss": 1.1981,
      "step": 1566
    },
    {
      "epoch": 0.0066956083304135295,
      "grad_norm": 4.325292587280273,
      "learning_rate": 0.00043308836096393776,
      "loss": 1.1435,
      "step": 1567
    },
    {
      "epoch": 0.006699881213840724,
      "grad_norm": 0.955357015132904,
      "learning_rate": 0.0004330456332250897,
      "loss": 0.3509,
      "step": 1568
    },
    {
      "epoch": 0.006704154097267918,
      "grad_norm": 1.1190955638885498,
      "learning_rate": 0.00043300290548624164,
      "loss": 0.3852,
      "step": 1569
    },
    {
      "epoch": 0.006708426980695113,
      "grad_norm": 2.3999030590057373,
      "learning_rate": 0.0004329601777473936,
      "loss": 0.6448,
      "step": 1570
    },
    {
      "epoch": 0.006712699864122307,
      "grad_norm": 0.4722782075405121,
      "learning_rate": 0.00043291745000854557,
      "loss": 0.103,
      "step": 1571
    },
    {
      "epoch": 0.006716972747549501,
      "grad_norm": 2.0868728160858154,
      "learning_rate": 0.0004328747222696975,
      "loss": 0.6363,
      "step": 1572
    },
    {
      "epoch": 0.006721245630976696,
      "grad_norm": 3.311319589614868,
      "learning_rate": 0.00043283199453084944,
      "loss": 0.9863,
      "step": 1573
    },
    {
      "epoch": 0.00672551851440389,
      "grad_norm": 4.247075080871582,
      "learning_rate": 0.00043278926679200135,
      "loss": 1.3251,
      "step": 1574
    },
    {
      "epoch": 0.006729791397831085,
      "grad_norm": 3.328153133392334,
      "learning_rate": 0.0004327465390531533,
      "loss": 0.9024,
      "step": 1575
    },
    {
      "epoch": 0.0067340642812582785,
      "grad_norm": 3.0501959323883057,
      "learning_rate": 0.00043270381131430523,
      "loss": 0.8376,
      "step": 1576
    },
    {
      "epoch": 0.006738337164685473,
      "grad_norm": 3.0170741081237793,
      "learning_rate": 0.0004326610835754572,
      "loss": 0.8118,
      "step": 1577
    },
    {
      "epoch": 0.006742610048112667,
      "grad_norm": 3.201756238937378,
      "learning_rate": 0.00043261835583660916,
      "loss": 0.7421,
      "step": 1578
    },
    {
      "epoch": 0.006746882931539862,
      "grad_norm": 1.6535965204238892,
      "learning_rate": 0.00043257562809776107,
      "loss": 0.533,
      "step": 1579
    },
    {
      "epoch": 0.006751155814967056,
      "grad_norm": 2.231503963470459,
      "learning_rate": 0.00043253290035891303,
      "loss": 1.103,
      "step": 1580
    },
    {
      "epoch": 0.00675542869839425,
      "grad_norm": 1.4223780632019043,
      "learning_rate": 0.00043249017262006494,
      "loss": 0.6291,
      "step": 1581
    },
    {
      "epoch": 0.006759701581821445,
      "grad_norm": 1.332741379737854,
      "learning_rate": 0.0004324474448812169,
      "loss": 0.3874,
      "step": 1582
    },
    {
      "epoch": 0.006763974465248639,
      "grad_norm": 1.6765968799591064,
      "learning_rate": 0.0004324047171423688,
      "loss": 0.7084,
      "step": 1583
    },
    {
      "epoch": 0.006768247348675834,
      "grad_norm": 1.333599328994751,
      "learning_rate": 0.0004323619894035208,
      "loss": 0.5967,
      "step": 1584
    },
    {
      "epoch": 0.0067725202321030275,
      "grad_norm": 2.8175485134124756,
      "learning_rate": 0.00043231926166467275,
      "loss": 1.0981,
      "step": 1585
    },
    {
      "epoch": 0.006776793115530222,
      "grad_norm": 1.4540431499481201,
      "learning_rate": 0.00043227653392582466,
      "loss": 0.4603,
      "step": 1586
    },
    {
      "epoch": 0.006781065998957416,
      "grad_norm": 3.4708330631256104,
      "learning_rate": 0.0004322338061869766,
      "loss": 1.1895,
      "step": 1587
    },
    {
      "epoch": 0.006785338882384611,
      "grad_norm": 1.3408325910568237,
      "learning_rate": 0.00043219107844812854,
      "loss": 0.5711,
      "step": 1588
    },
    {
      "epoch": 0.0067896117658118055,
      "grad_norm": 1.4629566669464111,
      "learning_rate": 0.0004321483507092805,
      "loss": 0.4208,
      "step": 1589
    },
    {
      "epoch": 0.006793884649238999,
      "grad_norm": 2.732290267944336,
      "learning_rate": 0.0004321056229704324,
      "loss": 0.8182,
      "step": 1590
    },
    {
      "epoch": 0.006798157532666194,
      "grad_norm": 2.10500431060791,
      "learning_rate": 0.0004320628952315844,
      "loss": 0.5443,
      "step": 1591
    },
    {
      "epoch": 0.006802430416093388,
      "grad_norm": 2.7695508003234863,
      "learning_rate": 0.00043202016749273634,
      "loss": 1.0765,
      "step": 1592
    },
    {
      "epoch": 0.006806703299520583,
      "grad_norm": 1.9189705848693848,
      "learning_rate": 0.00043197743975388825,
      "loss": 0.5552,
      "step": 1593
    },
    {
      "epoch": 0.0068109761829477766,
      "grad_norm": 1.998496174812317,
      "learning_rate": 0.0004319347120150402,
      "loss": 0.4472,
      "step": 1594
    },
    {
      "epoch": 0.006815249066374971,
      "grad_norm": 0.8114907741546631,
      "learning_rate": 0.00043189198427619213,
      "loss": 0.1838,
      "step": 1595
    },
    {
      "epoch": 0.006819521949802165,
      "grad_norm": 2.1601524353027344,
      "learning_rate": 0.00043184925653734404,
      "loss": 0.6614,
      "step": 1596
    },
    {
      "epoch": 0.00682379483322936,
      "grad_norm": 3.8383443355560303,
      "learning_rate": 0.00043180652879849595,
      "loss": 1.2335,
      "step": 1597
    },
    {
      "epoch": 0.006828067716656555,
      "grad_norm": 2.222907781600952,
      "learning_rate": 0.0004317638010596479,
      "loss": 1.0028,
      "step": 1598
    },
    {
      "epoch": 0.006832340600083748,
      "grad_norm": 2.424713134765625,
      "learning_rate": 0.0004317210733207998,
      "loss": 1.1524,
      "step": 1599
    },
    {
      "epoch": 0.006836613483510943,
      "grad_norm": 2.375746488571167,
      "learning_rate": 0.0004316783455819518,
      "loss": 1.0988,
      "step": 1600
    },
    {
      "epoch": 0.006840886366938137,
      "grad_norm": 0.7688713073730469,
      "learning_rate": 0.00043163561784310376,
      "loss": 0.1568,
      "step": 1601
    },
    {
      "epoch": 0.006845159250365332,
      "grad_norm": 2.494541883468628,
      "learning_rate": 0.00043159289010425567,
      "loss": 0.764,
      "step": 1602
    },
    {
      "epoch": 0.006849432133792526,
      "grad_norm": 0.5118924379348755,
      "learning_rate": 0.00043155016236540763,
      "loss": 0.0949,
      "step": 1603
    },
    {
      "epoch": 0.00685370501721972,
      "grad_norm": 11.020292282104492,
      "learning_rate": 0.00043150743462655954,
      "loss": 1.7437,
      "step": 1604
    },
    {
      "epoch": 0.006857977900646914,
      "grad_norm": 2.474606513977051,
      "learning_rate": 0.0004314647068877115,
      "loss": 0.8419,
      "step": 1605
    },
    {
      "epoch": 0.006862250784074109,
      "grad_norm": 2.658143997192383,
      "learning_rate": 0.0004314219791488634,
      "loss": 0.7261,
      "step": 1606
    },
    {
      "epoch": 0.006866523667501304,
      "grad_norm": 1.2775013446807861,
      "learning_rate": 0.0004313792514100154,
      "loss": 0.4423,
      "step": 1607
    },
    {
      "epoch": 0.0068707965509284975,
      "grad_norm": 2.2586829662323,
      "learning_rate": 0.00043133652367116735,
      "loss": 1.0967,
      "step": 1608
    },
    {
      "epoch": 0.006875069434355692,
      "grad_norm": 1.2887157201766968,
      "learning_rate": 0.00043129379593231926,
      "loss": 0.4267,
      "step": 1609
    },
    {
      "epoch": 0.006879342317782886,
      "grad_norm": 4.844096660614014,
      "learning_rate": 0.0004312510681934712,
      "loss": 1.4743,
      "step": 1610
    },
    {
      "epoch": 0.006883615201210081,
      "grad_norm": 4.630992412567139,
      "learning_rate": 0.00043120834045462313,
      "loss": 1.7221,
      "step": 1611
    },
    {
      "epoch": 0.006887888084637275,
      "grad_norm": 2.7320520877838135,
      "learning_rate": 0.0004311656127157751,
      "loss": 0.8799,
      "step": 1612
    },
    {
      "epoch": 0.006892160968064469,
      "grad_norm": 3.4662997722625732,
      "learning_rate": 0.000431122884976927,
      "loss": 1.1501,
      "step": 1613
    },
    {
      "epoch": 0.006896433851491663,
      "grad_norm": 2.4685800075531006,
      "learning_rate": 0.000431080157238079,
      "loss": 0.5843,
      "step": 1614
    },
    {
      "epoch": 0.006900706734918858,
      "grad_norm": 2.552981376647949,
      "learning_rate": 0.00043103742949923094,
      "loss": 1.0687,
      "step": 1615
    },
    {
      "epoch": 0.006904979618346053,
      "grad_norm": 5.593625545501709,
      "learning_rate": 0.00043099470176038285,
      "loss": 1.892,
      "step": 1616
    },
    {
      "epoch": 0.0069092525017732465,
      "grad_norm": 2.2348742485046387,
      "learning_rate": 0.0004309519740215348,
      "loss": 0.6286,
      "step": 1617
    },
    {
      "epoch": 0.006913525385200441,
      "grad_norm": 2.260723352432251,
      "learning_rate": 0.0004309092462826867,
      "loss": 0.4534,
      "step": 1618
    },
    {
      "epoch": 0.006917798268627635,
      "grad_norm": 1.355217695236206,
      "learning_rate": 0.0004308665185438387,
      "loss": 0.3705,
      "step": 1619
    },
    {
      "epoch": 0.00692207115205483,
      "grad_norm": 2.2176032066345215,
      "learning_rate": 0.0004308237908049906,
      "loss": 0.9823,
      "step": 1620
    },
    {
      "epoch": 0.006926344035482024,
      "grad_norm": 2.9265429973602295,
      "learning_rate": 0.00043078106306614257,
      "loss": 0.9472,
      "step": 1621
    },
    {
      "epoch": 0.006930616918909218,
      "grad_norm": 5.003279209136963,
      "learning_rate": 0.00043073833532729453,
      "loss": 1.1877,
      "step": 1622
    },
    {
      "epoch": 0.006934889802336413,
      "grad_norm": 1.745982050895691,
      "learning_rate": 0.00043069560758844644,
      "loss": 0.5408,
      "step": 1623
    },
    {
      "epoch": 0.006939162685763607,
      "grad_norm": 4.0439581871032715,
      "learning_rate": 0.0004306528798495984,
      "loss": 0.8776,
      "step": 1624
    },
    {
      "epoch": 0.006943435569190802,
      "grad_norm": 2.42785382270813,
      "learning_rate": 0.0004306101521107503,
      "loss": 0.7695,
      "step": 1625
    },
    {
      "epoch": 0.0069477084526179955,
      "grad_norm": 1.3985979557037354,
      "learning_rate": 0.0004305674243719023,
      "loss": 0.4797,
      "step": 1626
    },
    {
      "epoch": 0.00695198133604519,
      "grad_norm": 5.606457233428955,
      "learning_rate": 0.0004305246966330542,
      "loss": 2.815,
      "step": 1627
    },
    {
      "epoch": 0.006956254219472384,
      "grad_norm": 4.811792373657227,
      "learning_rate": 0.00043048196889420616,
      "loss": 0.7401,
      "step": 1628
    },
    {
      "epoch": 0.006960527102899579,
      "grad_norm": 3.0274276733398438,
      "learning_rate": 0.000430439241155358,
      "loss": 0.6697,
      "step": 1629
    },
    {
      "epoch": 0.006964799986326773,
      "grad_norm": 0.9765039682388306,
      "learning_rate": 0.00043039651341651,
      "loss": 0.1508,
      "step": 1630
    },
    {
      "epoch": 0.006969072869753967,
      "grad_norm": 3.165069580078125,
      "learning_rate": 0.00043035378567766195,
      "loss": 0.7946,
      "step": 1631
    },
    {
      "epoch": 0.006973345753181162,
      "grad_norm": 4.062500476837158,
      "learning_rate": 0.00043031105793881386,
      "loss": 0.4436,
      "step": 1632
    },
    {
      "epoch": 0.006977618636608356,
      "grad_norm": 3.4528865814208984,
      "learning_rate": 0.0004302683301999658,
      "loss": 0.8333,
      "step": 1633
    },
    {
      "epoch": 0.006981891520035551,
      "grad_norm": 2.2765376567840576,
      "learning_rate": 0.00043022560246111773,
      "loss": 0.9329,
      "step": 1634
    },
    {
      "epoch": 0.0069861644034627446,
      "grad_norm": 3.623843193054199,
      "learning_rate": 0.0004301828747222697,
      "loss": 1.1518,
      "step": 1635
    },
    {
      "epoch": 0.006990437286889939,
      "grad_norm": 1.389081597328186,
      "learning_rate": 0.0004301401469834216,
      "loss": 0.3405,
      "step": 1636
    },
    {
      "epoch": 0.006994710170317133,
      "grad_norm": 4.987768650054932,
      "learning_rate": 0.00043009741924457357,
      "loss": 1.4218,
      "step": 1637
    },
    {
      "epoch": 0.006998983053744328,
      "grad_norm": 4.762341022491455,
      "learning_rate": 0.00043005469150572554,
      "loss": 1.246,
      "step": 1638
    },
    {
      "epoch": 0.007003255937171522,
      "grad_norm": 5.1746625900268555,
      "learning_rate": 0.00043001196376687745,
      "loss": 1.7611,
      "step": 1639
    },
    {
      "epoch": 0.007007528820598716,
      "grad_norm": 1.85419499874115,
      "learning_rate": 0.0004299692360280294,
      "loss": 0.3175,
      "step": 1640
    },
    {
      "epoch": 0.007011801704025911,
      "grad_norm": 3.3622827529907227,
      "learning_rate": 0.0004299265082891813,
      "loss": 0.6593,
      "step": 1641
    },
    {
      "epoch": 0.007016074587453105,
      "grad_norm": 3.011568784713745,
      "learning_rate": 0.0004298837805503333,
      "loss": 0.5853,
      "step": 1642
    },
    {
      "epoch": 0.0070203474708803,
      "grad_norm": 1.4701471328735352,
      "learning_rate": 0.0004298410528114852,
      "loss": 0.3708,
      "step": 1643
    },
    {
      "epoch": 0.007024620354307494,
      "grad_norm": 3.204603672027588,
      "learning_rate": 0.00042979832507263716,
      "loss": 0.7962,
      "step": 1644
    },
    {
      "epoch": 0.007028893237734688,
      "grad_norm": 2.771616220474243,
      "learning_rate": 0.00042975559733378913,
      "loss": 1.1503,
      "step": 1645
    },
    {
      "epoch": 0.007033166121161882,
      "grad_norm": 1.3524852991104126,
      "learning_rate": 0.00042971286959494104,
      "loss": 0.3931,
      "step": 1646
    },
    {
      "epoch": 0.007037439004589077,
      "grad_norm": 1.3235434293746948,
      "learning_rate": 0.000429670141856093,
      "loss": 0.3095,
      "step": 1647
    },
    {
      "epoch": 0.007041711888016271,
      "grad_norm": 2.286353588104248,
      "learning_rate": 0.0004296274141172449,
      "loss": 0.9447,
      "step": 1648
    },
    {
      "epoch": 0.0070459847714434655,
      "grad_norm": 2.2652018070220947,
      "learning_rate": 0.0004295846863783969,
      "loss": 0.8895,
      "step": 1649
    },
    {
      "epoch": 0.00705025765487066,
      "grad_norm": 0.5134099125862122,
      "learning_rate": 0.0004295419586395488,
      "loss": 0.1031,
      "step": 1650
    },
    {
      "epoch": 0.007054530538297854,
      "grad_norm": 3.624537229537964,
      "learning_rate": 0.00042949923090070076,
      "loss": 1.1395,
      "step": 1651
    },
    {
      "epoch": 0.007058803421725049,
      "grad_norm": 2.3800039291381836,
      "learning_rate": 0.0004294565031618527,
      "loss": 1.0213,
      "step": 1652
    },
    {
      "epoch": 0.007063076305152243,
      "grad_norm": 3.2706727981567383,
      "learning_rate": 0.00042941377542300463,
      "loss": 1.0705,
      "step": 1653
    },
    {
      "epoch": 0.007067349188579437,
      "grad_norm": 2.9454102516174316,
      "learning_rate": 0.0004293710476841566,
      "loss": 0.7358,
      "step": 1654
    },
    {
      "epoch": 0.007071622072006631,
      "grad_norm": 5.511234760284424,
      "learning_rate": 0.0004293283199453085,
      "loss": 1.7781,
      "step": 1655
    },
    {
      "epoch": 0.007075894955433826,
      "grad_norm": 3.646524667739868,
      "learning_rate": 0.00042928559220646047,
      "loss": 3.61,
      "step": 1656
    },
    {
      "epoch": 0.00708016783886102,
      "grad_norm": 3.325700044631958,
      "learning_rate": 0.0004292428644676124,
      "loss": 1.0506,
      "step": 1657
    },
    {
      "epoch": 0.0070844407222882145,
      "grad_norm": 1.4651886224746704,
      "learning_rate": 0.00042920013672876435,
      "loss": 0.3614,
      "step": 1658
    },
    {
      "epoch": 0.007088713605715409,
      "grad_norm": 0.5803343057632446,
      "learning_rate": 0.0004291574089899163,
      "loss": 0.1272,
      "step": 1659
    },
    {
      "epoch": 0.007092986489142603,
      "grad_norm": 1.3993401527404785,
      "learning_rate": 0.0004291146812510682,
      "loss": 0.3309,
      "step": 1660
    },
    {
      "epoch": 0.007097259372569798,
      "grad_norm": 2.906252384185791,
      "learning_rate": 0.00042907195351222013,
      "loss": 0.8542,
      "step": 1661
    },
    {
      "epoch": 0.007101532255996992,
      "grad_norm": 1.9606800079345703,
      "learning_rate": 0.00042902922577337205,
      "loss": 0.4681,
      "step": 1662
    },
    {
      "epoch": 0.007105805139424186,
      "grad_norm": 2.031550407409668,
      "learning_rate": 0.000428986498034524,
      "loss": 1.1059,
      "step": 1663
    },
    {
      "epoch": 0.00711007802285138,
      "grad_norm": 3.247220039367676,
      "learning_rate": 0.0004289437702956759,
      "loss": 1.272,
      "step": 1664
    },
    {
      "epoch": 0.007114350906278575,
      "grad_norm": 0.5198496580123901,
      "learning_rate": 0.0004289010425568279,
      "loss": 0.106,
      "step": 1665
    },
    {
      "epoch": 0.00711862378970577,
      "grad_norm": 2.5850465297698975,
      "learning_rate": 0.0004288583148179798,
      "loss": 0.9641,
      "step": 1666
    },
    {
      "epoch": 0.0071228966731329635,
      "grad_norm": 0.449714720249176,
      "learning_rate": 0.00042881558707913176,
      "loss": 0.0809,
      "step": 1667
    },
    {
      "epoch": 0.007127169556560158,
      "grad_norm": 0.44251206517219543,
      "learning_rate": 0.0004287728593402837,
      "loss": 0.0859,
      "step": 1668
    },
    {
      "epoch": 0.007131442439987352,
      "grad_norm": 3.2796478271484375,
      "learning_rate": 0.00042873013160143564,
      "loss": 1.1511,
      "step": 1669
    },
    {
      "epoch": 0.007135715323414547,
      "grad_norm": 2.002202033996582,
      "learning_rate": 0.0004286874038625876,
      "loss": 0.9426,
      "step": 1670
    },
    {
      "epoch": 0.007139988206841741,
      "grad_norm": 2.6148786544799805,
      "learning_rate": 0.0004286446761237395,
      "loss": 0.6091,
      "step": 1671
    },
    {
      "epoch": 0.007144261090268935,
      "grad_norm": 3.136312961578369,
      "learning_rate": 0.0004286019483848915,
      "loss": 3.04,
      "step": 1672
    },
    {
      "epoch": 0.007148533973696129,
      "grad_norm": 1.4312639236450195,
      "learning_rate": 0.0004285592206460434,
      "loss": 0.3284,
      "step": 1673
    },
    {
      "epoch": 0.007152806857123324,
      "grad_norm": 2.629502534866333,
      "learning_rate": 0.00042851649290719535,
      "loss": 0.862,
      "step": 1674
    },
    {
      "epoch": 0.007157079740550519,
      "grad_norm": 2.0103418827056885,
      "learning_rate": 0.0004284737651683473,
      "loss": 0.8069,
      "step": 1675
    },
    {
      "epoch": 0.0071613526239777126,
      "grad_norm": 3.536165475845337,
      "learning_rate": 0.00042843103742949923,
      "loss": 1.9508,
      "step": 1676
    },
    {
      "epoch": 0.007165625507404907,
      "grad_norm": 1.8138811588287354,
      "learning_rate": 0.0004283883096906512,
      "loss": 0.6801,
      "step": 1677
    },
    {
      "epoch": 0.007169898390832101,
      "grad_norm": 2.277780055999756,
      "learning_rate": 0.0004283455819518031,
      "loss": 0.4966,
      "step": 1678
    },
    {
      "epoch": 0.007174171274259296,
      "grad_norm": 3.136695384979248,
      "learning_rate": 0.00042830285421295507,
      "loss": 0.9621,
      "step": 1679
    },
    {
      "epoch": 0.00717844415768649,
      "grad_norm": 6.056365013122559,
      "learning_rate": 0.000428260126474107,
      "loss": 2.7184,
      "step": 1680
    },
    {
      "epoch": 0.007182717041113684,
      "grad_norm": 2.0906126499176025,
      "learning_rate": 0.00042821739873525895,
      "loss": 1.0862,
      "step": 1681
    },
    {
      "epoch": 0.007186989924540878,
      "grad_norm": 1.9554470777511597,
      "learning_rate": 0.0004281746709964109,
      "loss": 0.4012,
      "step": 1682
    },
    {
      "epoch": 0.007191262807968073,
      "grad_norm": 3.2746405601501465,
      "learning_rate": 0.0004281319432575628,
      "loss": 1.0198,
      "step": 1683
    },
    {
      "epoch": 0.007195535691395268,
      "grad_norm": 1.1832308769226074,
      "learning_rate": 0.0004280892155187148,
      "loss": 0.5533,
      "step": 1684
    },
    {
      "epoch": 0.007199808574822462,
      "grad_norm": 2.3033504486083984,
      "learning_rate": 0.0004280464877798667,
      "loss": 0.8759,
      "step": 1685
    },
    {
      "epoch": 0.007204081458249656,
      "grad_norm": 3.2601354122161865,
      "learning_rate": 0.00042800376004101866,
      "loss": 1.4724,
      "step": 1686
    },
    {
      "epoch": 0.00720835434167685,
      "grad_norm": 1.4913442134857178,
      "learning_rate": 0.00042796103230217057,
      "loss": 0.6878,
      "step": 1687
    },
    {
      "epoch": 0.007212627225104045,
      "grad_norm": 1.7734631299972534,
      "learning_rate": 0.00042791830456332254,
      "loss": 0.7408,
      "step": 1688
    },
    {
      "epoch": 0.007216900108531239,
      "grad_norm": 1.4891526699066162,
      "learning_rate": 0.0004278755768244745,
      "loss": 0.3997,
      "step": 1689
    },
    {
      "epoch": 0.0072211729919584335,
      "grad_norm": 1.5965663194656372,
      "learning_rate": 0.0004278328490856264,
      "loss": 0.5244,
      "step": 1690
    },
    {
      "epoch": 0.007225445875385627,
      "grad_norm": 1.347894310951233,
      "learning_rate": 0.0004277901213467784,
      "loss": 0.3456,
      "step": 1691
    },
    {
      "epoch": 0.007229718758812822,
      "grad_norm": 1.1214215755462646,
      "learning_rate": 0.0004277473936079303,
      "loss": 0.4884,
      "step": 1692
    },
    {
      "epoch": 0.007233991642240017,
      "grad_norm": 1.8704477548599243,
      "learning_rate": 0.00042770466586908225,
      "loss": 0.5848,
      "step": 1693
    },
    {
      "epoch": 0.007238264525667211,
      "grad_norm": 4.5692362785339355,
      "learning_rate": 0.0004276619381302341,
      "loss": 1.8645,
      "step": 1694
    },
    {
      "epoch": 0.007242537409094405,
      "grad_norm": 2.341287851333618,
      "learning_rate": 0.0004276192103913861,
      "loss": 1.0325,
      "step": 1695
    },
    {
      "epoch": 0.007246810292521599,
      "grad_norm": 1.505567193031311,
      "learning_rate": 0.000427576482652538,
      "loss": 0.3579,
      "step": 1696
    },
    {
      "epoch": 0.007251083175948794,
      "grad_norm": 1.215073823928833,
      "learning_rate": 0.00042753375491368995,
      "loss": 0.3074,
      "step": 1697
    },
    {
      "epoch": 0.007255356059375988,
      "grad_norm": 1.165412425994873,
      "learning_rate": 0.0004274910271748419,
      "loss": 0.3545,
      "step": 1698
    },
    {
      "epoch": 0.0072596289428031825,
      "grad_norm": 1.0901538133621216,
      "learning_rate": 0.0004274482994359938,
      "loss": 0.2749,
      "step": 1699
    },
    {
      "epoch": 0.007263901826230377,
      "grad_norm": 1.9478834867477417,
      "learning_rate": 0.0004274055716971458,
      "loss": 0.7267,
      "step": 1700
    },
    {
      "epoch": 0.007268174709657571,
      "grad_norm": 1.4427720308303833,
      "learning_rate": 0.0004273628439582977,
      "loss": 0.3597,
      "step": 1701
    },
    {
      "epoch": 0.007272447593084766,
      "grad_norm": 3.13077449798584,
      "learning_rate": 0.00042732011621944967,
      "loss": 1.1722,
      "step": 1702
    },
    {
      "epoch": 0.00727672047651196,
      "grad_norm": 3.3607637882232666,
      "learning_rate": 0.0004272773884806016,
      "loss": 2.5244,
      "step": 1703
    },
    {
      "epoch": 0.007280993359939154,
      "grad_norm": 0.588567852973938,
      "learning_rate": 0.00042723466074175354,
      "loss": 0.1392,
      "step": 1704
    },
    {
      "epoch": 0.007285266243366348,
      "grad_norm": 1.0753856897354126,
      "learning_rate": 0.0004271919330029055,
      "loss": 0.4346,
      "step": 1705
    },
    {
      "epoch": 0.007289539126793543,
      "grad_norm": 2.1172354221343994,
      "learning_rate": 0.0004271492052640574,
      "loss": 0.8825,
      "step": 1706
    },
    {
      "epoch": 0.007293812010220737,
      "grad_norm": 1.9503904581069946,
      "learning_rate": 0.0004271064775252094,
      "loss": 0.7563,
      "step": 1707
    },
    {
      "epoch": 0.0072980848936479315,
      "grad_norm": 0.8565258383750916,
      "learning_rate": 0.0004270637497863613,
      "loss": 0.2824,
      "step": 1708
    },
    {
      "epoch": 0.007302357777075126,
      "grad_norm": 1.7375974655151367,
      "learning_rate": 0.00042702102204751326,
      "loss": 0.5079,
      "step": 1709
    },
    {
      "epoch": 0.00730663066050232,
      "grad_norm": 0.7920878529548645,
      "learning_rate": 0.00042697829430866517,
      "loss": 0.3189,
      "step": 1710
    },
    {
      "epoch": 0.007310903543929515,
      "grad_norm": 1.2894976139068604,
      "learning_rate": 0.00042693556656981714,
      "loss": 0.2982,
      "step": 1711
    },
    {
      "epoch": 0.007315176427356709,
      "grad_norm": 2.988050937652588,
      "learning_rate": 0.0004268928388309691,
      "loss": 1.1519,
      "step": 1712
    },
    {
      "epoch": 0.007319449310783903,
      "grad_norm": 0.5959323048591614,
      "learning_rate": 0.000426850111092121,
      "loss": 0.1983,
      "step": 1713
    },
    {
      "epoch": 0.007323722194211097,
      "grad_norm": 3.513279914855957,
      "learning_rate": 0.000426807383353273,
      "loss": 1.0812,
      "step": 1714
    },
    {
      "epoch": 0.007327995077638292,
      "grad_norm": 3.1319799423217773,
      "learning_rate": 0.0004267646556144249,
      "loss": 1.0797,
      "step": 1715
    },
    {
      "epoch": 0.007332267961065486,
      "grad_norm": 0.8651938438415527,
      "learning_rate": 0.00042672192787557685,
      "loss": 0.1045,
      "step": 1716
    },
    {
      "epoch": 0.0073365408444926805,
      "grad_norm": 3.4838709831237793,
      "learning_rate": 0.00042667920013672876,
      "loss": 1.0809,
      "step": 1717
    },
    {
      "epoch": 0.007340813727919875,
      "grad_norm": 3.2906548976898193,
      "learning_rate": 0.00042663647239788073,
      "loss": 0.9832,
      "step": 1718
    },
    {
      "epoch": 0.007345086611347069,
      "grad_norm": 1.840073585510254,
      "learning_rate": 0.0004265937446590327,
      "loss": 0.4113,
      "step": 1719
    },
    {
      "epoch": 0.007349359494774264,
      "grad_norm": 0.5586496591567993,
      "learning_rate": 0.0004265510169201846,
      "loss": 0.1873,
      "step": 1720
    },
    {
      "epoch": 0.007353632378201458,
      "grad_norm": 2.895171642303467,
      "learning_rate": 0.00042650828918133657,
      "loss": 0.8833,
      "step": 1721
    },
    {
      "epoch": 0.007357905261628652,
      "grad_norm": 0.6575693488121033,
      "learning_rate": 0.0004264655614424885,
      "loss": 0.2641,
      "step": 1722
    },
    {
      "epoch": 0.007362178145055846,
      "grad_norm": 1.80054771900177,
      "learning_rate": 0.00042642283370364044,
      "loss": 0.408,
      "step": 1723
    },
    {
      "epoch": 0.007366451028483041,
      "grad_norm": 1.5012884140014648,
      "learning_rate": 0.00042638010596479235,
      "loss": 0.5554,
      "step": 1724
    },
    {
      "epoch": 0.007370723911910235,
      "grad_norm": 1.7069483995437622,
      "learning_rate": 0.0004263373782259443,
      "loss": 0.5112,
      "step": 1725
    },
    {
      "epoch": 0.00737499679533743,
      "grad_norm": 3.3551175594329834,
      "learning_rate": 0.0004262946504870963,
      "loss": 1.854,
      "step": 1726
    },
    {
      "epoch": 0.007379269678764624,
      "grad_norm": 1.5624680519104004,
      "learning_rate": 0.00042625192274824814,
      "loss": 0.441,
      "step": 1727
    },
    {
      "epoch": 0.007383542562191818,
      "grad_norm": 1.3796863555908203,
      "learning_rate": 0.0004262091950094001,
      "loss": 0.3679,
      "step": 1728
    },
    {
      "epoch": 0.007387815445619013,
      "grad_norm": 1.9441618919372559,
      "learning_rate": 0.000426166467270552,
      "loss": 0.6245,
      "step": 1729
    },
    {
      "epoch": 0.007392088329046207,
      "grad_norm": 1.4033530950546265,
      "learning_rate": 0.000426123739531704,
      "loss": 0.4769,
      "step": 1730
    },
    {
      "epoch": 0.0073963612124734015,
      "grad_norm": 4.106290817260742,
      "learning_rate": 0.0004260810117928559,
      "loss": 0.8975,
      "step": 1731
    },
    {
      "epoch": 0.007400634095900595,
      "grad_norm": 3.508044719696045,
      "learning_rate": 0.00042603828405400786,
      "loss": 1.0794,
      "step": 1732
    },
    {
      "epoch": 0.00740490697932779,
      "grad_norm": 2.632910966873169,
      "learning_rate": 0.00042599555631515977,
      "loss": 1.1019,
      "step": 1733
    },
    {
      "epoch": 0.007409179862754984,
      "grad_norm": 2.2338526248931885,
      "learning_rate": 0.00042595282857631173,
      "loss": 0.7474,
      "step": 1734
    },
    {
      "epoch": 0.007413452746182179,
      "grad_norm": 3.3597538471221924,
      "learning_rate": 0.0004259101008374637,
      "loss": 1.1979,
      "step": 1735
    },
    {
      "epoch": 0.007417725629609373,
      "grad_norm": 0.8755422234535217,
      "learning_rate": 0.0004258673730986156,
      "loss": 0.29,
      "step": 1736
    },
    {
      "epoch": 0.007421998513036567,
      "grad_norm": 1.488716959953308,
      "learning_rate": 0.0004258246453597676,
      "loss": 0.4771,
      "step": 1737
    },
    {
      "epoch": 0.007426271396463762,
      "grad_norm": 3.1799848079681396,
      "learning_rate": 0.0004257819176209195,
      "loss": 1.2766,
      "step": 1738
    },
    {
      "epoch": 0.007430544279890956,
      "grad_norm": 1.0065158605575562,
      "learning_rate": 0.00042573918988207145,
      "loss": 0.303,
      "step": 1739
    },
    {
      "epoch": 0.0074348171633181505,
      "grad_norm": 2.0988035202026367,
      "learning_rate": 0.00042569646214322336,
      "loss": 0.8931,
      "step": 1740
    },
    {
      "epoch": 0.007439090046745344,
      "grad_norm": 4.116330146789551,
      "learning_rate": 0.0004256537344043753,
      "loss": 1.6583,
      "step": 1741
    },
    {
      "epoch": 0.007443362930172539,
      "grad_norm": 2.591710329055786,
      "learning_rate": 0.0004256110066655273,
      "loss": 0.844,
      "step": 1742
    },
    {
      "epoch": 0.007447635813599734,
      "grad_norm": 1.8339307308197021,
      "learning_rate": 0.0004255682789266792,
      "loss": 0.6324,
      "step": 1743
    },
    {
      "epoch": 0.007451908697026928,
      "grad_norm": 4.325413703918457,
      "learning_rate": 0.00042552555118783117,
      "loss": 1.5949,
      "step": 1744
    },
    {
      "epoch": 0.007456181580454122,
      "grad_norm": 3.378484010696411,
      "learning_rate": 0.0004254828234489831,
      "loss": 1.3482,
      "step": 1745
    },
    {
      "epoch": 0.007460454463881316,
      "grad_norm": 1.1773029565811157,
      "learning_rate": 0.00042544009571013504,
      "loss": 0.3295,
      "step": 1746
    },
    {
      "epoch": 0.007464727347308511,
      "grad_norm": 1.9276119470596313,
      "learning_rate": 0.00042539736797128695,
      "loss": 0.578,
      "step": 1747
    },
    {
      "epoch": 0.007469000230735705,
      "grad_norm": 2.437892198562622,
      "learning_rate": 0.0004253546402324389,
      "loss": 0.5482,
      "step": 1748
    },
    {
      "epoch": 0.0074732731141628995,
      "grad_norm": 1.640451192855835,
      "learning_rate": 0.0004253119124935909,
      "loss": 0.7293,
      "step": 1749
    },
    {
      "epoch": 0.007477545997590093,
      "grad_norm": 2.076138496398926,
      "learning_rate": 0.0004252691847547428,
      "loss": 0.739,
      "step": 1750
    },
    {
      "epoch": 0.007481818881017288,
      "grad_norm": 0.8875133395195007,
      "learning_rate": 0.00042522645701589476,
      "loss": 0.2292,
      "step": 1751
    },
    {
      "epoch": 0.007486091764444483,
      "grad_norm": 0.8954600691795349,
      "learning_rate": 0.00042518372927704667,
      "loss": 0.2284,
      "step": 1752
    },
    {
      "epoch": 0.007490364647871677,
      "grad_norm": 1.5926953554153442,
      "learning_rate": 0.00042514100153819863,
      "loss": 0.6416,
      "step": 1753
    },
    {
      "epoch": 0.007494637531298871,
      "grad_norm": 1.0740684270858765,
      "learning_rate": 0.00042509827379935054,
      "loss": 0.4682,
      "step": 1754
    },
    {
      "epoch": 0.007498910414726065,
      "grad_norm": 4.210289001464844,
      "learning_rate": 0.0004250555460605025,
      "loss": 1.4417,
      "step": 1755
    },
    {
      "epoch": 0.00750318329815326,
      "grad_norm": 1.4598976373672485,
      "learning_rate": 0.0004250128183216545,
      "loss": 0.4973,
      "step": 1756
    },
    {
      "epoch": 0.007507456181580454,
      "grad_norm": 1.7105357646942139,
      "learning_rate": 0.0004249700905828064,
      "loss": 0.4881,
      "step": 1757
    },
    {
      "epoch": 0.0075117290650076485,
      "grad_norm": 4.110013008117676,
      "learning_rate": 0.00042492736284395835,
      "loss": 1.3168,
      "step": 1758
    },
    {
      "epoch": 0.007516001948434842,
      "grad_norm": 1.7352312803268433,
      "learning_rate": 0.00042488463510511026,
      "loss": 0.5437,
      "step": 1759
    },
    {
      "epoch": 0.007520274831862037,
      "grad_norm": 3.7129926681518555,
      "learning_rate": 0.00042484190736626217,
      "loss": 1.1683,
      "step": 1760
    },
    {
      "epoch": 0.007524547715289232,
      "grad_norm": 2.7215614318847656,
      "learning_rate": 0.0004247991796274141,
      "loss": 1.0789,
      "step": 1761
    },
    {
      "epoch": 0.007528820598716426,
      "grad_norm": 1.3734594583511353,
      "learning_rate": 0.00042475645188856605,
      "loss": 0.472,
      "step": 1762
    },
    {
      "epoch": 0.00753309348214362,
      "grad_norm": 2.0171713829040527,
      "learning_rate": 0.00042471372414971796,
      "loss": 0.715,
      "step": 1763
    },
    {
      "epoch": 0.007537366365570814,
      "grad_norm": 4.411611557006836,
      "learning_rate": 0.0004246709964108699,
      "loss": 1.8738,
      "step": 1764
    },
    {
      "epoch": 0.007541639248998009,
      "grad_norm": 2.227576732635498,
      "learning_rate": 0.0004246282686720219,
      "loss": 1.3135,
      "step": 1765
    },
    {
      "epoch": 0.007545912132425203,
      "grad_norm": 1.8295567035675049,
      "learning_rate": 0.0004245855409331738,
      "loss": 0.7565,
      "step": 1766
    },
    {
      "epoch": 0.007550185015852398,
      "grad_norm": 2.540106773376465,
      "learning_rate": 0.00042454281319432576,
      "loss": 1.0643,
      "step": 1767
    },
    {
      "epoch": 0.007554457899279591,
      "grad_norm": 1.810985803604126,
      "learning_rate": 0.0004245000854554777,
      "loss": 0.538,
      "step": 1768
    },
    {
      "epoch": 0.007558730782706786,
      "grad_norm": 1.692267894744873,
      "learning_rate": 0.00042445735771662964,
      "loss": 0.5346,
      "step": 1769
    },
    {
      "epoch": 0.007563003666133981,
      "grad_norm": 3.4051873683929443,
      "learning_rate": 0.00042441462997778155,
      "loss": 1.1128,
      "step": 1770
    },
    {
      "epoch": 0.007567276549561175,
      "grad_norm": 1.2562252283096313,
      "learning_rate": 0.0004243719022389335,
      "loss": 0.3578,
      "step": 1771
    },
    {
      "epoch": 0.0075715494329883695,
      "grad_norm": 1.3770002126693726,
      "learning_rate": 0.0004243291745000855,
      "loss": 0.4659,
      "step": 1772
    },
    {
      "epoch": 0.007575822316415563,
      "grad_norm": 1.1149815320968628,
      "learning_rate": 0.0004242864467612374,
      "loss": 0.2971,
      "step": 1773
    },
    {
      "epoch": 0.007580095199842758,
      "grad_norm": 4.656495571136475,
      "learning_rate": 0.00042424371902238935,
      "loss": 1.4094,
      "step": 1774
    },
    {
      "epoch": 0.007584368083269952,
      "grad_norm": 1.2044956684112549,
      "learning_rate": 0.00042420099128354127,
      "loss": 0.5048,
      "step": 1775
    },
    {
      "epoch": 0.007588640966697147,
      "grad_norm": 3.814772844314575,
      "learning_rate": 0.00042415826354469323,
      "loss": 1.8177,
      "step": 1776
    },
    {
      "epoch": 0.007592913850124341,
      "grad_norm": 1.172839879989624,
      "learning_rate": 0.00042411553580584514,
      "loss": 0.4825,
      "step": 1777
    },
    {
      "epoch": 0.007597186733551535,
      "grad_norm": 1.1861352920532227,
      "learning_rate": 0.0004240728080669971,
      "loss": 0.3262,
      "step": 1778
    },
    {
      "epoch": 0.00760145961697873,
      "grad_norm": 1.5600138902664185,
      "learning_rate": 0.00042403008032814907,
      "loss": 0.5281,
      "step": 1779
    },
    {
      "epoch": 0.007605732500405924,
      "grad_norm": 1.9650704860687256,
      "learning_rate": 0.000423987352589301,
      "loss": 0.7623,
      "step": 1780
    },
    {
      "epoch": 0.0076100053838331185,
      "grad_norm": 2.6875267028808594,
      "learning_rate": 0.00042394462485045295,
      "loss": 1.0153,
      "step": 1781
    },
    {
      "epoch": 0.007614278267260312,
      "grad_norm": 1.433565616607666,
      "learning_rate": 0.00042390189711160486,
      "loss": 0.4627,
      "step": 1782
    },
    {
      "epoch": 0.007618551150687507,
      "grad_norm": 1.0033165216445923,
      "learning_rate": 0.0004238591693727568,
      "loss": 0.308,
      "step": 1783
    },
    {
      "epoch": 0.007622824034114701,
      "grad_norm": 5.372206211090088,
      "learning_rate": 0.00042381644163390873,
      "loss": 1.3375,
      "step": 1784
    },
    {
      "epoch": 0.007627096917541896,
      "grad_norm": 1.5576642751693726,
      "learning_rate": 0.0004237737138950607,
      "loss": 0.5279,
      "step": 1785
    },
    {
      "epoch": 0.00763136980096909,
      "grad_norm": 0.6858957409858704,
      "learning_rate": 0.00042373098615621266,
      "loss": 0.2314,
      "step": 1786
    },
    {
      "epoch": 0.007635642684396284,
      "grad_norm": 2.114057779312134,
      "learning_rate": 0.0004236882584173646,
      "loss": 0.7593,
      "step": 1787
    },
    {
      "epoch": 0.007639915567823479,
      "grad_norm": 2.0794100761413574,
      "learning_rate": 0.00042364553067851654,
      "loss": 0.7276,
      "step": 1788
    },
    {
      "epoch": 0.007644188451250673,
      "grad_norm": 2.2973012924194336,
      "learning_rate": 0.00042360280293966845,
      "loss": 0.6909,
      "step": 1789
    },
    {
      "epoch": 0.0076484613346778675,
      "grad_norm": 1.5652282238006592,
      "learning_rate": 0.0004235600752008204,
      "loss": 0.6323,
      "step": 1790
    },
    {
      "epoch": 0.007652734218105061,
      "grad_norm": 1.4169807434082031,
      "learning_rate": 0.0004235173474619723,
      "loss": 0.6594,
      "step": 1791
    },
    {
      "epoch": 0.007657007101532256,
      "grad_norm": 2.8822412490844727,
      "learning_rate": 0.0004234746197231243,
      "loss": 0.8701,
      "step": 1792
    },
    {
      "epoch": 0.00766127998495945,
      "grad_norm": 3.6459193229675293,
      "learning_rate": 0.0004234318919842762,
      "loss": 1.2311,
      "step": 1793
    },
    {
      "epoch": 0.007665552868386645,
      "grad_norm": 3.7154812812805176,
      "learning_rate": 0.0004233891642454281,
      "loss": 1.325,
      "step": 1794
    },
    {
      "epoch": 0.007669825751813839,
      "grad_norm": 1.616756558418274,
      "learning_rate": 0.0004233464365065801,
      "loss": 0.5444,
      "step": 1795
    },
    {
      "epoch": 0.007674098635241033,
      "grad_norm": 2.666651964187622,
      "learning_rate": 0.000423303708767732,
      "loss": 0.8776,
      "step": 1796
    },
    {
      "epoch": 0.007678371518668228,
      "grad_norm": 3.0867388248443604,
      "learning_rate": 0.00042326098102888395,
      "loss": 0.9956,
      "step": 1797
    },
    {
      "epoch": 0.007682644402095422,
      "grad_norm": 0.92730313539505,
      "learning_rate": 0.00042321825329003586,
      "loss": 0.3846,
      "step": 1798
    },
    {
      "epoch": 0.0076869172855226165,
      "grad_norm": 1.4254608154296875,
      "learning_rate": 0.00042317552555118783,
      "loss": 0.5945,
      "step": 1799
    },
    {
      "epoch": 0.00769119016894981,
      "grad_norm": 3.562162160873413,
      "learning_rate": 0.00042313279781233974,
      "loss": 1.1939,
      "step": 1800
    },
    {
      "epoch": 0.007695463052377005,
      "grad_norm": 4.083789348602295,
      "learning_rate": 0.0004230900700734917,
      "loss": 1.3473,
      "step": 1801
    },
    {
      "epoch": 0.007699735935804199,
      "grad_norm": 1.8597584962844849,
      "learning_rate": 0.00042304734233464367,
      "loss": 0.5076,
      "step": 1802
    },
    {
      "epoch": 0.007704008819231394,
      "grad_norm": 0.5211677551269531,
      "learning_rate": 0.0004230046145957956,
      "loss": 0.1929,
      "step": 1803
    },
    {
      "epoch": 0.007708281702658588,
      "grad_norm": 5.484533786773682,
      "learning_rate": 0.00042296188685694754,
      "loss": 2.1799,
      "step": 1804
    },
    {
      "epoch": 0.007712554586085782,
      "grad_norm": 2.7519586086273193,
      "learning_rate": 0.00042291915911809945,
      "loss": 0.8437,
      "step": 1805
    },
    {
      "epoch": 0.007716827469512977,
      "grad_norm": 2.5686051845550537,
      "learning_rate": 0.0004228764313792514,
      "loss": 0.879,
      "step": 1806
    },
    {
      "epoch": 0.007721100352940171,
      "grad_norm": 0.6833950281143188,
      "learning_rate": 0.00042283370364040333,
      "loss": 0.2,
      "step": 1807
    },
    {
      "epoch": 0.007725373236367366,
      "grad_norm": 2.0597643852233887,
      "learning_rate": 0.0004227909759015553,
      "loss": 1.222,
      "step": 1808
    },
    {
      "epoch": 0.007729646119794559,
      "grad_norm": 2.6434779167175293,
      "learning_rate": 0.00042274824816270726,
      "loss": 0.8249,
      "step": 1809
    },
    {
      "epoch": 0.007733919003221754,
      "grad_norm": 1.4707847833633423,
      "learning_rate": 0.00042270552042385917,
      "loss": 0.4697,
      "step": 1810
    },
    {
      "epoch": 0.007738191886648949,
      "grad_norm": 0.9160761833190918,
      "learning_rate": 0.00042266279268501114,
      "loss": 0.2088,
      "step": 1811
    },
    {
      "epoch": 0.007742464770076143,
      "grad_norm": 1.9745725393295288,
      "learning_rate": 0.00042262006494616305,
      "loss": 0.8078,
      "step": 1812
    },
    {
      "epoch": 0.0077467376535033374,
      "grad_norm": 1.140109896659851,
      "learning_rate": 0.000422577337207315,
      "loss": 0.4285,
      "step": 1813
    },
    {
      "epoch": 0.007751010536930531,
      "grad_norm": 4.211513042449951,
      "learning_rate": 0.0004225346094684669,
      "loss": 0.8664,
      "step": 1814
    },
    {
      "epoch": 0.007755283420357726,
      "grad_norm": 3.5955867767333984,
      "learning_rate": 0.0004224918817296189,
      "loss": 1.4372,
      "step": 1815
    },
    {
      "epoch": 0.00775955630378492,
      "grad_norm": 5.001898288726807,
      "learning_rate": 0.00042244915399077085,
      "loss": 1.4325,
      "step": 1816
    },
    {
      "epoch": 0.007763829187212115,
      "grad_norm": 1.880768060684204,
      "learning_rate": 0.00042240642625192276,
      "loss": 0.6624,
      "step": 1817
    },
    {
      "epoch": 0.0077681020706393085,
      "grad_norm": 1.4638603925704956,
      "learning_rate": 0.00042236369851307473,
      "loss": 0.428,
      "step": 1818
    },
    {
      "epoch": 0.007772374954066503,
      "grad_norm": 4.884130477905273,
      "learning_rate": 0.00042232097077422664,
      "loss": 1.4803,
      "step": 1819
    },
    {
      "epoch": 0.007776647837493698,
      "grad_norm": 0.44053205847740173,
      "learning_rate": 0.0004222782430353786,
      "loss": 0.1042,
      "step": 1820
    },
    {
      "epoch": 0.007780920720920892,
      "grad_norm": 2.1929373741149902,
      "learning_rate": 0.0004222355152965305,
      "loss": 0.6779,
      "step": 1821
    },
    {
      "epoch": 0.0077851936043480865,
      "grad_norm": 4.162524700164795,
      "learning_rate": 0.0004221927875576825,
      "loss": 1.1233,
      "step": 1822
    },
    {
      "epoch": 0.00778946648777528,
      "grad_norm": 2.0238916873931885,
      "learning_rate": 0.00042215005981883444,
      "loss": 0.6073,
      "step": 1823
    },
    {
      "epoch": 0.007793739371202475,
      "grad_norm": 1.4897773265838623,
      "learning_rate": 0.00042210733207998636,
      "loss": 0.3907,
      "step": 1824
    },
    {
      "epoch": 0.007798012254629669,
      "grad_norm": 1.3478530645370483,
      "learning_rate": 0.0004220646043411383,
      "loss": 0.334,
      "step": 1825
    },
    {
      "epoch": 0.007802285138056864,
      "grad_norm": 2.4144511222839355,
      "learning_rate": 0.0004220218766022902,
      "loss": 0.8865,
      "step": 1826
    },
    {
      "epoch": 0.0078065580214840575,
      "grad_norm": 2.344449043273926,
      "learning_rate": 0.00042197914886344214,
      "loss": 1.1829,
      "step": 1827
    },
    {
      "epoch": 0.007810830904911252,
      "grad_norm": 2.308473587036133,
      "learning_rate": 0.00042193642112459405,
      "loss": 0.8316,
      "step": 1828
    },
    {
      "epoch": 0.007815103788338447,
      "grad_norm": 5.080984115600586,
      "learning_rate": 0.000421893693385746,
      "loss": 2.0642,
      "step": 1829
    },
    {
      "epoch": 0.00781937667176564,
      "grad_norm": 2.132364511489868,
      "learning_rate": 0.00042185096564689793,
      "loss": 0.7796,
      "step": 1830
    },
    {
      "epoch": 0.007823649555192835,
      "grad_norm": 1.8975286483764648,
      "learning_rate": 0.0004218082379080499,
      "loss": 0.6375,
      "step": 1831
    },
    {
      "epoch": 0.00782792243862003,
      "grad_norm": 2.3140769004821777,
      "learning_rate": 0.00042176551016920186,
      "loss": 0.6906,
      "step": 1832
    },
    {
      "epoch": 0.007832195322047224,
      "grad_norm": 3.4208264350891113,
      "learning_rate": 0.00042172278243035377,
      "loss": 0.8555,
      "step": 1833
    },
    {
      "epoch": 0.007836468205474418,
      "grad_norm": 3.9734137058258057,
      "learning_rate": 0.00042168005469150573,
      "loss": 0.9937,
      "step": 1834
    },
    {
      "epoch": 0.007840741088901612,
      "grad_norm": 1.7995350360870361,
      "learning_rate": 0.00042163732695265764,
      "loss": 0.5456,
      "step": 1835
    },
    {
      "epoch": 0.007845013972328807,
      "grad_norm": 2.557474136352539,
      "learning_rate": 0.0004215945992138096,
      "loss": 0.756,
      "step": 1836
    },
    {
      "epoch": 0.007849286855756001,
      "grad_norm": 4.939148902893066,
      "learning_rate": 0.0004215518714749615,
      "loss": 1.5677,
      "step": 1837
    },
    {
      "epoch": 0.007853559739183195,
      "grad_norm": 0.8883951306343079,
      "learning_rate": 0.0004215091437361135,
      "loss": 0.3309,
      "step": 1838
    },
    {
      "epoch": 0.00785783262261039,
      "grad_norm": 3.135679006576538,
      "learning_rate": 0.00042146641599726545,
      "loss": 1.3078,
      "step": 1839
    },
    {
      "epoch": 0.007862105506037585,
      "grad_norm": 1.7831895351409912,
      "learning_rate": 0.00042142368825841736,
      "loss": 0.5776,
      "step": 1840
    },
    {
      "epoch": 0.007866378389464778,
      "grad_norm": 2.090886354446411,
      "learning_rate": 0.0004213809605195693,
      "loss": 0.6914,
      "step": 1841
    },
    {
      "epoch": 0.007870651272891972,
      "grad_norm": 3.520954132080078,
      "learning_rate": 0.00042133823278072124,
      "loss": 0.7185,
      "step": 1842
    },
    {
      "epoch": 0.007874924156319168,
      "grad_norm": 4.103842735290527,
      "learning_rate": 0.0004212955050418732,
      "loss": 1.2926,
      "step": 1843
    },
    {
      "epoch": 0.007879197039746362,
      "grad_norm": 2.3627004623413086,
      "learning_rate": 0.0004212527773030251,
      "loss": 0.7969,
      "step": 1844
    },
    {
      "epoch": 0.007883469923173556,
      "grad_norm": 2.5249273777008057,
      "learning_rate": 0.0004212100495641771,
      "loss": 0.8023,
      "step": 1845
    },
    {
      "epoch": 0.007887742806600751,
      "grad_norm": 0.7575253248214722,
      "learning_rate": 0.00042116732182532904,
      "loss": 0.1674,
      "step": 1846
    },
    {
      "epoch": 0.007892015690027945,
      "grad_norm": 0.7742931246757507,
      "learning_rate": 0.00042112459408648095,
      "loss": 0.1686,
      "step": 1847
    },
    {
      "epoch": 0.007896288573455139,
      "grad_norm": 3.081096887588501,
      "learning_rate": 0.0004210818663476329,
      "loss": 0.8057,
      "step": 1848
    },
    {
      "epoch": 0.007900561456882333,
      "grad_norm": 3.2659101486206055,
      "learning_rate": 0.00042103913860878483,
      "loss": 1.0821,
      "step": 1849
    },
    {
      "epoch": 0.007904834340309528,
      "grad_norm": 4.7190327644348145,
      "learning_rate": 0.0004209964108699368,
      "loss": 2.8264,
      "step": 1850
    },
    {
      "epoch": 0.007909107223736722,
      "grad_norm": 2.4309751987457275,
      "learning_rate": 0.0004209536831310887,
      "loss": 0.5421,
      "step": 1851
    },
    {
      "epoch": 0.007913380107163916,
      "grad_norm": 1.089219331741333,
      "learning_rate": 0.00042091095539224067,
      "loss": 0.4379,
      "step": 1852
    },
    {
      "epoch": 0.00791765299059111,
      "grad_norm": 2.725491523742676,
      "learning_rate": 0.00042086822765339263,
      "loss": 0.624,
      "step": 1853
    },
    {
      "epoch": 0.007921925874018305,
      "grad_norm": 4.323304653167725,
      "learning_rate": 0.00042082549991454454,
      "loss": 1.3739,
      "step": 1854
    },
    {
      "epoch": 0.0079261987574455,
      "grad_norm": 2.107818365097046,
      "learning_rate": 0.0004207827721756965,
      "loss": 0.6249,
      "step": 1855
    },
    {
      "epoch": 0.007930471640872693,
      "grad_norm": 4.028398513793945,
      "learning_rate": 0.0004207400444368484,
      "loss": 0.9258,
      "step": 1856
    },
    {
      "epoch": 0.007934744524299889,
      "grad_norm": 1.497418761253357,
      "learning_rate": 0.0004206973166980004,
      "loss": 0.3755,
      "step": 1857
    },
    {
      "epoch": 0.007939017407727083,
      "grad_norm": 2.072504758834839,
      "learning_rate": 0.0004206545889591523,
      "loss": 0.6253,
      "step": 1858
    },
    {
      "epoch": 0.007943290291154276,
      "grad_norm": 6.487736701965332,
      "learning_rate": 0.0004206118612203042,
      "loss": 1.4647,
      "step": 1859
    },
    {
      "epoch": 0.00794756317458147,
      "grad_norm": 1.9623042345046997,
      "learning_rate": 0.00042056913348145617,
      "loss": 0.5795,
      "step": 1860
    },
    {
      "epoch": 0.007951836058008666,
      "grad_norm": 1.5968985557556152,
      "learning_rate": 0.0004205264057426081,
      "loss": 0.4146,
      "step": 1861
    },
    {
      "epoch": 0.00795610894143586,
      "grad_norm": 1.9710760116577148,
      "learning_rate": 0.00042048367800376005,
      "loss": 0.8737,
      "step": 1862
    },
    {
      "epoch": 0.007960381824863054,
      "grad_norm": 1.4546806812286377,
      "learning_rate": 0.00042044095026491196,
      "loss": 0.3494,
      "step": 1863
    },
    {
      "epoch": 0.00796465470829025,
      "grad_norm": 3.6310627460479736,
      "learning_rate": 0.0004203982225260639,
      "loss": 0.6884,
      "step": 1864
    },
    {
      "epoch": 0.007968927591717443,
      "grad_norm": 3.975018262863159,
      "learning_rate": 0.00042035549478721583,
      "loss": 0.9664,
      "step": 1865
    },
    {
      "epoch": 0.007973200475144637,
      "grad_norm": 3.148446798324585,
      "learning_rate": 0.0004203127670483678,
      "loss": 0.41,
      "step": 1866
    },
    {
      "epoch": 0.00797747335857183,
      "grad_norm": 1.3201794624328613,
      "learning_rate": 0.0004202700393095197,
      "loss": 0.2785,
      "step": 1867
    },
    {
      "epoch": 0.007981746241999026,
      "grad_norm": 1.832159399986267,
      "learning_rate": 0.0004202273115706717,
      "loss": 0.4781,
      "step": 1868
    },
    {
      "epoch": 0.00798601912542622,
      "grad_norm": 3.896156072616577,
      "learning_rate": 0.00042018458383182364,
      "loss": 1.3086,
      "step": 1869
    },
    {
      "epoch": 0.007990292008853414,
      "grad_norm": 3.03542160987854,
      "learning_rate": 0.00042014185609297555,
      "loss": 0.8337,
      "step": 1870
    },
    {
      "epoch": 0.00799456489228061,
      "grad_norm": 3.0965187549591064,
      "learning_rate": 0.0004200991283541275,
      "loss": 0.9716,
      "step": 1871
    },
    {
      "epoch": 0.007998837775707804,
      "grad_norm": 0.7788171172142029,
      "learning_rate": 0.0004200564006152794,
      "loss": 0.1523,
      "step": 1872
    },
    {
      "epoch": 0.008003110659134997,
      "grad_norm": 0.8915556073188782,
      "learning_rate": 0.0004200136728764314,
      "loss": 0.3487,
      "step": 1873
    },
    {
      "epoch": 0.008007383542562191,
      "grad_norm": 2.801055908203125,
      "learning_rate": 0.0004199709451375833,
      "loss": 0.6681,
      "step": 1874
    },
    {
      "epoch": 0.008011656425989387,
      "grad_norm": 2.794891119003296,
      "learning_rate": 0.00041992821739873527,
      "loss": 0.6379,
      "step": 1875
    },
    {
      "epoch": 0.00801592930941658,
      "grad_norm": 0.5882481932640076,
      "learning_rate": 0.00041988548965988723,
      "loss": 0.1416,
      "step": 1876
    },
    {
      "epoch": 0.008020202192843775,
      "grad_norm": 3.1278138160705566,
      "learning_rate": 0.00041984276192103914,
      "loss": 1.5895,
      "step": 1877
    },
    {
      "epoch": 0.008024475076270968,
      "grad_norm": 1.2621506452560425,
      "learning_rate": 0.0004198000341821911,
      "loss": 0.2173,
      "step": 1878
    },
    {
      "epoch": 0.008028747959698164,
      "grad_norm": 0.530053973197937,
      "learning_rate": 0.000419757306443343,
      "loss": 0.1248,
      "step": 1879
    },
    {
      "epoch": 0.008033020843125358,
      "grad_norm": 5.99592399597168,
      "learning_rate": 0.000419714578704495,
      "loss": 2.407,
      "step": 1880
    },
    {
      "epoch": 0.008037293726552552,
      "grad_norm": 2.78584885597229,
      "learning_rate": 0.0004196718509656469,
      "loss": 0.6209,
      "step": 1881
    },
    {
      "epoch": 0.008041566609979747,
      "grad_norm": 3.2385733127593994,
      "learning_rate": 0.00041962912322679886,
      "loss": 0.9097,
      "step": 1882
    },
    {
      "epoch": 0.008045839493406941,
      "grad_norm": 3.2236859798431396,
      "learning_rate": 0.0004195863954879508,
      "loss": 0.9038,
      "step": 1883
    },
    {
      "epoch": 0.008050112376834135,
      "grad_norm": 4.665082931518555,
      "learning_rate": 0.00041954366774910273,
      "loss": 1.4755,
      "step": 1884
    },
    {
      "epoch": 0.008054385260261329,
      "grad_norm": 3.137342691421509,
      "learning_rate": 0.0004195009400102547,
      "loss": 0.7654,
      "step": 1885
    },
    {
      "epoch": 0.008058658143688524,
      "grad_norm": 0.773380696773529,
      "learning_rate": 0.0004194582122714066,
      "loss": 0.306,
      "step": 1886
    },
    {
      "epoch": 0.008062931027115718,
      "grad_norm": 3.2124760150909424,
      "learning_rate": 0.0004194154845325586,
      "loss": 0.9821,
      "step": 1887
    },
    {
      "epoch": 0.008067203910542912,
      "grad_norm": 3.112194299697876,
      "learning_rate": 0.0004193727567937105,
      "loss": 0.9297,
      "step": 1888
    },
    {
      "epoch": 0.008071476793970108,
      "grad_norm": 4.806793689727783,
      "learning_rate": 0.00041933002905486245,
      "loss": 1.8341,
      "step": 1889
    },
    {
      "epoch": 0.008075749677397302,
      "grad_norm": 0.6569995284080505,
      "learning_rate": 0.0004192873013160144,
      "loss": 0.2709,
      "step": 1890
    },
    {
      "epoch": 0.008080022560824495,
      "grad_norm": 0.5269420742988586,
      "learning_rate": 0.0004192445735771663,
      "loss": 0.1458,
      "step": 1891
    },
    {
      "epoch": 0.00808429544425169,
      "grad_norm": 2.3122775554656982,
      "learning_rate": 0.00041920184583831824,
      "loss": 0.6539,
      "step": 1892
    },
    {
      "epoch": 0.008088568327678885,
      "grad_norm": 1.6146812438964844,
      "learning_rate": 0.00041915911809947015,
      "loss": 0.3107,
      "step": 1893
    },
    {
      "epoch": 0.008092841211106079,
      "grad_norm": 0.5445823669433594,
      "learning_rate": 0.0004191163903606221,
      "loss": 0.1721,
      "step": 1894
    },
    {
      "epoch": 0.008097114094533273,
      "grad_norm": 2.793623685836792,
      "learning_rate": 0.000419073662621774,
      "loss": 1.0467,
      "step": 1895
    },
    {
      "epoch": 0.008101386977960466,
      "grad_norm": 2.4664270877838135,
      "learning_rate": 0.000419030934882926,
      "loss": 0.7566,
      "step": 1896
    },
    {
      "epoch": 0.008105659861387662,
      "grad_norm": 4.239918231964111,
      "learning_rate": 0.00041898820714407795,
      "loss": 1.415,
      "step": 1897
    },
    {
      "epoch": 0.008109932744814856,
      "grad_norm": 3.8496217727661133,
      "learning_rate": 0.00041894547940522986,
      "loss": 1.1557,
      "step": 1898
    },
    {
      "epoch": 0.00811420562824205,
      "grad_norm": 3.411742925643921,
      "learning_rate": 0.00041890275166638183,
      "loss": 0.587,
      "step": 1899
    },
    {
      "epoch": 0.008118478511669245,
      "grad_norm": 3.0324628353118896,
      "learning_rate": 0.00041886002392753374,
      "loss": 1.2738,
      "step": 1900
    },
    {
      "epoch": 0.00812275139509644,
      "grad_norm": 2.3678343296051025,
      "learning_rate": 0.0004188172961886857,
      "loss": 1.4066,
      "step": 1901
    },
    {
      "epoch": 0.008127024278523633,
      "grad_norm": 0.5458803176879883,
      "learning_rate": 0.0004187745684498376,
      "loss": 0.2521,
      "step": 1902
    },
    {
      "epoch": 0.008131297161950827,
      "grad_norm": 2.089977264404297,
      "learning_rate": 0.0004187318407109896,
      "loss": 1.2839,
      "step": 1903
    },
    {
      "epoch": 0.008135570045378022,
      "grad_norm": 3.073946475982666,
      "learning_rate": 0.0004186891129721415,
      "loss": 0.9497,
      "step": 1904
    },
    {
      "epoch": 0.008139842928805216,
      "grad_norm": 4.40834379196167,
      "learning_rate": 0.00041864638523329346,
      "loss": 1.644,
      "step": 1905
    },
    {
      "epoch": 0.00814411581223241,
      "grad_norm": 2.7578208446502686,
      "learning_rate": 0.0004186036574944454,
      "loss": 0.7381,
      "step": 1906
    },
    {
      "epoch": 0.008148388695659606,
      "grad_norm": 3.611325979232788,
      "learning_rate": 0.00041856092975559733,
      "loss": 0.8034,
      "step": 1907
    },
    {
      "epoch": 0.0081526615790868,
      "grad_norm": 1.7537152767181396,
      "learning_rate": 0.0004185182020167493,
      "loss": 1.1316,
      "step": 1908
    },
    {
      "epoch": 0.008156934462513993,
      "grad_norm": 2.6268882751464844,
      "learning_rate": 0.0004184754742779012,
      "loss": 0.7625,
      "step": 1909
    },
    {
      "epoch": 0.008161207345941187,
      "grad_norm": 1.6694715023040771,
      "learning_rate": 0.00041843274653905317,
      "loss": 0.6636,
      "step": 1910
    },
    {
      "epoch": 0.008165480229368383,
      "grad_norm": 2.551115036010742,
      "learning_rate": 0.0004183900188002051,
      "loss": 0.6984,
      "step": 1911
    },
    {
      "epoch": 0.008169753112795577,
      "grad_norm": 2.1831326484680176,
      "learning_rate": 0.00041834729106135705,
      "loss": 0.5119,
      "step": 1912
    },
    {
      "epoch": 0.00817402599622277,
      "grad_norm": 4.0344648361206055,
      "learning_rate": 0.000418304563322509,
      "loss": 1.5795,
      "step": 1913
    },
    {
      "epoch": 0.008178298879649966,
      "grad_norm": 3.6043105125427246,
      "learning_rate": 0.0004182618355836609,
      "loss": 1.0881,
      "step": 1914
    },
    {
      "epoch": 0.00818257176307716,
      "grad_norm": 0.9454892873764038,
      "learning_rate": 0.0004182191078448129,
      "loss": 0.2287,
      "step": 1915
    },
    {
      "epoch": 0.008186844646504354,
      "grad_norm": 1.6419954299926758,
      "learning_rate": 0.0004181763801059648,
      "loss": 0.6613,
      "step": 1916
    },
    {
      "epoch": 0.008191117529931548,
      "grad_norm": 2.773615598678589,
      "learning_rate": 0.00041813365236711676,
      "loss": 1.0241,
      "step": 1917
    },
    {
      "epoch": 0.008195390413358743,
      "grad_norm": 2.1009533405303955,
      "learning_rate": 0.0004180909246282687,
      "loss": 0.6685,
      "step": 1918
    },
    {
      "epoch": 0.008199663296785937,
      "grad_norm": 3.717315673828125,
      "learning_rate": 0.00041804819688942064,
      "loss": 1.3855,
      "step": 1919
    },
    {
      "epoch": 0.008203936180213131,
      "grad_norm": 1.055793046951294,
      "learning_rate": 0.0004180054691505726,
      "loss": 0.2059,
      "step": 1920
    },
    {
      "epoch": 0.008208209063640325,
      "grad_norm": 1.4748175144195557,
      "learning_rate": 0.0004179627414117245,
      "loss": 0.984,
      "step": 1921
    },
    {
      "epoch": 0.00821248194706752,
      "grad_norm": 1.950803518295288,
      "learning_rate": 0.0004179200136728765,
      "loss": 0.5081,
      "step": 1922
    },
    {
      "epoch": 0.008216754830494714,
      "grad_norm": 1.9516693353652954,
      "learning_rate": 0.0004178772859340284,
      "loss": 0.5366,
      "step": 1923
    },
    {
      "epoch": 0.008221027713921908,
      "grad_norm": 1.0354105234146118,
      "learning_rate": 0.0004178345581951803,
      "loss": 0.2628,
      "step": 1924
    },
    {
      "epoch": 0.008225300597349104,
      "grad_norm": 1.904247760772705,
      "learning_rate": 0.0004177918304563322,
      "loss": 0.6218,
      "step": 1925
    },
    {
      "epoch": 0.008229573480776298,
      "grad_norm": 0.8089122176170349,
      "learning_rate": 0.0004177491027174842,
      "loss": 0.2196,
      "step": 1926
    },
    {
      "epoch": 0.008233846364203492,
      "grad_norm": 1.5548585653305054,
      "learning_rate": 0.00041770637497863614,
      "loss": 0.6217,
      "step": 1927
    },
    {
      "epoch": 0.008238119247630685,
      "grad_norm": 3.705526351928711,
      "learning_rate": 0.00041766364723978805,
      "loss": 1.2506,
      "step": 1928
    },
    {
      "epoch": 0.008242392131057881,
      "grad_norm": 0.7596653699874878,
      "learning_rate": 0.00041762091950094,
      "loss": 0.1935,
      "step": 1929
    },
    {
      "epoch": 0.008246665014485075,
      "grad_norm": 3.2255051136016846,
      "learning_rate": 0.00041757819176209193,
      "loss": 1.3294,
      "step": 1930
    },
    {
      "epoch": 0.008250937897912269,
      "grad_norm": 1.5076957941055298,
      "learning_rate": 0.0004175354640232439,
      "loss": 0.6102,
      "step": 1931
    },
    {
      "epoch": 0.008255210781339464,
      "grad_norm": 2.0322816371917725,
      "learning_rate": 0.0004174927362843958,
      "loss": 0.3893,
      "step": 1932
    },
    {
      "epoch": 0.008259483664766658,
      "grad_norm": 1.6113954782485962,
      "learning_rate": 0.00041745000854554777,
      "loss": 0.435,
      "step": 1933
    },
    {
      "epoch": 0.008263756548193852,
      "grad_norm": 0.9717481136322021,
      "learning_rate": 0.0004174072808066997,
      "loss": 0.2885,
      "step": 1934
    },
    {
      "epoch": 0.008268029431621046,
      "grad_norm": 1.5109928846359253,
      "learning_rate": 0.00041736455306785165,
      "loss": 0.5909,
      "step": 1935
    },
    {
      "epoch": 0.008272302315048241,
      "grad_norm": 2.7344467639923096,
      "learning_rate": 0.0004173218253290036,
      "loss": 1.239,
      "step": 1936
    },
    {
      "epoch": 0.008276575198475435,
      "grad_norm": 3.371066093444824,
      "learning_rate": 0.0004172790975901555,
      "loss": 0.957,
      "step": 1937
    },
    {
      "epoch": 0.00828084808190263,
      "grad_norm": 3.2988715171813965,
      "learning_rate": 0.0004172363698513075,
      "loss": 1.6904,
      "step": 1938
    },
    {
      "epoch": 0.008285120965329823,
      "grad_norm": 0.9906087517738342,
      "learning_rate": 0.0004171936421124594,
      "loss": 0.417,
      "step": 1939
    },
    {
      "epoch": 0.008289393848757019,
      "grad_norm": 4.8391876220703125,
      "learning_rate": 0.00041715091437361136,
      "loss": 0.9398,
      "step": 1940
    },
    {
      "epoch": 0.008293666732184212,
      "grad_norm": 2.245518684387207,
      "learning_rate": 0.00041710818663476327,
      "loss": 0.5091,
      "step": 1941
    },
    {
      "epoch": 0.008297939615611406,
      "grad_norm": 1.7299997806549072,
      "learning_rate": 0.00041706545889591524,
      "loss": 0.6396,
      "step": 1942
    },
    {
      "epoch": 0.008302212499038602,
      "grad_norm": 0.8307541012763977,
      "learning_rate": 0.0004170227311570672,
      "loss": 0.2199,
      "step": 1943
    },
    {
      "epoch": 0.008306485382465796,
      "grad_norm": 1.4700682163238525,
      "learning_rate": 0.0004169800034182191,
      "loss": 1.18,
      "step": 1944
    },
    {
      "epoch": 0.00831075826589299,
      "grad_norm": 3.897928237915039,
      "learning_rate": 0.0004169372756793711,
      "loss": 0.9728,
      "step": 1945
    },
    {
      "epoch": 0.008315031149320183,
      "grad_norm": 2.5486836433410645,
      "learning_rate": 0.000416894547940523,
      "loss": 1.0818,
      "step": 1946
    },
    {
      "epoch": 0.008319304032747379,
      "grad_norm": 2.0572288036346436,
      "learning_rate": 0.00041685182020167495,
      "loss": 0.3529,
      "step": 1947
    },
    {
      "epoch": 0.008323576916174573,
      "grad_norm": 4.1470746994018555,
      "learning_rate": 0.00041680909246282686,
      "loss": 1.3667,
      "step": 1948
    },
    {
      "epoch": 0.008327849799601767,
      "grad_norm": 1.4439774751663208,
      "learning_rate": 0.00041676636472397883,
      "loss": 1.0826,
      "step": 1949
    },
    {
      "epoch": 0.008332122683028962,
      "grad_norm": 0.6675686240196228,
      "learning_rate": 0.0004167236369851308,
      "loss": 0.2969,
      "step": 1950
    },
    {
      "epoch": 0.008336395566456156,
      "grad_norm": 2.38987135887146,
      "learning_rate": 0.0004166809092462827,
      "loss": 0.7198,
      "step": 1951
    },
    {
      "epoch": 0.00834066844988335,
      "grad_norm": 4.014992713928223,
      "learning_rate": 0.00041663818150743467,
      "loss": 1.3313,
      "step": 1952
    },
    {
      "epoch": 0.008344941333310544,
      "grad_norm": 0.520399808883667,
      "learning_rate": 0.0004165954537685866,
      "loss": 0.1605,
      "step": 1953
    },
    {
      "epoch": 0.00834921421673774,
      "grad_norm": 1.6017119884490967,
      "learning_rate": 0.00041655272602973855,
      "loss": 0.3153,
      "step": 1954
    },
    {
      "epoch": 0.008353487100164933,
      "grad_norm": 1.5758401155471802,
      "learning_rate": 0.00041650999829089046,
      "loss": 0.9627,
      "step": 1955
    },
    {
      "epoch": 0.008357759983592127,
      "grad_norm": 2.3857381343841553,
      "learning_rate": 0.0004164672705520424,
      "loss": 0.4752,
      "step": 1956
    },
    {
      "epoch": 0.008362032867019323,
      "grad_norm": 1.5205663442611694,
      "learning_rate": 0.00041642454281319433,
      "loss": 0.6176,
      "step": 1957
    },
    {
      "epoch": 0.008366305750446517,
      "grad_norm": 1.367416262626648,
      "learning_rate": 0.00041638181507434624,
      "loss": 0.4738,
      "step": 1958
    },
    {
      "epoch": 0.00837057863387371,
      "grad_norm": 1.9848870038986206,
      "learning_rate": 0.0004163390873354982,
      "loss": 0.6864,
      "step": 1959
    },
    {
      "epoch": 0.008374851517300904,
      "grad_norm": 2.430769920349121,
      "learning_rate": 0.0004162963595966501,
      "loss": 0.9429,
      "step": 1960
    },
    {
      "epoch": 0.0083791244007281,
      "grad_norm": 1.5455232858657837,
      "learning_rate": 0.0004162536318578021,
      "loss": 0.6176,
      "step": 1961
    },
    {
      "epoch": 0.008383397284155294,
      "grad_norm": 1.0099014043807983,
      "learning_rate": 0.000416210904118954,
      "loss": 0.1768,
      "step": 1962
    },
    {
      "epoch": 0.008387670167582488,
      "grad_norm": 2.1659278869628906,
      "learning_rate": 0.00041616817638010596,
      "loss": 0.5743,
      "step": 1963
    },
    {
      "epoch": 0.008391943051009682,
      "grad_norm": 0.8182559609413147,
      "learning_rate": 0.0004161254486412579,
      "loss": 0.1504,
      "step": 1964
    },
    {
      "epoch": 0.008396215934436877,
      "grad_norm": 3.5232696533203125,
      "learning_rate": 0.00041608272090240983,
      "loss": 1.0369,
      "step": 1965
    },
    {
      "epoch": 0.008400488817864071,
      "grad_norm": 1.7915443181991577,
      "learning_rate": 0.0004160399931635618,
      "loss": 1.048,
      "step": 1966
    },
    {
      "epoch": 0.008404761701291265,
      "grad_norm": 3.2141494750976562,
      "learning_rate": 0.0004159972654247137,
      "loss": 1.0248,
      "step": 1967
    },
    {
      "epoch": 0.00840903458471846,
      "grad_norm": 1.7874637842178345,
      "learning_rate": 0.0004159545376858657,
      "loss": 1.0484,
      "step": 1968
    },
    {
      "epoch": 0.008413307468145654,
      "grad_norm": 3.697204828262329,
      "learning_rate": 0.0004159118099470176,
      "loss": 0.6053,
      "step": 1969
    },
    {
      "epoch": 0.008417580351572848,
      "grad_norm": 2.3360671997070312,
      "learning_rate": 0.00041586908220816955,
      "loss": 0.5666,
      "step": 1970
    },
    {
      "epoch": 0.008421853235000042,
      "grad_norm": 1.5679457187652588,
      "learning_rate": 0.00041582635446932146,
      "loss": 0.4528,
      "step": 1971
    },
    {
      "epoch": 0.008426126118427238,
      "grad_norm": 0.8052334189414978,
      "learning_rate": 0.0004157836267304734,
      "loss": 0.1624,
      "step": 1972
    },
    {
      "epoch": 0.008430399001854431,
      "grad_norm": 1.3894153833389282,
      "learning_rate": 0.0004157408989916254,
      "loss": 0.3666,
      "step": 1973
    },
    {
      "epoch": 0.008434671885281625,
      "grad_norm": 5.417437553405762,
      "learning_rate": 0.0004156981712527773,
      "loss": 2.3637,
      "step": 1974
    },
    {
      "epoch": 0.00843894476870882,
      "grad_norm": 2.278336524963379,
      "learning_rate": 0.00041565544351392927,
      "loss": 0.7288,
      "step": 1975
    },
    {
      "epoch": 0.008443217652136015,
      "grad_norm": 0.44283199310302734,
      "learning_rate": 0.0004156127157750812,
      "loss": 0.1106,
      "step": 1976
    },
    {
      "epoch": 0.008447490535563209,
      "grad_norm": 2.829824447631836,
      "learning_rate": 0.00041556998803623314,
      "loss": 1.1238,
      "step": 1977
    },
    {
      "epoch": 0.008451763418990402,
      "grad_norm": 1.7795522212982178,
      "learning_rate": 0.00041552726029738505,
      "loss": 0.4855,
      "step": 1978
    },
    {
      "epoch": 0.008456036302417598,
      "grad_norm": 3.6164634227752686,
      "learning_rate": 0.000415484532558537,
      "loss": 1.2815,
      "step": 1979
    },
    {
      "epoch": 0.008460309185844792,
      "grad_norm": 4.684942722320557,
      "learning_rate": 0.000415441804819689,
      "loss": 1.5964,
      "step": 1980
    },
    {
      "epoch": 0.008464582069271986,
      "grad_norm": 3.1691513061523438,
      "learning_rate": 0.0004153990770808409,
      "loss": 0.9889,
      "step": 1981
    },
    {
      "epoch": 0.008468854952699181,
      "grad_norm": 1.3376128673553467,
      "learning_rate": 0.00041535634934199286,
      "loss": 0.2919,
      "step": 1982
    },
    {
      "epoch": 0.008473127836126375,
      "grad_norm": 3.337545156478882,
      "learning_rate": 0.00041531362160314477,
      "loss": 0.8554,
      "step": 1983
    },
    {
      "epoch": 0.008477400719553569,
      "grad_norm": 2.914050817489624,
      "learning_rate": 0.00041527089386429674,
      "loss": 1.0077,
      "step": 1984
    },
    {
      "epoch": 0.008481673602980763,
      "grad_norm": 1.77079176902771,
      "learning_rate": 0.00041522816612544865,
      "loss": 0.713,
      "step": 1985
    },
    {
      "epoch": 0.008485946486407958,
      "grad_norm": 1.760683298110962,
      "learning_rate": 0.0004151854383866006,
      "loss": 0.7129,
      "step": 1986
    },
    {
      "epoch": 0.008490219369835152,
      "grad_norm": 3.090989589691162,
      "learning_rate": 0.0004151427106477526,
      "loss": 0.7906,
      "step": 1987
    },
    {
      "epoch": 0.008494492253262346,
      "grad_norm": 1.6714365482330322,
      "learning_rate": 0.0004150999829089045,
      "loss": 0.6649,
      "step": 1988
    },
    {
      "epoch": 0.00849876513668954,
      "grad_norm": 1.6850680112838745,
      "learning_rate": 0.00041505725517005645,
      "loss": 0.4849,
      "step": 1989
    },
    {
      "epoch": 0.008503038020116736,
      "grad_norm": 1.6817841529846191,
      "learning_rate": 0.0004150145274312083,
      "loss": 0.6117,
      "step": 1990
    },
    {
      "epoch": 0.00850731090354393,
      "grad_norm": 0.7807717323303223,
      "learning_rate": 0.0004149717996923603,
      "loss": 0.155,
      "step": 1991
    },
    {
      "epoch": 0.008511583786971123,
      "grad_norm": 1.6441247463226318,
      "learning_rate": 0.0004149290719535122,
      "loss": 0.6804,
      "step": 1992
    },
    {
      "epoch": 0.008515856670398319,
      "grad_norm": 1.507550835609436,
      "learning_rate": 0.00041488634421466415,
      "loss": 0.5716,
      "step": 1993
    },
    {
      "epoch": 0.008520129553825513,
      "grad_norm": 2.796504020690918,
      "learning_rate": 0.0004148436164758161,
      "loss": 1.1337,
      "step": 1994
    },
    {
      "epoch": 0.008524402437252707,
      "grad_norm": 2.2110416889190674,
      "learning_rate": 0.000414800888736968,
      "loss": 0.8809,
      "step": 1995
    },
    {
      "epoch": 0.0085286753206799,
      "grad_norm": 3.730572462081909,
      "learning_rate": 0.00041475816099812,
      "loss": 1.1125,
      "step": 1996
    },
    {
      "epoch": 0.008532948204107096,
      "grad_norm": 3.7421677112579346,
      "learning_rate": 0.0004147154332592719,
      "loss": 1.0428,
      "step": 1997
    },
    {
      "epoch": 0.00853722108753429,
      "grad_norm": 1.7052053213119507,
      "learning_rate": 0.00041467270552042386,
      "loss": 0.6451,
      "step": 1998
    },
    {
      "epoch": 0.008541493970961484,
      "grad_norm": 2.0943117141723633,
      "learning_rate": 0.0004146299777815758,
      "loss": 0.5494,
      "step": 1999
    },
    {
      "epoch": 0.00854576685438868,
      "grad_norm": 2.7406461238861084,
      "learning_rate": 0.00041458725004272774,
      "loss": 0.7151,
      "step": 2000
    },
    {
      "epoch": 0.008550039737815873,
      "grad_norm": 1.6625922918319702,
      "learning_rate": 0.0004145445223038797,
      "loss": 0.6456,
      "step": 2001
    },
    {
      "epoch": 0.008554312621243067,
      "grad_norm": 1.3998445272445679,
      "learning_rate": 0.0004145017945650316,
      "loss": 0.4761,
      "step": 2002
    },
    {
      "epoch": 0.008558585504670261,
      "grad_norm": 1.6497937440872192,
      "learning_rate": 0.0004144590668261836,
      "loss": 1.0217,
      "step": 2003
    },
    {
      "epoch": 0.008562858388097457,
      "grad_norm": 1.6185247898101807,
      "learning_rate": 0.0004144163390873355,
      "loss": 1.001,
      "step": 2004
    },
    {
      "epoch": 0.00856713127152465,
      "grad_norm": 1.5569932460784912,
      "learning_rate": 0.00041437361134848746,
      "loss": 0.5926,
      "step": 2005
    },
    {
      "epoch": 0.008571404154951844,
      "grad_norm": 3.056328535079956,
      "learning_rate": 0.00041433088360963937,
      "loss": 0.4562,
      "step": 2006
    },
    {
      "epoch": 0.008575677038379038,
      "grad_norm": 1.319799780845642,
      "learning_rate": 0.00041428815587079133,
      "loss": 0.4594,
      "step": 2007
    },
    {
      "epoch": 0.008579949921806234,
      "grad_norm": 1.1048367023468018,
      "learning_rate": 0.00041424542813194324,
      "loss": 0.4817,
      "step": 2008
    },
    {
      "epoch": 0.008584222805233428,
      "grad_norm": 3.2574846744537354,
      "learning_rate": 0.0004142027003930952,
      "loss": 1.1181,
      "step": 2009
    },
    {
      "epoch": 0.008588495688660621,
      "grad_norm": 5.129786014556885,
      "learning_rate": 0.0004141599726542472,
      "loss": 1.3399,
      "step": 2010
    },
    {
      "epoch": 0.008592768572087817,
      "grad_norm": 1.9383820295333862,
      "learning_rate": 0.0004141172449153991,
      "loss": 0.3246,
      "step": 2011
    },
    {
      "epoch": 0.00859704145551501,
      "grad_norm": 3.581676483154297,
      "learning_rate": 0.00041407451717655105,
      "loss": 0.975,
      "step": 2012
    },
    {
      "epoch": 0.008601314338942205,
      "grad_norm": 1.287458062171936,
      "learning_rate": 0.00041403178943770296,
      "loss": 0.4296,
      "step": 2013
    },
    {
      "epoch": 0.008605587222369399,
      "grad_norm": 3.404205083847046,
      "learning_rate": 0.0004139890616988549,
      "loss": 0.9231,
      "step": 2014
    },
    {
      "epoch": 0.008609860105796594,
      "grad_norm": 1.7087794542312622,
      "learning_rate": 0.00041394633396000684,
      "loss": 0.9219,
      "step": 2015
    },
    {
      "epoch": 0.008614132989223788,
      "grad_norm": 64.48017883300781,
      "learning_rate": 0.0004139036062211588,
      "loss": 1.0136,
      "step": 2016
    },
    {
      "epoch": 0.008618405872650982,
      "grad_norm": 0.7878955602645874,
      "learning_rate": 0.00041386087848231077,
      "loss": 0.169,
      "step": 2017
    },
    {
      "epoch": 0.008622678756078177,
      "grad_norm": 0.6685250401496887,
      "learning_rate": 0.0004138181507434627,
      "loss": 0.153,
      "step": 2018
    },
    {
      "epoch": 0.008626951639505371,
      "grad_norm": 1.3113796710968018,
      "learning_rate": 0.00041377542300461464,
      "loss": 0.3965,
      "step": 2019
    },
    {
      "epoch": 0.008631224522932565,
      "grad_norm": 2.351627826690674,
      "learning_rate": 0.00041373269526576655,
      "loss": 0.6836,
      "step": 2020
    },
    {
      "epoch": 0.008635497406359759,
      "grad_norm": 2.1742568016052246,
      "learning_rate": 0.0004136899675269185,
      "loss": 0.5289,
      "step": 2021
    },
    {
      "epoch": 0.008639770289786955,
      "grad_norm": 3.4934604167938232,
      "learning_rate": 0.00041364723978807043,
      "loss": 1.5008,
      "step": 2022
    },
    {
      "epoch": 0.008644043173214148,
      "grad_norm": 1.3816430568695068,
      "learning_rate": 0.00041360451204922234,
      "loss": 0.3735,
      "step": 2023
    },
    {
      "epoch": 0.008648316056641342,
      "grad_norm": 1.91139817237854,
      "learning_rate": 0.0004135617843103743,
      "loss": 0.8886,
      "step": 2024
    },
    {
      "epoch": 0.008652588940068538,
      "grad_norm": 0.9762119650840759,
      "learning_rate": 0.0004135190565715262,
      "loss": 0.4173,
      "step": 2025
    },
    {
      "epoch": 0.008656861823495732,
      "grad_norm": 2.6391918659210205,
      "learning_rate": 0.0004134763288326782,
      "loss": 0.8077,
      "step": 2026
    },
    {
      "epoch": 0.008661134706922926,
      "grad_norm": 2.062314510345459,
      "learning_rate": 0.0004134336010938301,
      "loss": 0.8887,
      "step": 2027
    },
    {
      "epoch": 0.00866540759035012,
      "grad_norm": 2.427522659301758,
      "learning_rate": 0.00041339087335498205,
      "loss": 0.4287,
      "step": 2028
    },
    {
      "epoch": 0.008669680473777315,
      "grad_norm": 0.5818435549736023,
      "learning_rate": 0.00041334814561613397,
      "loss": 0.1387,
      "step": 2029
    },
    {
      "epoch": 0.008673953357204509,
      "grad_norm": 1.2551192045211792,
      "learning_rate": 0.00041330541787728593,
      "loss": 0.3048,
      "step": 2030
    },
    {
      "epoch": 0.008678226240631703,
      "grad_norm": 7.009501934051514,
      "learning_rate": 0.0004132626901384379,
      "loss": 3.6396,
      "step": 2031
    },
    {
      "epoch": 0.008682499124058897,
      "grad_norm": 2.436718225479126,
      "learning_rate": 0.0004132199623995898,
      "loss": 0.5508,
      "step": 2032
    },
    {
      "epoch": 0.008686772007486092,
      "grad_norm": 1.0236074924468994,
      "learning_rate": 0.00041317723466074177,
      "loss": 0.35,
      "step": 2033
    },
    {
      "epoch": 0.008691044890913286,
      "grad_norm": 1.8607147932052612,
      "learning_rate": 0.0004131345069218937,
      "loss": 0.3274,
      "step": 2034
    },
    {
      "epoch": 0.00869531777434048,
      "grad_norm": 3.021430015563965,
      "learning_rate": 0.00041309177918304565,
      "loss": 1.507,
      "step": 2035
    },
    {
      "epoch": 0.008699590657767675,
      "grad_norm": 1.5536741018295288,
      "learning_rate": 0.00041304905144419756,
      "loss": 0.5133,
      "step": 2036
    },
    {
      "epoch": 0.00870386354119487,
      "grad_norm": 2.435689687728882,
      "learning_rate": 0.0004130063237053495,
      "loss": 0.6526,
      "step": 2037
    },
    {
      "epoch": 0.008708136424622063,
      "grad_norm": 6.122888088226318,
      "learning_rate": 0.00041296359596650143,
      "loss": 2.9138,
      "step": 2038
    },
    {
      "epoch": 0.008712409308049257,
      "grad_norm": 1.9902886152267456,
      "learning_rate": 0.0004129208682276534,
      "loss": 0.494,
      "step": 2039
    },
    {
      "epoch": 0.008716682191476453,
      "grad_norm": 1.2384339570999146,
      "learning_rate": 0.00041287814048880536,
      "loss": 0.3162,
      "step": 2040
    },
    {
      "epoch": 0.008720955074903647,
      "grad_norm": 1.2123744487762451,
      "learning_rate": 0.0004128354127499573,
      "loss": 0.3014,
      "step": 2041
    },
    {
      "epoch": 0.00872522795833084,
      "grad_norm": 1.5203253030776978,
      "learning_rate": 0.00041279268501110924,
      "loss": 0.3676,
      "step": 2042
    },
    {
      "epoch": 0.008729500841758036,
      "grad_norm": 1.301892638206482,
      "learning_rate": 0.00041274995727226115,
      "loss": 0.3178,
      "step": 2043
    },
    {
      "epoch": 0.00873377372518523,
      "grad_norm": 2.3559398651123047,
      "learning_rate": 0.0004127072295334131,
      "loss": 0.5381,
      "step": 2044
    },
    {
      "epoch": 0.008738046608612424,
      "grad_norm": 3.404013156890869,
      "learning_rate": 0.000412664501794565,
      "loss": 0.9115,
      "step": 2045
    },
    {
      "epoch": 0.008742319492039618,
      "grad_norm": 0.9818021655082703,
      "learning_rate": 0.000412621774055717,
      "loss": 0.2742,
      "step": 2046
    },
    {
      "epoch": 0.008746592375466813,
      "grad_norm": 6.240073204040527,
      "learning_rate": 0.00041257904631686895,
      "loss": 2.7662,
      "step": 2047
    },
    {
      "epoch": 0.008750865258894007,
      "grad_norm": 4.851039409637451,
      "learning_rate": 0.00041253631857802087,
      "loss": 1.4994,
      "step": 2048
    },
    {
      "epoch": 0.0087551381423212,
      "grad_norm": 0.670017421245575,
      "learning_rate": 0.00041249359083917283,
      "loss": 0.1935,
      "step": 2049
    },
    {
      "epoch": 0.008759411025748395,
      "grad_norm": 2.7979326248168945,
      "learning_rate": 0.00041245086310032474,
      "loss": 0.6929,
      "step": 2050
    },
    {
      "epoch": 0.00876368390917559,
      "grad_norm": 2.74112868309021,
      "learning_rate": 0.0004124081353614767,
      "loss": 0.7684,
      "step": 2051
    },
    {
      "epoch": 0.008767956792602784,
      "grad_norm": 3.572385787963867,
      "learning_rate": 0.0004123654076226286,
      "loss": 0.8817,
      "step": 2052
    },
    {
      "epoch": 0.008772229676029978,
      "grad_norm": 3.4287800788879395,
      "learning_rate": 0.0004123226798837806,
      "loss": 0.9025,
      "step": 2053
    },
    {
      "epoch": 0.008776502559457174,
      "grad_norm": 2.9392635822296143,
      "learning_rate": 0.00041227995214493255,
      "loss": 0.6479,
      "step": 2054
    },
    {
      "epoch": 0.008780775442884367,
      "grad_norm": 5.79780912399292,
      "learning_rate": 0.00041223722440608446,
      "loss": 2.6503,
      "step": 2055
    },
    {
      "epoch": 0.008785048326311561,
      "grad_norm": 1.1925156116485596,
      "learning_rate": 0.00041219449666723637,
      "loss": 0.2138,
      "step": 2056
    },
    {
      "epoch": 0.008789321209738755,
      "grad_norm": 0.37183067202568054,
      "learning_rate": 0.0004121517689283883,
      "loss": 0.1237,
      "step": 2057
    },
    {
      "epoch": 0.00879359409316595,
      "grad_norm": 5.086085319519043,
      "learning_rate": 0.00041210904118954024,
      "loss": 1.4837,
      "step": 2058
    },
    {
      "epoch": 0.008797866976593145,
      "grad_norm": 0.30248725414276123,
      "learning_rate": 0.00041206631345069215,
      "loss": 0.1016,
      "step": 2059
    },
    {
      "epoch": 0.008802139860020338,
      "grad_norm": 2.8668103218078613,
      "learning_rate": 0.0004120235857118441,
      "loss": 0.6584,
      "step": 2060
    },
    {
      "epoch": 0.008806412743447534,
      "grad_norm": 2.6560614109039307,
      "learning_rate": 0.0004119808579729961,
      "loss": 0.5535,
      "step": 2061
    },
    {
      "epoch": 0.008810685626874728,
      "grad_norm": 0.5647245049476624,
      "learning_rate": 0.000411938130234148,
      "loss": 0.1418,
      "step": 2062
    },
    {
      "epoch": 0.008814958510301922,
      "grad_norm": 2.7354800701141357,
      "learning_rate": 0.00041189540249529996,
      "loss": 0.8697,
      "step": 2063
    },
    {
      "epoch": 0.008819231393729116,
      "grad_norm": 1.2277088165283203,
      "learning_rate": 0.00041185267475645187,
      "loss": 0.1936,
      "step": 2064
    },
    {
      "epoch": 0.008823504277156311,
      "grad_norm": 5.260418891906738,
      "learning_rate": 0.00041180994701760384,
      "loss": 1.5145,
      "step": 2065
    },
    {
      "epoch": 0.008827777160583505,
      "grad_norm": 1.9981400966644287,
      "learning_rate": 0.00041176721927875575,
      "loss": 0.4744,
      "step": 2066
    },
    {
      "epoch": 0.008832050044010699,
      "grad_norm": 0.25036272406578064,
      "learning_rate": 0.0004117244915399077,
      "loss": 0.067,
      "step": 2067
    },
    {
      "epoch": 0.008836322927437894,
      "grad_norm": 1.7814435958862305,
      "learning_rate": 0.0004116817638010597,
      "loss": 0.5489,
      "step": 2068
    },
    {
      "epoch": 0.008840595810865088,
      "grad_norm": 2.8065133094787598,
      "learning_rate": 0.0004116390360622116,
      "loss": 0.7263,
      "step": 2069
    },
    {
      "epoch": 0.008844868694292282,
      "grad_norm": 4.534285068511963,
      "learning_rate": 0.00041159630832336355,
      "loss": 1.2813,
      "step": 2070
    },
    {
      "epoch": 0.008849141577719476,
      "grad_norm": 2.115638494491577,
      "learning_rate": 0.00041155358058451546,
      "loss": 0.5822,
      "step": 2071
    },
    {
      "epoch": 0.008853414461146672,
      "grad_norm": 2.28802227973938,
      "learning_rate": 0.00041151085284566743,
      "loss": 1.0994,
      "step": 2072
    },
    {
      "epoch": 0.008857687344573865,
      "grad_norm": 1.709159255027771,
      "learning_rate": 0.00041146812510681934,
      "loss": 0.4446,
      "step": 2073
    },
    {
      "epoch": 0.00886196022800106,
      "grad_norm": 1.9653762578964233,
      "learning_rate": 0.0004114253973679713,
      "loss": 0.8613,
      "step": 2074
    },
    {
      "epoch": 0.008866233111428253,
      "grad_norm": 1.8011471033096313,
      "learning_rate": 0.0004113826696291232,
      "loss": 0.4547,
      "step": 2075
    },
    {
      "epoch": 0.008870505994855449,
      "grad_norm": 1.3593182563781738,
      "learning_rate": 0.0004113399418902752,
      "loss": 0.1999,
      "step": 2076
    },
    {
      "epoch": 0.008874778878282643,
      "grad_norm": 3.066608190536499,
      "learning_rate": 0.00041129721415142714,
      "loss": 1.4515,
      "step": 2077
    },
    {
      "epoch": 0.008879051761709836,
      "grad_norm": 1.3671618700027466,
      "learning_rate": 0.00041125448641257905,
      "loss": 0.2015,
      "step": 2078
    },
    {
      "epoch": 0.008883324645137032,
      "grad_norm": 2.141201972961426,
      "learning_rate": 0.000411211758673731,
      "loss": 0.6317,
      "step": 2079
    },
    {
      "epoch": 0.008887597528564226,
      "grad_norm": 3.1773908138275146,
      "learning_rate": 0.00041116903093488293,
      "loss": 1.3252,
      "step": 2080
    },
    {
      "epoch": 0.00889187041199142,
      "grad_norm": 1.9439342021942139,
      "learning_rate": 0.0004111263031960349,
      "loss": 1.0115,
      "step": 2081
    },
    {
      "epoch": 0.008896143295418614,
      "grad_norm": 1.2397295236587524,
      "learning_rate": 0.0004110835754571868,
      "loss": 0.5809,
      "step": 2082
    },
    {
      "epoch": 0.00890041617884581,
      "grad_norm": 1.8160501718521118,
      "learning_rate": 0.00041104084771833877,
      "loss": 0.9638,
      "step": 2083
    },
    {
      "epoch": 0.008904689062273003,
      "grad_norm": 4.078653812408447,
      "learning_rate": 0.00041099811997949074,
      "loss": 1.1513,
      "step": 2084
    },
    {
      "epoch": 0.008908961945700197,
      "grad_norm": 1.6055139303207397,
      "learning_rate": 0.00041095539224064265,
      "loss": 0.924,
      "step": 2085
    },
    {
      "epoch": 0.008913234829127393,
      "grad_norm": 1.918523907661438,
      "learning_rate": 0.0004109126645017946,
      "loss": 0.3826,
      "step": 2086
    },
    {
      "epoch": 0.008917507712554586,
      "grad_norm": 2.265657901763916,
      "learning_rate": 0.0004108699367629465,
      "loss": 0.7586,
      "step": 2087
    },
    {
      "epoch": 0.00892178059598178,
      "grad_norm": 2.062523365020752,
      "learning_rate": 0.0004108272090240985,
      "loss": 0.5104,
      "step": 2088
    },
    {
      "epoch": 0.008926053479408974,
      "grad_norm": 3.8747692108154297,
      "learning_rate": 0.00041078448128525034,
      "loss": 1.4945,
      "step": 2089
    },
    {
      "epoch": 0.00893032636283617,
      "grad_norm": 4.612390041351318,
      "learning_rate": 0.0004107417535464023,
      "loss": 1.2351,
      "step": 2090
    },
    {
      "epoch": 0.008934599246263364,
      "grad_norm": 2.3171396255493164,
      "learning_rate": 0.0004106990258075543,
      "loss": 0.7984,
      "step": 2091
    },
    {
      "epoch": 0.008938872129690557,
      "grad_norm": 38.503475189208984,
      "learning_rate": 0.0004106562980687062,
      "loss": 5.6234,
      "step": 2092
    },
    {
      "epoch": 0.008943145013117753,
      "grad_norm": 3.39609432220459,
      "learning_rate": 0.00041061357032985815,
      "loss": 0.9638,
      "step": 2093
    },
    {
      "epoch": 0.008947417896544947,
      "grad_norm": 1.5774061679840088,
      "learning_rate": 0.00041057084259101006,
      "loss": 0.3666,
      "step": 2094
    },
    {
      "epoch": 0.00895169077997214,
      "grad_norm": 1.3523025512695312,
      "learning_rate": 0.000410528114852162,
      "loss": 0.4518,
      "step": 2095
    },
    {
      "epoch": 0.008955963663399335,
      "grad_norm": 2.5516839027404785,
      "learning_rate": 0.00041048538711331394,
      "loss": 0.919,
      "step": 2096
    },
    {
      "epoch": 0.00896023654682653,
      "grad_norm": 2.748100757598877,
      "learning_rate": 0.0004104426593744659,
      "loss": 1.1028,
      "step": 2097
    },
    {
      "epoch": 0.008964509430253724,
      "grad_norm": 3.9243547916412354,
      "learning_rate": 0.00041039993163561787,
      "loss": 1.9148,
      "step": 2098
    },
    {
      "epoch": 0.008968782313680918,
      "grad_norm": 4.216087818145752,
      "learning_rate": 0.0004103572038967698,
      "loss": 1.5414,
      "step": 2099
    },
    {
      "epoch": 0.008973055197108112,
      "grad_norm": 1.199567198753357,
      "learning_rate": 0.00041031447615792174,
      "loss": 0.3272,
      "step": 2100
    },
    {
      "epoch": 0.008977328080535307,
      "grad_norm": 1.0173301696777344,
      "learning_rate": 0.00041027174841907365,
      "loss": 0.3032,
      "step": 2101
    },
    {
      "epoch": 0.008981600963962501,
      "grad_norm": 3.904007911682129,
      "learning_rate": 0.0004102290206802256,
      "loss": 1.1254,
      "step": 2102
    },
    {
      "epoch": 0.008985873847389695,
      "grad_norm": 3.199021816253662,
      "learning_rate": 0.00041018629294137753,
      "loss": 1.0146,
      "step": 2103
    },
    {
      "epoch": 0.00899014673081689,
      "grad_norm": 3.8964357376098633,
      "learning_rate": 0.0004101435652025295,
      "loss": 1.3928,
      "step": 2104
    },
    {
      "epoch": 0.008994419614244084,
      "grad_norm": 3.888904333114624,
      "learning_rate": 0.00041010083746368146,
      "loss": 1.3465,
      "step": 2105
    },
    {
      "epoch": 0.008998692497671278,
      "grad_norm": 3.2926981449127197,
      "learning_rate": 0.00041005810972483337,
      "loss": 0.696,
      "step": 2106
    },
    {
      "epoch": 0.009002965381098472,
      "grad_norm": 1.7483406066894531,
      "learning_rate": 0.00041001538198598533,
      "loss": 0.3326,
      "step": 2107
    },
    {
      "epoch": 0.009007238264525668,
      "grad_norm": 1.5193426609039307,
      "learning_rate": 0.00040997265424713724,
      "loss": 0.3431,
      "step": 2108
    },
    {
      "epoch": 0.009011511147952862,
      "grad_norm": 3.03519344329834,
      "learning_rate": 0.0004099299265082892,
      "loss": 1.127,
      "step": 2109
    },
    {
      "epoch": 0.009015784031380055,
      "grad_norm": 2.933088779449463,
      "learning_rate": 0.0004098871987694411,
      "loss": 0.4552,
      "step": 2110
    },
    {
      "epoch": 0.009020056914807251,
      "grad_norm": 2.223012924194336,
      "learning_rate": 0.0004098444710305931,
      "loss": 0.511,
      "step": 2111
    },
    {
      "epoch": 0.009024329798234445,
      "grad_norm": 0.9753744006156921,
      "learning_rate": 0.000409801743291745,
      "loss": 0.1359,
      "step": 2112
    },
    {
      "epoch": 0.009028602681661639,
      "grad_norm": 3.6514556407928467,
      "learning_rate": 0.00040975901555289696,
      "loss": 1.0343,
      "step": 2113
    },
    {
      "epoch": 0.009032875565088833,
      "grad_norm": 3.9891550540924072,
      "learning_rate": 0.0004097162878140489,
      "loss": 1.1303,
      "step": 2114
    },
    {
      "epoch": 0.009037148448516028,
      "grad_norm": 4.526089191436768,
      "learning_rate": 0.00040967356007520084,
      "loss": 1.2982,
      "step": 2115
    },
    {
      "epoch": 0.009041421331943222,
      "grad_norm": 3.663911819458008,
      "learning_rate": 0.0004096308323363528,
      "loss": 0.9561,
      "step": 2116
    },
    {
      "epoch": 0.009045694215370416,
      "grad_norm": 0.7004820704460144,
      "learning_rate": 0.0004095881045975047,
      "loss": 0.0915,
      "step": 2117
    },
    {
      "epoch": 0.00904996709879761,
      "grad_norm": 2.245621681213379,
      "learning_rate": 0.0004095453768586567,
      "loss": 0.6467,
      "step": 2118
    },
    {
      "epoch": 0.009054239982224805,
      "grad_norm": 3.9479968547821045,
      "learning_rate": 0.0004095026491198086,
      "loss": 1.2661,
      "step": 2119
    },
    {
      "epoch": 0.009058512865652,
      "grad_norm": 3.762415647506714,
      "learning_rate": 0.00040945992138096055,
      "loss": 1.2017,
      "step": 2120
    },
    {
      "epoch": 0.009062785749079193,
      "grad_norm": 3.3773250579833984,
      "learning_rate": 0.0004094171936421125,
      "loss": 1.1023,
      "step": 2121
    },
    {
      "epoch": 0.009067058632506389,
      "grad_norm": 2.2171566486358643,
      "learning_rate": 0.0004093744659032644,
      "loss": 0.9727,
      "step": 2122
    },
    {
      "epoch": 0.009071331515933583,
      "grad_norm": 2.8872013092041016,
      "learning_rate": 0.00040933173816441634,
      "loss": 0.608,
      "step": 2123
    },
    {
      "epoch": 0.009075604399360776,
      "grad_norm": 2.5690784454345703,
      "learning_rate": 0.00040928901042556825,
      "loss": 0.847,
      "step": 2124
    },
    {
      "epoch": 0.00907987728278797,
      "grad_norm": 3.918565273284912,
      "learning_rate": 0.0004092462826867202,
      "loss": 0.8477,
      "step": 2125
    },
    {
      "epoch": 0.009084150166215166,
      "grad_norm": 4.316182613372803,
      "learning_rate": 0.0004092035549478721,
      "loss": 1.757,
      "step": 2126
    },
    {
      "epoch": 0.00908842304964236,
      "grad_norm": 3.799670934677124,
      "learning_rate": 0.0004091608272090241,
      "loss": 0.829,
      "step": 2127
    },
    {
      "epoch": 0.009092695933069554,
      "grad_norm": 4.16835880279541,
      "learning_rate": 0.00040911809947017606,
      "loss": 1.2346,
      "step": 2128
    },
    {
      "epoch": 0.009096968816496749,
      "grad_norm": 2.6791000366210938,
      "learning_rate": 0.00040907537173132797,
      "loss": 0.5909,
      "step": 2129
    },
    {
      "epoch": 0.009101241699923943,
      "grad_norm": 3.0139737129211426,
      "learning_rate": 0.00040903264399247993,
      "loss": 0.7416,
      "step": 2130
    },
    {
      "epoch": 0.009105514583351137,
      "grad_norm": 1.5212531089782715,
      "learning_rate": 0.00040898991625363184,
      "loss": 0.467,
      "step": 2131
    },
    {
      "epoch": 0.00910978746677833,
      "grad_norm": 1.9105397462844849,
      "learning_rate": 0.0004089471885147838,
      "loss": 0.7456,
      "step": 2132
    },
    {
      "epoch": 0.009114060350205526,
      "grad_norm": 1.364564299583435,
      "learning_rate": 0.0004089044607759357,
      "loss": 1.0983,
      "step": 2133
    },
    {
      "epoch": 0.00911833323363272,
      "grad_norm": 1.986046314239502,
      "learning_rate": 0.0004088617330370877,
      "loss": 0.4796,
      "step": 2134
    },
    {
      "epoch": 0.009122606117059914,
      "grad_norm": 1.4243336915969849,
      "learning_rate": 0.00040881900529823965,
      "loss": 0.338,
      "step": 2135
    },
    {
      "epoch": 0.00912687900048711,
      "grad_norm": 3.825875759124756,
      "learning_rate": 0.00040877627755939156,
      "loss": 1.2092,
      "step": 2136
    },
    {
      "epoch": 0.009131151883914303,
      "grad_norm": 3.087413787841797,
      "learning_rate": 0.0004087335498205435,
      "loss": 1.6092,
      "step": 2137
    },
    {
      "epoch": 0.009135424767341497,
      "grad_norm": 1.8587993383407593,
      "learning_rate": 0.00040869082208169543,
      "loss": 0.5124,
      "step": 2138
    },
    {
      "epoch": 0.009139697650768691,
      "grad_norm": 3.774080991744995,
      "learning_rate": 0.0004086480943428474,
      "loss": 1.1698,
      "step": 2139
    },
    {
      "epoch": 0.009143970534195887,
      "grad_norm": 3.2022998332977295,
      "learning_rate": 0.0004086053666039993,
      "loss": 1.4338,
      "step": 2140
    },
    {
      "epoch": 0.00914824341762308,
      "grad_norm": 1.4758455753326416,
      "learning_rate": 0.0004085626388651513,
      "loss": 0.4204,
      "step": 2141
    },
    {
      "epoch": 0.009152516301050274,
      "grad_norm": 1.2434039115905762,
      "learning_rate": 0.0004085199111263032,
      "loss": 0.3541,
      "step": 2142
    },
    {
      "epoch": 0.009156789184477468,
      "grad_norm": 4.102501392364502,
      "learning_rate": 0.00040847718338745515,
      "loss": 1.4489,
      "step": 2143
    },
    {
      "epoch": 0.009161062067904664,
      "grad_norm": 0.9185124635696411,
      "learning_rate": 0.0004084344556486071,
      "loss": 0.2341,
      "step": 2144
    },
    {
      "epoch": 0.009165334951331858,
      "grad_norm": 4.057361125946045,
      "learning_rate": 0.000408391727909759,
      "loss": 1.2389,
      "step": 2145
    },
    {
      "epoch": 0.009169607834759052,
      "grad_norm": 2.642897367477417,
      "learning_rate": 0.000408349000170911,
      "loss": 0.9738,
      "step": 2146
    },
    {
      "epoch": 0.009173880718186247,
      "grad_norm": 0.5913333296775818,
      "learning_rate": 0.0004083062724320629,
      "loss": 0.1484,
      "step": 2147
    },
    {
      "epoch": 0.009178153601613441,
      "grad_norm": 0.9958721399307251,
      "learning_rate": 0.00040826354469321487,
      "loss": 0.3133,
      "step": 2148
    },
    {
      "epoch": 0.009182426485040635,
      "grad_norm": 1.3690259456634521,
      "learning_rate": 0.0004082208169543668,
      "loss": 0.26,
      "step": 2149
    },
    {
      "epoch": 0.009186699368467829,
      "grad_norm": 2.456437349319458,
      "learning_rate": 0.00040817808921551874,
      "loss": 0.8791,
      "step": 2150
    },
    {
      "epoch": 0.009190972251895024,
      "grad_norm": 1.3707855939865112,
      "learning_rate": 0.0004081353614766707,
      "loss": 0.4757,
      "step": 2151
    },
    {
      "epoch": 0.009195245135322218,
      "grad_norm": 2.38360333442688,
      "learning_rate": 0.0004080926337378226,
      "loss": 0.5977,
      "step": 2152
    },
    {
      "epoch": 0.009199518018749412,
      "grad_norm": 1.0714350938796997,
      "learning_rate": 0.0004080499059989746,
      "loss": 0.3084,
      "step": 2153
    },
    {
      "epoch": 0.009203790902176608,
      "grad_norm": 1.181066870689392,
      "learning_rate": 0.0004080071782601265,
      "loss": 0.3915,
      "step": 2154
    },
    {
      "epoch": 0.009208063785603801,
      "grad_norm": 4.2523579597473145,
      "learning_rate": 0.0004079644505212784,
      "loss": 1.2141,
      "step": 2155
    },
    {
      "epoch": 0.009212336669030995,
      "grad_norm": 0.8984615802764893,
      "learning_rate": 0.0004079217227824303,
      "loss": 0.2527,
      "step": 2156
    },
    {
      "epoch": 0.00921660955245819,
      "grad_norm": 0.4585486948490143,
      "learning_rate": 0.0004078789950435823,
      "loss": 0.1188,
      "step": 2157
    },
    {
      "epoch": 0.009220882435885385,
      "grad_norm": 4.1034040451049805,
      "learning_rate": 0.00040783626730473424,
      "loss": 1.1134,
      "step": 2158
    },
    {
      "epoch": 0.009225155319312579,
      "grad_norm": 1.812402367591858,
      "learning_rate": 0.00040779353956588616,
      "loss": 0.559,
      "step": 2159
    },
    {
      "epoch": 0.009229428202739772,
      "grad_norm": 2.758173704147339,
      "learning_rate": 0.0004077508118270381,
      "loss": 0.9426,
      "step": 2160
    },
    {
      "epoch": 0.009233701086166966,
      "grad_norm": 2.103408098220825,
      "learning_rate": 0.00040770808408819003,
      "loss": 1.0065,
      "step": 2161
    },
    {
      "epoch": 0.009237973969594162,
      "grad_norm": 1.8108052015304565,
      "learning_rate": 0.000407665356349342,
      "loss": 0.5346,
      "step": 2162
    },
    {
      "epoch": 0.009242246853021356,
      "grad_norm": 0.8436015248298645,
      "learning_rate": 0.0004076226286104939,
      "loss": 0.0922,
      "step": 2163
    },
    {
      "epoch": 0.00924651973644855,
      "grad_norm": 3.2193853855133057,
      "learning_rate": 0.00040757990087164587,
      "loss": 1.2557,
      "step": 2164
    },
    {
      "epoch": 0.009250792619875745,
      "grad_norm": 1.319411277770996,
      "learning_rate": 0.00040753717313279784,
      "loss": 0.9895,
      "step": 2165
    },
    {
      "epoch": 0.009255065503302939,
      "grad_norm": 0.5464508533477783,
      "learning_rate": 0.00040749444539394975,
      "loss": 0.0509,
      "step": 2166
    },
    {
      "epoch": 0.009259338386730133,
      "grad_norm": 2.704394817352295,
      "learning_rate": 0.0004074517176551017,
      "loss": 0.6382,
      "step": 2167
    },
    {
      "epoch": 0.009263611270157327,
      "grad_norm": 0.6651676893234253,
      "learning_rate": 0.0004074089899162536,
      "loss": 0.1585,
      "step": 2168
    },
    {
      "epoch": 0.009267884153584522,
      "grad_norm": 6.522916316986084,
      "learning_rate": 0.0004073662621774056,
      "loss": 2.1319,
      "step": 2169
    },
    {
      "epoch": 0.009272157037011716,
      "grad_norm": 2.481335401535034,
      "learning_rate": 0.0004073235344385575,
      "loss": 0.902,
      "step": 2170
    },
    {
      "epoch": 0.00927642992043891,
      "grad_norm": 1.8837854862213135,
      "learning_rate": 0.00040728080669970946,
      "loss": 0.6818,
      "step": 2171
    },
    {
      "epoch": 0.009280702803866106,
      "grad_norm": 2.9795315265655518,
      "learning_rate": 0.00040723807896086143,
      "loss": 1.0307,
      "step": 2172
    },
    {
      "epoch": 0.0092849756872933,
      "grad_norm": 3.6521799564361572,
      "learning_rate": 0.00040719535122201334,
      "loss": 0.9252,
      "step": 2173
    },
    {
      "epoch": 0.009289248570720493,
      "grad_norm": 3.937117576599121,
      "learning_rate": 0.0004071526234831653,
      "loss": 1.0624,
      "step": 2174
    },
    {
      "epoch": 0.009293521454147687,
      "grad_norm": 3.8366432189941406,
      "learning_rate": 0.0004071098957443172,
      "loss": 1.0756,
      "step": 2175
    },
    {
      "epoch": 0.009297794337574883,
      "grad_norm": 3.007598876953125,
      "learning_rate": 0.0004070671680054692,
      "loss": 0.7278,
      "step": 2176
    },
    {
      "epoch": 0.009302067221002077,
      "grad_norm": 1.3218834400177002,
      "learning_rate": 0.0004070244402666211,
      "loss": 0.3648,
      "step": 2177
    },
    {
      "epoch": 0.00930634010442927,
      "grad_norm": 1.3298308849334717,
      "learning_rate": 0.00040698171252777306,
      "loss": 0.5216,
      "step": 2178
    },
    {
      "epoch": 0.009310612987856466,
      "grad_norm": 1.2894717454910278,
      "learning_rate": 0.00040693898478892497,
      "loss": 0.3578,
      "step": 2179
    },
    {
      "epoch": 0.00931488587128366,
      "grad_norm": 2.7928900718688965,
      "learning_rate": 0.00040689625705007693,
      "loss": 0.6564,
      "step": 2180
    },
    {
      "epoch": 0.009319158754710854,
      "grad_norm": 5.274302005767822,
      "learning_rate": 0.0004068535293112289,
      "loss": 1.5181,
      "step": 2181
    },
    {
      "epoch": 0.009323431638138048,
      "grad_norm": 1.2727612257003784,
      "learning_rate": 0.0004068108015723808,
      "loss": 0.5042,
      "step": 2182
    },
    {
      "epoch": 0.009327704521565243,
      "grad_norm": 0.6688374280929565,
      "learning_rate": 0.00040676807383353277,
      "loss": 0.0682,
      "step": 2183
    },
    {
      "epoch": 0.009331977404992437,
      "grad_norm": 0.8938537240028381,
      "learning_rate": 0.0004067253460946847,
      "loss": 0.3525,
      "step": 2184
    },
    {
      "epoch": 0.009336250288419631,
      "grad_norm": 2.73657488822937,
      "learning_rate": 0.00040668261835583665,
      "loss": 0.975,
      "step": 2185
    },
    {
      "epoch": 0.009340523171846825,
      "grad_norm": 3.988532066345215,
      "learning_rate": 0.00040663989061698856,
      "loss": 1.1446,
      "step": 2186
    },
    {
      "epoch": 0.00934479605527402,
      "grad_norm": 1.3675734996795654,
      "learning_rate": 0.00040659716287814047,
      "loss": 0.6096,
      "step": 2187
    },
    {
      "epoch": 0.009349068938701214,
      "grad_norm": 3.820873498916626,
      "learning_rate": 0.00040655443513929243,
      "loss": 1.0335,
      "step": 2188
    },
    {
      "epoch": 0.009353341822128408,
      "grad_norm": 1.0367388725280762,
      "learning_rate": 0.00040651170740044435,
      "loss": 0.3537,
      "step": 2189
    },
    {
      "epoch": 0.009357614705555604,
      "grad_norm": 1.1368051767349243,
      "learning_rate": 0.0004064689796615963,
      "loss": 0.3913,
      "step": 2190
    },
    {
      "epoch": 0.009361887588982798,
      "grad_norm": 1.9302536249160767,
      "learning_rate": 0.0004064262519227482,
      "loss": 0.6205,
      "step": 2191
    },
    {
      "epoch": 0.009366160472409991,
      "grad_norm": 5.104858875274658,
      "learning_rate": 0.0004063835241839002,
      "loss": 1.8787,
      "step": 2192
    },
    {
      "epoch": 0.009370433355837185,
      "grad_norm": 3.3923561573028564,
      "learning_rate": 0.0004063407964450521,
      "loss": 0.6217,
      "step": 2193
    },
    {
      "epoch": 0.009374706239264381,
      "grad_norm": 2.0112009048461914,
      "learning_rate": 0.00040629806870620406,
      "loss": 0.7369,
      "step": 2194
    },
    {
      "epoch": 0.009378979122691575,
      "grad_norm": 0.7262333631515503,
      "learning_rate": 0.000406255340967356,
      "loss": 0.2446,
      "step": 2195
    },
    {
      "epoch": 0.009383252006118769,
      "grad_norm": 1.3784499168395996,
      "learning_rate": 0.00040621261322850794,
      "loss": 0.2682,
      "step": 2196
    },
    {
      "epoch": 0.009387524889545964,
      "grad_norm": 1.5180130004882812,
      "learning_rate": 0.0004061698854896599,
      "loss": 0.5568,
      "step": 2197
    },
    {
      "epoch": 0.009391797772973158,
      "grad_norm": 2.3807761669158936,
      "learning_rate": 0.0004061271577508118,
      "loss": 0.8414,
      "step": 2198
    },
    {
      "epoch": 0.009396070656400352,
      "grad_norm": 1.9275742769241333,
      "learning_rate": 0.0004060844300119638,
      "loss": 0.6536,
      "step": 2199
    },
    {
      "epoch": 0.009400343539827546,
      "grad_norm": 3.9787375926971436,
      "learning_rate": 0.0004060417022731157,
      "loss": 1.4817,
      "step": 2200
    },
    {
      "epoch": 0.009404616423254741,
      "grad_norm": 3.2805793285369873,
      "learning_rate": 0.00040599897453426765,
      "loss": 0.8657,
      "step": 2201
    },
    {
      "epoch": 0.009408889306681935,
      "grad_norm": 2.030008316040039,
      "learning_rate": 0.0004059562467954196,
      "loss": 0.6284,
      "step": 2202
    },
    {
      "epoch": 0.009413162190109129,
      "grad_norm": 2.59895920753479,
      "learning_rate": 0.00040591351905657153,
      "loss": 1.0769,
      "step": 2203
    },
    {
      "epoch": 0.009417435073536325,
      "grad_norm": 3.1336069107055664,
      "learning_rate": 0.0004058707913177235,
      "loss": 0.8541,
      "step": 2204
    },
    {
      "epoch": 0.009421707956963518,
      "grad_norm": 1.4839179515838623,
      "learning_rate": 0.0004058280635788754,
      "loss": 1.0019,
      "step": 2205
    },
    {
      "epoch": 0.009425980840390712,
      "grad_norm": 2.1233270168304443,
      "learning_rate": 0.00040578533584002737,
      "loss": 0.5335,
      "step": 2206
    },
    {
      "epoch": 0.009430253723817906,
      "grad_norm": 0.6627354621887207,
      "learning_rate": 0.0004057426081011793,
      "loss": 0.273,
      "step": 2207
    },
    {
      "epoch": 0.009434526607245102,
      "grad_norm": 3.606961727142334,
      "learning_rate": 0.00040569988036233125,
      "loss": 1.0952,
      "step": 2208
    },
    {
      "epoch": 0.009438799490672296,
      "grad_norm": 0.40643852949142456,
      "learning_rate": 0.00040565715262348316,
      "loss": 0.1445,
      "step": 2209
    },
    {
      "epoch": 0.00944307237409949,
      "grad_norm": 1.6314725875854492,
      "learning_rate": 0.0004056144248846351,
      "loss": 0.5573,
      "step": 2210
    },
    {
      "epoch": 0.009447345257526683,
      "grad_norm": 1.4626284837722778,
      "learning_rate": 0.0004055716971457871,
      "loss": 0.8957,
      "step": 2211
    },
    {
      "epoch": 0.009451618140953879,
      "grad_norm": 1.9261740446090698,
      "learning_rate": 0.000405528969406939,
      "loss": 0.182,
      "step": 2212
    },
    {
      "epoch": 0.009455891024381073,
      "grad_norm": 5.004432201385498,
      "learning_rate": 0.00040548624166809096,
      "loss": 1.7926,
      "step": 2213
    },
    {
      "epoch": 0.009460163907808267,
      "grad_norm": 2.026787042617798,
      "learning_rate": 0.00040544351392924287,
      "loss": 0.7028,
      "step": 2214
    },
    {
      "epoch": 0.009464436791235462,
      "grad_norm": 4.532351970672607,
      "learning_rate": 0.00040540078619039484,
      "loss": 2.352,
      "step": 2215
    },
    {
      "epoch": 0.009468709674662656,
      "grad_norm": 2.1840124130249023,
      "learning_rate": 0.00040535805845154675,
      "loss": 0.6618,
      "step": 2216
    },
    {
      "epoch": 0.00947298255808985,
      "grad_norm": 6.26666259765625,
      "learning_rate": 0.0004053153307126987,
      "loss": 2.2324,
      "step": 2217
    },
    {
      "epoch": 0.009477255441517044,
      "grad_norm": 2.095703363418579,
      "learning_rate": 0.0004052726029738507,
      "loss": 0.7146,
      "step": 2218
    },
    {
      "epoch": 0.00948152832494424,
      "grad_norm": 1.4712600708007812,
      "learning_rate": 0.0004052298752350026,
      "loss": 0.7725,
      "step": 2219
    },
    {
      "epoch": 0.009485801208371433,
      "grad_norm": 1.0404542684555054,
      "learning_rate": 0.0004051871474961545,
      "loss": 0.3231,
      "step": 2220
    },
    {
      "epoch": 0.009490074091798627,
      "grad_norm": 1.0874016284942627,
      "learning_rate": 0.0004051444197573064,
      "loss": 0.3229,
      "step": 2221
    },
    {
      "epoch": 0.009494346975225823,
      "grad_norm": 1.437177300453186,
      "learning_rate": 0.0004051016920184584,
      "loss": 0.3669,
      "step": 2222
    },
    {
      "epoch": 0.009498619858653017,
      "grad_norm": 1.523470163345337,
      "learning_rate": 0.0004050589642796103,
      "loss": 0.6262,
      "step": 2223
    },
    {
      "epoch": 0.00950289274208021,
      "grad_norm": 2.047671318054199,
      "learning_rate": 0.00040501623654076225,
      "loss": 0.4999,
      "step": 2224
    },
    {
      "epoch": 0.009507165625507404,
      "grad_norm": 3.7898571491241455,
      "learning_rate": 0.0004049735088019142,
      "loss": 1.3461,
      "step": 2225
    },
    {
      "epoch": 0.0095114385089346,
      "grad_norm": 5.09990930557251,
      "learning_rate": 0.0004049307810630661,
      "loss": 1.094,
      "step": 2226
    },
    {
      "epoch": 0.009515711392361794,
      "grad_norm": 4.043152332305908,
      "learning_rate": 0.0004048880533242181,
      "loss": 1.2039,
      "step": 2227
    },
    {
      "epoch": 0.009519984275788988,
      "grad_norm": 2.163980722427368,
      "learning_rate": 0.00040484532558537,
      "loss": 0.6292,
      "step": 2228
    },
    {
      "epoch": 0.009524257159216181,
      "grad_norm": 1.5627444982528687,
      "learning_rate": 0.00040480259784652197,
      "loss": 0.6314,
      "step": 2229
    },
    {
      "epoch": 0.009528530042643377,
      "grad_norm": 2.00236439704895,
      "learning_rate": 0.0004047598701076739,
      "loss": 0.6559,
      "step": 2230
    },
    {
      "epoch": 0.00953280292607057,
      "grad_norm": 2.96152663230896,
      "learning_rate": 0.00040471714236882584,
      "loss": 0.3344,
      "step": 2231
    },
    {
      "epoch": 0.009537075809497765,
      "grad_norm": 1.862276554107666,
      "learning_rate": 0.0004046744146299778,
      "loss": 0.3701,
      "step": 2232
    },
    {
      "epoch": 0.00954134869292496,
      "grad_norm": 4.314467906951904,
      "learning_rate": 0.0004046316868911297,
      "loss": 1.2015,
      "step": 2233
    },
    {
      "epoch": 0.009545621576352154,
      "grad_norm": 0.8875663876533508,
      "learning_rate": 0.0004045889591522817,
      "loss": 0.2946,
      "step": 2234
    },
    {
      "epoch": 0.009549894459779348,
      "grad_norm": 0.6761209964752197,
      "learning_rate": 0.0004045462314134336,
      "loss": 0.1927,
      "step": 2235
    },
    {
      "epoch": 0.009554167343206542,
      "grad_norm": 1.7983800172805786,
      "learning_rate": 0.00040450350367458556,
      "loss": 0.4339,
      "step": 2236
    },
    {
      "epoch": 0.009558440226633737,
      "grad_norm": 0.6710881590843201,
      "learning_rate": 0.00040446077593573747,
      "loss": 0.1802,
      "step": 2237
    },
    {
      "epoch": 0.009562713110060931,
      "grad_norm": 1.9730507135391235,
      "learning_rate": 0.00040441804819688943,
      "loss": 0.7828,
      "step": 2238
    },
    {
      "epoch": 0.009566985993488125,
      "grad_norm": 2.45328426361084,
      "learning_rate": 0.0004043753204580414,
      "loss": 0.725,
      "step": 2239
    },
    {
      "epoch": 0.00957125887691532,
      "grad_norm": 3.053398847579956,
      "learning_rate": 0.0004043325927191933,
      "loss": 0.8064,
      "step": 2240
    },
    {
      "epoch": 0.009575531760342515,
      "grad_norm": 1.8665080070495605,
      "learning_rate": 0.0004042898649803453,
      "loss": 0.5935,
      "step": 2241
    },
    {
      "epoch": 0.009579804643769708,
      "grad_norm": 3.172175884246826,
      "learning_rate": 0.0004042471372414972,
      "loss": 0.976,
      "step": 2242
    },
    {
      "epoch": 0.009584077527196902,
      "grad_norm": 2.9417200088500977,
      "learning_rate": 0.00040420440950264915,
      "loss": 0.6941,
      "step": 2243
    },
    {
      "epoch": 0.009588350410624098,
      "grad_norm": 2.610043525695801,
      "learning_rate": 0.00040416168176380106,
      "loss": 0.8943,
      "step": 2244
    },
    {
      "epoch": 0.009592623294051292,
      "grad_norm": 0.6898647546768188,
      "learning_rate": 0.000404118954024953,
      "loss": 0.185,
      "step": 2245
    },
    {
      "epoch": 0.009596896177478486,
      "grad_norm": 1.8365273475646973,
      "learning_rate": 0.00040407622628610494,
      "loss": 0.4996,
      "step": 2246
    },
    {
      "epoch": 0.009601169060905681,
      "grad_norm": 2.5545403957366943,
      "learning_rate": 0.0004040334985472569,
      "loss": 0.8384,
      "step": 2247
    },
    {
      "epoch": 0.009605441944332875,
      "grad_norm": 2.2124264240264893,
      "learning_rate": 0.00040399077080840887,
      "loss": 0.7075,
      "step": 2248
    },
    {
      "epoch": 0.009609714827760069,
      "grad_norm": 0.9270886182785034,
      "learning_rate": 0.0004039480430695608,
      "loss": 0.3376,
      "step": 2249
    },
    {
      "epoch": 0.009613987711187263,
      "grad_norm": 2.397083282470703,
      "learning_rate": 0.00040390531533071274,
      "loss": 0.7025,
      "step": 2250
    },
    {
      "epoch": 0.009618260594614458,
      "grad_norm": 3.3520541191101074,
      "learning_rate": 0.00040386258759186465,
      "loss": 0.7796,
      "step": 2251
    },
    {
      "epoch": 0.009622533478041652,
      "grad_norm": 2.6835131645202637,
      "learning_rate": 0.0004038198598530166,
      "loss": 0.7241,
      "step": 2252
    },
    {
      "epoch": 0.009626806361468846,
      "grad_norm": 0.8356049060821533,
      "learning_rate": 0.0004037771321141685,
      "loss": 0.2869,
      "step": 2253
    },
    {
      "epoch": 0.00963107924489604,
      "grad_norm": 2.6638946533203125,
      "learning_rate": 0.00040373440437532044,
      "loss": 0.6874,
      "step": 2254
    },
    {
      "epoch": 0.009635352128323236,
      "grad_norm": 1.811729073524475,
      "learning_rate": 0.0004036916766364724,
      "loss": 0.6215,
      "step": 2255
    },
    {
      "epoch": 0.00963962501175043,
      "grad_norm": 2.645700693130493,
      "learning_rate": 0.0004036489488976243,
      "loss": 0.7311,
      "step": 2256
    },
    {
      "epoch": 0.009643897895177623,
      "grad_norm": 5.600140571594238,
      "learning_rate": 0.0004036062211587763,
      "loss": 1.2763,
      "step": 2257
    },
    {
      "epoch": 0.009648170778604819,
      "grad_norm": 1.7905174493789673,
      "learning_rate": 0.0004035634934199282,
      "loss": 0.4196,
      "step": 2258
    },
    {
      "epoch": 0.009652443662032013,
      "grad_norm": 43.12307357788086,
      "learning_rate": 0.00040352076568108016,
      "loss": 4.3279,
      "step": 2259
    },
    {
      "epoch": 0.009656716545459207,
      "grad_norm": 2.2987470626831055,
      "learning_rate": 0.00040347803794223207,
      "loss": 0.5244,
      "step": 2260
    },
    {
      "epoch": 0.0096609894288864,
      "grad_norm": 3.5244715213775635,
      "learning_rate": 0.00040343531020338403,
      "loss": 1.288,
      "step": 2261
    },
    {
      "epoch": 0.009665262312313596,
      "grad_norm": 5.297297477722168,
      "learning_rate": 0.000403392582464536,
      "loss": 1.6326,
      "step": 2262
    },
    {
      "epoch": 0.00966953519574079,
      "grad_norm": 1.5795166492462158,
      "learning_rate": 0.0004033498547256879,
      "loss": 0.4257,
      "step": 2263
    },
    {
      "epoch": 0.009673808079167984,
      "grad_norm": 3.498654365539551,
      "learning_rate": 0.0004033071269868399,
      "loss": 1.1931,
      "step": 2264
    },
    {
      "epoch": 0.00967808096259518,
      "grad_norm": 4.16067361831665,
      "learning_rate": 0.0004032643992479918,
      "loss": 1.4053,
      "step": 2265
    },
    {
      "epoch": 0.009682353846022373,
      "grad_norm": 2.200625419616699,
      "learning_rate": 0.00040322167150914375,
      "loss": 0.4737,
      "step": 2266
    },
    {
      "epoch": 0.009686626729449567,
      "grad_norm": 1.715570092201233,
      "learning_rate": 0.00040317894377029566,
      "loss": 0.3558,
      "step": 2267
    },
    {
      "epoch": 0.00969089961287676,
      "grad_norm": 5.239670276641846,
      "learning_rate": 0.0004031362160314476,
      "loss": 1.594,
      "step": 2268
    },
    {
      "epoch": 0.009695172496303956,
      "grad_norm": 1.2382750511169434,
      "learning_rate": 0.0004030934882925996,
      "loss": 0.3671,
      "step": 2269
    },
    {
      "epoch": 0.00969944537973115,
      "grad_norm": 1.2055556774139404,
      "learning_rate": 0.0004030507605537515,
      "loss": 0.2565,
      "step": 2270
    },
    {
      "epoch": 0.009703718263158344,
      "grad_norm": 1.629043459892273,
      "learning_rate": 0.00040300803281490346,
      "loss": 0.4955,
      "step": 2271
    },
    {
      "epoch": 0.009707991146585538,
      "grad_norm": 2.41638445854187,
      "learning_rate": 0.0004029653050760554,
      "loss": 0.619,
      "step": 2272
    },
    {
      "epoch": 0.009712264030012734,
      "grad_norm": 2.3499670028686523,
      "learning_rate": 0.00040292257733720734,
      "loss": 0.5983,
      "step": 2273
    },
    {
      "epoch": 0.009716536913439927,
      "grad_norm": 1.5944838523864746,
      "learning_rate": 0.00040287984959835925,
      "loss": 0.582,
      "step": 2274
    },
    {
      "epoch": 0.009720809796867121,
      "grad_norm": 1.507851004600525,
      "learning_rate": 0.0004028371218595112,
      "loss": 0.5417,
      "step": 2275
    },
    {
      "epoch": 0.009725082680294317,
      "grad_norm": 1.8428786993026733,
      "learning_rate": 0.0004027943941206632,
      "loss": 0.4679,
      "step": 2276
    },
    {
      "epoch": 0.00972935556372151,
      "grad_norm": 4.368420600891113,
      "learning_rate": 0.0004027516663818151,
      "loss": 1.1515,
      "step": 2277
    },
    {
      "epoch": 0.009733628447148705,
      "grad_norm": 1.5018303394317627,
      "learning_rate": 0.00040270893864296706,
      "loss": 0.5327,
      "step": 2278
    },
    {
      "epoch": 0.009737901330575898,
      "grad_norm": 1.3195472955703735,
      "learning_rate": 0.00040266621090411897,
      "loss": 0.4208,
      "step": 2279
    },
    {
      "epoch": 0.009742174214003094,
      "grad_norm": 1.690374732017517,
      "learning_rate": 0.00040262348316527093,
      "loss": 0.3839,
      "step": 2280
    },
    {
      "epoch": 0.009746447097430288,
      "grad_norm": 2.9945149421691895,
      "learning_rate": 0.00040258075542642284,
      "loss": 0.7597,
      "step": 2281
    },
    {
      "epoch": 0.009750719980857482,
      "grad_norm": 1.3611215353012085,
      "learning_rate": 0.0004025380276875748,
      "loss": 0.3412,
      "step": 2282
    },
    {
      "epoch": 0.009754992864284677,
      "grad_norm": 1.6886141300201416,
      "learning_rate": 0.0004024952999487267,
      "loss": 0.5432,
      "step": 2283
    },
    {
      "epoch": 0.009759265747711871,
      "grad_norm": 1.4596318006515503,
      "learning_rate": 0.0004024525722098787,
      "loss": 0.4174,
      "step": 2284
    },
    {
      "epoch": 0.009763538631139065,
      "grad_norm": 5.9412922859191895,
      "learning_rate": 0.00040240984447103065,
      "loss": 2.006,
      "step": 2285
    },
    {
      "epoch": 0.009767811514566259,
      "grad_norm": 1.3300938606262207,
      "learning_rate": 0.0004023671167321825,
      "loss": 0.4021,
      "step": 2286
    },
    {
      "epoch": 0.009772084397993454,
      "grad_norm": 2.7702243328094482,
      "learning_rate": 0.00040232438899333447,
      "loss": 0.7517,
      "step": 2287
    },
    {
      "epoch": 0.009776357281420648,
      "grad_norm": 1.6719146966934204,
      "learning_rate": 0.0004022816612544864,
      "loss": 0.5922,
      "step": 2288
    },
    {
      "epoch": 0.009780630164847842,
      "grad_norm": 4.012261390686035,
      "learning_rate": 0.00040223893351563835,
      "loss": 1.3621,
      "step": 2289
    },
    {
      "epoch": 0.009784903048275038,
      "grad_norm": 1.326903223991394,
      "learning_rate": 0.00040219620577679026,
      "loss": 0.3358,
      "step": 2290
    },
    {
      "epoch": 0.009789175931702232,
      "grad_norm": 1.1939724683761597,
      "learning_rate": 0.0004021534780379422,
      "loss": 0.3055,
      "step": 2291
    },
    {
      "epoch": 0.009793448815129425,
      "grad_norm": 1.4177696704864502,
      "learning_rate": 0.0004021107502990942,
      "loss": 0.3415,
      "step": 2292
    },
    {
      "epoch": 0.00979772169855662,
      "grad_norm": 4.157130241394043,
      "learning_rate": 0.0004020680225602461,
      "loss": 1.0544,
      "step": 2293
    },
    {
      "epoch": 0.009801994581983815,
      "grad_norm": 2.0248851776123047,
      "learning_rate": 0.00040202529482139806,
      "loss": 0.5045,
      "step": 2294
    },
    {
      "epoch": 0.009806267465411009,
      "grad_norm": 6.600478649139404,
      "learning_rate": 0.00040198256708255,
      "loss": 2.2637,
      "step": 2295
    },
    {
      "epoch": 0.009810540348838203,
      "grad_norm": 0.5344017148017883,
      "learning_rate": 0.00040193983934370194,
      "loss": 0.1681,
      "step": 2296
    },
    {
      "epoch": 0.009814813232265397,
      "grad_norm": 4.133338451385498,
      "learning_rate": 0.00040189711160485385,
      "loss": 0.868,
      "step": 2297
    },
    {
      "epoch": 0.009819086115692592,
      "grad_norm": 2.3497400283813477,
      "learning_rate": 0.0004018543838660058,
      "loss": 0.6245,
      "step": 2298
    },
    {
      "epoch": 0.009823358999119786,
      "grad_norm": 0.7367721796035767,
      "learning_rate": 0.0004018116561271578,
      "loss": 0.2776,
      "step": 2299
    },
    {
      "epoch": 0.00982763188254698,
      "grad_norm": 5.231048583984375,
      "learning_rate": 0.0004017689283883097,
      "loss": 1.7493,
      "step": 2300
    },
    {
      "epoch": 0.009831904765974175,
      "grad_norm": 5.197198867797852,
      "learning_rate": 0.00040172620064946165,
      "loss": 1.5733,
      "step": 2301
    },
    {
      "epoch": 0.00983617764940137,
      "grad_norm": 2.041133165359497,
      "learning_rate": 0.00040168347291061357,
      "loss": 0.5429,
      "step": 2302
    },
    {
      "epoch": 0.009840450532828563,
      "grad_norm": 1.9786673784255981,
      "learning_rate": 0.00040164074517176553,
      "loss": 0.4749,
      "step": 2303
    },
    {
      "epoch": 0.009844723416255757,
      "grad_norm": 5.164703845977783,
      "learning_rate": 0.00040159801743291744,
      "loss": 1.5325,
      "step": 2304
    },
    {
      "epoch": 0.009848996299682953,
      "grad_norm": 4.232532501220703,
      "learning_rate": 0.0004015552896940694,
      "loss": 1.2564,
      "step": 2305
    },
    {
      "epoch": 0.009853269183110146,
      "grad_norm": 0.8742725253105164,
      "learning_rate": 0.00040151256195522137,
      "loss": 0.3051,
      "step": 2306
    },
    {
      "epoch": 0.00985754206653734,
      "grad_norm": 2.0980515480041504,
      "learning_rate": 0.0004014698342163733,
      "loss": 0.4819,
      "step": 2307
    },
    {
      "epoch": 0.009861814949964536,
      "grad_norm": 2.5081160068511963,
      "learning_rate": 0.00040142710647752525,
      "loss": 0.5268,
      "step": 2308
    },
    {
      "epoch": 0.00986608783339173,
      "grad_norm": 1.827600359916687,
      "learning_rate": 0.00040138437873867716,
      "loss": 0.4885,
      "step": 2309
    },
    {
      "epoch": 0.009870360716818924,
      "grad_norm": 1.8228740692138672,
      "learning_rate": 0.0004013416509998291,
      "loss": 0.4311,
      "step": 2310
    },
    {
      "epoch": 0.009874633600246117,
      "grad_norm": 1.649472951889038,
      "learning_rate": 0.00040129892326098103,
      "loss": 0.6522,
      "step": 2311
    },
    {
      "epoch": 0.009878906483673313,
      "grad_norm": 1.6036628484725952,
      "learning_rate": 0.000401256195522133,
      "loss": 0.4304,
      "step": 2312
    },
    {
      "epoch": 0.009883179367100507,
      "grad_norm": 4.7677741050720215,
      "learning_rate": 0.0004012134677832849,
      "loss": 0.9288,
      "step": 2313
    },
    {
      "epoch": 0.0098874522505277,
      "grad_norm": 4.6005353927612305,
      "learning_rate": 0.0004011707400444369,
      "loss": 1.3512,
      "step": 2314
    },
    {
      "epoch": 0.009891725133954896,
      "grad_norm": 3.610131025314331,
      "learning_rate": 0.00040112801230558884,
      "loss": 1.3455,
      "step": 2315
    },
    {
      "epoch": 0.00989599801738209,
      "grad_norm": 4.515862464904785,
      "learning_rate": 0.00040108528456674075,
      "loss": 1.2654,
      "step": 2316
    },
    {
      "epoch": 0.009900270900809284,
      "grad_norm": 1.0908197164535522,
      "learning_rate": 0.0004010425568278927,
      "loss": 0.3195,
      "step": 2317
    },
    {
      "epoch": 0.009904543784236478,
      "grad_norm": 3.16867995262146,
      "learning_rate": 0.0004009998290890446,
      "loss": 1.1734,
      "step": 2318
    },
    {
      "epoch": 0.009908816667663673,
      "grad_norm": 2.8048319816589355,
      "learning_rate": 0.00040095710135019654,
      "loss": 0.9246,
      "step": 2319
    },
    {
      "epoch": 0.009913089551090867,
      "grad_norm": 4.436756134033203,
      "learning_rate": 0.00040091437361134845,
      "loss": 1.3123,
      "step": 2320
    },
    {
      "epoch": 0.009917362434518061,
      "grad_norm": 1.6574616432189941,
      "learning_rate": 0.0004008716458725004,
      "loss": 0.4755,
      "step": 2321
    },
    {
      "epoch": 0.009921635317945255,
      "grad_norm": 2.269634246826172,
      "learning_rate": 0.0004008289181336524,
      "loss": 0.8378,
      "step": 2322
    },
    {
      "epoch": 0.00992590820137245,
      "grad_norm": 2.9617087841033936,
      "learning_rate": 0.0004007861903948043,
      "loss": 1.2564,
      "step": 2323
    },
    {
      "epoch": 0.009930181084799644,
      "grad_norm": 1.52312433719635,
      "learning_rate": 0.00040074346265595625,
      "loss": 0.6539,
      "step": 2324
    },
    {
      "epoch": 0.009934453968226838,
      "grad_norm": 0.6524385213851929,
      "learning_rate": 0.00040070073491710816,
      "loss": 0.1992,
      "step": 2325
    },
    {
      "epoch": 0.009938726851654034,
      "grad_norm": 4.417508602142334,
      "learning_rate": 0.00040065800717826013,
      "loss": 1.6093,
      "step": 2326
    },
    {
      "epoch": 0.009942999735081228,
      "grad_norm": 2.968820571899414,
      "learning_rate": 0.00040061527943941204,
      "loss": 0.9226,
      "step": 2327
    },
    {
      "epoch": 0.009947272618508422,
      "grad_norm": 4.38751745223999,
      "learning_rate": 0.000400572551700564,
      "loss": 1.4744,
      "step": 2328
    },
    {
      "epoch": 0.009951545501935615,
      "grad_norm": 0.4745495319366455,
      "learning_rate": 0.00040052982396171597,
      "loss": 0.1489,
      "step": 2329
    },
    {
      "epoch": 0.009955818385362811,
      "grad_norm": 1.6704376935958862,
      "learning_rate": 0.0004004870962228679,
      "loss": 0.5928,
      "step": 2330
    },
    {
      "epoch": 0.009960091268790005,
      "grad_norm": 2.8322060108184814,
      "learning_rate": 0.00040044436848401984,
      "loss": 0.9826,
      "step": 2331
    },
    {
      "epoch": 0.009964364152217199,
      "grad_norm": 1.356084942817688,
      "learning_rate": 0.00040040164074517175,
      "loss": 0.5122,
      "step": 2332
    },
    {
      "epoch": 0.009968637035644394,
      "grad_norm": 3.9193942546844482,
      "learning_rate": 0.0004003589130063237,
      "loss": 1.1997,
      "step": 2333
    },
    {
      "epoch": 0.009972909919071588,
      "grad_norm": 1.2506319284439087,
      "learning_rate": 0.00040031618526747563,
      "loss": 0.4358,
      "step": 2334
    },
    {
      "epoch": 0.009977182802498782,
      "grad_norm": 3.971315860748291,
      "learning_rate": 0.0004002734575286276,
      "loss": 1.1963,
      "step": 2335
    },
    {
      "epoch": 0.009981455685925976,
      "grad_norm": 2.160900354385376,
      "learning_rate": 0.00040023072978977956,
      "loss": 0.7329,
      "step": 2336
    },
    {
      "epoch": 0.009985728569353172,
      "grad_norm": 3.548584461212158,
      "learning_rate": 0.00040018800205093147,
      "loss": 1.1406,
      "step": 2337
    },
    {
      "epoch": 0.009990001452780365,
      "grad_norm": 3.6147356033325195,
      "learning_rate": 0.00040014527431208344,
      "loss": 1.1882,
      "step": 2338
    },
    {
      "epoch": 0.00999427433620756,
      "grad_norm": 2.215709686279297,
      "learning_rate": 0.00040010254657323535,
      "loss": 0.8612,
      "step": 2339
    },
    {
      "epoch": 0.009998547219634753,
      "grad_norm": 0.7798365354537964,
      "learning_rate": 0.0004000598188343873,
      "loss": 0.3845,
      "step": 2340
    },
    {
      "epoch": 0.010002820103061949,
      "grad_norm": 2.616438388824463,
      "learning_rate": 0.0004000170910955392,
      "loss": 0.7267,
      "step": 2341
    },
    {
      "epoch": 0.010007092986489143,
      "grad_norm": 1.735231876373291,
      "learning_rate": 0.0003999743633566912,
      "loss": 0.5244,
      "step": 2342
    },
    {
      "epoch": 0.010011365869916336,
      "grad_norm": 0.48830923438072205,
      "learning_rate": 0.00039993163561784315,
      "loss": 0.1306,
      "step": 2343
    },
    {
      "epoch": 0.010015638753343532,
      "grad_norm": 2.2403032779693604,
      "learning_rate": 0.00039988890787899506,
      "loss": 0.6153,
      "step": 2344
    },
    {
      "epoch": 0.010019911636770726,
      "grad_norm": 2.4429290294647217,
      "learning_rate": 0.00039984618014014703,
      "loss": 1.0271,
      "step": 2345
    },
    {
      "epoch": 0.01002418452019792,
      "grad_norm": 1.5765659809112549,
      "learning_rate": 0.00039980345240129894,
      "loss": 0.4331,
      "step": 2346
    },
    {
      "epoch": 0.010028457403625114,
      "grad_norm": 0.5462549328804016,
      "learning_rate": 0.0003997607246624509,
      "loss": 0.1528,
      "step": 2347
    },
    {
      "epoch": 0.010032730287052309,
      "grad_norm": 2.1840691566467285,
      "learning_rate": 0.0003997179969236028,
      "loss": 0.5649,
      "step": 2348
    },
    {
      "epoch": 0.010037003170479503,
      "grad_norm": 3.599257230758667,
      "learning_rate": 0.0003996752691847548,
      "loss": 1.0981,
      "step": 2349
    },
    {
      "epoch": 0.010041276053906697,
      "grad_norm": 1.5394943952560425,
      "learning_rate": 0.0003996325414459067,
      "loss": 0.6158,
      "step": 2350
    },
    {
      "epoch": 0.010045548937333892,
      "grad_norm": 1.8052961826324463,
      "learning_rate": 0.00039958981370705865,
      "loss": 0.738,
      "step": 2351
    },
    {
      "epoch": 0.010049821820761086,
      "grad_norm": 1.6918045282363892,
      "learning_rate": 0.00039954708596821057,
      "loss": 0.6452,
      "step": 2352
    },
    {
      "epoch": 0.01005409470418828,
      "grad_norm": 0.7830593585968018,
      "learning_rate": 0.0003995043582293625,
      "loss": 0.3482,
      "step": 2353
    },
    {
      "epoch": 0.010058367587615474,
      "grad_norm": 1.2396177053451538,
      "learning_rate": 0.00039946163049051444,
      "loss": 0.3848,
      "step": 2354
    },
    {
      "epoch": 0.01006264047104267,
      "grad_norm": 0.8836748003959656,
      "learning_rate": 0.00039941890275166635,
      "loss": 0.4078,
      "step": 2355
    },
    {
      "epoch": 0.010066913354469863,
      "grad_norm": 3.119659185409546,
      "learning_rate": 0.0003993761750128183,
      "loss": 0.9263,
      "step": 2356
    },
    {
      "epoch": 0.010071186237897057,
      "grad_norm": 3.058490037918091,
      "learning_rate": 0.00039933344727397023,
      "loss": 0.9309,
      "step": 2357
    },
    {
      "epoch": 0.010075459121324253,
      "grad_norm": 0.927253782749176,
      "learning_rate": 0.0003992907195351222,
      "loss": 0.3104,
      "step": 2358
    },
    {
      "epoch": 0.010079732004751447,
      "grad_norm": 0.8614965677261353,
      "learning_rate": 0.00039924799179627416,
      "loss": 0.2945,
      "step": 2359
    },
    {
      "epoch": 0.01008400488817864,
      "grad_norm": 1.9561752080917358,
      "learning_rate": 0.00039920526405742607,
      "loss": 0.6701,
      "step": 2360
    },
    {
      "epoch": 0.010088277771605834,
      "grad_norm": 1.6631160974502563,
      "learning_rate": 0.00039916253631857803,
      "loss": 0.5174,
      "step": 2361
    },
    {
      "epoch": 0.01009255065503303,
      "grad_norm": 3.676992893218994,
      "learning_rate": 0.00039911980857972994,
      "loss": 0.7532,
      "step": 2362
    },
    {
      "epoch": 0.010096823538460224,
      "grad_norm": 0.8356866836547852,
      "learning_rate": 0.0003990770808408819,
      "loss": 0.292,
      "step": 2363
    },
    {
      "epoch": 0.010101096421887418,
      "grad_norm": 3.7479491233825684,
      "learning_rate": 0.0003990343531020338,
      "loss": 1.3072,
      "step": 2364
    },
    {
      "epoch": 0.010105369305314612,
      "grad_norm": 1.8986918926239014,
      "learning_rate": 0.0003989916253631858,
      "loss": 0.6633,
      "step": 2365
    },
    {
      "epoch": 0.010109642188741807,
      "grad_norm": 0.5693468451499939,
      "learning_rate": 0.00039894889762433775,
      "loss": 0.1888,
      "step": 2366
    },
    {
      "epoch": 0.010113915072169001,
      "grad_norm": 2.577045440673828,
      "learning_rate": 0.00039890616988548966,
      "loss": 0.6384,
      "step": 2367
    },
    {
      "epoch": 0.010118187955596195,
      "grad_norm": 2.0139079093933105,
      "learning_rate": 0.0003988634421466416,
      "loss": 0.689,
      "step": 2368
    },
    {
      "epoch": 0.01012246083902339,
      "grad_norm": 3.59967041015625,
      "learning_rate": 0.00039882071440779354,
      "loss": 1.0416,
      "step": 2369
    },
    {
      "epoch": 0.010126733722450584,
      "grad_norm": 1.8767282962799072,
      "learning_rate": 0.0003987779866689455,
      "loss": 0.7333,
      "step": 2370
    },
    {
      "epoch": 0.010131006605877778,
      "grad_norm": 1.880184531211853,
      "learning_rate": 0.0003987352589300974,
      "loss": 0.9588,
      "step": 2371
    },
    {
      "epoch": 0.010135279489304972,
      "grad_norm": 1.9311858415603638,
      "learning_rate": 0.0003986925311912494,
      "loss": 0.6713,
      "step": 2372
    },
    {
      "epoch": 0.010139552372732168,
      "grad_norm": 0.6814453601837158,
      "learning_rate": 0.00039864980345240134,
      "loss": 0.2388,
      "step": 2373
    },
    {
      "epoch": 0.010143825256159361,
      "grad_norm": 0.697605550289154,
      "learning_rate": 0.00039860707571355325,
      "loss": 0.2224,
      "step": 2374
    },
    {
      "epoch": 0.010148098139586555,
      "grad_norm": 3.691850185394287,
      "learning_rate": 0.0003985643479747052,
      "loss": 1.1005,
      "step": 2375
    },
    {
      "epoch": 0.010152371023013751,
      "grad_norm": 3.5127413272857666,
      "learning_rate": 0.00039852162023585713,
      "loss": 0.8675,
      "step": 2376
    },
    {
      "epoch": 0.010156643906440945,
      "grad_norm": 0.5939238667488098,
      "learning_rate": 0.0003984788924970091,
      "loss": 0.1629,
      "step": 2377
    },
    {
      "epoch": 0.010160916789868139,
      "grad_norm": 1.6253854036331177,
      "learning_rate": 0.000398436164758161,
      "loss": 0.5133,
      "step": 2378
    },
    {
      "epoch": 0.010165189673295333,
      "grad_norm": 4.545941352844238,
      "learning_rate": 0.00039839343701931297,
      "loss": 1.454,
      "step": 2379
    },
    {
      "epoch": 0.010169462556722528,
      "grad_norm": 1.9540634155273438,
      "learning_rate": 0.00039835070928046493,
      "loss": 0.6532,
      "step": 2380
    },
    {
      "epoch": 0.010173735440149722,
      "grad_norm": 0.6730220913887024,
      "learning_rate": 0.00039830798154161684,
      "loss": 0.1787,
      "step": 2381
    },
    {
      "epoch": 0.010178008323576916,
      "grad_norm": 2.9336323738098145,
      "learning_rate": 0.0003982652538027688,
      "loss": 0.8463,
      "step": 2382
    },
    {
      "epoch": 0.01018228120700411,
      "grad_norm": 3.571463108062744,
      "learning_rate": 0.0003982225260639207,
      "loss": 1.1647,
      "step": 2383
    },
    {
      "epoch": 0.010186554090431305,
      "grad_norm": 1.8838486671447754,
      "learning_rate": 0.0003981797983250727,
      "loss": 0.3997,
      "step": 2384
    },
    {
      "epoch": 0.010190826973858499,
      "grad_norm": 2.2643816471099854,
      "learning_rate": 0.00039813707058622454,
      "loss": 0.7566,
      "step": 2385
    },
    {
      "epoch": 0.010195099857285693,
      "grad_norm": 2.1643295288085938,
      "learning_rate": 0.0003980943428473765,
      "loss": 0.7816,
      "step": 2386
    },
    {
      "epoch": 0.010199372740712889,
      "grad_norm": 1.8175921440124512,
      "learning_rate": 0.0003980516151085284,
      "loss": 0.5757,
      "step": 2387
    },
    {
      "epoch": 0.010203645624140082,
      "grad_norm": 1.5572165250778198,
      "learning_rate": 0.0003980088873696804,
      "loss": 0.4359,
      "step": 2388
    },
    {
      "epoch": 0.010207918507567276,
      "grad_norm": 2.6794118881225586,
      "learning_rate": 0.00039796615963083235,
      "loss": 0.8751,
      "step": 2389
    },
    {
      "epoch": 0.01021219139099447,
      "grad_norm": 1.1594014167785645,
      "learning_rate": 0.00039792343189198426,
      "loss": 0.3637,
      "step": 2390
    },
    {
      "epoch": 0.010216464274421666,
      "grad_norm": 0.9369937777519226,
      "learning_rate": 0.0003978807041531362,
      "loss": 0.2405,
      "step": 2391
    },
    {
      "epoch": 0.01022073715784886,
      "grad_norm": 1.2968330383300781,
      "learning_rate": 0.00039783797641428813,
      "loss": 0.3658,
      "step": 2392
    },
    {
      "epoch": 0.010225010041276053,
      "grad_norm": 2.646179676055908,
      "learning_rate": 0.0003977952486754401,
      "loss": 0.8159,
      "step": 2393
    },
    {
      "epoch": 0.010229282924703249,
      "grad_norm": 1.1252961158752441,
      "learning_rate": 0.000397752520936592,
      "loss": 0.3634,
      "step": 2394
    },
    {
      "epoch": 0.010233555808130443,
      "grad_norm": 1.126758098602295,
      "learning_rate": 0.000397709793197744,
      "loss": 0.3652,
      "step": 2395
    },
    {
      "epoch": 0.010237828691557637,
      "grad_norm": 2.267615556716919,
      "learning_rate": 0.00039766706545889594,
      "loss": 0.7136,
      "step": 2396
    },
    {
      "epoch": 0.01024210157498483,
      "grad_norm": 3.0494282245635986,
      "learning_rate": 0.00039762433772004785,
      "loss": 1.1146,
      "step": 2397
    },
    {
      "epoch": 0.010246374458412026,
      "grad_norm": 0.8594717383384705,
      "learning_rate": 0.0003975816099811998,
      "loss": 0.2538,
      "step": 2398
    },
    {
      "epoch": 0.01025064734183922,
      "grad_norm": 2.343055486679077,
      "learning_rate": 0.0003975388822423517,
      "loss": 0.6973,
      "step": 2399
    },
    {
      "epoch": 0.010254920225266414,
      "grad_norm": 0.8314029574394226,
      "learning_rate": 0.0003974961545035037,
      "loss": 0.2878,
      "step": 2400
    },
    {
      "epoch": 0.01025919310869361,
      "grad_norm": 2.883610248565674,
      "learning_rate": 0.0003974534267646556,
      "loss": 0.8736,
      "step": 2401
    },
    {
      "epoch": 0.010263465992120803,
      "grad_norm": 4.547396183013916,
      "learning_rate": 0.00039741069902580757,
      "loss": 1.4149,
      "step": 2402
    },
    {
      "epoch": 0.010267738875547997,
      "grad_norm": 1.4060282707214355,
      "learning_rate": 0.00039736797128695953,
      "loss": 0.5007,
      "step": 2403
    },
    {
      "epoch": 0.010272011758975191,
      "grad_norm": 2.413713216781616,
      "learning_rate": 0.00039732524354811144,
      "loss": 0.6402,
      "step": 2404
    },
    {
      "epoch": 0.010276284642402387,
      "grad_norm": 4.203840255737305,
      "learning_rate": 0.0003972825158092634,
      "loss": 2.8535,
      "step": 2405
    },
    {
      "epoch": 0.01028055752582958,
      "grad_norm": 0.5345556139945984,
      "learning_rate": 0.0003972397880704153,
      "loss": 0.1916,
      "step": 2406
    },
    {
      "epoch": 0.010284830409256774,
      "grad_norm": 1.9841358661651611,
      "learning_rate": 0.0003971970603315673,
      "loss": 0.6233,
      "step": 2407
    },
    {
      "epoch": 0.010289103292683968,
      "grad_norm": 2.077939510345459,
      "learning_rate": 0.0003971543325927192,
      "loss": 0.6284,
      "step": 2408
    },
    {
      "epoch": 0.010293376176111164,
      "grad_norm": 3.1107845306396484,
      "learning_rate": 0.00039711160485387116,
      "loss": 0.7943,
      "step": 2409
    },
    {
      "epoch": 0.010297649059538358,
      "grad_norm": 1.4823404550552368,
      "learning_rate": 0.0003970688771150231,
      "loss": 0.2721,
      "step": 2410
    },
    {
      "epoch": 0.010301921942965551,
      "grad_norm": 2.908057689666748,
      "learning_rate": 0.00039702614937617503,
      "loss": 0.7899,
      "step": 2411
    },
    {
      "epoch": 0.010306194826392747,
      "grad_norm": 2.668877124786377,
      "learning_rate": 0.000396983421637327,
      "loss": 0.6083,
      "step": 2412
    },
    {
      "epoch": 0.010310467709819941,
      "grad_norm": 1.95785653591156,
      "learning_rate": 0.0003969406938984789,
      "loss": 0.7426,
      "step": 2413
    },
    {
      "epoch": 0.010314740593247135,
      "grad_norm": 5.4748759269714355,
      "learning_rate": 0.0003968979661596309,
      "loss": 2.8884,
      "step": 2414
    },
    {
      "epoch": 0.010319013476674329,
      "grad_norm": 0.40824708342552185,
      "learning_rate": 0.0003968552384207828,
      "loss": 0.1334,
      "step": 2415
    },
    {
      "epoch": 0.010323286360101524,
      "grad_norm": 2.1306955814361572,
      "learning_rate": 0.00039681251068193475,
      "loss": 0.6331,
      "step": 2416
    },
    {
      "epoch": 0.010327559243528718,
      "grad_norm": 1.7323598861694336,
      "learning_rate": 0.00039676978294308666,
      "loss": 0.9908,
      "step": 2417
    },
    {
      "epoch": 0.010331832126955912,
      "grad_norm": 0.7605850696563721,
      "learning_rate": 0.00039672705520423857,
      "loss": 0.3493,
      "step": 2418
    },
    {
      "epoch": 0.010336105010383108,
      "grad_norm": 0.4768759310245514,
      "learning_rate": 0.00039668432746539054,
      "loss": 0.1491,
      "step": 2419
    },
    {
      "epoch": 0.010340377893810301,
      "grad_norm": 1.771822214126587,
      "learning_rate": 0.00039664159972654245,
      "loss": 0.5823,
      "step": 2420
    },
    {
      "epoch": 0.010344650777237495,
      "grad_norm": 1.801506757736206,
      "learning_rate": 0.0003965988719876944,
      "loss": 0.5875,
      "step": 2421
    },
    {
      "epoch": 0.010348923660664689,
      "grad_norm": 1.712867021560669,
      "learning_rate": 0.0003965561442488463,
      "loss": 0.9549,
      "step": 2422
    },
    {
      "epoch": 0.010353196544091885,
      "grad_norm": 0.5465852618217468,
      "learning_rate": 0.0003965134165099983,
      "loss": 0.1616,
      "step": 2423
    },
    {
      "epoch": 0.010357469427519079,
      "grad_norm": 3.0288169384002686,
      "learning_rate": 0.0003964706887711502,
      "loss": 1.4849,
      "step": 2424
    },
    {
      "epoch": 0.010361742310946272,
      "grad_norm": 1.6968979835510254,
      "learning_rate": 0.00039642796103230216,
      "loss": 0.6617,
      "step": 2425
    },
    {
      "epoch": 0.010366015194373468,
      "grad_norm": 1.7713087797164917,
      "learning_rate": 0.00039638523329345413,
      "loss": 0.541,
      "step": 2426
    },
    {
      "epoch": 0.010370288077800662,
      "grad_norm": 2.1530582904815674,
      "learning_rate": 0.00039634250555460604,
      "loss": 0.7496,
      "step": 2427
    },
    {
      "epoch": 0.010374560961227856,
      "grad_norm": 2.5526204109191895,
      "learning_rate": 0.000396299777815758,
      "loss": 1.1138,
      "step": 2428
    },
    {
      "epoch": 0.01037883384465505,
      "grad_norm": 0.6937088370323181,
      "learning_rate": 0.0003962570500769099,
      "loss": 0.1862,
      "step": 2429
    },
    {
      "epoch": 0.010383106728082245,
      "grad_norm": 0.6043403744697571,
      "learning_rate": 0.0003962143223380619,
      "loss": 0.1676,
      "step": 2430
    },
    {
      "epoch": 0.010387379611509439,
      "grad_norm": 2.1543431282043457,
      "learning_rate": 0.0003961715945992138,
      "loss": 0.8179,
      "step": 2431
    },
    {
      "epoch": 0.010391652494936633,
      "grad_norm": 0.5931056141853333,
      "learning_rate": 0.00039612886686036576,
      "loss": 0.1589,
      "step": 2432
    },
    {
      "epoch": 0.010395925378363827,
      "grad_norm": 2.223745822906494,
      "learning_rate": 0.0003960861391215177,
      "loss": 0.8491,
      "step": 2433
    },
    {
      "epoch": 0.010400198261791022,
      "grad_norm": 1.8670158386230469,
      "learning_rate": 0.00039604341138266963,
      "loss": 0.7319,
      "step": 2434
    },
    {
      "epoch": 0.010404471145218216,
      "grad_norm": 0.7670379281044006,
      "learning_rate": 0.0003960006836438216,
      "loss": 0.3326,
      "step": 2435
    },
    {
      "epoch": 0.01040874402864541,
      "grad_norm": 3.045135736465454,
      "learning_rate": 0.0003959579559049735,
      "loss": 1.2523,
      "step": 2436
    },
    {
      "epoch": 0.010413016912072606,
      "grad_norm": 5.125687599182129,
      "learning_rate": 0.00039591522816612547,
      "loss": 1.7003,
      "step": 2437
    },
    {
      "epoch": 0.0104172897954998,
      "grad_norm": 1.5874334573745728,
      "learning_rate": 0.0003958725004272774,
      "loss": 0.4803,
      "step": 2438
    },
    {
      "epoch": 0.010421562678926993,
      "grad_norm": 0.4588867425918579,
      "learning_rate": 0.00039582977268842935,
      "loss": 0.1395,
      "step": 2439
    },
    {
      "epoch": 0.010425835562354187,
      "grad_norm": 3.858668565750122,
      "learning_rate": 0.0003957870449495813,
      "loss": 1.1562,
      "step": 2440
    },
    {
      "epoch": 0.010430108445781383,
      "grad_norm": 1.7922935485839844,
      "learning_rate": 0.0003957443172107332,
      "loss": 0.808,
      "step": 2441
    },
    {
      "epoch": 0.010434381329208577,
      "grad_norm": 0.28268083930015564,
      "learning_rate": 0.0003957015894718852,
      "loss": 0.0937,
      "step": 2442
    },
    {
      "epoch": 0.01043865421263577,
      "grad_norm": 2.86464262008667,
      "learning_rate": 0.0003956588617330371,
      "loss": 1.4344,
      "step": 2443
    },
    {
      "epoch": 0.010442927096062966,
      "grad_norm": 0.39271965622901917,
      "learning_rate": 0.00039561613399418906,
      "loss": 0.1201,
      "step": 2444
    },
    {
      "epoch": 0.01044719997949016,
      "grad_norm": 1.6043113470077515,
      "learning_rate": 0.000395573406255341,
      "loss": 0.567,
      "step": 2445
    },
    {
      "epoch": 0.010451472862917354,
      "grad_norm": 2.7434635162353516,
      "learning_rate": 0.00039553067851649294,
      "loss": 0.8462,
      "step": 2446
    },
    {
      "epoch": 0.010455745746344548,
      "grad_norm": 1.4290237426757812,
      "learning_rate": 0.0003954879507776449,
      "loss": 0.3774,
      "step": 2447
    },
    {
      "epoch": 0.010460018629771743,
      "grad_norm": 0.8170132637023926,
      "learning_rate": 0.0003954452230387968,
      "loss": 0.4036,
      "step": 2448
    },
    {
      "epoch": 0.010464291513198937,
      "grad_norm": 1.86979079246521,
      "learning_rate": 0.0003954024952999488,
      "loss": 0.6214,
      "step": 2449
    },
    {
      "epoch": 0.010468564396626131,
      "grad_norm": 2.705387830734253,
      "learning_rate": 0.00039535976756110064,
      "loss": 0.8462,
      "step": 2450
    },
    {
      "epoch": 0.010472837280053325,
      "grad_norm": 3.913168430328369,
      "learning_rate": 0.0003953170398222526,
      "loss": 1.696,
      "step": 2451
    },
    {
      "epoch": 0.01047711016348052,
      "grad_norm": 3.4768917560577393,
      "learning_rate": 0.0003952743120834045,
      "loss": 1.3558,
      "step": 2452
    },
    {
      "epoch": 0.010481383046907714,
      "grad_norm": 2.5053341388702393,
      "learning_rate": 0.0003952315843445565,
      "loss": 1.2266,
      "step": 2453
    },
    {
      "epoch": 0.010485655930334908,
      "grad_norm": 2.1571199893951416,
      "learning_rate": 0.0003951888566057084,
      "loss": 1.0078,
      "step": 2454
    },
    {
      "epoch": 0.010489928813762104,
      "grad_norm": 1.6086851358413696,
      "learning_rate": 0.00039514612886686035,
      "loss": 0.4546,
      "step": 2455
    },
    {
      "epoch": 0.010494201697189297,
      "grad_norm": 2.11716365814209,
      "learning_rate": 0.0003951034011280123,
      "loss": 0.9493,
      "step": 2456
    },
    {
      "epoch": 0.010498474580616491,
      "grad_norm": 2.514986276626587,
      "learning_rate": 0.00039506067338916423,
      "loss": 0.94,
      "step": 2457
    },
    {
      "epoch": 0.010502747464043685,
      "grad_norm": 1.3738720417022705,
      "learning_rate": 0.0003950179456503162,
      "loss": 0.4746,
      "step": 2458
    },
    {
      "epoch": 0.01050702034747088,
      "grad_norm": 2.333179235458374,
      "learning_rate": 0.0003949752179114681,
      "loss": 0.7975,
      "step": 2459
    },
    {
      "epoch": 0.010511293230898075,
      "grad_norm": 1.8167649507522583,
      "learning_rate": 0.00039493249017262007,
      "loss": 0.5487,
      "step": 2460
    },
    {
      "epoch": 0.010515566114325268,
      "grad_norm": 0.5270624160766602,
      "learning_rate": 0.000394889762433772,
      "loss": 0.1593,
      "step": 2461
    },
    {
      "epoch": 0.010519838997752464,
      "grad_norm": 2.4675533771514893,
      "learning_rate": 0.00039484703469492395,
      "loss": 0.7483,
      "step": 2462
    },
    {
      "epoch": 0.010524111881179658,
      "grad_norm": 1.6276682615280151,
      "learning_rate": 0.0003948043069560759,
      "loss": 0.6037,
      "step": 2463
    },
    {
      "epoch": 0.010528384764606852,
      "grad_norm": 1.126596450805664,
      "learning_rate": 0.0003947615792172278,
      "loss": 0.3664,
      "step": 2464
    },
    {
      "epoch": 0.010532657648034046,
      "grad_norm": 1.4749916791915894,
      "learning_rate": 0.0003947188514783798,
      "loss": 0.4316,
      "step": 2465
    },
    {
      "epoch": 0.010536930531461241,
      "grad_norm": 1.2001855373382568,
      "learning_rate": 0.0003946761237395317,
      "loss": 0.5758,
      "step": 2466
    },
    {
      "epoch": 0.010541203414888435,
      "grad_norm": 1.9065234661102295,
      "learning_rate": 0.00039463339600068366,
      "loss": 0.613,
      "step": 2467
    },
    {
      "epoch": 0.010545476298315629,
      "grad_norm": 2.0489261150360107,
      "learning_rate": 0.00039459066826183557,
      "loss": 0.7034,
      "step": 2468
    },
    {
      "epoch": 0.010549749181742825,
      "grad_norm": 1.4856051206588745,
      "learning_rate": 0.00039454794052298754,
      "loss": 0.4019,
      "step": 2469
    },
    {
      "epoch": 0.010554022065170018,
      "grad_norm": 1.5205973386764526,
      "learning_rate": 0.0003945052127841395,
      "loss": 0.3666,
      "step": 2470
    },
    {
      "epoch": 0.010558294948597212,
      "grad_norm": 1.0497413873672485,
      "learning_rate": 0.0003944624850452914,
      "loss": 0.3352,
      "step": 2471
    },
    {
      "epoch": 0.010562567832024406,
      "grad_norm": 0.36506882309913635,
      "learning_rate": 0.0003944197573064434,
      "loss": 0.121,
      "step": 2472
    },
    {
      "epoch": 0.010566840715451602,
      "grad_norm": 1.156557559967041,
      "learning_rate": 0.0003943770295675953,
      "loss": 0.5625,
      "step": 2473
    },
    {
      "epoch": 0.010571113598878796,
      "grad_norm": 1.7118892669677734,
      "learning_rate": 0.00039433430182874725,
      "loss": 0.578,
      "step": 2474
    },
    {
      "epoch": 0.01057538648230599,
      "grad_norm": 2.2374625205993652,
      "learning_rate": 0.00039429157408989916,
      "loss": 0.8572,
      "step": 2475
    },
    {
      "epoch": 0.010579659365733183,
      "grad_norm": 2.624305009841919,
      "learning_rate": 0.00039424884635105113,
      "loss": 0.7338,
      "step": 2476
    },
    {
      "epoch": 0.010583932249160379,
      "grad_norm": 0.6434544324874878,
      "learning_rate": 0.0003942061186122031,
      "loss": 0.2951,
      "step": 2477
    },
    {
      "epoch": 0.010588205132587573,
      "grad_norm": 0.8222237229347229,
      "learning_rate": 0.000394163390873355,
      "loss": 0.2747,
      "step": 2478
    },
    {
      "epoch": 0.010592478016014767,
      "grad_norm": 0.8589500188827515,
      "learning_rate": 0.00039412066313450697,
      "loss": 0.2751,
      "step": 2479
    },
    {
      "epoch": 0.010596750899441962,
      "grad_norm": 3.416487455368042,
      "learning_rate": 0.0003940779353956589,
      "loss": 1.205,
      "step": 2480
    },
    {
      "epoch": 0.010601023782869156,
      "grad_norm": 3.1194655895233154,
      "learning_rate": 0.00039403520765681085,
      "loss": 0.9838,
      "step": 2481
    },
    {
      "epoch": 0.01060529666629635,
      "grad_norm": 1.7139220237731934,
      "learning_rate": 0.00039399247991796276,
      "loss": 0.4243,
      "step": 2482
    },
    {
      "epoch": 0.010609569549723544,
      "grad_norm": 4.114542007446289,
      "learning_rate": 0.00039394975217911467,
      "loss": 1.0399,
      "step": 2483
    },
    {
      "epoch": 0.01061384243315074,
      "grad_norm": 1.2223975658416748,
      "learning_rate": 0.0003939070244402666,
      "loss": 0.5308,
      "step": 2484
    },
    {
      "epoch": 0.010618115316577933,
      "grad_norm": 1.38339364528656,
      "learning_rate": 0.00039386429670141854,
      "loss": 0.413,
      "step": 2485
    },
    {
      "epoch": 0.010622388200005127,
      "grad_norm": 5.342604637145996,
      "learning_rate": 0.0003938215689625705,
      "loss": 2.3789,
      "step": 2486
    },
    {
      "epoch": 0.010626661083432323,
      "grad_norm": 1.1184929609298706,
      "learning_rate": 0.0003937788412237224,
      "loss": 0.4728,
      "step": 2487
    },
    {
      "epoch": 0.010630933966859516,
      "grad_norm": 2.9590866565704346,
      "learning_rate": 0.0003937361134848744,
      "loss": 1.1161,
      "step": 2488
    },
    {
      "epoch": 0.01063520685028671,
      "grad_norm": 1.7804251909255981,
      "learning_rate": 0.0003936933857460263,
      "loss": 0.9672,
      "step": 2489
    },
    {
      "epoch": 0.010639479733713904,
      "grad_norm": 2.729010581970215,
      "learning_rate": 0.00039365065800717826,
      "loss": 0.7833,
      "step": 2490
    },
    {
      "epoch": 0.0106437526171411,
      "grad_norm": 0.8171082735061646,
      "learning_rate": 0.00039360793026833017,
      "loss": 0.2782,
      "step": 2491
    },
    {
      "epoch": 0.010648025500568294,
      "grad_norm": 1.4184502363204956,
      "learning_rate": 0.00039356520252948213,
      "loss": 0.3434,
      "step": 2492
    },
    {
      "epoch": 0.010652298383995487,
      "grad_norm": 2.5745856761932373,
      "learning_rate": 0.0003935224747906341,
      "loss": 0.7491,
      "step": 2493
    },
    {
      "epoch": 0.010656571267422681,
      "grad_norm": 0.7033034563064575,
      "learning_rate": 0.000393479747051786,
      "loss": 0.2524,
      "step": 2494
    },
    {
      "epoch": 0.010660844150849877,
      "grad_norm": 0.6172347068786621,
      "learning_rate": 0.000393437019312938,
      "loss": 0.231,
      "step": 2495
    },
    {
      "epoch": 0.01066511703427707,
      "grad_norm": 2.4225528240203857,
      "learning_rate": 0.0003933942915740899,
      "loss": 0.7053,
      "step": 2496
    },
    {
      "epoch": 0.010669389917704265,
      "grad_norm": 1.7821359634399414,
      "learning_rate": 0.00039335156383524185,
      "loss": 0.7319,
      "step": 2497
    },
    {
      "epoch": 0.01067366280113146,
      "grad_norm": 2.1949024200439453,
      "learning_rate": 0.00039330883609639376,
      "loss": 0.7995,
      "step": 2498
    },
    {
      "epoch": 0.010677935684558654,
      "grad_norm": 0.5018695592880249,
      "learning_rate": 0.0003932661083575457,
      "loss": 0.1845,
      "step": 2499
    },
    {
      "epoch": 0.010682208567985848,
      "grad_norm": 5.011537075042725,
      "learning_rate": 0.0003932233806186977,
      "loss": 2.1093,
      "step": 2500
    },
    {
      "epoch": 0.010686481451413042,
      "grad_norm": 1.5376207828521729,
      "learning_rate": 0.0003931806528798496,
      "loss": 0.5871,
      "step": 2501
    },
    {
      "epoch": 0.010690754334840237,
      "grad_norm": 4.891579627990723,
      "learning_rate": 0.00039313792514100157,
      "loss": 1.3451,
      "step": 2502
    },
    {
      "epoch": 0.010695027218267431,
      "grad_norm": 1.8185794353485107,
      "learning_rate": 0.0003930951974021535,
      "loss": 0.6447,
      "step": 2503
    },
    {
      "epoch": 0.010699300101694625,
      "grad_norm": 2.6053526401519775,
      "learning_rate": 0.00039305246966330544,
      "loss": 0.6216,
      "step": 2504
    },
    {
      "epoch": 0.01070357298512182,
      "grad_norm": 3.495919942855835,
      "learning_rate": 0.00039300974192445735,
      "loss": 0.9079,
      "step": 2505
    },
    {
      "epoch": 0.010707845868549015,
      "grad_norm": 2.7471354007720947,
      "learning_rate": 0.0003929670141856093,
      "loss": 1.2924,
      "step": 2506
    },
    {
      "epoch": 0.010712118751976208,
      "grad_norm": 2.680417776107788,
      "learning_rate": 0.0003929242864467613,
      "loss": 0.7879,
      "step": 2507
    },
    {
      "epoch": 0.010716391635403402,
      "grad_norm": 1.611678123474121,
      "learning_rate": 0.0003928815587079132,
      "loss": 0.5582,
      "step": 2508
    },
    {
      "epoch": 0.010720664518830598,
      "grad_norm": 3.2220587730407715,
      "learning_rate": 0.00039283883096906516,
      "loss": 0.8265,
      "step": 2509
    },
    {
      "epoch": 0.010724937402257792,
      "grad_norm": 1.6781078577041626,
      "learning_rate": 0.00039279610323021707,
      "loss": 0.7561,
      "step": 2510
    },
    {
      "epoch": 0.010729210285684986,
      "grad_norm": 1.5214252471923828,
      "learning_rate": 0.00039275337549136903,
      "loss": 0.3902,
      "step": 2511
    },
    {
      "epoch": 0.010733483169112181,
      "grad_norm": 1.5344089269638062,
      "learning_rate": 0.00039271064775252095,
      "loss": 0.5633,
      "step": 2512
    },
    {
      "epoch": 0.010737756052539375,
      "grad_norm": 1.6178549528121948,
      "learning_rate": 0.0003926679200136729,
      "loss": 0.583,
      "step": 2513
    },
    {
      "epoch": 0.010742028935966569,
      "grad_norm": 2.30458402633667,
      "learning_rate": 0.0003926251922748249,
      "loss": 0.6048,
      "step": 2514
    },
    {
      "epoch": 0.010746301819393763,
      "grad_norm": 1.4882220029830933,
      "learning_rate": 0.0003925824645359768,
      "loss": 0.5421,
      "step": 2515
    },
    {
      "epoch": 0.010750574702820958,
      "grad_norm": 0.9237990379333496,
      "learning_rate": 0.0003925397367971287,
      "loss": 0.3824,
      "step": 2516
    },
    {
      "epoch": 0.010754847586248152,
      "grad_norm": 0.6475211381912231,
      "learning_rate": 0.0003924970090582806,
      "loss": 0.1803,
      "step": 2517
    },
    {
      "epoch": 0.010759120469675346,
      "grad_norm": 1.9072985649108887,
      "learning_rate": 0.00039245428131943257,
      "loss": 0.6021,
      "step": 2518
    },
    {
      "epoch": 0.01076339335310254,
      "grad_norm": 4.615581035614014,
      "learning_rate": 0.0003924115535805845,
      "loss": 1.7663,
      "step": 2519
    },
    {
      "epoch": 0.010767666236529735,
      "grad_norm": 2.5132503509521484,
      "learning_rate": 0.00039236882584173645,
      "loss": 1.1028,
      "step": 2520
    },
    {
      "epoch": 0.01077193911995693,
      "grad_norm": 3.163241386413574,
      "learning_rate": 0.00039232609810288836,
      "loss": 0.968,
      "step": 2521
    },
    {
      "epoch": 0.010776212003384123,
      "grad_norm": 2.094952344894409,
      "learning_rate": 0.0003922833703640403,
      "loss": 1.0317,
      "step": 2522
    },
    {
      "epoch": 0.010780484886811319,
      "grad_norm": 1.8069238662719727,
      "learning_rate": 0.0003922406426251923,
      "loss": 0.5403,
      "step": 2523
    },
    {
      "epoch": 0.010784757770238513,
      "grad_norm": 3.11501145362854,
      "learning_rate": 0.0003921979148863442,
      "loss": 0.9058,
      "step": 2524
    },
    {
      "epoch": 0.010789030653665706,
      "grad_norm": 1.6396561861038208,
      "learning_rate": 0.00039215518714749616,
      "loss": 0.7344,
      "step": 2525
    },
    {
      "epoch": 0.0107933035370929,
      "grad_norm": 2.9688515663146973,
      "learning_rate": 0.0003921124594086481,
      "loss": 1.1266,
      "step": 2526
    },
    {
      "epoch": 0.010797576420520096,
      "grad_norm": 1.4038035869598389,
      "learning_rate": 0.00039206973166980004,
      "loss": 0.4202,
      "step": 2527
    },
    {
      "epoch": 0.01080184930394729,
      "grad_norm": 0.7056607007980347,
      "learning_rate": 0.00039202700393095195,
      "loss": 0.1978,
      "step": 2528
    },
    {
      "epoch": 0.010806122187374484,
      "grad_norm": 0.8129916191101074,
      "learning_rate": 0.0003919842761921039,
      "loss": 0.3573,
      "step": 2529
    },
    {
      "epoch": 0.01081039507080168,
      "grad_norm": 0.6012633442878723,
      "learning_rate": 0.0003919415484532559,
      "loss": 0.2129,
      "step": 2530
    },
    {
      "epoch": 0.010814667954228873,
      "grad_norm": 1.2324137687683105,
      "learning_rate": 0.0003918988207144078,
      "loss": 0.4201,
      "step": 2531
    },
    {
      "epoch": 0.010818940837656067,
      "grad_norm": 2.7627134323120117,
      "learning_rate": 0.00039185609297555976,
      "loss": 1.0053,
      "step": 2532
    },
    {
      "epoch": 0.01082321372108326,
      "grad_norm": 3.2453908920288086,
      "learning_rate": 0.00039181336523671167,
      "loss": 1.2715,
      "step": 2533
    },
    {
      "epoch": 0.010827486604510456,
      "grad_norm": 2.373906135559082,
      "learning_rate": 0.00039177063749786363,
      "loss": 0.7545,
      "step": 2534
    },
    {
      "epoch": 0.01083175948793765,
      "grad_norm": 1.7380731105804443,
      "learning_rate": 0.00039172790975901554,
      "loss": 0.4559,
      "step": 2535
    },
    {
      "epoch": 0.010836032371364844,
      "grad_norm": 1.652564525604248,
      "learning_rate": 0.0003916851820201675,
      "loss": 0.4197,
      "step": 2536
    },
    {
      "epoch": 0.01084030525479204,
      "grad_norm": 0.8410190343856812,
      "learning_rate": 0.0003916424542813195,
      "loss": 0.3128,
      "step": 2537
    },
    {
      "epoch": 0.010844578138219233,
      "grad_norm": 3.714754581451416,
      "learning_rate": 0.0003915997265424714,
      "loss": 1.2766,
      "step": 2538
    },
    {
      "epoch": 0.010848851021646427,
      "grad_norm": 5.4249267578125,
      "learning_rate": 0.00039155699880362335,
      "loss": 2.0618,
      "step": 2539
    },
    {
      "epoch": 0.010853123905073621,
      "grad_norm": 3.3081185817718506,
      "learning_rate": 0.00039151427106477526,
      "loss": 1.3972,
      "step": 2540
    },
    {
      "epoch": 0.010857396788500817,
      "grad_norm": 1.292989730834961,
      "learning_rate": 0.0003914715433259272,
      "loss": 0.4215,
      "step": 2541
    },
    {
      "epoch": 0.01086166967192801,
      "grad_norm": 1.686957836151123,
      "learning_rate": 0.00039142881558707914,
      "loss": 0.7999,
      "step": 2542
    },
    {
      "epoch": 0.010865942555355204,
      "grad_norm": 1.5979273319244385,
      "learning_rate": 0.0003913860878482311,
      "loss": 0.7834,
      "step": 2543
    },
    {
      "epoch": 0.010870215438782398,
      "grad_norm": 3.02947998046875,
      "learning_rate": 0.00039134336010938307,
      "loss": 0.843,
      "step": 2544
    },
    {
      "epoch": 0.010874488322209594,
      "grad_norm": 1.9391462802886963,
      "learning_rate": 0.000391300632370535,
      "loss": 1.1493,
      "step": 2545
    },
    {
      "epoch": 0.010878761205636788,
      "grad_norm": 1.5662959814071655,
      "learning_rate": 0.00039125790463168694,
      "loss": 0.3235,
      "step": 2546
    },
    {
      "epoch": 0.010883034089063982,
      "grad_norm": 2.004441976547241,
      "learning_rate": 0.00039121517689283885,
      "loss": 0.7728,
      "step": 2547
    },
    {
      "epoch": 0.010887306972491177,
      "grad_norm": 1.3356549739837646,
      "learning_rate": 0.0003911724491539908,
      "loss": 0.3291,
      "step": 2548
    },
    {
      "epoch": 0.010891579855918371,
      "grad_norm": 1.741652250289917,
      "learning_rate": 0.0003911297214151427,
      "loss": 0.5429,
      "step": 2549
    },
    {
      "epoch": 0.010895852739345565,
      "grad_norm": 1.416225790977478,
      "learning_rate": 0.00039108699367629464,
      "loss": 0.4398,
      "step": 2550
    },
    {
      "epoch": 0.010900125622772759,
      "grad_norm": 2.149127721786499,
      "learning_rate": 0.00039104426593744655,
      "loss": 1.0963,
      "step": 2551
    },
    {
      "epoch": 0.010904398506199954,
      "grad_norm": 3.1308460235595703,
      "learning_rate": 0.0003910015381985985,
      "loss": 1.095,
      "step": 2552
    },
    {
      "epoch": 0.010908671389627148,
      "grad_norm": 2.0895845890045166,
      "learning_rate": 0.0003909588104597505,
      "loss": 1.0444,
      "step": 2553
    },
    {
      "epoch": 0.010912944273054342,
      "grad_norm": 4.659573554992676,
      "learning_rate": 0.0003909160827209024,
      "loss": 0.9841,
      "step": 2554
    },
    {
      "epoch": 0.010917217156481538,
      "grad_norm": 0.738549530506134,
      "learning_rate": 0.00039087335498205435,
      "loss": 0.1404,
      "step": 2555
    },
    {
      "epoch": 0.010921490039908732,
      "grad_norm": 2.5157883167266846,
      "learning_rate": 0.00039083062724320626,
      "loss": 0.4757,
      "step": 2556
    },
    {
      "epoch": 0.010925762923335925,
      "grad_norm": 1.6070014238357544,
      "learning_rate": 0.00039078789950435823,
      "loss": 0.4012,
      "step": 2557
    },
    {
      "epoch": 0.01093003580676312,
      "grad_norm": 2.505735158920288,
      "learning_rate": 0.00039074517176551014,
      "loss": 0.9222,
      "step": 2558
    },
    {
      "epoch": 0.010934308690190315,
      "grad_norm": 1.6063553094863892,
      "learning_rate": 0.0003907024440266621,
      "loss": 0.5413,
      "step": 2559
    },
    {
      "epoch": 0.010938581573617509,
      "grad_norm": 3.5001344680786133,
      "learning_rate": 0.00039065971628781407,
      "loss": 1.5554,
      "step": 2560
    },
    {
      "epoch": 0.010942854457044703,
      "grad_norm": 3.3530433177948,
      "learning_rate": 0.000390616988548966,
      "loss": 0.8354,
      "step": 2561
    },
    {
      "epoch": 0.010947127340471896,
      "grad_norm": 3.3265366554260254,
      "learning_rate": 0.00039057426081011795,
      "loss": 0.8083,
      "step": 2562
    },
    {
      "epoch": 0.010951400223899092,
      "grad_norm": 4.788977146148682,
      "learning_rate": 0.00039053153307126986,
      "loss": 1.1502,
      "step": 2563
    },
    {
      "epoch": 0.010955673107326286,
      "grad_norm": 1.5951272249221802,
      "learning_rate": 0.0003904888053324218,
      "loss": 0.4492,
      "step": 2564
    },
    {
      "epoch": 0.01095994599075348,
      "grad_norm": 2.136965274810791,
      "learning_rate": 0.00039044607759357373,
      "loss": 0.8951,
      "step": 2565
    },
    {
      "epoch": 0.010964218874180675,
      "grad_norm": 3.736584186553955,
      "learning_rate": 0.0003904033498547257,
      "loss": 1.5436,
      "step": 2566
    },
    {
      "epoch": 0.01096849175760787,
      "grad_norm": 3.3834242820739746,
      "learning_rate": 0.00039036062211587766,
      "loss": 1.1786,
      "step": 2567
    },
    {
      "epoch": 0.010972764641035063,
      "grad_norm": 2.1032650470733643,
      "learning_rate": 0.0003903178943770296,
      "loss": 0.4384,
      "step": 2568
    },
    {
      "epoch": 0.010977037524462257,
      "grad_norm": 1.7783355712890625,
      "learning_rate": 0.00039027516663818154,
      "loss": 0.6065,
      "step": 2569
    },
    {
      "epoch": 0.010981310407889452,
      "grad_norm": 1.044946312904358,
      "learning_rate": 0.00039023243889933345,
      "loss": 0.2076,
      "step": 2570
    },
    {
      "epoch": 0.010985583291316646,
      "grad_norm": 2.0382113456726074,
      "learning_rate": 0.0003901897111604854,
      "loss": 0.5096,
      "step": 2571
    },
    {
      "epoch": 0.01098985617474384,
      "grad_norm": 1.870303988456726,
      "learning_rate": 0.0003901469834216373,
      "loss": 0.7403,
      "step": 2572
    },
    {
      "epoch": 0.010994129058171036,
      "grad_norm": 2.740938425064087,
      "learning_rate": 0.0003901042556827893,
      "loss": 1.1234,
      "step": 2573
    },
    {
      "epoch": 0.01099840194159823,
      "grad_norm": 2.5640413761138916,
      "learning_rate": 0.00039006152794394125,
      "loss": 0.6122,
      "step": 2574
    },
    {
      "epoch": 0.011002674825025423,
      "grad_norm": 3.202634334564209,
      "learning_rate": 0.00039001880020509317,
      "loss": 0.8933,
      "step": 2575
    },
    {
      "epoch": 0.011006947708452617,
      "grad_norm": 3.81059193611145,
      "learning_rate": 0.00038997607246624513,
      "loss": 1.6316,
      "step": 2576
    },
    {
      "epoch": 0.011011220591879813,
      "grad_norm": 2.9823131561279297,
      "learning_rate": 0.00038993334472739704,
      "loss": 1.0624,
      "step": 2577
    },
    {
      "epoch": 0.011015493475307007,
      "grad_norm": 1.3123862743377686,
      "learning_rate": 0.000389890616988549,
      "loss": 0.3167,
      "step": 2578
    },
    {
      "epoch": 0.0110197663587342,
      "grad_norm": 1.1543893814086914,
      "learning_rate": 0.0003898478892497009,
      "loss": 0.2727,
      "step": 2579
    },
    {
      "epoch": 0.011024039242161396,
      "grad_norm": 1.7355629205703735,
      "learning_rate": 0.0003898051615108529,
      "loss": 0.4751,
      "step": 2580
    },
    {
      "epoch": 0.01102831212558859,
      "grad_norm": 4.000865936279297,
      "learning_rate": 0.00038976243377200485,
      "loss": 1.2854,
      "step": 2581
    },
    {
      "epoch": 0.011032585009015784,
      "grad_norm": 3.9916882514953613,
      "learning_rate": 0.0003897197060331567,
      "loss": 1.2174,
      "step": 2582
    },
    {
      "epoch": 0.011036857892442978,
      "grad_norm": 1.8768672943115234,
      "learning_rate": 0.00038967697829430867,
      "loss": 0.5124,
      "step": 2583
    },
    {
      "epoch": 0.011041130775870173,
      "grad_norm": 3.162794828414917,
      "learning_rate": 0.0003896342505554606,
      "loss": 1.0597,
      "step": 2584
    },
    {
      "epoch": 0.011045403659297367,
      "grad_norm": 0.3925929069519043,
      "learning_rate": 0.00038959152281661254,
      "loss": 0.0899,
      "step": 2585
    },
    {
      "epoch": 0.011049676542724561,
      "grad_norm": 2.6052725315093994,
      "learning_rate": 0.00038954879507776445,
      "loss": 0.7348,
      "step": 2586
    },
    {
      "epoch": 0.011053949426151755,
      "grad_norm": 1.7486587762832642,
      "learning_rate": 0.0003895060673389164,
      "loss": 0.3894,
      "step": 2587
    },
    {
      "epoch": 0.01105822230957895,
      "grad_norm": 4.072211742401123,
      "learning_rate": 0.00038946333960006833,
      "loss": 1.1957,
      "step": 2588
    },
    {
      "epoch": 0.011062495193006144,
      "grad_norm": 2.5783281326293945,
      "learning_rate": 0.0003894206118612203,
      "loss": 0.6554,
      "step": 2589
    },
    {
      "epoch": 0.011066768076433338,
      "grad_norm": 1.3301221132278442,
      "learning_rate": 0.00038937788412237226,
      "loss": 0.4282,
      "step": 2590
    },
    {
      "epoch": 0.011071040959860534,
      "grad_norm": 3.913606882095337,
      "learning_rate": 0.00038933515638352417,
      "loss": 0.7578,
      "step": 2591
    },
    {
      "epoch": 0.011075313843287728,
      "grad_norm": 2.6358447074890137,
      "learning_rate": 0.00038929242864467614,
      "loss": 0.7865,
      "step": 2592
    },
    {
      "epoch": 0.011079586726714922,
      "grad_norm": 1.5916186571121216,
      "learning_rate": 0.00038924970090582805,
      "loss": 0.253,
      "step": 2593
    },
    {
      "epoch": 0.011083859610142115,
      "grad_norm": 2.329887628555298,
      "learning_rate": 0.00038920697316698,
      "loss": 0.6499,
      "step": 2594
    },
    {
      "epoch": 0.011088132493569311,
      "grad_norm": 0.5243700742721558,
      "learning_rate": 0.0003891642454281319,
      "loss": 0.1261,
      "step": 2595
    },
    {
      "epoch": 0.011092405376996505,
      "grad_norm": 2.2924041748046875,
      "learning_rate": 0.0003891215176892839,
      "loss": 0.7097,
      "step": 2596
    },
    {
      "epoch": 0.011096678260423699,
      "grad_norm": 1.108270525932312,
      "learning_rate": 0.00038907878995043585,
      "loss": 0.3399,
      "step": 2597
    },
    {
      "epoch": 0.011100951143850894,
      "grad_norm": 1.5541976690292358,
      "learning_rate": 0.00038903606221158776,
      "loss": 0.4448,
      "step": 2598
    },
    {
      "epoch": 0.011105224027278088,
      "grad_norm": 1.0007622241973877,
      "learning_rate": 0.00038899333447273973,
      "loss": 0.285,
      "step": 2599
    },
    {
      "epoch": 0.011109496910705282,
      "grad_norm": 4.1127448081970215,
      "learning_rate": 0.00038895060673389164,
      "loss": 1.4862,
      "step": 2600
    },
    {
      "epoch": 0.011113769794132476,
      "grad_norm": 3.050145149230957,
      "learning_rate": 0.0003889078789950436,
      "loss": 0.8353,
      "step": 2601
    },
    {
      "epoch": 0.011118042677559671,
      "grad_norm": 3.039463996887207,
      "learning_rate": 0.0003888651512561955,
      "loss": 0.9666,
      "step": 2602
    },
    {
      "epoch": 0.011122315560986865,
      "grad_norm": 3.1814377307891846,
      "learning_rate": 0.0003888224235173475,
      "loss": 0.7499,
      "step": 2603
    },
    {
      "epoch": 0.011126588444414059,
      "grad_norm": 2.9858319759368896,
      "learning_rate": 0.00038877969577849944,
      "loss": 0.941,
      "step": 2604
    },
    {
      "epoch": 0.011130861327841253,
      "grad_norm": 0.8874408602714539,
      "learning_rate": 0.00038873696803965135,
      "loss": 0.2484,
      "step": 2605
    },
    {
      "epoch": 0.011135134211268449,
      "grad_norm": 0.8888586163520813,
      "learning_rate": 0.0003886942403008033,
      "loss": 0.2494,
      "step": 2606
    },
    {
      "epoch": 0.011139407094695642,
      "grad_norm": 2.540956735610962,
      "learning_rate": 0.00038865151256195523,
      "loss": 1.028,
      "step": 2607
    },
    {
      "epoch": 0.011143679978122836,
      "grad_norm": 2.9362080097198486,
      "learning_rate": 0.0003886087848231072,
      "loss": 0.7693,
      "step": 2608
    },
    {
      "epoch": 0.011147952861550032,
      "grad_norm": 4.4566426277160645,
      "learning_rate": 0.0003885660570842591,
      "loss": 1.0942,
      "step": 2609
    },
    {
      "epoch": 0.011152225744977226,
      "grad_norm": 2.2494266033172607,
      "learning_rate": 0.00038852332934541107,
      "loss": 0.593,
      "step": 2610
    },
    {
      "epoch": 0.01115649862840442,
      "grad_norm": 2.5541574954986572,
      "learning_rate": 0.00038848060160656304,
      "loss": 0.7694,
      "step": 2611
    },
    {
      "epoch": 0.011160771511831613,
      "grad_norm": 2.3168931007385254,
      "learning_rate": 0.00038843787386771495,
      "loss": 0.6128,
      "step": 2612
    },
    {
      "epoch": 0.011165044395258809,
      "grad_norm": 2.1676461696624756,
      "learning_rate": 0.0003883951461288669,
      "loss": 0.7372,
      "step": 2613
    },
    {
      "epoch": 0.011169317278686003,
      "grad_norm": 3.7027666568756104,
      "learning_rate": 0.0003883524183900188,
      "loss": 1.1856,
      "step": 2614
    },
    {
      "epoch": 0.011173590162113197,
      "grad_norm": 2.3783934116363525,
      "learning_rate": 0.00038830969065117073,
      "loss": 0.6578,
      "step": 2615
    },
    {
      "epoch": 0.011177863045540392,
      "grad_norm": 3.220606803894043,
      "learning_rate": 0.00038826696291232264,
      "loss": 0.992,
      "step": 2616
    },
    {
      "epoch": 0.011182135928967586,
      "grad_norm": 3.5589566230773926,
      "learning_rate": 0.0003882242351734746,
      "loss": 0.9952,
      "step": 2617
    },
    {
      "epoch": 0.01118640881239478,
      "grad_norm": 2.1977598667144775,
      "learning_rate": 0.0003881815074346265,
      "loss": 0.5617,
      "step": 2618
    },
    {
      "epoch": 0.011190681695821974,
      "grad_norm": 2.2052831649780273,
      "learning_rate": 0.0003881387796957785,
      "loss": 0.5617,
      "step": 2619
    },
    {
      "epoch": 0.01119495457924917,
      "grad_norm": 1.7281899452209473,
      "learning_rate": 0.00038809605195693045,
      "loss": 0.4737,
      "step": 2620
    },
    {
      "epoch": 0.011199227462676363,
      "grad_norm": 0.8296561241149902,
      "learning_rate": 0.00038805332421808236,
      "loss": 0.2346,
      "step": 2621
    },
    {
      "epoch": 0.011203500346103557,
      "grad_norm": 2.768538236618042,
      "learning_rate": 0.0003880105964792343,
      "loss": 0.6456,
      "step": 2622
    },
    {
      "epoch": 0.011207773229530753,
      "grad_norm": 2.793879270553589,
      "learning_rate": 0.00038796786874038624,
      "loss": 0.7281,
      "step": 2623
    },
    {
      "epoch": 0.011212046112957947,
      "grad_norm": 1.2149162292480469,
      "learning_rate": 0.0003879251410015382,
      "loss": 0.2929,
      "step": 2624
    },
    {
      "epoch": 0.01121631899638514,
      "grad_norm": 1.5000489950180054,
      "learning_rate": 0.0003878824132626901,
      "loss": 0.2333,
      "step": 2625
    },
    {
      "epoch": 0.011220591879812334,
      "grad_norm": 2.444951057434082,
      "learning_rate": 0.0003878396855238421,
      "loss": 0.6576,
      "step": 2626
    },
    {
      "epoch": 0.01122486476323953,
      "grad_norm": 0.8366962671279907,
      "learning_rate": 0.00038779695778499404,
      "loss": 0.2569,
      "step": 2627
    },
    {
      "epoch": 0.011229137646666724,
      "grad_norm": 4.224447727203369,
      "learning_rate": 0.00038775423004614595,
      "loss": 1.5696,
      "step": 2628
    },
    {
      "epoch": 0.011233410530093918,
      "grad_norm": 2.3934245109558105,
      "learning_rate": 0.0003877115023072979,
      "loss": 0.6422,
      "step": 2629
    },
    {
      "epoch": 0.011237683413521111,
      "grad_norm": 4.655716419219971,
      "learning_rate": 0.00038766877456844983,
      "loss": 1.0657,
      "step": 2630
    },
    {
      "epoch": 0.011241956296948307,
      "grad_norm": 2.5354559421539307,
      "learning_rate": 0.0003876260468296018,
      "loss": 1.0395,
      "step": 2631
    },
    {
      "epoch": 0.011246229180375501,
      "grad_norm": 1.4936718940734863,
      "learning_rate": 0.0003875833190907537,
      "loss": 0.2336,
      "step": 2632
    },
    {
      "epoch": 0.011250502063802695,
      "grad_norm": 3.073198080062866,
      "learning_rate": 0.00038754059135190567,
      "loss": 0.7371,
      "step": 2633
    },
    {
      "epoch": 0.01125477494722989,
      "grad_norm": 2.565269947052002,
      "learning_rate": 0.00038749786361305763,
      "loss": 0.5355,
      "step": 2634
    },
    {
      "epoch": 0.011259047830657084,
      "grad_norm": 2.358349561691284,
      "learning_rate": 0.00038745513587420954,
      "loss": 0.9631,
      "step": 2635
    },
    {
      "epoch": 0.011263320714084278,
      "grad_norm": 2.1395928859710693,
      "learning_rate": 0.0003874124081353615,
      "loss": 0.5882,
      "step": 2636
    },
    {
      "epoch": 0.011267593597511472,
      "grad_norm": 2.541266441345215,
      "learning_rate": 0.0003873696803965134,
      "loss": 0.9183,
      "step": 2637
    },
    {
      "epoch": 0.011271866480938668,
      "grad_norm": 5.25977087020874,
      "learning_rate": 0.0003873269526576654,
      "loss": 1.9942,
      "step": 2638
    },
    {
      "epoch": 0.011276139364365861,
      "grad_norm": 3.229814291000366,
      "learning_rate": 0.0003872842249188173,
      "loss": 0.8694,
      "step": 2639
    },
    {
      "epoch": 0.011280412247793055,
      "grad_norm": 1.84657883644104,
      "learning_rate": 0.00038724149717996926,
      "loss": 0.5399,
      "step": 2640
    },
    {
      "epoch": 0.01128468513122025,
      "grad_norm": 1.4344767332077026,
      "learning_rate": 0.0003871987694411212,
      "loss": 0.4062,
      "step": 2641
    },
    {
      "epoch": 0.011288958014647445,
      "grad_norm": 2.4721813201904297,
      "learning_rate": 0.00038715604170227314,
      "loss": 1.0134,
      "step": 2642
    },
    {
      "epoch": 0.011293230898074639,
      "grad_norm": 2.9940273761749268,
      "learning_rate": 0.0003871133139634251,
      "loss": 1.312,
      "step": 2643
    },
    {
      "epoch": 0.011297503781501832,
      "grad_norm": 4.397890567779541,
      "learning_rate": 0.000387070586224577,
      "loss": 1.5374,
      "step": 2644
    },
    {
      "epoch": 0.011301776664929028,
      "grad_norm": 0.9970608949661255,
      "learning_rate": 0.000387027858485729,
      "loss": 0.279,
      "step": 2645
    },
    {
      "epoch": 0.011306049548356222,
      "grad_norm": 0.7410922646522522,
      "learning_rate": 0.0003869851307468809,
      "loss": 0.2374,
      "step": 2646
    },
    {
      "epoch": 0.011310322431783416,
      "grad_norm": 1.1832524538040161,
      "learning_rate": 0.00038694240300803285,
      "loss": 0.3088,
      "step": 2647
    },
    {
      "epoch": 0.01131459531521061,
      "grad_norm": 2.241878032684326,
      "learning_rate": 0.00038689967526918476,
      "loss": 0.5394,
      "step": 2648
    },
    {
      "epoch": 0.011318868198637805,
      "grad_norm": 1.084841251373291,
      "learning_rate": 0.0003868569475303367,
      "loss": 0.2737,
      "step": 2649
    },
    {
      "epoch": 0.011323141082064999,
      "grad_norm": 1.53017258644104,
      "learning_rate": 0.00038681421979148864,
      "loss": 0.444,
      "step": 2650
    },
    {
      "epoch": 0.011327413965492193,
      "grad_norm": 2.2182860374450684,
      "learning_rate": 0.00038677149205264055,
      "loss": 0.3526,
      "step": 2651
    },
    {
      "epoch": 0.011331686848919388,
      "grad_norm": 4.755188465118408,
      "learning_rate": 0.0003867287643137925,
      "loss": 1.7649,
      "step": 2652
    },
    {
      "epoch": 0.011335959732346582,
      "grad_norm": 3.049466848373413,
      "learning_rate": 0.0003866860365749444,
      "loss": 0.9633,
      "step": 2653
    },
    {
      "epoch": 0.011340232615773776,
      "grad_norm": 2.533109188079834,
      "learning_rate": 0.0003866433088360964,
      "loss": 0.8479,
      "step": 2654
    },
    {
      "epoch": 0.01134450549920097,
      "grad_norm": 1.4435511827468872,
      "learning_rate": 0.0003866005810972483,
      "loss": 0.3693,
      "step": 2655
    },
    {
      "epoch": 0.011348778382628166,
      "grad_norm": 2.145148992538452,
      "learning_rate": 0.00038655785335840027,
      "loss": 0.625,
      "step": 2656
    },
    {
      "epoch": 0.01135305126605536,
      "grad_norm": 4.852813720703125,
      "learning_rate": 0.00038651512561955223,
      "loss": 1.2122,
      "step": 2657
    },
    {
      "epoch": 0.011357324149482553,
      "grad_norm": 4.322885513305664,
      "learning_rate": 0.00038647239788070414,
      "loss": 1.1758,
      "step": 2658
    },
    {
      "epoch": 0.011361597032909749,
      "grad_norm": 3.4211323261260986,
      "learning_rate": 0.0003864296701418561,
      "loss": 1.1268,
      "step": 2659
    },
    {
      "epoch": 0.011365869916336943,
      "grad_norm": 3.3591833114624023,
      "learning_rate": 0.000386386942403008,
      "loss": 0.965,
      "step": 2660
    },
    {
      "epoch": 0.011370142799764137,
      "grad_norm": 1.1929789781570435,
      "learning_rate": 0.00038634421466416,
      "loss": 0.2459,
      "step": 2661
    },
    {
      "epoch": 0.01137441568319133,
      "grad_norm": 2.0867526531219482,
      "learning_rate": 0.0003863014869253119,
      "loss": 0.9524,
      "step": 2662
    },
    {
      "epoch": 0.011378688566618526,
      "grad_norm": 3.784459352493286,
      "learning_rate": 0.00038625875918646386,
      "loss": 1.3011,
      "step": 2663
    },
    {
      "epoch": 0.01138296145004572,
      "grad_norm": 1.9064842462539673,
      "learning_rate": 0.0003862160314476158,
      "loss": 0.5841,
      "step": 2664
    },
    {
      "epoch": 0.011387234333472914,
      "grad_norm": 3.006333112716675,
      "learning_rate": 0.00038617330370876773,
      "loss": 1.2864,
      "step": 2665
    },
    {
      "epoch": 0.01139150721690011,
      "grad_norm": 4.825088024139404,
      "learning_rate": 0.0003861305759699197,
      "loss": 0.9945,
      "step": 2666
    },
    {
      "epoch": 0.011395780100327303,
      "grad_norm": 4.1113762855529785,
      "learning_rate": 0.0003860878482310716,
      "loss": 1.2088,
      "step": 2667
    },
    {
      "epoch": 0.011400052983754497,
      "grad_norm": 3.2466583251953125,
      "learning_rate": 0.0003860451204922236,
      "loss": 0.9391,
      "step": 2668
    },
    {
      "epoch": 0.011404325867181691,
      "grad_norm": 3.2638590335845947,
      "learning_rate": 0.0003860023927533755,
      "loss": 1.3139,
      "step": 2669
    },
    {
      "epoch": 0.011408598750608887,
      "grad_norm": 1.3554552793502808,
      "learning_rate": 0.00038595966501452745,
      "loss": 0.3139,
      "step": 2670
    },
    {
      "epoch": 0.01141287163403608,
      "grad_norm": 1.9166959524154663,
      "learning_rate": 0.0003859169372756794,
      "loss": 0.597,
      "step": 2671
    },
    {
      "epoch": 0.011417144517463274,
      "grad_norm": 1.0998330116271973,
      "learning_rate": 0.0003858742095368313,
      "loss": 0.2157,
      "step": 2672
    },
    {
      "epoch": 0.011421417400890468,
      "grad_norm": 1.948815941810608,
      "learning_rate": 0.0003858314817979833,
      "loss": 0.8751,
      "step": 2673
    },
    {
      "epoch": 0.011425690284317664,
      "grad_norm": 1.4365296363830566,
      "learning_rate": 0.0003857887540591352,
      "loss": 0.3416,
      "step": 2674
    },
    {
      "epoch": 0.011429963167744858,
      "grad_norm": 1.8364393711090088,
      "learning_rate": 0.00038574602632028717,
      "loss": 0.5735,
      "step": 2675
    },
    {
      "epoch": 0.011434236051172051,
      "grad_norm": 3.832340717315674,
      "learning_rate": 0.0003857032985814391,
      "loss": 1.4367,
      "step": 2676
    },
    {
      "epoch": 0.011438508934599247,
      "grad_norm": 0.9636892676353455,
      "learning_rate": 0.00038566057084259104,
      "loss": 0.1985,
      "step": 2677
    },
    {
      "epoch": 0.01144278181802644,
      "grad_norm": 1.4001808166503906,
      "learning_rate": 0.000385617843103743,
      "loss": 0.4737,
      "step": 2678
    },
    {
      "epoch": 0.011447054701453635,
      "grad_norm": 3.481808662414551,
      "learning_rate": 0.0003855751153648949,
      "loss": 1.0789,
      "step": 2679
    },
    {
      "epoch": 0.011451327584880829,
      "grad_norm": 1.2446610927581787,
      "learning_rate": 0.00038553238762604683,
      "loss": 0.2898,
      "step": 2680
    },
    {
      "epoch": 0.011455600468308024,
      "grad_norm": 3.2072508335113525,
      "learning_rate": 0.00038548965988719874,
      "loss": 0.9492,
      "step": 2681
    },
    {
      "epoch": 0.011459873351735218,
      "grad_norm": 2.3306381702423096,
      "learning_rate": 0.0003854469321483507,
      "loss": 0.6395,
      "step": 2682
    },
    {
      "epoch": 0.011464146235162412,
      "grad_norm": 1.6391245126724243,
      "learning_rate": 0.0003854042044095026,
      "loss": 0.6863,
      "step": 2683
    },
    {
      "epoch": 0.011468419118589607,
      "grad_norm": 1.3912241458892822,
      "learning_rate": 0.0003853614766706546,
      "loss": 0.3412,
      "step": 2684
    },
    {
      "epoch": 0.011472692002016801,
      "grad_norm": 2.777033805847168,
      "learning_rate": 0.0003853187489318065,
      "loss": 0.8885,
      "step": 2685
    },
    {
      "epoch": 0.011476964885443995,
      "grad_norm": 1.056129813194275,
      "learning_rate": 0.00038527602119295846,
      "loss": 0.3621,
      "step": 2686
    },
    {
      "epoch": 0.011481237768871189,
      "grad_norm": 2.175895929336548,
      "learning_rate": 0.0003852332934541104,
      "loss": 0.658,
      "step": 2687
    },
    {
      "epoch": 0.011485510652298385,
      "grad_norm": 3.1438424587249756,
      "learning_rate": 0.00038519056571526233,
      "loss": 1.3216,
      "step": 2688
    },
    {
      "epoch": 0.011489783535725578,
      "grad_norm": 3.7337162494659424,
      "learning_rate": 0.0003851478379764143,
      "loss": 1.0605,
      "step": 2689
    },
    {
      "epoch": 0.011494056419152772,
      "grad_norm": 1.5911693572998047,
      "learning_rate": 0.0003851051102375662,
      "loss": 0.4176,
      "step": 2690
    },
    {
      "epoch": 0.011498329302579968,
      "grad_norm": 0.9476587772369385,
      "learning_rate": 0.00038506238249871817,
      "loss": 0.3063,
      "step": 2691
    },
    {
      "epoch": 0.011502602186007162,
      "grad_norm": 2.9897570610046387,
      "learning_rate": 0.0003850196547598701,
      "loss": 0.8776,
      "step": 2692
    },
    {
      "epoch": 0.011506875069434356,
      "grad_norm": 1.2278521060943604,
      "learning_rate": 0.00038497692702102205,
      "loss": 0.3359,
      "step": 2693
    },
    {
      "epoch": 0.01151114795286155,
      "grad_norm": 2.2802529335021973,
      "learning_rate": 0.000384934199282174,
      "loss": 0.7405,
      "step": 2694
    },
    {
      "epoch": 0.011515420836288745,
      "grad_norm": 1.68878173828125,
      "learning_rate": 0.0003848914715433259,
      "loss": 0.8632,
      "step": 2695
    },
    {
      "epoch": 0.011519693719715939,
      "grad_norm": 3.645066976547241,
      "learning_rate": 0.0003848487438044779,
      "loss": 1.2962,
      "step": 2696
    },
    {
      "epoch": 0.011523966603143133,
      "grad_norm": 3.5314738750457764,
      "learning_rate": 0.0003848060160656298,
      "loss": 1.7157,
      "step": 2697
    },
    {
      "epoch": 0.011528239486570327,
      "grad_norm": 2.620509147644043,
      "learning_rate": 0.00038476328832678176,
      "loss": 0.8803,
      "step": 2698
    },
    {
      "epoch": 0.011532512369997522,
      "grad_norm": 0.9996734261512756,
      "learning_rate": 0.0003847205605879337,
      "loss": 0.3272,
      "step": 2699
    },
    {
      "epoch": 0.011536785253424716,
      "grad_norm": 1.6970783472061157,
      "learning_rate": 0.00038467783284908564,
      "loss": 0.4762,
      "step": 2700
    },
    {
      "epoch": 0.01154105813685191,
      "grad_norm": 3.6045494079589844,
      "learning_rate": 0.0003846351051102376,
      "loss": 1.5251,
      "step": 2701
    },
    {
      "epoch": 0.011545331020279105,
      "grad_norm": 2.772630453109741,
      "learning_rate": 0.0003845923773713895,
      "loss": 1.2814,
      "step": 2702
    },
    {
      "epoch": 0.0115496039037063,
      "grad_norm": 0.8349756002426147,
      "learning_rate": 0.0003845496496325415,
      "loss": 0.2386,
      "step": 2703
    },
    {
      "epoch": 0.011553876787133493,
      "grad_norm": 1.8160420656204224,
      "learning_rate": 0.0003845069218936934,
      "loss": 0.4697,
      "step": 2704
    },
    {
      "epoch": 0.011558149670560687,
      "grad_norm": 2.835784912109375,
      "learning_rate": 0.00038446419415484536,
      "loss": 0.7948,
      "step": 2705
    },
    {
      "epoch": 0.011562422553987883,
      "grad_norm": 2.940544605255127,
      "learning_rate": 0.00038442146641599727,
      "loss": 1.0288,
      "step": 2706
    },
    {
      "epoch": 0.011566695437415076,
      "grad_norm": 1.6280477046966553,
      "learning_rate": 0.00038437873867714923,
      "loss": 0.3817,
      "step": 2707
    },
    {
      "epoch": 0.01157096832084227,
      "grad_norm": 2.60862135887146,
      "learning_rate": 0.0003843360109383012,
      "loss": 0.7878,
      "step": 2708
    },
    {
      "epoch": 0.011575241204269466,
      "grad_norm": 2.435584306716919,
      "learning_rate": 0.0003842932831994531,
      "loss": 1.1291,
      "step": 2709
    },
    {
      "epoch": 0.01157951408769666,
      "grad_norm": 0.6883028745651245,
      "learning_rate": 0.00038425055546060507,
      "loss": 0.215,
      "step": 2710
    },
    {
      "epoch": 0.011583786971123854,
      "grad_norm": 2.2497432231903076,
      "learning_rate": 0.000384207827721757,
      "loss": 0.6372,
      "step": 2711
    },
    {
      "epoch": 0.011588059854551047,
      "grad_norm": 2.1792969703674316,
      "learning_rate": 0.00038416509998290895,
      "loss": 0.9176,
      "step": 2712
    },
    {
      "epoch": 0.011592332737978243,
      "grad_norm": 1.593498945236206,
      "learning_rate": 0.0003841223722440608,
      "loss": 0.3817,
      "step": 2713
    },
    {
      "epoch": 0.011596605621405437,
      "grad_norm": 4.5685715675354,
      "learning_rate": 0.00038407964450521277,
      "loss": 1.8695,
      "step": 2714
    },
    {
      "epoch": 0.01160087850483263,
      "grad_norm": 1.6142075061798096,
      "learning_rate": 0.00038403691676636473,
      "loss": 0.4727,
      "step": 2715
    },
    {
      "epoch": 0.011605151388259825,
      "grad_norm": 1.24615478515625,
      "learning_rate": 0.00038399418902751664,
      "loss": 0.3355,
      "step": 2716
    },
    {
      "epoch": 0.01160942427168702,
      "grad_norm": 0.8034467101097107,
      "learning_rate": 0.0003839514612886686,
      "loss": 0.2387,
      "step": 2717
    },
    {
      "epoch": 0.011613697155114214,
      "grad_norm": 1.5758589506149292,
      "learning_rate": 0.0003839087335498205,
      "loss": 0.5776,
      "step": 2718
    },
    {
      "epoch": 0.011617970038541408,
      "grad_norm": 0.7611658573150635,
      "learning_rate": 0.0003838660058109725,
      "loss": 0.2241,
      "step": 2719
    },
    {
      "epoch": 0.011622242921968604,
      "grad_norm": 0.7600988745689392,
      "learning_rate": 0.0003838232780721244,
      "loss": 0.2247,
      "step": 2720
    },
    {
      "epoch": 0.011626515805395797,
      "grad_norm": 0.9723448753356934,
      "learning_rate": 0.00038378055033327636,
      "loss": 0.3422,
      "step": 2721
    },
    {
      "epoch": 0.011630788688822991,
      "grad_norm": 1.6506016254425049,
      "learning_rate": 0.00038373782259442827,
      "loss": 0.3675,
      "step": 2722
    },
    {
      "epoch": 0.011635061572250185,
      "grad_norm": 1.3796151876449585,
      "learning_rate": 0.00038369509485558024,
      "loss": 0.4836,
      "step": 2723
    },
    {
      "epoch": 0.01163933445567738,
      "grad_norm": 3.8120522499084473,
      "learning_rate": 0.0003836523671167322,
      "loss": 1.2638,
      "step": 2724
    },
    {
      "epoch": 0.011643607339104575,
      "grad_norm": 0.7622561454772949,
      "learning_rate": 0.0003836096393778841,
      "loss": 0.1879,
      "step": 2725
    },
    {
      "epoch": 0.011647880222531768,
      "grad_norm": 3.034883737564087,
      "learning_rate": 0.0003835669116390361,
      "loss": 0.9047,
      "step": 2726
    },
    {
      "epoch": 0.011652153105958964,
      "grad_norm": 2.805727481842041,
      "learning_rate": 0.000383524183900188,
      "loss": 0.7234,
      "step": 2727
    },
    {
      "epoch": 0.011656425989386158,
      "grad_norm": 2.608797073364258,
      "learning_rate": 0.00038348145616133995,
      "loss": 0.7472,
      "step": 2728
    },
    {
      "epoch": 0.011660698872813352,
      "grad_norm": 1.4529496431350708,
      "learning_rate": 0.00038343872842249186,
      "loss": 0.5782,
      "step": 2729
    },
    {
      "epoch": 0.011664971756240546,
      "grad_norm": 1.9744025468826294,
      "learning_rate": 0.00038339600068364383,
      "loss": 0.6362,
      "step": 2730
    },
    {
      "epoch": 0.011669244639667741,
      "grad_norm": 3.8913400173187256,
      "learning_rate": 0.0003833532729447958,
      "loss": 1.4549,
      "step": 2731
    },
    {
      "epoch": 0.011673517523094935,
      "grad_norm": 1.899664282798767,
      "learning_rate": 0.0003833105452059477,
      "loss": 0.9134,
      "step": 2732
    },
    {
      "epoch": 0.011677790406522129,
      "grad_norm": 4.518564224243164,
      "learning_rate": 0.00038326781746709967,
      "loss": 1.715,
      "step": 2733
    },
    {
      "epoch": 0.011682063289949324,
      "grad_norm": 3.2014293670654297,
      "learning_rate": 0.0003832250897282516,
      "loss": 0.9145,
      "step": 2734
    },
    {
      "epoch": 0.011686336173376518,
      "grad_norm": 1.6252872943878174,
      "learning_rate": 0.00038318236198940355,
      "loss": 0.3516,
      "step": 2735
    },
    {
      "epoch": 0.011690609056803712,
      "grad_norm": 1.7218304872512817,
      "learning_rate": 0.00038313963425055546,
      "loss": 0.5799,
      "step": 2736
    },
    {
      "epoch": 0.011694881940230906,
      "grad_norm": 3.11102294921875,
      "learning_rate": 0.0003830969065117074,
      "loss": 1.3781,
      "step": 2737
    },
    {
      "epoch": 0.011699154823658102,
      "grad_norm": 0.3820081055164337,
      "learning_rate": 0.0003830541787728594,
      "loss": 0.1115,
      "step": 2738
    },
    {
      "epoch": 0.011703427707085295,
      "grad_norm": 2.302469491958618,
      "learning_rate": 0.0003830114510340113,
      "loss": 0.7595,
      "step": 2739
    },
    {
      "epoch": 0.01170770059051249,
      "grad_norm": 2.258558988571167,
      "learning_rate": 0.00038296872329516326,
      "loss": 0.8217,
      "step": 2740
    },
    {
      "epoch": 0.011711973473939683,
      "grad_norm": 1.6565167903900146,
      "learning_rate": 0.00038292599555631517,
      "loss": 0.8473,
      "step": 2741
    },
    {
      "epoch": 0.011716246357366879,
      "grad_norm": 2.2402615547180176,
      "learning_rate": 0.00038288326781746714,
      "loss": 0.7232,
      "step": 2742
    },
    {
      "epoch": 0.011720519240794073,
      "grad_norm": 0.7057660818099976,
      "learning_rate": 0.00038284054007861905,
      "loss": 0.1583,
      "step": 2743
    },
    {
      "epoch": 0.011724792124221266,
      "grad_norm": 3.6936397552490234,
      "learning_rate": 0.000382797812339771,
      "loss": 1.1754,
      "step": 2744
    },
    {
      "epoch": 0.011729065007648462,
      "grad_norm": 1.5774952173233032,
      "learning_rate": 0.000382755084600923,
      "loss": 0.3645,
      "step": 2745
    },
    {
      "epoch": 0.011733337891075656,
      "grad_norm": 0.3203648328781128,
      "learning_rate": 0.00038271235686207483,
      "loss": 0.0771,
      "step": 2746
    },
    {
      "epoch": 0.01173761077450285,
      "grad_norm": 4.3589186668396,
      "learning_rate": 0.0003826696291232268,
      "loss": 1.7344,
      "step": 2747
    },
    {
      "epoch": 0.011741883657930044,
      "grad_norm": 2.226945400238037,
      "learning_rate": 0.0003826269013843787,
      "loss": 0.5649,
      "step": 2748
    },
    {
      "epoch": 0.01174615654135724,
      "grad_norm": 0.9181610345840454,
      "learning_rate": 0.0003825841736455307,
      "loss": 0.1704,
      "step": 2749
    },
    {
      "epoch": 0.011750429424784433,
      "grad_norm": 1.9133679866790771,
      "learning_rate": 0.0003825414459066826,
      "loss": 0.5658,
      "step": 2750
    },
    {
      "epoch": 0.011754702308211627,
      "grad_norm": 1.7690356969833374,
      "learning_rate": 0.00038249871816783455,
      "loss": 0.8418,
      "step": 2751
    },
    {
      "epoch": 0.011758975191638823,
      "grad_norm": 1.873231291770935,
      "learning_rate": 0.0003824559904289865,
      "loss": 0.4477,
      "step": 2752
    },
    {
      "epoch": 0.011763248075066016,
      "grad_norm": 4.2628092765808105,
      "learning_rate": 0.0003824132626901384,
      "loss": 1.6086,
      "step": 2753
    },
    {
      "epoch": 0.01176752095849321,
      "grad_norm": 2.6882240772247314,
      "learning_rate": 0.0003823705349512904,
      "loss": 0.9181,
      "step": 2754
    },
    {
      "epoch": 0.011771793841920404,
      "grad_norm": 1.7727757692337036,
      "learning_rate": 0.0003823278072124423,
      "loss": 0.8118,
      "step": 2755
    },
    {
      "epoch": 0.0117760667253476,
      "grad_norm": 2.8464391231536865,
      "learning_rate": 0.00038228507947359427,
      "loss": 0.9156,
      "step": 2756
    },
    {
      "epoch": 0.011780339608774794,
      "grad_norm": 0.221674382686615,
      "learning_rate": 0.0003822423517347462,
      "loss": 0.0374,
      "step": 2757
    },
    {
      "epoch": 0.011784612492201987,
      "grad_norm": 1.3177837133407593,
      "learning_rate": 0.00038219962399589814,
      "loss": 0.3109,
      "step": 2758
    },
    {
      "epoch": 0.011788885375629181,
      "grad_norm": 2.160888195037842,
      "learning_rate": 0.00038215689625705005,
      "loss": 0.7516,
      "step": 2759
    },
    {
      "epoch": 0.011793158259056377,
      "grad_norm": 1.6220629215240479,
      "learning_rate": 0.000382114168518202,
      "loss": 0.7165,
      "step": 2760
    },
    {
      "epoch": 0.01179743114248357,
      "grad_norm": 1.838886022567749,
      "learning_rate": 0.000382071440779354,
      "loss": 0.652,
      "step": 2761
    },
    {
      "epoch": 0.011801704025910765,
      "grad_norm": 5.075514793395996,
      "learning_rate": 0.0003820287130405059,
      "loss": 1.5136,
      "step": 2762
    },
    {
      "epoch": 0.01180597690933796,
      "grad_norm": 2.5954794883728027,
      "learning_rate": 0.00038198598530165786,
      "loss": 0.729,
      "step": 2763
    },
    {
      "epoch": 0.011810249792765154,
      "grad_norm": 2.089660406112671,
      "learning_rate": 0.00038194325756280977,
      "loss": 0.7175,
      "step": 2764
    },
    {
      "epoch": 0.011814522676192348,
      "grad_norm": 1.8641809225082397,
      "learning_rate": 0.00038190052982396173,
      "loss": 0.809,
      "step": 2765
    },
    {
      "epoch": 0.011818795559619542,
      "grad_norm": 2.123540163040161,
      "learning_rate": 0.00038185780208511365,
      "loss": 0.7174,
      "step": 2766
    },
    {
      "epoch": 0.011823068443046737,
      "grad_norm": 1.525970697402954,
      "learning_rate": 0.0003818150743462656,
      "loss": 0.5687,
      "step": 2767
    },
    {
      "epoch": 0.011827341326473931,
      "grad_norm": 1.9226237535476685,
      "learning_rate": 0.0003817723466074176,
      "loss": 0.7956,
      "step": 2768
    },
    {
      "epoch": 0.011831614209901125,
      "grad_norm": 2.0925416946411133,
      "learning_rate": 0.0003817296188685695,
      "loss": 0.7189,
      "step": 2769
    },
    {
      "epoch": 0.01183588709332832,
      "grad_norm": 5.230830669403076,
      "learning_rate": 0.00038168689112972145,
      "loss": 2.3934,
      "step": 2770
    },
    {
      "epoch": 0.011840159976755514,
      "grad_norm": 2.4343786239624023,
      "learning_rate": 0.00038164416339087336,
      "loss": 0.9047,
      "step": 2771
    },
    {
      "epoch": 0.011844432860182708,
      "grad_norm": 0.5357069969177246,
      "learning_rate": 0.0003816014356520253,
      "loss": 0.1082,
      "step": 2772
    },
    {
      "epoch": 0.011848705743609902,
      "grad_norm": 1.9807902574539185,
      "learning_rate": 0.00038155870791317724,
      "loss": 0.4899,
      "step": 2773
    },
    {
      "epoch": 0.011852978627037098,
      "grad_norm": 1.9969760179519653,
      "learning_rate": 0.0003815159801743292,
      "loss": 0.6623,
      "step": 2774
    },
    {
      "epoch": 0.011857251510464292,
      "grad_norm": 0.5424493551254272,
      "learning_rate": 0.00038147325243548117,
      "loss": 0.1236,
      "step": 2775
    },
    {
      "epoch": 0.011861524393891485,
      "grad_norm": 2.6925442218780518,
      "learning_rate": 0.0003814305246966331,
      "loss": 0.8006,
      "step": 2776
    },
    {
      "epoch": 0.011865797277318681,
      "grad_norm": 2.1428160667419434,
      "learning_rate": 0.00038138779695778504,
      "loss": 0.4746,
      "step": 2777
    },
    {
      "epoch": 0.011870070160745875,
      "grad_norm": 2.0339794158935547,
      "learning_rate": 0.00038134506921893695,
      "loss": 0.8082,
      "step": 2778
    },
    {
      "epoch": 0.011874343044173069,
      "grad_norm": 1.052782654762268,
      "learning_rate": 0.00038130234148008886,
      "loss": 0.384,
      "step": 2779
    },
    {
      "epoch": 0.011878615927600263,
      "grad_norm": 3.410933017730713,
      "learning_rate": 0.0003812596137412408,
      "loss": 1.265,
      "step": 2780
    },
    {
      "epoch": 0.011882888811027458,
      "grad_norm": 2.0201592445373535,
      "learning_rate": 0.00038121688600239274,
      "loss": 0.3717,
      "step": 2781
    },
    {
      "epoch": 0.011887161694454652,
      "grad_norm": 2.8930723667144775,
      "learning_rate": 0.0003811741582635447,
      "loss": 1.0458,
      "step": 2782
    },
    {
      "epoch": 0.011891434577881846,
      "grad_norm": 3.298076629638672,
      "learning_rate": 0.0003811314305246966,
      "loss": 1.1562,
      "step": 2783
    },
    {
      "epoch": 0.01189570746130904,
      "grad_norm": 3.782212018966675,
      "learning_rate": 0.0003810887027858486,
      "loss": 1.2321,
      "step": 2784
    },
    {
      "epoch": 0.011899980344736235,
      "grad_norm": 2.0261778831481934,
      "learning_rate": 0.0003810459750470005,
      "loss": 0.6383,
      "step": 2785
    },
    {
      "epoch": 0.01190425322816343,
      "grad_norm": 1.3140504360198975,
      "learning_rate": 0.00038100324730815246,
      "loss": 0.3407,
      "step": 2786
    },
    {
      "epoch": 0.011908526111590623,
      "grad_norm": 2.066056728363037,
      "learning_rate": 0.00038096051956930437,
      "loss": 0.9276,
      "step": 2787
    },
    {
      "epoch": 0.011912798995017819,
      "grad_norm": 1.2925859689712524,
      "learning_rate": 0.00038091779183045633,
      "loss": 0.4028,
      "step": 2788
    },
    {
      "epoch": 0.011917071878445012,
      "grad_norm": 2.0377604961395264,
      "learning_rate": 0.00038087506409160824,
      "loss": 0.9278,
      "step": 2789
    },
    {
      "epoch": 0.011921344761872206,
      "grad_norm": 3.3060688972473145,
      "learning_rate": 0.0003808323363527602,
      "loss": 1.0382,
      "step": 2790
    },
    {
      "epoch": 0.0119256176452994,
      "grad_norm": 1.7876310348510742,
      "learning_rate": 0.00038078960861391217,
      "loss": 0.7426,
      "step": 2791
    },
    {
      "epoch": 0.011929890528726596,
      "grad_norm": 1.478295087814331,
      "learning_rate": 0.0003807468808750641,
      "loss": 0.3539,
      "step": 2792
    },
    {
      "epoch": 0.01193416341215379,
      "grad_norm": 1.742828607559204,
      "learning_rate": 0.00038070415313621605,
      "loss": 0.6372,
      "step": 2793
    },
    {
      "epoch": 0.011938436295580983,
      "grad_norm": 1.1494265794754028,
      "learning_rate": 0.00038066142539736796,
      "loss": 0.3473,
      "step": 2794
    },
    {
      "epoch": 0.011942709179008179,
      "grad_norm": 3.561697006225586,
      "learning_rate": 0.0003806186976585199,
      "loss": 2.668,
      "step": 2795
    },
    {
      "epoch": 0.011946982062435373,
      "grad_norm": 2.717839241027832,
      "learning_rate": 0.00038057596991967183,
      "loss": 1.1397,
      "step": 2796
    },
    {
      "epoch": 0.011951254945862567,
      "grad_norm": 2.705467462539673,
      "learning_rate": 0.0003805332421808238,
      "loss": 1.0592,
      "step": 2797
    },
    {
      "epoch": 0.01195552782928976,
      "grad_norm": 1.8153293132781982,
      "learning_rate": 0.00038049051444197576,
      "loss": 0.5766,
      "step": 2798
    },
    {
      "epoch": 0.011959800712716956,
      "grad_norm": 1.9263627529144287,
      "learning_rate": 0.0003804477867031277,
      "loss": 0.6437,
      "step": 2799
    },
    {
      "epoch": 0.01196407359614415,
      "grad_norm": 1.8938647508621216,
      "learning_rate": 0.00038040505896427964,
      "loss": 0.6163,
      "step": 2800
    },
    {
      "epoch": 0.011968346479571344,
      "grad_norm": 4.382844924926758,
      "learning_rate": 0.00038036233122543155,
      "loss": 1.4122,
      "step": 2801
    },
    {
      "epoch": 0.01197261936299854,
      "grad_norm": 1.1635639667510986,
      "learning_rate": 0.0003803196034865835,
      "loss": 0.2931,
      "step": 2802
    },
    {
      "epoch": 0.011976892246425733,
      "grad_norm": 2.484128713607788,
      "learning_rate": 0.0003802768757477354,
      "loss": 1.0352,
      "step": 2803
    },
    {
      "epoch": 0.011981165129852927,
      "grad_norm": 4.838106632232666,
      "learning_rate": 0.0003802341480088874,
      "loss": 2.1596,
      "step": 2804
    },
    {
      "epoch": 0.011985438013280121,
      "grad_norm": 2.1817569732666016,
      "learning_rate": 0.00038019142027003936,
      "loss": 0.7511,
      "step": 2805
    },
    {
      "epoch": 0.011989710896707317,
      "grad_norm": 1.7075380086898804,
      "learning_rate": 0.00038014869253119127,
      "loss": 0.4426,
      "step": 2806
    },
    {
      "epoch": 0.01199398378013451,
      "grad_norm": 0.9751560091972351,
      "learning_rate": 0.00038010596479234323,
      "loss": 0.2326,
      "step": 2807
    },
    {
      "epoch": 0.011998256663561704,
      "grad_norm": 1.5288208723068237,
      "learning_rate": 0.00038006323705349514,
      "loss": 0.665,
      "step": 2808
    },
    {
      "epoch": 0.012002529546988898,
      "grad_norm": 2.678323745727539,
      "learning_rate": 0.0003800205093146471,
      "loss": 0.8816,
      "step": 2809
    },
    {
      "epoch": 0.012006802430416094,
      "grad_norm": 1.7065099477767944,
      "learning_rate": 0.000379977781575799,
      "loss": 0.4479,
      "step": 2810
    },
    {
      "epoch": 0.012011075313843288,
      "grad_norm": 1.1241083145141602,
      "learning_rate": 0.000379935053836951,
      "loss": 0.3843,
      "step": 2811
    },
    {
      "epoch": 0.012015348197270482,
      "grad_norm": 1.4642878770828247,
      "learning_rate": 0.0003798923260981029,
      "loss": 0.7319,
      "step": 2812
    },
    {
      "epoch": 0.012019621080697677,
      "grad_norm": 1.228659987449646,
      "learning_rate": 0.0003798495983592548,
      "loss": 0.4319,
      "step": 2813
    },
    {
      "epoch": 0.012023893964124871,
      "grad_norm": 2.2425410747528076,
      "learning_rate": 0.00037980687062040677,
      "loss": 0.6968,
      "step": 2814
    },
    {
      "epoch": 0.012028166847552065,
      "grad_norm": 2.945319414138794,
      "learning_rate": 0.0003797641428815587,
      "loss": 1.0089,
      "step": 2815
    },
    {
      "epoch": 0.012032439730979259,
      "grad_norm": 1.1623796224594116,
      "learning_rate": 0.00037972141514271065,
      "loss": 0.3925,
      "step": 2816
    },
    {
      "epoch": 0.012036712614406454,
      "grad_norm": 4.215325355529785,
      "learning_rate": 0.00037967868740386256,
      "loss": 1.1877,
      "step": 2817
    },
    {
      "epoch": 0.012040985497833648,
      "grad_norm": 1.45200514793396,
      "learning_rate": 0.0003796359596650145,
      "loss": 0.4475,
      "step": 2818
    },
    {
      "epoch": 0.012045258381260842,
      "grad_norm": 2.5352320671081543,
      "learning_rate": 0.0003795932319261665,
      "loss": 0.85,
      "step": 2819
    },
    {
      "epoch": 0.012049531264688038,
      "grad_norm": 3.8451826572418213,
      "learning_rate": 0.0003795505041873184,
      "loss": 1.043,
      "step": 2820
    },
    {
      "epoch": 0.012053804148115231,
      "grad_norm": 2.597203254699707,
      "learning_rate": 0.00037950777644847036,
      "loss": 1.0013,
      "step": 2821
    },
    {
      "epoch": 0.012058077031542425,
      "grad_norm": 3.6275970935821533,
      "learning_rate": 0.0003794650487096223,
      "loss": 1.257,
      "step": 2822
    },
    {
      "epoch": 0.01206234991496962,
      "grad_norm": 1.6236090660095215,
      "learning_rate": 0.00037942232097077424,
      "loss": 0.4316,
      "step": 2823
    },
    {
      "epoch": 0.012066622798396815,
      "grad_norm": 1.4812637567520142,
      "learning_rate": 0.00037937959323192615,
      "loss": 0.6023,
      "step": 2824
    },
    {
      "epoch": 0.012070895681824009,
      "grad_norm": 4.875049114227295,
      "learning_rate": 0.0003793368654930781,
      "loss": 1.937,
      "step": 2825
    },
    {
      "epoch": 0.012075168565251202,
      "grad_norm": 1.4359792470932007,
      "learning_rate": 0.00037929413775423,
      "loss": 0.4533,
      "step": 2826
    },
    {
      "epoch": 0.012079441448678396,
      "grad_norm": 2.1347477436065674,
      "learning_rate": 0.000379251410015382,
      "loss": 0.9557,
      "step": 2827
    },
    {
      "epoch": 0.012083714332105592,
      "grad_norm": 4.188868522644043,
      "learning_rate": 0.00037920868227653395,
      "loss": 1.5714,
      "step": 2828
    },
    {
      "epoch": 0.012087987215532786,
      "grad_norm": 2.070765733718872,
      "learning_rate": 0.00037916595453768586,
      "loss": 0.5454,
      "step": 2829
    },
    {
      "epoch": 0.01209226009895998,
      "grad_norm": 2.738351345062256,
      "learning_rate": 0.00037912322679883783,
      "loss": 0.8511,
      "step": 2830
    },
    {
      "epoch": 0.012096532982387175,
      "grad_norm": 1.686797022819519,
      "learning_rate": 0.00037908049905998974,
      "loss": 0.3121,
      "step": 2831
    },
    {
      "epoch": 0.012100805865814369,
      "grad_norm": 1.025968074798584,
      "learning_rate": 0.0003790377713211417,
      "loss": 0.2413,
      "step": 2832
    },
    {
      "epoch": 0.012105078749241563,
      "grad_norm": 1.6253639459609985,
      "learning_rate": 0.0003789950435822936,
      "loss": 0.2853,
      "step": 2833
    },
    {
      "epoch": 0.012109351632668757,
      "grad_norm": 1.7269096374511719,
      "learning_rate": 0.0003789523158434456,
      "loss": 0.5307,
      "step": 2834
    },
    {
      "epoch": 0.012113624516095952,
      "grad_norm": 1.8367407321929932,
      "learning_rate": 0.00037890958810459755,
      "loss": 0.574,
      "step": 2835
    },
    {
      "epoch": 0.012117897399523146,
      "grad_norm": 3.4784140586853027,
      "learning_rate": 0.00037886686036574946,
      "loss": 1.4944,
      "step": 2836
    },
    {
      "epoch": 0.01212217028295034,
      "grad_norm": 1.4818764925003052,
      "learning_rate": 0.0003788241326269014,
      "loss": 0.6888,
      "step": 2837
    },
    {
      "epoch": 0.012126443166377536,
      "grad_norm": 3.9058337211608887,
      "learning_rate": 0.00037878140488805333,
      "loss": 1.4356,
      "step": 2838
    },
    {
      "epoch": 0.01213071604980473,
      "grad_norm": 2.856750726699829,
      "learning_rate": 0.0003787386771492053,
      "loss": 0.9913,
      "step": 2839
    },
    {
      "epoch": 0.012134988933231923,
      "grad_norm": 4.180445671081543,
      "learning_rate": 0.0003786959494103572,
      "loss": 2.3838,
      "step": 2840
    },
    {
      "epoch": 0.012139261816659117,
      "grad_norm": 1.4421775341033936,
      "learning_rate": 0.0003786532216715092,
      "loss": 0.6217,
      "step": 2841
    },
    {
      "epoch": 0.012143534700086313,
      "grad_norm": 1.9170098304748535,
      "learning_rate": 0.00037861049393266114,
      "loss": 0.5728,
      "step": 2842
    },
    {
      "epoch": 0.012147807583513507,
      "grad_norm": 1.2533694505691528,
      "learning_rate": 0.00037856776619381305,
      "loss": 0.3398,
      "step": 2843
    },
    {
      "epoch": 0.0121520804669407,
      "grad_norm": 1.4420454502105713,
      "learning_rate": 0.000378525038454965,
      "loss": 0.5709,
      "step": 2844
    },
    {
      "epoch": 0.012156353350367896,
      "grad_norm": 0.9202257394790649,
      "learning_rate": 0.00037848231071611687,
      "loss": 0.3277,
      "step": 2845
    },
    {
      "epoch": 0.01216062623379509,
      "grad_norm": 1.8881425857543945,
      "learning_rate": 0.00037843958297726884,
      "loss": 0.6506,
      "step": 2846
    },
    {
      "epoch": 0.012164899117222284,
      "grad_norm": 1.367737889289856,
      "learning_rate": 0.00037839685523842075,
      "loss": 0.4049,
      "step": 2847
    },
    {
      "epoch": 0.012169172000649478,
      "grad_norm": 1.3484655618667603,
      "learning_rate": 0.0003783541274995727,
      "loss": 0.4208,
      "step": 2848
    },
    {
      "epoch": 0.012173444884076673,
      "grad_norm": 3.420928716659546,
      "learning_rate": 0.0003783113997607247,
      "loss": 1.3237,
      "step": 2849
    },
    {
      "epoch": 0.012177717767503867,
      "grad_norm": 1.4761745929718018,
      "learning_rate": 0.0003782686720218766,
      "loss": 0.4405,
      "step": 2850
    },
    {
      "epoch": 0.012181990650931061,
      "grad_norm": 1.882819414138794,
      "learning_rate": 0.00037822594428302855,
      "loss": 0.5694,
      "step": 2851
    },
    {
      "epoch": 0.012186263534358255,
      "grad_norm": 2.556609630584717,
      "learning_rate": 0.00037818321654418046,
      "loss": 1.1566,
      "step": 2852
    },
    {
      "epoch": 0.01219053641778545,
      "grad_norm": 3.221845865249634,
      "learning_rate": 0.00037814048880533243,
      "loss": 1.017,
      "step": 2853
    },
    {
      "epoch": 0.012194809301212644,
      "grad_norm": 1.4728277921676636,
      "learning_rate": 0.00037809776106648434,
      "loss": 0.3596,
      "step": 2854
    },
    {
      "epoch": 0.012199082184639838,
      "grad_norm": 2.2042503356933594,
      "learning_rate": 0.0003780550333276363,
      "loss": 0.7538,
      "step": 2855
    },
    {
      "epoch": 0.012203355068067034,
      "grad_norm": 0.9197901487350464,
      "learning_rate": 0.00037801230558878827,
      "loss": 0.2677,
      "step": 2856
    },
    {
      "epoch": 0.012207627951494228,
      "grad_norm": 3.3006184101104736,
      "learning_rate": 0.0003779695778499402,
      "loss": 1.0178,
      "step": 2857
    },
    {
      "epoch": 0.012211900834921421,
      "grad_norm": 1.6396713256835938,
      "learning_rate": 0.00037792685011109214,
      "loss": 0.5137,
      "step": 2858
    },
    {
      "epoch": 0.012216173718348615,
      "grad_norm": 0.710724413394928,
      "learning_rate": 0.00037788412237224405,
      "loss": 0.2003,
      "step": 2859
    },
    {
      "epoch": 0.01222044660177581,
      "grad_norm": 1.746980905532837,
      "learning_rate": 0.000377841394633396,
      "loss": 0.5567,
      "step": 2860
    },
    {
      "epoch": 0.012224719485203005,
      "grad_norm": 3.2461318969726562,
      "learning_rate": 0.00037779866689454793,
      "loss": 1.2562,
      "step": 2861
    },
    {
      "epoch": 0.012228992368630199,
      "grad_norm": 4.250312328338623,
      "learning_rate": 0.0003777559391556999,
      "loss": 1.6572,
      "step": 2862
    },
    {
      "epoch": 0.012233265252057394,
      "grad_norm": 1.9640758037567139,
      "learning_rate": 0.0003777132114168518,
      "loss": 0.744,
      "step": 2863
    },
    {
      "epoch": 0.012237538135484588,
      "grad_norm": 3.7267110347747803,
      "learning_rate": 0.00037767048367800377,
      "loss": 0.8697,
      "step": 2864
    },
    {
      "epoch": 0.012241811018911782,
      "grad_norm": 0.8476888537406921,
      "learning_rate": 0.00037762775593915574,
      "loss": 0.305,
      "step": 2865
    },
    {
      "epoch": 0.012246083902338976,
      "grad_norm": 1.4872081279754639,
      "learning_rate": 0.00037758502820030765,
      "loss": 0.2746,
      "step": 2866
    },
    {
      "epoch": 0.012250356785766171,
      "grad_norm": 3.7223033905029297,
      "learning_rate": 0.0003775423004614596,
      "loss": 1.6153,
      "step": 2867
    },
    {
      "epoch": 0.012254629669193365,
      "grad_norm": 1.4778612852096558,
      "learning_rate": 0.0003774995727226115,
      "loss": 0.2592,
      "step": 2868
    },
    {
      "epoch": 0.012258902552620559,
      "grad_norm": 0.6482308506965637,
      "learning_rate": 0.0003774568449837635,
      "loss": 0.249,
      "step": 2869
    },
    {
      "epoch": 0.012263175436047753,
      "grad_norm": 3.7813923358917236,
      "learning_rate": 0.0003774141172449154,
      "loss": 1.1604,
      "step": 2870
    },
    {
      "epoch": 0.012267448319474948,
      "grad_norm": 1.779746413230896,
      "learning_rate": 0.00037737138950606736,
      "loss": 0.5041,
      "step": 2871
    },
    {
      "epoch": 0.012271721202902142,
      "grad_norm": 3.8600873947143555,
      "learning_rate": 0.00037732866176721933,
      "loss": 1.3268,
      "step": 2872
    },
    {
      "epoch": 0.012275994086329336,
      "grad_norm": 2.9545960426330566,
      "learning_rate": 0.00037728593402837124,
      "loss": 0.7823,
      "step": 2873
    },
    {
      "epoch": 0.012280266969756532,
      "grad_norm": 1.904229760169983,
      "learning_rate": 0.0003772432062895232,
      "loss": 0.713,
      "step": 2874
    },
    {
      "epoch": 0.012284539853183726,
      "grad_norm": 3.7954232692718506,
      "learning_rate": 0.0003772004785506751,
      "loss": 1.1439,
      "step": 2875
    },
    {
      "epoch": 0.01228881273661092,
      "grad_norm": 1.6703686714172363,
      "learning_rate": 0.0003771577508118271,
      "loss": 0.5,
      "step": 2876
    },
    {
      "epoch": 0.012293085620038113,
      "grad_norm": 2.151055097579956,
      "learning_rate": 0.000377115023072979,
      "loss": 0.8396,
      "step": 2877
    },
    {
      "epoch": 0.012297358503465309,
      "grad_norm": 1.349586844444275,
      "learning_rate": 0.0003770722953341309,
      "loss": 0.206,
      "step": 2878
    },
    {
      "epoch": 0.012301631386892503,
      "grad_norm": 0.8659555315971375,
      "learning_rate": 0.00037702956759528287,
      "loss": 0.3056,
      "step": 2879
    },
    {
      "epoch": 0.012305904270319697,
      "grad_norm": 1.3224401473999023,
      "learning_rate": 0.0003769868398564348,
      "loss": 0.4075,
      "step": 2880
    },
    {
      "epoch": 0.012310177153746892,
      "grad_norm": 1.6742016077041626,
      "learning_rate": 0.00037694411211758674,
      "loss": 0.4897,
      "step": 2881
    },
    {
      "epoch": 0.012314450037174086,
      "grad_norm": 0.8625317215919495,
      "learning_rate": 0.00037690138437873865,
      "loss": 0.3234,
      "step": 2882
    },
    {
      "epoch": 0.01231872292060128,
      "grad_norm": 1.3623270988464355,
      "learning_rate": 0.0003768586566398906,
      "loss": 0.1964,
      "step": 2883
    },
    {
      "epoch": 0.012322995804028474,
      "grad_norm": 3.7960188388824463,
      "learning_rate": 0.00037681592890104253,
      "loss": 1.1249,
      "step": 2884
    },
    {
      "epoch": 0.01232726868745567,
      "grad_norm": 2.847215175628662,
      "learning_rate": 0.0003767732011621945,
      "loss": 0.8825,
      "step": 2885
    },
    {
      "epoch": 0.012331541570882863,
      "grad_norm": 3.6079788208007812,
      "learning_rate": 0.00037673047342334646,
      "loss": 0.5837,
      "step": 2886
    },
    {
      "epoch": 0.012335814454310057,
      "grad_norm": 2.866988182067871,
      "learning_rate": 0.00037668774568449837,
      "loss": 0.7936,
      "step": 2887
    },
    {
      "epoch": 0.012340087337737253,
      "grad_norm": 1.1808680295944214,
      "learning_rate": 0.00037664501794565033,
      "loss": 0.32,
      "step": 2888
    },
    {
      "epoch": 0.012344360221164447,
      "grad_norm": 3.4961471557617188,
      "learning_rate": 0.00037660229020680224,
      "loss": 1.142,
      "step": 2889
    },
    {
      "epoch": 0.01234863310459164,
      "grad_norm": 4.267301082611084,
      "learning_rate": 0.0003765595624679542,
      "loss": 1.2445,
      "step": 2890
    },
    {
      "epoch": 0.012352905988018834,
      "grad_norm": 2.3328018188476562,
      "learning_rate": 0.0003765168347291061,
      "loss": 0.7374,
      "step": 2891
    },
    {
      "epoch": 0.01235717887144603,
      "grad_norm": 4.154350757598877,
      "learning_rate": 0.0003764741069902581,
      "loss": 1.1926,
      "step": 2892
    },
    {
      "epoch": 0.012361451754873224,
      "grad_norm": 4.071198463439941,
      "learning_rate": 0.00037643137925141,
      "loss": 1.1777,
      "step": 2893
    },
    {
      "epoch": 0.012365724638300418,
      "grad_norm": 4.530990123748779,
      "learning_rate": 0.00037638865151256196,
      "loss": 0.9581,
      "step": 2894
    },
    {
      "epoch": 0.012369997521727611,
      "grad_norm": 2.8069345951080322,
      "learning_rate": 0.0003763459237737139,
      "loss": 0.7814,
      "step": 2895
    },
    {
      "epoch": 0.012374270405154807,
      "grad_norm": 2.2287275791168213,
      "learning_rate": 0.00037630319603486584,
      "loss": 0.6525,
      "step": 2896
    },
    {
      "epoch": 0.012378543288582,
      "grad_norm": 4.314441204071045,
      "learning_rate": 0.0003762604682960178,
      "loss": 1.5371,
      "step": 2897
    },
    {
      "epoch": 0.012382816172009195,
      "grad_norm": 4.049295902252197,
      "learning_rate": 0.0003762177405571697,
      "loss": 1.4498,
      "step": 2898
    },
    {
      "epoch": 0.01238708905543639,
      "grad_norm": 1.3009703159332275,
      "learning_rate": 0.0003761750128183217,
      "loss": 0.2974,
      "step": 2899
    },
    {
      "epoch": 0.012391361938863584,
      "grad_norm": 4.526175022125244,
      "learning_rate": 0.0003761322850794736,
      "loss": 1.8288,
      "step": 2900
    },
    {
      "epoch": 0.012395634822290778,
      "grad_norm": 1.8458513021469116,
      "learning_rate": 0.00037608955734062555,
      "loss": 0.567,
      "step": 2901
    },
    {
      "epoch": 0.012399907705717972,
      "grad_norm": 1.6943271160125732,
      "learning_rate": 0.0003760468296017775,
      "loss": 0.4894,
      "step": 2902
    },
    {
      "epoch": 0.012404180589145167,
      "grad_norm": 1.6018880605697632,
      "learning_rate": 0.00037600410186292943,
      "loss": 0.5041,
      "step": 2903
    },
    {
      "epoch": 0.012408453472572361,
      "grad_norm": 4.532155990600586,
      "learning_rate": 0.0003759613741240814,
      "loss": 1.4725,
      "step": 2904
    },
    {
      "epoch": 0.012412726355999555,
      "grad_norm": 1.7290828227996826,
      "learning_rate": 0.0003759186463852333,
      "loss": 0.6734,
      "step": 2905
    },
    {
      "epoch": 0.01241699923942675,
      "grad_norm": 1.764163613319397,
      "learning_rate": 0.00037587591864638527,
      "loss": 0.343,
      "step": 2906
    },
    {
      "epoch": 0.012421272122853945,
      "grad_norm": 0.8025105595588684,
      "learning_rate": 0.0003758331909075372,
      "loss": 0.2531,
      "step": 2907
    },
    {
      "epoch": 0.012425545006281138,
      "grad_norm": 0.8835787773132324,
      "learning_rate": 0.00037579046316868914,
      "loss": 0.2774,
      "step": 2908
    },
    {
      "epoch": 0.012429817889708332,
      "grad_norm": 3.908082962036133,
      "learning_rate": 0.0003757477354298411,
      "loss": 1.4476,
      "step": 2909
    },
    {
      "epoch": 0.012434090773135528,
      "grad_norm": 1.126798152923584,
      "learning_rate": 0.000375705007690993,
      "loss": 0.4062,
      "step": 2910
    },
    {
      "epoch": 0.012438363656562722,
      "grad_norm": 3.944378614425659,
      "learning_rate": 0.00037566227995214493,
      "loss": 1.5937,
      "step": 2911
    },
    {
      "epoch": 0.012442636539989916,
      "grad_norm": 2.1171600818634033,
      "learning_rate": 0.00037561955221329684,
      "loss": 0.5272,
      "step": 2912
    },
    {
      "epoch": 0.012446909423417111,
      "grad_norm": 0.5464980602264404,
      "learning_rate": 0.0003755768244744488,
      "loss": 0.1903,
      "step": 2913
    },
    {
      "epoch": 0.012451182306844305,
      "grad_norm": 1.9547940492630005,
      "learning_rate": 0.0003755340967356007,
      "loss": 0.6145,
      "step": 2914
    },
    {
      "epoch": 0.012455455190271499,
      "grad_norm": 5.3968024253845215,
      "learning_rate": 0.0003754913689967527,
      "loss": 1.7359,
      "step": 2915
    },
    {
      "epoch": 0.012459728073698693,
      "grad_norm": 2.356212615966797,
      "learning_rate": 0.00037544864125790465,
      "loss": 0.8179,
      "step": 2916
    },
    {
      "epoch": 0.012464000957125888,
      "grad_norm": 3.1288750171661377,
      "learning_rate": 0.00037540591351905656,
      "loss": 1.355,
      "step": 2917
    },
    {
      "epoch": 0.012468273840553082,
      "grad_norm": 1.0157816410064697,
      "learning_rate": 0.0003753631857802085,
      "loss": 0.3348,
      "step": 2918
    },
    {
      "epoch": 0.012472546723980276,
      "grad_norm": 1.948346734046936,
      "learning_rate": 0.00037532045804136043,
      "loss": 0.5945,
      "step": 2919
    },
    {
      "epoch": 0.01247681960740747,
      "grad_norm": 1.4832303524017334,
      "learning_rate": 0.0003752777303025124,
      "loss": 0.5347,
      "step": 2920
    },
    {
      "epoch": 0.012481092490834666,
      "grad_norm": 0.9744424223899841,
      "learning_rate": 0.0003752350025636643,
      "loss": 0.3197,
      "step": 2921
    },
    {
      "epoch": 0.01248536537426186,
      "grad_norm": 2.1410326957702637,
      "learning_rate": 0.0003751922748248163,
      "loss": 1.0582,
      "step": 2922
    },
    {
      "epoch": 0.012489638257689053,
      "grad_norm": 0.9679207801818848,
      "learning_rate": 0.00037514954708596824,
      "loss": 0.4773,
      "step": 2923
    },
    {
      "epoch": 0.012493911141116249,
      "grad_norm": 3.9595160484313965,
      "learning_rate": 0.00037510681934712015,
      "loss": 1.0971,
      "step": 2924
    },
    {
      "epoch": 0.012498184024543443,
      "grad_norm": 1.5462092161178589,
      "learning_rate": 0.0003750640916082721,
      "loss": 1.0869,
      "step": 2925
    },
    {
      "epoch": 0.012502456907970637,
      "grad_norm": 0.8366488218307495,
      "learning_rate": 0.000375021363869424,
      "loss": 0.2206,
      "step": 2926
    },
    {
      "epoch": 0.01250672979139783,
      "grad_norm": 2.070749282836914,
      "learning_rate": 0.000374978636130576,
      "loss": 0.7137,
      "step": 2927
    },
    {
      "epoch": 0.012511002674825026,
      "grad_norm": 3.565843343734741,
      "learning_rate": 0.0003749359083917279,
      "loss": 1.0238,
      "step": 2928
    },
    {
      "epoch": 0.01251527555825222,
      "grad_norm": 3.155750274658203,
      "learning_rate": 0.00037489318065287987,
      "loss": 1.1853,
      "step": 2929
    },
    {
      "epoch": 0.012519548441679414,
      "grad_norm": 3.8139138221740723,
      "learning_rate": 0.0003748504529140318,
      "loss": 0.9229,
      "step": 2930
    },
    {
      "epoch": 0.01252382132510661,
      "grad_norm": 1.9538953304290771,
      "learning_rate": 0.00037480772517518374,
      "loss": 0.6833,
      "step": 2931
    },
    {
      "epoch": 0.012528094208533803,
      "grad_norm": 0.833999514579773,
      "learning_rate": 0.0003747649974363357,
      "loss": 0.4185,
      "step": 2932
    },
    {
      "epoch": 0.012532367091960997,
      "grad_norm": 1.99297034740448,
      "learning_rate": 0.0003747222696974876,
      "loss": 0.9673,
      "step": 2933
    },
    {
      "epoch": 0.01253663997538819,
      "grad_norm": 1.9621338844299316,
      "learning_rate": 0.0003746795419586396,
      "loss": 0.7528,
      "step": 2934
    },
    {
      "epoch": 0.012540912858815386,
      "grad_norm": 3.987091541290283,
      "learning_rate": 0.0003746368142197915,
      "loss": 0.9357,
      "step": 2935
    },
    {
      "epoch": 0.01254518574224258,
      "grad_norm": 3.923398017883301,
      "learning_rate": 0.00037459408648094346,
      "loss": 0.9089,
      "step": 2936
    },
    {
      "epoch": 0.012549458625669774,
      "grad_norm": 1.4038320779800415,
      "learning_rate": 0.00037455135874209537,
      "loss": 0.3305,
      "step": 2937
    },
    {
      "epoch": 0.012553731509096968,
      "grad_norm": 1.916987657546997,
      "learning_rate": 0.00037450863100324733,
      "loss": 0.5496,
      "step": 2938
    },
    {
      "epoch": 0.012558004392524164,
      "grad_norm": 1.286906361579895,
      "learning_rate": 0.0003744659032643993,
      "loss": 0.2659,
      "step": 2939
    },
    {
      "epoch": 0.012562277275951357,
      "grad_norm": 2.7985615730285645,
      "learning_rate": 0.0003744231755255512,
      "loss": 1.0213,
      "step": 2940
    },
    {
      "epoch": 0.012566550159378551,
      "grad_norm": 1.660894751548767,
      "learning_rate": 0.0003743804477867032,
      "loss": 0.5542,
      "step": 2941
    },
    {
      "epoch": 0.012570823042805747,
      "grad_norm": 1.9599286317825317,
      "learning_rate": 0.0003743377200478551,
      "loss": 0.7288,
      "step": 2942
    },
    {
      "epoch": 0.01257509592623294,
      "grad_norm": 2.0507736206054688,
      "learning_rate": 0.000374294992309007,
      "loss": 0.7059,
      "step": 2943
    },
    {
      "epoch": 0.012579368809660135,
      "grad_norm": 0.831895112991333,
      "learning_rate": 0.0003742522645701589,
      "loss": 0.2064,
      "step": 2944
    },
    {
      "epoch": 0.012583641693087328,
      "grad_norm": 2.3823463916778564,
      "learning_rate": 0.00037420953683131087,
      "loss": 0.6528,
      "step": 2945
    },
    {
      "epoch": 0.012587914576514524,
      "grad_norm": 2.3702354431152344,
      "learning_rate": 0.00037416680909246284,
      "loss": 0.6503,
      "step": 2946
    },
    {
      "epoch": 0.012592187459941718,
      "grad_norm": 1.6194720268249512,
      "learning_rate": 0.00037412408135361475,
      "loss": 0.5649,
      "step": 2947
    },
    {
      "epoch": 0.012596460343368912,
      "grad_norm": 3.6099209785461426,
      "learning_rate": 0.0003740813536147667,
      "loss": 0.7634,
      "step": 2948
    },
    {
      "epoch": 0.012600733226796107,
      "grad_norm": 2.819157123565674,
      "learning_rate": 0.0003740386258759186,
      "loss": 1.1999,
      "step": 2949
    },
    {
      "epoch": 0.012605006110223301,
      "grad_norm": 1.7036105394363403,
      "learning_rate": 0.0003739958981370706,
      "loss": 0.5837,
      "step": 2950
    },
    {
      "epoch": 0.012609278993650495,
      "grad_norm": 1.6743123531341553,
      "learning_rate": 0.0003739531703982225,
      "loss": 0.5519,
      "step": 2951
    },
    {
      "epoch": 0.012613551877077689,
      "grad_norm": 2.3647286891937256,
      "learning_rate": 0.00037391044265937446,
      "loss": 0.8038,
      "step": 2952
    },
    {
      "epoch": 0.012617824760504884,
      "grad_norm": 2.256760597229004,
      "learning_rate": 0.00037386771492052643,
      "loss": 0.6004,
      "step": 2953
    },
    {
      "epoch": 0.012622097643932078,
      "grad_norm": 0.48838359117507935,
      "learning_rate": 0.00037382498718167834,
      "loss": 0.1088,
      "step": 2954
    },
    {
      "epoch": 0.012626370527359272,
      "grad_norm": 1.8905558586120605,
      "learning_rate": 0.0003737822594428303,
      "loss": 0.9097,
      "step": 2955
    },
    {
      "epoch": 0.012630643410786468,
      "grad_norm": 1.892434000968933,
      "learning_rate": 0.0003737395317039822,
      "loss": 0.9097,
      "step": 2956
    },
    {
      "epoch": 0.012634916294213662,
      "grad_norm": 4.103435039520264,
      "learning_rate": 0.0003736968039651342,
      "loss": 1.1659,
      "step": 2957
    },
    {
      "epoch": 0.012639189177640855,
      "grad_norm": 1.695467233657837,
      "learning_rate": 0.0003736540762262861,
      "loss": 0.4891,
      "step": 2958
    },
    {
      "epoch": 0.01264346206106805,
      "grad_norm": 1.156975507736206,
      "learning_rate": 0.00037361134848743806,
      "loss": 0.2533,
      "step": 2959
    },
    {
      "epoch": 0.012647734944495245,
      "grad_norm": 3.4790494441986084,
      "learning_rate": 0.00037356862074859,
      "loss": 1.1846,
      "step": 2960
    },
    {
      "epoch": 0.012652007827922439,
      "grad_norm": 1.1913152933120728,
      "learning_rate": 0.00037352589300974193,
      "loss": 0.362,
      "step": 2961
    },
    {
      "epoch": 0.012656280711349633,
      "grad_norm": 1.7238726615905762,
      "learning_rate": 0.0003734831652708939,
      "loss": 0.9803,
      "step": 2962
    },
    {
      "epoch": 0.012660553594776826,
      "grad_norm": 1.1549807786941528,
      "learning_rate": 0.0003734404375320458,
      "loss": 0.3435,
      "step": 2963
    },
    {
      "epoch": 0.012664826478204022,
      "grad_norm": 1.9631696939468384,
      "learning_rate": 0.00037339770979319777,
      "loss": 0.8047,
      "step": 2964
    },
    {
      "epoch": 0.012669099361631216,
      "grad_norm": 1.936266541481018,
      "learning_rate": 0.0003733549820543497,
      "loss": 0.7873,
      "step": 2965
    },
    {
      "epoch": 0.01267337224505841,
      "grad_norm": 4.681224822998047,
      "learning_rate": 0.00037331225431550165,
      "loss": 2.6623,
      "step": 2966
    },
    {
      "epoch": 0.012677645128485605,
      "grad_norm": 5.141765594482422,
      "learning_rate": 0.00037326952657665356,
      "loss": 0.9677,
      "step": 2967
    },
    {
      "epoch": 0.0126819180119128,
      "grad_norm": 0.8707472085952759,
      "learning_rate": 0.0003732267988378055,
      "loss": 0.2104,
      "step": 2968
    },
    {
      "epoch": 0.012686190895339993,
      "grad_norm": 2.8976047039031982,
      "learning_rate": 0.0003731840710989575,
      "loss": 0.8783,
      "step": 2969
    },
    {
      "epoch": 0.012690463778767187,
      "grad_norm": 3.542583465576172,
      "learning_rate": 0.0003731413433601094,
      "loss": 0.9681,
      "step": 2970
    },
    {
      "epoch": 0.012694736662194383,
      "grad_norm": 4.4666008949279785,
      "learning_rate": 0.00037309861562126136,
      "loss": 1.2813,
      "step": 2971
    },
    {
      "epoch": 0.012699009545621576,
      "grad_norm": 0.8290522694587708,
      "learning_rate": 0.0003730558878824133,
      "loss": 0.2069,
      "step": 2972
    },
    {
      "epoch": 0.01270328242904877,
      "grad_norm": 2.1240978240966797,
      "learning_rate": 0.00037301316014356524,
      "loss": 0.6703,
      "step": 2973
    },
    {
      "epoch": 0.012707555312475966,
      "grad_norm": 1.9658869504928589,
      "learning_rate": 0.00037297043240471715,
      "loss": 0.6852,
      "step": 2974
    },
    {
      "epoch": 0.01271182819590316,
      "grad_norm": 1.4009093046188354,
      "learning_rate": 0.0003729277046658691,
      "loss": 0.2401,
      "step": 2975
    },
    {
      "epoch": 0.012716101079330354,
      "grad_norm": 3.7469699382781982,
      "learning_rate": 0.000372884976927021,
      "loss": 1.0131,
      "step": 2976
    },
    {
      "epoch": 0.012720373962757547,
      "grad_norm": 2.670754909515381,
      "learning_rate": 0.00037284224918817294,
      "loss": 0.6814,
      "step": 2977
    },
    {
      "epoch": 0.012724646846184743,
      "grad_norm": 1.384973406791687,
      "learning_rate": 0.0003727995214493249,
      "loss": 0.209,
      "step": 2978
    },
    {
      "epoch": 0.012728919729611937,
      "grad_norm": 3.280975818634033,
      "learning_rate": 0.0003727567937104768,
      "loss": 0.832,
      "step": 2979
    },
    {
      "epoch": 0.01273319261303913,
      "grad_norm": 4.336636066436768,
      "learning_rate": 0.0003727140659716288,
      "loss": 1.8021,
      "step": 2980
    },
    {
      "epoch": 0.012737465496466325,
      "grad_norm": 5.3822712898254395,
      "learning_rate": 0.0003726713382327807,
      "loss": 1.3753,
      "step": 2981
    },
    {
      "epoch": 0.01274173837989352,
      "grad_norm": 0.8004788756370544,
      "learning_rate": 0.00037262861049393265,
      "loss": 0.2545,
      "step": 2982
    },
    {
      "epoch": 0.012746011263320714,
      "grad_norm": 6.044205188751221,
      "learning_rate": 0.0003725858827550846,
      "loss": 1.1596,
      "step": 2983
    },
    {
      "epoch": 0.012750284146747908,
      "grad_norm": 4.086915493011475,
      "learning_rate": 0.00037254315501623653,
      "loss": 1.7431,
      "step": 2984
    },
    {
      "epoch": 0.012754557030175103,
      "grad_norm": 0.7687281966209412,
      "learning_rate": 0.0003725004272773885,
      "loss": 0.2356,
      "step": 2985
    },
    {
      "epoch": 0.012758829913602297,
      "grad_norm": 3.3810977935791016,
      "learning_rate": 0.0003724576995385404,
      "loss": 1.1177,
      "step": 2986
    },
    {
      "epoch": 0.012763102797029491,
      "grad_norm": 3.705203056335449,
      "learning_rate": 0.00037241497179969237,
      "loss": 1.4844,
      "step": 2987
    },
    {
      "epoch": 0.012767375680456685,
      "grad_norm": 0.804902195930481,
      "learning_rate": 0.0003723722440608443,
      "loss": 0.2205,
      "step": 2988
    },
    {
      "epoch": 0.01277164856388388,
      "grad_norm": 0.5809627175331116,
      "learning_rate": 0.00037232951632199624,
      "loss": 0.1348,
      "step": 2989
    },
    {
      "epoch": 0.012775921447311074,
      "grad_norm": 2.0606634616851807,
      "learning_rate": 0.0003722867885831482,
      "loss": 0.5481,
      "step": 2990
    },
    {
      "epoch": 0.012780194330738268,
      "grad_norm": 3.9392659664154053,
      "learning_rate": 0.0003722440608443001,
      "loss": 0.9112,
      "step": 2991
    },
    {
      "epoch": 0.012784467214165464,
      "grad_norm": 3.4462296962738037,
      "learning_rate": 0.0003722013331054521,
      "loss": 1.2691,
      "step": 2992
    },
    {
      "epoch": 0.012788740097592658,
      "grad_norm": 5.390508651733398,
      "learning_rate": 0.000372158605366604,
      "loss": 0.9499,
      "step": 2993
    },
    {
      "epoch": 0.012793012981019852,
      "grad_norm": 0.7971832752227783,
      "learning_rate": 0.00037211587762775596,
      "loss": 0.2239,
      "step": 2994
    },
    {
      "epoch": 0.012797285864447045,
      "grad_norm": 4.633699417114258,
      "learning_rate": 0.00037207314988890787,
      "loss": 1.3271,
      "step": 2995
    },
    {
      "epoch": 0.012801558747874241,
      "grad_norm": 2.171800374984741,
      "learning_rate": 0.00037203042215005984,
      "loss": 0.6864,
      "step": 2996
    },
    {
      "epoch": 0.012805831631301435,
      "grad_norm": 1.9125839471817017,
      "learning_rate": 0.00037198769441121175,
      "loss": 0.6129,
      "step": 2997
    },
    {
      "epoch": 0.012810104514728629,
      "grad_norm": 1.1944471597671509,
      "learning_rate": 0.0003719449666723637,
      "loss": 0.6848,
      "step": 2998
    },
    {
      "epoch": 0.012814377398155824,
      "grad_norm": 3.0636703968048096,
      "learning_rate": 0.0003719022389335157,
      "loss": 0.8342,
      "step": 2999
    },
    {
      "epoch": 0.012818650281583018,
      "grad_norm": 0.716110348701477,
      "learning_rate": 0.0003718595111946676,
      "loss": 0.1865,
      "step": 3000
    },
    {
      "epoch": 0.012822923165010212,
      "grad_norm": 2.0910258293151855,
      "learning_rate": 0.00037181678345581955,
      "loss": 0.8398,
      "step": 3001
    },
    {
      "epoch": 0.012827196048437406,
      "grad_norm": 3.1801490783691406,
      "learning_rate": 0.00037177405571697146,
      "loss": 1.0219,
      "step": 3002
    },
    {
      "epoch": 0.012831468931864601,
      "grad_norm": 1.90681791305542,
      "learning_rate": 0.00037173132797812343,
      "loss": 0.527,
      "step": 3003
    },
    {
      "epoch": 0.012835741815291795,
      "grad_norm": 2.496406078338623,
      "learning_rate": 0.00037168860023927534,
      "loss": 0.652,
      "step": 3004
    },
    {
      "epoch": 0.01284001469871899,
      "grad_norm": 2.6563241481781006,
      "learning_rate": 0.0003716458725004273,
      "loss": 0.9897,
      "step": 3005
    },
    {
      "epoch": 0.012844287582146183,
      "grad_norm": 1.7530635595321655,
      "learning_rate": 0.00037160314476157927,
      "loss": 0.3858,
      "step": 3006
    },
    {
      "epoch": 0.012848560465573379,
      "grad_norm": 3.8998353481292725,
      "learning_rate": 0.0003715604170227312,
      "loss": 0.8707,
      "step": 3007
    },
    {
      "epoch": 0.012852833349000573,
      "grad_norm": 2.206343173980713,
      "learning_rate": 0.00037151768928388315,
      "loss": 0.7192,
      "step": 3008
    },
    {
      "epoch": 0.012857106232427766,
      "grad_norm": 2.8351941108703613,
      "learning_rate": 0.000371474961545035,
      "loss": 0.8585,
      "step": 3009
    },
    {
      "epoch": 0.012861379115854962,
      "grad_norm": 0.6169930696487427,
      "learning_rate": 0.00037143223380618697,
      "loss": 0.1542,
      "step": 3010
    },
    {
      "epoch": 0.012865651999282156,
      "grad_norm": 1.0509271621704102,
      "learning_rate": 0.0003713895060673389,
      "loss": 0.3322,
      "step": 3011
    },
    {
      "epoch": 0.01286992488270935,
      "grad_norm": 2.7340738773345947,
      "learning_rate": 0.00037134677832849084,
      "loss": 0.8109,
      "step": 3012
    },
    {
      "epoch": 0.012874197766136544,
      "grad_norm": 1.7874945402145386,
      "learning_rate": 0.0003713040505896428,
      "loss": 0.4429,
      "step": 3013
    },
    {
      "epoch": 0.012878470649563739,
      "grad_norm": 35.845577239990234,
      "learning_rate": 0.0003712613228507947,
      "loss": 5.1105,
      "step": 3014
    },
    {
      "epoch": 0.012882743532990933,
      "grad_norm": 0.6002996563911438,
      "learning_rate": 0.0003712185951119467,
      "loss": 0.1331,
      "step": 3015
    },
    {
      "epoch": 0.012887016416418127,
      "grad_norm": 4.198338508605957,
      "learning_rate": 0.0003711758673730986,
      "loss": 1.4389,
      "step": 3016
    },
    {
      "epoch": 0.012891289299845322,
      "grad_norm": 1.7627655267715454,
      "learning_rate": 0.00037113313963425056,
      "loss": 0.687,
      "step": 3017
    },
    {
      "epoch": 0.012895562183272516,
      "grad_norm": 2.1327059268951416,
      "learning_rate": 0.00037109041189540247,
      "loss": 0.6829,
      "step": 3018
    },
    {
      "epoch": 0.01289983506669971,
      "grad_norm": 1.0101372003555298,
      "learning_rate": 0.00037104768415655443,
      "loss": 0.269,
      "step": 3019
    },
    {
      "epoch": 0.012904107950126904,
      "grad_norm": 4.1311235427856445,
      "learning_rate": 0.0003710049564177064,
      "loss": 1.273,
      "step": 3020
    },
    {
      "epoch": 0.0129083808335541,
      "grad_norm": 2.671124219894409,
      "learning_rate": 0.0003709622286788583,
      "loss": 0.7886,
      "step": 3021
    },
    {
      "epoch": 0.012912653716981293,
      "grad_norm": 2.6705634593963623,
      "learning_rate": 0.0003709195009400103,
      "loss": 0.9722,
      "step": 3022
    },
    {
      "epoch": 0.012916926600408487,
      "grad_norm": 2.5649213790893555,
      "learning_rate": 0.0003708767732011622,
      "loss": 0.789,
      "step": 3023
    },
    {
      "epoch": 0.012921199483835683,
      "grad_norm": 1.2828682661056519,
      "learning_rate": 0.00037083404546231415,
      "loss": 0.4065,
      "step": 3024
    },
    {
      "epoch": 0.012925472367262877,
      "grad_norm": 1.2553592920303345,
      "learning_rate": 0.00037079131772346606,
      "loss": 0.3851,
      "step": 3025
    },
    {
      "epoch": 0.01292974525069007,
      "grad_norm": 1.4387929439544678,
      "learning_rate": 0.000370748589984618,
      "loss": 0.6341,
      "step": 3026
    },
    {
      "epoch": 0.012934018134117264,
      "grad_norm": 3.452160358428955,
      "learning_rate": 0.00037070586224577,
      "loss": 1.325,
      "step": 3027
    },
    {
      "epoch": 0.01293829101754446,
      "grad_norm": 2.1901602745056152,
      "learning_rate": 0.0003706631345069219,
      "loss": 0.727,
      "step": 3028
    },
    {
      "epoch": 0.012942563900971654,
      "grad_norm": 2.16204833984375,
      "learning_rate": 0.00037062040676807387,
      "loss": 0.5206,
      "step": 3029
    },
    {
      "epoch": 0.012946836784398848,
      "grad_norm": 3.6180102825164795,
      "learning_rate": 0.0003705776790292258,
      "loss": 2.1894,
      "step": 3030
    },
    {
      "epoch": 0.012951109667826042,
      "grad_norm": 1.6851623058319092,
      "learning_rate": 0.00037053495129037774,
      "loss": 0.5111,
      "step": 3031
    },
    {
      "epoch": 0.012955382551253237,
      "grad_norm": 2.252875328063965,
      "learning_rate": 0.00037049222355152965,
      "loss": 0.5861,
      "step": 3032
    },
    {
      "epoch": 0.012959655434680431,
      "grad_norm": 4.0609354972839355,
      "learning_rate": 0.0003704494958126816,
      "loss": 0.9848,
      "step": 3033
    },
    {
      "epoch": 0.012963928318107625,
      "grad_norm": 4.04249906539917,
      "learning_rate": 0.00037040676807383353,
      "loss": 0.7851,
      "step": 3034
    },
    {
      "epoch": 0.01296820120153482,
      "grad_norm": 2.367464303970337,
      "learning_rate": 0.0003703640403349855,
      "loss": 0.6007,
      "step": 3035
    },
    {
      "epoch": 0.012972474084962014,
      "grad_norm": 0.7804028391838074,
      "learning_rate": 0.00037032131259613746,
      "loss": 0.2662,
      "step": 3036
    },
    {
      "epoch": 0.012976746968389208,
      "grad_norm": 3.59770131111145,
      "learning_rate": 0.00037027858485728937,
      "loss": 1.9069,
      "step": 3037
    },
    {
      "epoch": 0.012981019851816402,
      "grad_norm": 0.4689517617225647,
      "learning_rate": 0.00037023585711844133,
      "loss": 0.1522,
      "step": 3038
    },
    {
      "epoch": 0.012985292735243598,
      "grad_norm": 4.109208583831787,
      "learning_rate": 0.00037019312937959325,
      "loss": 0.9697,
      "step": 3039
    },
    {
      "epoch": 0.012989565618670791,
      "grad_norm": 0.9745352268218994,
      "learning_rate": 0.0003701504016407452,
      "loss": 0.3361,
      "step": 3040
    },
    {
      "epoch": 0.012993838502097985,
      "grad_norm": 4.279012203216553,
      "learning_rate": 0.0003701076739018971,
      "loss": 1.5529,
      "step": 3041
    },
    {
      "epoch": 0.012998111385525181,
      "grad_norm": 0.49288472533226013,
      "learning_rate": 0.00037006494616304903,
      "loss": 0.1514,
      "step": 3042
    },
    {
      "epoch": 0.013002384268952375,
      "grad_norm": 1.725107192993164,
      "learning_rate": 0.000370022218424201,
      "loss": 0.5731,
      "step": 3043
    },
    {
      "epoch": 0.013006657152379569,
      "grad_norm": 2.233917713165283,
      "learning_rate": 0.0003699794906853529,
      "loss": 0.7186,
      "step": 3044
    },
    {
      "epoch": 0.013010930035806762,
      "grad_norm": 2.252530813217163,
      "learning_rate": 0.00036993676294650487,
      "loss": 0.7244,
      "step": 3045
    },
    {
      "epoch": 0.013015202919233958,
      "grad_norm": 1.4305349588394165,
      "learning_rate": 0.0003698940352076568,
      "loss": 0.5838,
      "step": 3046
    },
    {
      "epoch": 0.013019475802661152,
      "grad_norm": 0.6253552436828613,
      "learning_rate": 0.00036985130746880875,
      "loss": 0.207,
      "step": 3047
    },
    {
      "epoch": 0.013023748686088346,
      "grad_norm": 2.661024332046509,
      "learning_rate": 0.00036980857972996066,
      "loss": 0.7332,
      "step": 3048
    },
    {
      "epoch": 0.01302802156951554,
      "grad_norm": 5.8360676765441895,
      "learning_rate": 0.0003697658519911126,
      "loss": 1.4298,
      "step": 3049
    },
    {
      "epoch": 0.013032294452942735,
      "grad_norm": 1.85056471824646,
      "learning_rate": 0.0003697231242522646,
      "loss": 0.4241,
      "step": 3050
    },
    {
      "epoch": 0.013036567336369929,
      "grad_norm": 2.7609188556671143,
      "learning_rate": 0.0003696803965134165,
      "loss": 0.7406,
      "step": 3051
    },
    {
      "epoch": 0.013040840219797123,
      "grad_norm": 4.147908687591553,
      "learning_rate": 0.00036963766877456846,
      "loss": 1.2183,
      "step": 3052
    },
    {
      "epoch": 0.013045113103224319,
      "grad_norm": 4.232577800750732,
      "learning_rate": 0.0003695949410357204,
      "loss": 1.5243,
      "step": 3053
    },
    {
      "epoch": 0.013049385986651512,
      "grad_norm": 4.030705451965332,
      "learning_rate": 0.00036955221329687234,
      "loss": 1.193,
      "step": 3054
    },
    {
      "epoch": 0.013053658870078706,
      "grad_norm": 2.347538709640503,
      "learning_rate": 0.00036950948555802425,
      "loss": 0.6377,
      "step": 3055
    },
    {
      "epoch": 0.0130579317535059,
      "grad_norm": 2.264601230621338,
      "learning_rate": 0.0003694667578191762,
      "loss": 0.567,
      "step": 3056
    },
    {
      "epoch": 0.013062204636933096,
      "grad_norm": 2.120469331741333,
      "learning_rate": 0.0003694240300803282,
      "loss": 0.5355,
      "step": 3057
    },
    {
      "epoch": 0.01306647752036029,
      "grad_norm": 1.7004379034042358,
      "learning_rate": 0.0003693813023414801,
      "loss": 0.4448,
      "step": 3058
    },
    {
      "epoch": 0.013070750403787483,
      "grad_norm": 1.4386062622070312,
      "learning_rate": 0.00036933857460263206,
      "loss": 0.4201,
      "step": 3059
    },
    {
      "epoch": 0.013075023287214679,
      "grad_norm": 2.0813143253326416,
      "learning_rate": 0.00036929584686378397,
      "loss": 0.7085,
      "step": 3060
    },
    {
      "epoch": 0.013079296170641873,
      "grad_norm": 2.868567943572998,
      "learning_rate": 0.00036925311912493593,
      "loss": 0.7705,
      "step": 3061
    },
    {
      "epoch": 0.013083569054069067,
      "grad_norm": 0.8309544920921326,
      "learning_rate": 0.00036921039138608784,
      "loss": 0.2948,
      "step": 3062
    },
    {
      "epoch": 0.01308784193749626,
      "grad_norm": 2.855928897857666,
      "learning_rate": 0.0003691676636472398,
      "loss": 0.7638,
      "step": 3063
    },
    {
      "epoch": 0.013092114820923456,
      "grad_norm": 3.856997489929199,
      "learning_rate": 0.00036912493590839177,
      "loss": 1.1521,
      "step": 3064
    },
    {
      "epoch": 0.01309638770435065,
      "grad_norm": 1.5303618907928467,
      "learning_rate": 0.0003690822081695437,
      "loss": 0.3405,
      "step": 3065
    },
    {
      "epoch": 0.013100660587777844,
      "grad_norm": 3.4311118125915527,
      "learning_rate": 0.00036903948043069565,
      "loss": 1.2017,
      "step": 3066
    },
    {
      "epoch": 0.01310493347120504,
      "grad_norm": 2.622744083404541,
      "learning_rate": 0.00036899675269184756,
      "loss": 0.6765,
      "step": 3067
    },
    {
      "epoch": 0.013109206354632233,
      "grad_norm": 0.5017039775848389,
      "learning_rate": 0.0003689540249529995,
      "loss": 0.1897,
      "step": 3068
    },
    {
      "epoch": 0.013113479238059427,
      "grad_norm": 3.7615132331848145,
      "learning_rate": 0.00036891129721415143,
      "loss": 1.6074,
      "step": 3069
    },
    {
      "epoch": 0.013117752121486621,
      "grad_norm": 2.5142369270324707,
      "learning_rate": 0.0003688685694753034,
      "loss": 0.8839,
      "step": 3070
    },
    {
      "epoch": 0.013122025004913817,
      "grad_norm": 2.2553889751434326,
      "learning_rate": 0.0003688258417364553,
      "loss": 0.6046,
      "step": 3071
    },
    {
      "epoch": 0.01312629788834101,
      "grad_norm": 2.488970994949341,
      "learning_rate": 0.0003687831139976073,
      "loss": 0.8278,
      "step": 3072
    },
    {
      "epoch": 0.013130570771768204,
      "grad_norm": 3.209204912185669,
      "learning_rate": 0.00036874038625875924,
      "loss": 0.7631,
      "step": 3073
    },
    {
      "epoch": 0.013134843655195398,
      "grad_norm": 0.7962155342102051,
      "learning_rate": 0.00036869765851991115,
      "loss": 0.2556,
      "step": 3074
    },
    {
      "epoch": 0.013139116538622594,
      "grad_norm": 1.6690033674240112,
      "learning_rate": 0.00036865493078106306,
      "loss": 0.6194,
      "step": 3075
    },
    {
      "epoch": 0.013143389422049788,
      "grad_norm": 2.196092367172241,
      "learning_rate": 0.00036861220304221497,
      "loss": 0.557,
      "step": 3076
    },
    {
      "epoch": 0.013147662305476981,
      "grad_norm": 3.4558119773864746,
      "learning_rate": 0.00036856947530336694,
      "loss": 1.4823,
      "step": 3077
    },
    {
      "epoch": 0.013151935188904177,
      "grad_norm": 1.1208363771438599,
      "learning_rate": 0.00036852674756451885,
      "loss": 0.1971,
      "step": 3078
    },
    {
      "epoch": 0.013156208072331371,
      "grad_norm": 2.1937646865844727,
      "learning_rate": 0.0003684840198256708,
      "loss": 0.8204,
      "step": 3079
    },
    {
      "epoch": 0.013160480955758565,
      "grad_norm": 2.424701452255249,
      "learning_rate": 0.0003684412920868228,
      "loss": 1.0441,
      "step": 3080
    },
    {
      "epoch": 0.013164753839185759,
      "grad_norm": 2.350632429122925,
      "learning_rate": 0.0003683985643479747,
      "loss": 0.9832,
      "step": 3081
    },
    {
      "epoch": 0.013169026722612954,
      "grad_norm": 2.0494539737701416,
      "learning_rate": 0.00036835583660912665,
      "loss": 0.7459,
      "step": 3082
    },
    {
      "epoch": 0.013173299606040148,
      "grad_norm": 3.405538558959961,
      "learning_rate": 0.00036831310887027856,
      "loss": 1.119,
      "step": 3083
    },
    {
      "epoch": 0.013177572489467342,
      "grad_norm": 1.864802360534668,
      "learning_rate": 0.00036827038113143053,
      "loss": 0.4307,
      "step": 3084
    },
    {
      "epoch": 0.013181845372894537,
      "grad_norm": 1.8202651739120483,
      "learning_rate": 0.00036822765339258244,
      "loss": 0.5832,
      "step": 3085
    },
    {
      "epoch": 0.013186118256321731,
      "grad_norm": 3.706430673599243,
      "learning_rate": 0.0003681849256537344,
      "loss": 1.0709,
      "step": 3086
    },
    {
      "epoch": 0.013190391139748925,
      "grad_norm": 2.9741294384002686,
      "learning_rate": 0.00036814219791488637,
      "loss": 1.1472,
      "step": 3087
    },
    {
      "epoch": 0.013194664023176119,
      "grad_norm": 2.001169204711914,
      "learning_rate": 0.0003680994701760383,
      "loss": 0.5193,
      "step": 3088
    },
    {
      "epoch": 0.013198936906603315,
      "grad_norm": 1.1962823867797852,
      "learning_rate": 0.00036805674243719025,
      "loss": 0.3951,
      "step": 3089
    },
    {
      "epoch": 0.013203209790030508,
      "grad_norm": 1.5894614458084106,
      "learning_rate": 0.00036801401469834216,
      "loss": 0.3676,
      "step": 3090
    },
    {
      "epoch": 0.013207482673457702,
      "grad_norm": 1.621385097503662,
      "learning_rate": 0.0003679712869594941,
      "loss": 0.2943,
      "step": 3091
    },
    {
      "epoch": 0.013211755556884896,
      "grad_norm": 2.5905697345733643,
      "learning_rate": 0.00036792855922064603,
      "loss": 0.5907,
      "step": 3092
    },
    {
      "epoch": 0.013216028440312092,
      "grad_norm": 2.5486631393432617,
      "learning_rate": 0.000367885831481798,
      "loss": 0.8755,
      "step": 3093
    },
    {
      "epoch": 0.013220301323739286,
      "grad_norm": 1.603224515914917,
      "learning_rate": 0.00036784310374294996,
      "loss": 0.4236,
      "step": 3094
    },
    {
      "epoch": 0.01322457420716648,
      "grad_norm": 1.2164322137832642,
      "learning_rate": 0.0003678003760041019,
      "loss": 0.242,
      "step": 3095
    },
    {
      "epoch": 0.013228847090593675,
      "grad_norm": 1.645340919494629,
      "learning_rate": 0.00036775764826525384,
      "loss": 0.459,
      "step": 3096
    },
    {
      "epoch": 0.013233119974020869,
      "grad_norm": 2.806785821914673,
      "learning_rate": 0.00036771492052640575,
      "loss": 1.0757,
      "step": 3097
    },
    {
      "epoch": 0.013237392857448063,
      "grad_norm": 1.3438999652862549,
      "learning_rate": 0.0003676721927875577,
      "loss": 0.3941,
      "step": 3098
    },
    {
      "epoch": 0.013241665740875257,
      "grad_norm": 1.8042055368423462,
      "learning_rate": 0.0003676294650487096,
      "loss": 0.4146,
      "step": 3099
    },
    {
      "epoch": 0.013245938624302452,
      "grad_norm": 2.0050747394561768,
      "learning_rate": 0.0003675867373098616,
      "loss": 0.6333,
      "step": 3100
    },
    {
      "epoch": 0.013250211507729646,
      "grad_norm": 18.018550872802734,
      "learning_rate": 0.0003675440095710135,
      "loss": 1.7718,
      "step": 3101
    },
    {
      "epoch": 0.01325448439115684,
      "grad_norm": 2.2857894897460938,
      "learning_rate": 0.00036750128183216546,
      "loss": 0.4394,
      "step": 3102
    },
    {
      "epoch": 0.013258757274584036,
      "grad_norm": 1.9188868999481201,
      "learning_rate": 0.00036745855409331743,
      "loss": 0.5308,
      "step": 3103
    },
    {
      "epoch": 0.01326303015801123,
      "grad_norm": 1.8270647525787354,
      "learning_rate": 0.00036741582635446934,
      "loss": 0.4308,
      "step": 3104
    },
    {
      "epoch": 0.013267303041438423,
      "grad_norm": 2.324941873550415,
      "learning_rate": 0.0003673730986156213,
      "loss": 0.6111,
      "step": 3105
    },
    {
      "epoch": 0.013271575924865617,
      "grad_norm": 3.1984329223632812,
      "learning_rate": 0.0003673303708767732,
      "loss": 1.3461,
      "step": 3106
    },
    {
      "epoch": 0.013275848808292813,
      "grad_norm": 1.567922592163086,
      "learning_rate": 0.0003672876431379252,
      "loss": 0.3287,
      "step": 3107
    },
    {
      "epoch": 0.013280121691720007,
      "grad_norm": 3.4849472045898438,
      "learning_rate": 0.00036724491539907704,
      "loss": 1.1518,
      "step": 3108
    },
    {
      "epoch": 0.0132843945751472,
      "grad_norm": 3.7966067790985107,
      "learning_rate": 0.000367202187660229,
      "loss": 1.316,
      "step": 3109
    },
    {
      "epoch": 0.013288667458574396,
      "grad_norm": 2.0798697471618652,
      "learning_rate": 0.00036715945992138097,
      "loss": 0.6684,
      "step": 3110
    },
    {
      "epoch": 0.01329294034200159,
      "grad_norm": 2.0251593589782715,
      "learning_rate": 0.0003671167321825329,
      "loss": 0.3429,
      "step": 3111
    },
    {
      "epoch": 0.013297213225428784,
      "grad_norm": 1.6817598342895508,
      "learning_rate": 0.00036707400444368484,
      "loss": 0.3826,
      "step": 3112
    },
    {
      "epoch": 0.013301486108855978,
      "grad_norm": 3.328348398208618,
      "learning_rate": 0.00036703127670483675,
      "loss": 1.1125,
      "step": 3113
    },
    {
      "epoch": 0.013305758992283173,
      "grad_norm": 1.7535954713821411,
      "learning_rate": 0.0003669885489659887,
      "loss": 0.2961,
      "step": 3114
    },
    {
      "epoch": 0.013310031875710367,
      "grad_norm": 1.6427369117736816,
      "learning_rate": 0.00036694582122714063,
      "loss": 0.4391,
      "step": 3115
    },
    {
      "epoch": 0.013314304759137561,
      "grad_norm": 1.6066950559616089,
      "learning_rate": 0.0003669030934882926,
      "loss": 0.4194,
      "step": 3116
    },
    {
      "epoch": 0.013318577642564755,
      "grad_norm": 3.2263171672821045,
      "learning_rate": 0.00036686036574944456,
      "loss": 1.5259,
      "step": 3117
    },
    {
      "epoch": 0.01332285052599195,
      "grad_norm": 1.7761660814285278,
      "learning_rate": 0.00036681763801059647,
      "loss": 0.4333,
      "step": 3118
    },
    {
      "epoch": 0.013327123409419144,
      "grad_norm": 1.3424259424209595,
      "learning_rate": 0.00036677491027174844,
      "loss": 0.2772,
      "step": 3119
    },
    {
      "epoch": 0.013331396292846338,
      "grad_norm": 1.4763423204421997,
      "learning_rate": 0.00036673218253290035,
      "loss": 0.3706,
      "step": 3120
    },
    {
      "epoch": 0.013335669176273534,
      "grad_norm": 2.6125082969665527,
      "learning_rate": 0.0003666894547940523,
      "loss": 1.1419,
      "step": 3121
    },
    {
      "epoch": 0.013339942059700727,
      "grad_norm": 2.365673780441284,
      "learning_rate": 0.0003666467270552042,
      "loss": 0.7514,
      "step": 3122
    },
    {
      "epoch": 0.013344214943127921,
      "grad_norm": 3.6367478370666504,
      "learning_rate": 0.0003666039993163562,
      "loss": 0.9486,
      "step": 3123
    },
    {
      "epoch": 0.013348487826555115,
      "grad_norm": 1.9180396795272827,
      "learning_rate": 0.00036656127157750815,
      "loss": 1.1873,
      "step": 3124
    },
    {
      "epoch": 0.01335276070998231,
      "grad_norm": 1.1305123567581177,
      "learning_rate": 0.00036651854383866006,
      "loss": 0.2573,
      "step": 3125
    },
    {
      "epoch": 0.013357033593409505,
      "grad_norm": 2.5801987648010254,
      "learning_rate": 0.00036647581609981203,
      "loss": 0.5743,
      "step": 3126
    },
    {
      "epoch": 0.013361306476836698,
      "grad_norm": 2.246535301208496,
      "learning_rate": 0.00036643308836096394,
      "loss": 0.5871,
      "step": 3127
    },
    {
      "epoch": 0.013365579360263894,
      "grad_norm": 0.7990070581436157,
      "learning_rate": 0.0003663903606221159,
      "loss": 0.2438,
      "step": 3128
    },
    {
      "epoch": 0.013369852243691088,
      "grad_norm": 2.9723377227783203,
      "learning_rate": 0.0003663476328832678,
      "loss": 0.7938,
      "step": 3129
    },
    {
      "epoch": 0.013374125127118282,
      "grad_norm": 2.6021928787231445,
      "learning_rate": 0.0003663049051444198,
      "loss": 0.96,
      "step": 3130
    },
    {
      "epoch": 0.013378398010545476,
      "grad_norm": 0.7173303365707397,
      "learning_rate": 0.00036626217740557174,
      "loss": 0.1916,
      "step": 3131
    },
    {
      "epoch": 0.013382670893972671,
      "grad_norm": 3.2625784873962402,
      "learning_rate": 0.00036621944966672365,
      "loss": 0.5981,
      "step": 3132
    },
    {
      "epoch": 0.013386943777399865,
      "grad_norm": 3.635676145553589,
      "learning_rate": 0.0003661767219278756,
      "loss": 1.1164,
      "step": 3133
    },
    {
      "epoch": 0.013391216660827059,
      "grad_norm": 1.5386205911636353,
      "learning_rate": 0.00036613399418902753,
      "loss": 1.0641,
      "step": 3134
    },
    {
      "epoch": 0.013395489544254255,
      "grad_norm": 2.1458115577697754,
      "learning_rate": 0.0003660912664501795,
      "loss": 0.8835,
      "step": 3135
    },
    {
      "epoch": 0.013399762427681448,
      "grad_norm": 3.1445508003234863,
      "learning_rate": 0.0003660485387113314,
      "loss": 0.6711,
      "step": 3136
    },
    {
      "epoch": 0.013404035311108642,
      "grad_norm": 2.8461437225341797,
      "learning_rate": 0.00036600581097248337,
      "loss": 0.5736,
      "step": 3137
    },
    {
      "epoch": 0.013408308194535836,
      "grad_norm": 5.005611419677734,
      "learning_rate": 0.0003659630832336353,
      "loss": 1.4022,
      "step": 3138
    },
    {
      "epoch": 0.013412581077963032,
      "grad_norm": 2.2207653522491455,
      "learning_rate": 0.00036592035549478725,
      "loss": 0.9948,
      "step": 3139
    },
    {
      "epoch": 0.013416853961390226,
      "grad_norm": 2.898425817489624,
      "learning_rate": 0.0003658776277559392,
      "loss": 0.6834,
      "step": 3140
    },
    {
      "epoch": 0.01342112684481742,
      "grad_norm": 2.11234974861145,
      "learning_rate": 0.00036583490001709107,
      "loss": 0.7966,
      "step": 3141
    },
    {
      "epoch": 0.013425399728244613,
      "grad_norm": 2.476323127746582,
      "learning_rate": 0.00036579217227824303,
      "loss": 0.6379,
      "step": 3142
    },
    {
      "epoch": 0.013429672611671809,
      "grad_norm": 1.9365688562393188,
      "learning_rate": 0.00036574944453939494,
      "loss": 0.7719,
      "step": 3143
    },
    {
      "epoch": 0.013433945495099003,
      "grad_norm": 1.406269907951355,
      "learning_rate": 0.0003657067168005469,
      "loss": 0.3641,
      "step": 3144
    },
    {
      "epoch": 0.013438218378526197,
      "grad_norm": 1.9849140644073486,
      "learning_rate": 0.0003656639890616988,
      "loss": 0.7702,
      "step": 3145
    },
    {
      "epoch": 0.013442491261953392,
      "grad_norm": 1.817501425743103,
      "learning_rate": 0.0003656212613228508,
      "loss": 0.6348,
      "step": 3146
    },
    {
      "epoch": 0.013446764145380586,
      "grad_norm": 3.649024486541748,
      "learning_rate": 0.00036557853358400275,
      "loss": 0.9192,
      "step": 3147
    },
    {
      "epoch": 0.01345103702880778,
      "grad_norm": 12.279190063476562,
      "learning_rate": 0.00036553580584515466,
      "loss": 0.7694,
      "step": 3148
    },
    {
      "epoch": 0.013455309912234974,
      "grad_norm": 1.8210581541061401,
      "learning_rate": 0.0003654930781063066,
      "loss": 0.7517,
      "step": 3149
    },
    {
      "epoch": 0.01345958279566217,
      "grad_norm": 2.022346019744873,
      "learning_rate": 0.00036545035036745854,
      "loss": 0.3642,
      "step": 3150
    },
    {
      "epoch": 0.013463855679089363,
      "grad_norm": 2.9648454189300537,
      "learning_rate": 0.0003654076226286105,
      "loss": 0.9654,
      "step": 3151
    },
    {
      "epoch": 0.013468128562516557,
      "grad_norm": 1.6510149240493774,
      "learning_rate": 0.0003653648948897624,
      "loss": 0.6456,
      "step": 3152
    },
    {
      "epoch": 0.013472401445943753,
      "grad_norm": 1.641129493713379,
      "learning_rate": 0.0003653221671509144,
      "loss": 0.3818,
      "step": 3153
    },
    {
      "epoch": 0.013476674329370946,
      "grad_norm": 1.6622790098190308,
      "learning_rate": 0.00036527943941206634,
      "loss": 0.7226,
      "step": 3154
    },
    {
      "epoch": 0.01348094721279814,
      "grad_norm": 3.12229323387146,
      "learning_rate": 0.00036523671167321825,
      "loss": 1.3025,
      "step": 3155
    },
    {
      "epoch": 0.013485220096225334,
      "grad_norm": 1.6775974035263062,
      "learning_rate": 0.0003651939839343702,
      "loss": 0.4592,
      "step": 3156
    },
    {
      "epoch": 0.01348949297965253,
      "grad_norm": 1.5955743789672852,
      "learning_rate": 0.00036515125619552213,
      "loss": 0.3652,
      "step": 3157
    },
    {
      "epoch": 0.013493765863079724,
      "grad_norm": 2.6143882274627686,
      "learning_rate": 0.0003651085284566741,
      "loss": 0.7662,
      "step": 3158
    },
    {
      "epoch": 0.013498038746506917,
      "grad_norm": 1.1349256038665771,
      "learning_rate": 0.000365065800717826,
      "loss": 0.272,
      "step": 3159
    },
    {
      "epoch": 0.013502311629934111,
      "grad_norm": 2.0820086002349854,
      "learning_rate": 0.00036502307297897797,
      "loss": 0.8109,
      "step": 3160
    },
    {
      "epoch": 0.013506584513361307,
      "grad_norm": 1.6312065124511719,
      "learning_rate": 0.00036498034524012993,
      "loss": 0.5426,
      "step": 3161
    },
    {
      "epoch": 0.0135108573967885,
      "grad_norm": 2.7338831424713135,
      "learning_rate": 0.00036493761750128184,
      "loss": 1.0923,
      "step": 3162
    },
    {
      "epoch": 0.013515130280215695,
      "grad_norm": 4.128474712371826,
      "learning_rate": 0.0003648948897624338,
      "loss": 1.0864,
      "step": 3163
    },
    {
      "epoch": 0.01351940316364289,
      "grad_norm": 3.843251943588257,
      "learning_rate": 0.0003648521620235857,
      "loss": 1.6386,
      "step": 3164
    },
    {
      "epoch": 0.013523676047070084,
      "grad_norm": 1.9343879222869873,
      "learning_rate": 0.0003648094342847377,
      "loss": 0.6222,
      "step": 3165
    },
    {
      "epoch": 0.013527948930497278,
      "grad_norm": 2.069502830505371,
      "learning_rate": 0.0003647667065458896,
      "loss": 0.5944,
      "step": 3166
    },
    {
      "epoch": 0.013532221813924472,
      "grad_norm": 1.9334181547164917,
      "learning_rate": 0.00036472397880704156,
      "loss": 0.8107,
      "step": 3167
    },
    {
      "epoch": 0.013536494697351667,
      "grad_norm": 1.5038862228393555,
      "learning_rate": 0.0003646812510681935,
      "loss": 0.604,
      "step": 3168
    },
    {
      "epoch": 0.013540767580778861,
      "grad_norm": 4.8138837814331055,
      "learning_rate": 0.00036463852332934544,
      "loss": 2.0888,
      "step": 3169
    },
    {
      "epoch": 0.013545040464206055,
      "grad_norm": 1.0161892175674438,
      "learning_rate": 0.0003645957955904974,
      "loss": 0.2182,
      "step": 3170
    },
    {
      "epoch": 0.01354931334763325,
      "grad_norm": 0.887021005153656,
      "learning_rate": 0.0003645530678516493,
      "loss": 0.2118,
      "step": 3171
    },
    {
      "epoch": 0.013553586231060444,
      "grad_norm": 2.119403839111328,
      "learning_rate": 0.0003645103401128013,
      "loss": 0.7706,
      "step": 3172
    },
    {
      "epoch": 0.013557859114487638,
      "grad_norm": 1.2544353008270264,
      "learning_rate": 0.0003644676123739532,
      "loss": 0.4815,
      "step": 3173
    },
    {
      "epoch": 0.013562131997914832,
      "grad_norm": 1.543700933456421,
      "learning_rate": 0.0003644248846351051,
      "loss": 0.6521,
      "step": 3174
    },
    {
      "epoch": 0.013566404881342028,
      "grad_norm": 2.985421895980835,
      "learning_rate": 0.000364382156896257,
      "loss": 0.6408,
      "step": 3175
    },
    {
      "epoch": 0.013570677764769222,
      "grad_norm": 0.4901139438152313,
      "learning_rate": 0.000364339429157409,
      "loss": 0.1209,
      "step": 3176
    },
    {
      "epoch": 0.013574950648196416,
      "grad_norm": 1.0119801759719849,
      "learning_rate": 0.00036429670141856094,
      "loss": 0.3618,
      "step": 3177
    },
    {
      "epoch": 0.013579223531623611,
      "grad_norm": 3.0395328998565674,
      "learning_rate": 0.00036425397367971285,
      "loss": 0.8177,
      "step": 3178
    },
    {
      "epoch": 0.013583496415050805,
      "grad_norm": 2.179823398590088,
      "learning_rate": 0.0003642112459408648,
      "loss": 0.8682,
      "step": 3179
    },
    {
      "epoch": 0.013587769298477999,
      "grad_norm": 1.4959677457809448,
      "learning_rate": 0.0003641685182020167,
      "loss": 0.366,
      "step": 3180
    },
    {
      "epoch": 0.013592042181905193,
      "grad_norm": 2.8127851486206055,
      "learning_rate": 0.0003641257904631687,
      "loss": 1.1405,
      "step": 3181
    },
    {
      "epoch": 0.013596315065332388,
      "grad_norm": 3.574526309967041,
      "learning_rate": 0.0003640830627243206,
      "loss": 0.9803,
      "step": 3182
    },
    {
      "epoch": 0.013600587948759582,
      "grad_norm": 2.949051856994629,
      "learning_rate": 0.00036404033498547257,
      "loss": 0.6408,
      "step": 3183
    },
    {
      "epoch": 0.013604860832186776,
      "grad_norm": 0.4683346152305603,
      "learning_rate": 0.00036399760724662453,
      "loss": 0.1209,
      "step": 3184
    },
    {
      "epoch": 0.01360913371561397,
      "grad_norm": 1.814948558807373,
      "learning_rate": 0.00036395487950777644,
      "loss": 0.7061,
      "step": 3185
    },
    {
      "epoch": 0.013613406599041165,
      "grad_norm": 1.2824265956878662,
      "learning_rate": 0.0003639121517689284,
      "loss": 0.5327,
      "step": 3186
    },
    {
      "epoch": 0.01361767948246836,
      "grad_norm": 2.484203577041626,
      "learning_rate": 0.0003638694240300803,
      "loss": 0.524,
      "step": 3187
    },
    {
      "epoch": 0.013621952365895553,
      "grad_norm": 2.2856569290161133,
      "learning_rate": 0.0003638266962912323,
      "loss": 0.4647,
      "step": 3188
    },
    {
      "epoch": 0.013626225249322749,
      "grad_norm": 2.956667423248291,
      "learning_rate": 0.0003637839685523842,
      "loss": 1.0274,
      "step": 3189
    },
    {
      "epoch": 0.013630498132749943,
      "grad_norm": 1.2816412448883057,
      "learning_rate": 0.00036374124081353616,
      "loss": 0.4321,
      "step": 3190
    },
    {
      "epoch": 0.013634771016177136,
      "grad_norm": 1.3966189622879028,
      "learning_rate": 0.0003636985130746881,
      "loss": 0.3247,
      "step": 3191
    },
    {
      "epoch": 0.01363904389960433,
      "grad_norm": 1.4199459552764893,
      "learning_rate": 0.00036365578533584003,
      "loss": 0.45,
      "step": 3192
    },
    {
      "epoch": 0.013643316783031526,
      "grad_norm": 1.215383768081665,
      "learning_rate": 0.000363613057596992,
      "loss": 0.4947,
      "step": 3193
    },
    {
      "epoch": 0.01364758966645872,
      "grad_norm": 0.9433335065841675,
      "learning_rate": 0.0003635703298581439,
      "loss": 0.1995,
      "step": 3194
    },
    {
      "epoch": 0.013651862549885914,
      "grad_norm": 3.597623109817505,
      "learning_rate": 0.0003635276021192959,
      "loss": 1.4633,
      "step": 3195
    },
    {
      "epoch": 0.01365613543331311,
      "grad_norm": 3.7443788051605225,
      "learning_rate": 0.0003634848743804478,
      "loss": 0.9188,
      "step": 3196
    },
    {
      "epoch": 0.013660408316740303,
      "grad_norm": 2.080324649810791,
      "learning_rate": 0.00036344214664159975,
      "loss": 0.561,
      "step": 3197
    },
    {
      "epoch": 0.013664681200167497,
      "grad_norm": 2.93851637840271,
      "learning_rate": 0.0003633994189027517,
      "loss": 0.8935,
      "step": 3198
    },
    {
      "epoch": 0.01366895408359469,
      "grad_norm": 1.6011189222335815,
      "learning_rate": 0.0003633566911639036,
      "loss": 0.3428,
      "step": 3199
    },
    {
      "epoch": 0.013673226967021886,
      "grad_norm": 2.9679622650146484,
      "learning_rate": 0.0003633139634250556,
      "loss": 0.8316,
      "step": 3200
    },
    {
      "epoch": 0.01367749985044908,
      "grad_norm": 2.2742421627044678,
      "learning_rate": 0.0003632712356862075,
      "loss": 0.773,
      "step": 3201
    },
    {
      "epoch": 0.013681772733876274,
      "grad_norm": 2.0077757835388184,
      "learning_rate": 0.00036322850794735947,
      "loss": 0.6998,
      "step": 3202
    },
    {
      "epoch": 0.013686045617303468,
      "grad_norm": 1.2982341051101685,
      "learning_rate": 0.0003631857802085114,
      "loss": 0.4645,
      "step": 3203
    },
    {
      "epoch": 0.013690318500730663,
      "grad_norm": 1.2710989713668823,
      "learning_rate": 0.00036314305246966334,
      "loss": 0.4266,
      "step": 3204
    },
    {
      "epoch": 0.013694591384157857,
      "grad_norm": 2.5990707874298096,
      "learning_rate": 0.00036310032473081525,
      "loss": 0.7867,
      "step": 3205
    },
    {
      "epoch": 0.013698864267585051,
      "grad_norm": 1.2657040357589722,
      "learning_rate": 0.00036305759699196716,
      "loss": 0.3612,
      "step": 3206
    },
    {
      "epoch": 0.013703137151012247,
      "grad_norm": 3.549237012863159,
      "learning_rate": 0.00036301486925311913,
      "loss": 1.1448,
      "step": 3207
    },
    {
      "epoch": 0.01370741003443944,
      "grad_norm": 4.146829128265381,
      "learning_rate": 0.00036297214151427104,
      "loss": 1.3254,
      "step": 3208
    },
    {
      "epoch": 0.013711682917866634,
      "grad_norm": 1.851325511932373,
      "learning_rate": 0.000362929413775423,
      "loss": 0.7511,
      "step": 3209
    },
    {
      "epoch": 0.013715955801293828,
      "grad_norm": 2.635399341583252,
      "learning_rate": 0.0003628866860365749,
      "loss": 0.7938,
      "step": 3210
    },
    {
      "epoch": 0.013720228684721024,
      "grad_norm": 1.141983985900879,
      "learning_rate": 0.0003628439582977269,
      "loss": 0.3843,
      "step": 3211
    },
    {
      "epoch": 0.013724501568148218,
      "grad_norm": 4.349792957305908,
      "learning_rate": 0.0003628012305588788,
      "loss": 1.9535,
      "step": 3212
    },
    {
      "epoch": 0.013728774451575412,
      "grad_norm": 2.027848243713379,
      "learning_rate": 0.00036275850282003076,
      "loss": 1.2344,
      "step": 3213
    },
    {
      "epoch": 0.013733047335002607,
      "grad_norm": 0.95072340965271,
      "learning_rate": 0.0003627157750811827,
      "loss": 0.2521,
      "step": 3214
    },
    {
      "epoch": 0.013737320218429801,
      "grad_norm": 0.7381824254989624,
      "learning_rate": 0.00036267304734233463,
      "loss": 0.2249,
      "step": 3215
    },
    {
      "epoch": 0.013741593101856995,
      "grad_norm": 4.868534088134766,
      "learning_rate": 0.0003626303196034866,
      "loss": 2.4725,
      "step": 3216
    },
    {
      "epoch": 0.013745865985284189,
      "grad_norm": 3.596683979034424,
      "learning_rate": 0.0003625875918646385,
      "loss": 1.4292,
      "step": 3217
    },
    {
      "epoch": 0.013750138868711384,
      "grad_norm": 3.0531907081604004,
      "learning_rate": 0.00036254486412579047,
      "loss": 0.8164,
      "step": 3218
    },
    {
      "epoch": 0.013754411752138578,
      "grad_norm": 4.10195779800415,
      "learning_rate": 0.0003625021363869424,
      "loss": 1.4771,
      "step": 3219
    },
    {
      "epoch": 0.013758684635565772,
      "grad_norm": 1.8270626068115234,
      "learning_rate": 0.00036245940864809435,
      "loss": 0.649,
      "step": 3220
    },
    {
      "epoch": 0.013762957518992968,
      "grad_norm": 3.404202938079834,
      "learning_rate": 0.0003624166809092463,
      "loss": 1.2618,
      "step": 3221
    },
    {
      "epoch": 0.013767230402420162,
      "grad_norm": 0.5002328157424927,
      "learning_rate": 0.0003623739531703982,
      "loss": 0.1646,
      "step": 3222
    },
    {
      "epoch": 0.013771503285847355,
      "grad_norm": 2.0671892166137695,
      "learning_rate": 0.0003623312254315502,
      "loss": 0.4924,
      "step": 3223
    },
    {
      "epoch": 0.01377577616927455,
      "grad_norm": 1.5549789667129517,
      "learning_rate": 0.0003622884976927021,
      "loss": 0.5356,
      "step": 3224
    },
    {
      "epoch": 0.013780049052701745,
      "grad_norm": 2.0563747882843018,
      "learning_rate": 0.00036224576995385406,
      "loss": 1.0551,
      "step": 3225
    },
    {
      "epoch": 0.013784321936128939,
      "grad_norm": 1.5584875345230103,
      "learning_rate": 0.000362203042215006,
      "loss": 0.8941,
      "step": 3226
    },
    {
      "epoch": 0.013788594819556133,
      "grad_norm": 0.6896238327026367,
      "learning_rate": 0.00036216031447615794,
      "loss": 0.2904,
      "step": 3227
    },
    {
      "epoch": 0.013792867702983326,
      "grad_norm": 2.7328884601593018,
      "learning_rate": 0.0003621175867373099,
      "loss": 0.7773,
      "step": 3228
    },
    {
      "epoch": 0.013797140586410522,
      "grad_norm": 0.6191868782043457,
      "learning_rate": 0.0003620748589984618,
      "loss": 0.2461,
      "step": 3229
    },
    {
      "epoch": 0.013801413469837716,
      "grad_norm": 4.849890232086182,
      "learning_rate": 0.0003620321312596138,
      "loss": 1.4569,
      "step": 3230
    },
    {
      "epoch": 0.01380568635326491,
      "grad_norm": 1.8740593194961548,
      "learning_rate": 0.0003619894035207657,
      "loss": 0.7612,
      "step": 3231
    },
    {
      "epoch": 0.013809959236692105,
      "grad_norm": 1.4757484197616577,
      "learning_rate": 0.00036194667578191766,
      "loss": 0.8179,
      "step": 3232
    },
    {
      "epoch": 0.013814232120119299,
      "grad_norm": 3.0140421390533447,
      "learning_rate": 0.00036190394804306957,
      "loss": 0.7888,
      "step": 3233
    },
    {
      "epoch": 0.013818505003546493,
      "grad_norm": 2.5188558101654053,
      "learning_rate": 0.00036186122030422153,
      "loss": 0.6883,
      "step": 3234
    },
    {
      "epoch": 0.013822777886973687,
      "grad_norm": 1.1851032972335815,
      "learning_rate": 0.0003618184925653735,
      "loss": 0.5318,
      "step": 3235
    },
    {
      "epoch": 0.013827050770400882,
      "grad_norm": 0.5819365382194519,
      "learning_rate": 0.0003617757648265254,
      "loss": 0.2278,
      "step": 3236
    },
    {
      "epoch": 0.013831323653828076,
      "grad_norm": 2.7209417819976807,
      "learning_rate": 0.00036173303708767737,
      "loss": 1.1248,
      "step": 3237
    },
    {
      "epoch": 0.01383559653725527,
      "grad_norm": 3.070612907409668,
      "learning_rate": 0.0003616903093488293,
      "loss": 0.7658,
      "step": 3238
    },
    {
      "epoch": 0.013839869420682466,
      "grad_norm": 1.7691782712936401,
      "learning_rate": 0.0003616475816099812,
      "loss": 0.4998,
      "step": 3239
    },
    {
      "epoch": 0.01384414230410966,
      "grad_norm": 3.0272440910339355,
      "learning_rate": 0.0003616048538711331,
      "loss": 0.9886,
      "step": 3240
    },
    {
      "epoch": 0.013848415187536853,
      "grad_norm": 1.648836374282837,
      "learning_rate": 0.00036156212613228507,
      "loss": 1.0652,
      "step": 3241
    },
    {
      "epoch": 0.013852688070964047,
      "grad_norm": 1.7821818590164185,
      "learning_rate": 0.000361519398393437,
      "loss": 0.7613,
      "step": 3242
    },
    {
      "epoch": 0.013856960954391243,
      "grad_norm": 4.833991050720215,
      "learning_rate": 0.00036147667065458894,
      "loss": 0.9777,
      "step": 3243
    },
    {
      "epoch": 0.013861233837818437,
      "grad_norm": 1.7522963285446167,
      "learning_rate": 0.0003614339429157409,
      "loss": 0.5128,
      "step": 3244
    },
    {
      "epoch": 0.01386550672124563,
      "grad_norm": 3.996189832687378,
      "learning_rate": 0.0003613912151768928,
      "loss": 1.1308,
      "step": 3245
    },
    {
      "epoch": 0.013869779604672826,
      "grad_norm": 3.2915468215942383,
      "learning_rate": 0.0003613484874380448,
      "loss": 0.7292,
      "step": 3246
    },
    {
      "epoch": 0.01387405248810002,
      "grad_norm": 2.0636940002441406,
      "learning_rate": 0.0003613057596991967,
      "loss": 0.7249,
      "step": 3247
    },
    {
      "epoch": 0.013878325371527214,
      "grad_norm": 3.143683433532715,
      "learning_rate": 0.00036126303196034866,
      "loss": 1.2993,
      "step": 3248
    },
    {
      "epoch": 0.013882598254954408,
      "grad_norm": 1.232756495475769,
      "learning_rate": 0.00036122030422150057,
      "loss": 0.5667,
      "step": 3249
    },
    {
      "epoch": 0.013886871138381603,
      "grad_norm": 2.1511096954345703,
      "learning_rate": 0.00036117757648265254,
      "loss": 0.7782,
      "step": 3250
    },
    {
      "epoch": 0.013891144021808797,
      "grad_norm": 2.901746988296509,
      "learning_rate": 0.0003611348487438045,
      "loss": 0.8705,
      "step": 3251
    },
    {
      "epoch": 0.013895416905235991,
      "grad_norm": 1.4228744506835938,
      "learning_rate": 0.0003610921210049564,
      "loss": 0.7867,
      "step": 3252
    },
    {
      "epoch": 0.013899689788663185,
      "grad_norm": 4.034314155578613,
      "learning_rate": 0.0003610493932661084,
      "loss": 2.5026,
      "step": 3253
    },
    {
      "epoch": 0.01390396267209038,
      "grad_norm": 3.0250630378723145,
      "learning_rate": 0.0003610066655272603,
      "loss": 0.734,
      "step": 3254
    },
    {
      "epoch": 0.013908235555517574,
      "grad_norm": 2.611147880554199,
      "learning_rate": 0.00036096393778841225,
      "loss": 0.6644,
      "step": 3255
    },
    {
      "epoch": 0.013912508438944768,
      "grad_norm": 4.6345648765563965,
      "learning_rate": 0.00036092121004956416,
      "loss": 0.643,
      "step": 3256
    },
    {
      "epoch": 0.013916781322371964,
      "grad_norm": 0.626186192035675,
      "learning_rate": 0.00036087848231071613,
      "loss": 0.2396,
      "step": 3257
    },
    {
      "epoch": 0.013921054205799158,
      "grad_norm": 2.958366870880127,
      "learning_rate": 0.0003608357545718681,
      "loss": 0.5994,
      "step": 3258
    },
    {
      "epoch": 0.013925327089226351,
      "grad_norm": 3.0250964164733887,
      "learning_rate": 0.00036079302683302,
      "loss": 0.8021,
      "step": 3259
    },
    {
      "epoch": 0.013929599972653545,
      "grad_norm": 2.2224631309509277,
      "learning_rate": 0.00036075029909417197,
      "loss": 0.5747,
      "step": 3260
    },
    {
      "epoch": 0.013933872856080741,
      "grad_norm": 0.6403116583824158,
      "learning_rate": 0.0003607075713553239,
      "loss": 0.2416,
      "step": 3261
    },
    {
      "epoch": 0.013938145739507935,
      "grad_norm": 3.4971909523010254,
      "learning_rate": 0.00036066484361647584,
      "loss": 1.3795,
      "step": 3262
    },
    {
      "epoch": 0.013942418622935129,
      "grad_norm": 3.13728928565979,
      "learning_rate": 0.00036062211587762776,
      "loss": 0.9069,
      "step": 3263
    },
    {
      "epoch": 0.013946691506362324,
      "grad_norm": 3.8849897384643555,
      "learning_rate": 0.0003605793881387797,
      "loss": 1.3215,
      "step": 3264
    },
    {
      "epoch": 0.013950964389789518,
      "grad_norm": 2.412639617919922,
      "learning_rate": 0.0003605366603999317,
      "loss": 0.6518,
      "step": 3265
    },
    {
      "epoch": 0.013955237273216712,
      "grad_norm": 1.882503867149353,
      "learning_rate": 0.0003604939326610836,
      "loss": 0.5354,
      "step": 3266
    },
    {
      "epoch": 0.013959510156643906,
      "grad_norm": 4.321911334991455,
      "learning_rate": 0.00036045120492223556,
      "loss": 1.5079,
      "step": 3267
    },
    {
      "epoch": 0.013963783040071101,
      "grad_norm": 4.990087032318115,
      "learning_rate": 0.00036040847718338747,
      "loss": 1.5508,
      "step": 3268
    },
    {
      "epoch": 0.013968055923498295,
      "grad_norm": 3.772324323654175,
      "learning_rate": 0.00036036574944453944,
      "loss": 1.4968,
      "step": 3269
    },
    {
      "epoch": 0.013972328806925489,
      "grad_norm": 3.5571746826171875,
      "learning_rate": 0.00036032302170569135,
      "loss": 1.0637,
      "step": 3270
    },
    {
      "epoch": 0.013976601690352683,
      "grad_norm": 3.919597864151001,
      "learning_rate": 0.0003602802939668433,
      "loss": 1.5665,
      "step": 3271
    },
    {
      "epoch": 0.013980874573779879,
      "grad_norm": 1.4330252408981323,
      "learning_rate": 0.00036023756622799517,
      "loss": 0.7491,
      "step": 3272
    },
    {
      "epoch": 0.013985147457207072,
      "grad_norm": 2.7404720783233643,
      "learning_rate": 0.00036019483848914713,
      "loss": 0.8012,
      "step": 3273
    },
    {
      "epoch": 0.013989420340634266,
      "grad_norm": 2.6259586811065674,
      "learning_rate": 0.0003601521107502991,
      "loss": 0.8498,
      "step": 3274
    },
    {
      "epoch": 0.013993693224061462,
      "grad_norm": 0.9049558639526367,
      "learning_rate": 0.000360109383011451,
      "loss": 0.3651,
      "step": 3275
    },
    {
      "epoch": 0.013997966107488656,
      "grad_norm": 1.2524394989013672,
      "learning_rate": 0.000360066655272603,
      "loss": 0.4902,
      "step": 3276
    },
    {
      "epoch": 0.01400223899091585,
      "grad_norm": 1.6908880472183228,
      "learning_rate": 0.0003600239275337549,
      "loss": 0.621,
      "step": 3277
    },
    {
      "epoch": 0.014006511874343043,
      "grad_norm": 1.634838342666626,
      "learning_rate": 0.00035998119979490685,
      "loss": 0.5118,
      "step": 3278
    },
    {
      "epoch": 0.014010784757770239,
      "grad_norm": 2.406482219696045,
      "learning_rate": 0.00035993847205605876,
      "loss": 0.986,
      "step": 3279
    },
    {
      "epoch": 0.014015057641197433,
      "grad_norm": 0.7162878513336182,
      "learning_rate": 0.0003598957443172107,
      "loss": 0.243,
      "step": 3280
    },
    {
      "epoch": 0.014019330524624627,
      "grad_norm": 2.3705408573150635,
      "learning_rate": 0.0003598530165783627,
      "loss": 0.6853,
      "step": 3281
    },
    {
      "epoch": 0.014023603408051822,
      "grad_norm": 1.7577667236328125,
      "learning_rate": 0.0003598102888395146,
      "loss": 0.7488,
      "step": 3282
    },
    {
      "epoch": 0.014027876291479016,
      "grad_norm": 0.8397157788276672,
      "learning_rate": 0.00035976756110066657,
      "loss": 0.3371,
      "step": 3283
    },
    {
      "epoch": 0.01403214917490621,
      "grad_norm": 1.4944651126861572,
      "learning_rate": 0.0003597248333618185,
      "loss": 0.5044,
      "step": 3284
    },
    {
      "epoch": 0.014036422058333404,
      "grad_norm": 3.1938416957855225,
      "learning_rate": 0.00035968210562297044,
      "loss": 0.9608,
      "step": 3285
    },
    {
      "epoch": 0.0140406949417606,
      "grad_norm": 0.9250503182411194,
      "learning_rate": 0.00035963937788412235,
      "loss": 0.3522,
      "step": 3286
    },
    {
      "epoch": 0.014044967825187793,
      "grad_norm": 2.2188515663146973,
      "learning_rate": 0.0003595966501452743,
      "loss": 0.8584,
      "step": 3287
    },
    {
      "epoch": 0.014049240708614987,
      "grad_norm": 1.0233858823776245,
      "learning_rate": 0.0003595539224064263,
      "loss": 0.4173,
      "step": 3288
    },
    {
      "epoch": 0.014053513592042183,
      "grad_norm": 2.9167537689208984,
      "learning_rate": 0.0003595111946675782,
      "loss": 1.8886,
      "step": 3289
    },
    {
      "epoch": 0.014057786475469377,
      "grad_norm": 1.1272271871566772,
      "learning_rate": 0.00035946846692873016,
      "loss": 0.6258,
      "step": 3290
    },
    {
      "epoch": 0.01406205935889657,
      "grad_norm": 2.330646276473999,
      "learning_rate": 0.00035942573918988207,
      "loss": 0.7923,
      "step": 3291
    },
    {
      "epoch": 0.014066332242323764,
      "grad_norm": 3.5703072547912598,
      "learning_rate": 0.00035938301145103403,
      "loss": 0.9174,
      "step": 3292
    },
    {
      "epoch": 0.01407060512575096,
      "grad_norm": 1.724269986152649,
      "learning_rate": 0.00035934028371218595,
      "loss": 0.6008,
      "step": 3293
    },
    {
      "epoch": 0.014074878009178154,
      "grad_norm": 1.3525947332382202,
      "learning_rate": 0.0003592975559733379,
      "loss": 0.5656,
      "step": 3294
    },
    {
      "epoch": 0.014079150892605348,
      "grad_norm": 2.282738447189331,
      "learning_rate": 0.0003592548282344899,
      "loss": 0.6226,
      "step": 3295
    },
    {
      "epoch": 0.014083423776032541,
      "grad_norm": 0.874691903591156,
      "learning_rate": 0.0003592121004956418,
      "loss": 0.3349,
      "step": 3296
    },
    {
      "epoch": 0.014087696659459737,
      "grad_norm": 0.8032770156860352,
      "learning_rate": 0.00035916937275679375,
      "loss": 0.3059,
      "step": 3297
    },
    {
      "epoch": 0.014091969542886931,
      "grad_norm": 2.432725429534912,
      "learning_rate": 0.00035912664501794566,
      "loss": 1.0906,
      "step": 3298
    },
    {
      "epoch": 0.014096242426314125,
      "grad_norm": 2.8478598594665527,
      "learning_rate": 0.0003590839172790976,
      "loss": 1.6016,
      "step": 3299
    },
    {
      "epoch": 0.01410051530974132,
      "grad_norm": 1.077304720878601,
      "learning_rate": 0.00035904118954024954,
      "loss": 0.4052,
      "step": 3300
    },
    {
      "epoch": 0.014104788193168514,
      "grad_norm": 1.808747410774231,
      "learning_rate": 0.0003589984618014015,
      "loss": 0.7403,
      "step": 3301
    },
    {
      "epoch": 0.014109061076595708,
      "grad_norm": 1.798304796218872,
      "learning_rate": 0.00035895573406255347,
      "loss": 0.7068,
      "step": 3302
    },
    {
      "epoch": 0.014113333960022902,
      "grad_norm": 5.367770671844482,
      "learning_rate": 0.0003589130063237054,
      "loss": 2.6441,
      "step": 3303
    },
    {
      "epoch": 0.014117606843450098,
      "grad_norm": 2.1816647052764893,
      "learning_rate": 0.00035887027858485734,
      "loss": 0.6322,
      "step": 3304
    },
    {
      "epoch": 0.014121879726877291,
      "grad_norm": 2.5739197731018066,
      "learning_rate": 0.0003588275508460092,
      "loss": 0.7831,
      "step": 3305
    },
    {
      "epoch": 0.014126152610304485,
      "grad_norm": 0.6655890345573425,
      "learning_rate": 0.00035878482310716116,
      "loss": 0.2408,
      "step": 3306
    },
    {
      "epoch": 0.01413042549373168,
      "grad_norm": 1.424651861190796,
      "learning_rate": 0.0003587420953683131,
      "loss": 0.4493,
      "step": 3307
    },
    {
      "epoch": 0.014134698377158875,
      "grad_norm": 1.2449909448623657,
      "learning_rate": 0.00035869936762946504,
      "loss": 0.5507,
      "step": 3308
    },
    {
      "epoch": 0.014138971260586069,
      "grad_norm": 2.079063653945923,
      "learning_rate": 0.00035865663989061695,
      "loss": 0.7886,
      "step": 3309
    },
    {
      "epoch": 0.014143244144013262,
      "grad_norm": 0.6894196271896362,
      "learning_rate": 0.0003586139121517689,
      "loss": 0.229,
      "step": 3310
    },
    {
      "epoch": 0.014147517027440458,
      "grad_norm": 0.76741623878479,
      "learning_rate": 0.0003585711844129209,
      "loss": 0.2543,
      "step": 3311
    },
    {
      "epoch": 0.014151789910867652,
      "grad_norm": 2.2954156398773193,
      "learning_rate": 0.0003585284566740728,
      "loss": 0.7124,
      "step": 3312
    },
    {
      "epoch": 0.014156062794294846,
      "grad_norm": 2.5187463760375977,
      "learning_rate": 0.00035848572893522476,
      "loss": 0.5862,
      "step": 3313
    },
    {
      "epoch": 0.01416033567772204,
      "grad_norm": 0.9417055249214172,
      "learning_rate": 0.00035844300119637667,
      "loss": 0.4309,
      "step": 3314
    },
    {
      "epoch": 0.014164608561149235,
      "grad_norm": 0.8185428977012634,
      "learning_rate": 0.00035840027345752863,
      "loss": 0.4047,
      "step": 3315
    },
    {
      "epoch": 0.014168881444576429,
      "grad_norm": 0.7851570844650269,
      "learning_rate": 0.00035835754571868054,
      "loss": 0.384,
      "step": 3316
    },
    {
      "epoch": 0.014173154328003623,
      "grad_norm": 3.6279232501983643,
      "learning_rate": 0.0003583148179798325,
      "loss": 1.6701,
      "step": 3317
    },
    {
      "epoch": 0.014177427211430818,
      "grad_norm": 0.9335620999336243,
      "learning_rate": 0.00035827209024098447,
      "loss": 0.5777,
      "step": 3318
    },
    {
      "epoch": 0.014181700094858012,
      "grad_norm": 3.7871007919311523,
      "learning_rate": 0.0003582293625021364,
      "loss": 1.46,
      "step": 3319
    },
    {
      "epoch": 0.014185972978285206,
      "grad_norm": 1.7124813795089722,
      "learning_rate": 0.00035818663476328835,
      "loss": 0.7028,
      "step": 3320
    },
    {
      "epoch": 0.0141902458617124,
      "grad_norm": 2.5467967987060547,
      "learning_rate": 0.00035814390702444026,
      "loss": 0.9493,
      "step": 3321
    },
    {
      "epoch": 0.014194518745139596,
      "grad_norm": 1.5752524137496948,
      "learning_rate": 0.0003581011792855922,
      "loss": 1.0635,
      "step": 3322
    },
    {
      "epoch": 0.01419879162856679,
      "grad_norm": 57.56992721557617,
      "learning_rate": 0.00035805845154674413,
      "loss": 1.7054,
      "step": 3323
    },
    {
      "epoch": 0.014203064511993983,
      "grad_norm": 1.667029857635498,
      "learning_rate": 0.0003580157238078961,
      "loss": 0.6778,
      "step": 3324
    },
    {
      "epoch": 0.014207337395421179,
      "grad_norm": 1.6166330575942993,
      "learning_rate": 0.00035797299606904806,
      "loss": 0.5953,
      "step": 3325
    },
    {
      "epoch": 0.014211610278848373,
      "grad_norm": 2.5644309520721436,
      "learning_rate": 0.0003579302683302,
      "loss": 0.8106,
      "step": 3326
    },
    {
      "epoch": 0.014215883162275567,
      "grad_norm": 3.176401376724243,
      "learning_rate": 0.00035788754059135194,
      "loss": 0.7958,
      "step": 3327
    },
    {
      "epoch": 0.01422015604570276,
      "grad_norm": 1.284982442855835,
      "learning_rate": 0.00035784481285250385,
      "loss": 0.8302,
      "step": 3328
    },
    {
      "epoch": 0.014224428929129956,
      "grad_norm": 0.3815567195415497,
      "learning_rate": 0.0003578020851136558,
      "loss": 0.1729,
      "step": 3329
    },
    {
      "epoch": 0.01422870181255715,
      "grad_norm": 0.3617493510246277,
      "learning_rate": 0.0003577593573748077,
      "loss": 0.178,
      "step": 3330
    },
    {
      "epoch": 0.014232974695984344,
      "grad_norm": 0.42494675517082214,
      "learning_rate": 0.0003577166296359597,
      "loss": 0.2405,
      "step": 3331
    },
    {
      "epoch": 0.01423724757941154,
      "grad_norm": 1.628829002380371,
      "learning_rate": 0.00035767390189711166,
      "loss": 1.0359,
      "step": 3332
    },
    {
      "epoch": 0.014241520462838733,
      "grad_norm": 1.7226194143295288,
      "learning_rate": 0.00035763117415826357,
      "loss": 0.7225,
      "step": 3333
    },
    {
      "epoch": 0.014245793346265927,
      "grad_norm": 4.076986789703369,
      "learning_rate": 0.00035758844641941553,
      "loss": 2.222,
      "step": 3334
    },
    {
      "epoch": 0.014250066229693121,
      "grad_norm": 1.69112229347229,
      "learning_rate": 0.00035754571868056744,
      "loss": 0.7585,
      "step": 3335
    },
    {
      "epoch": 0.014254339113120316,
      "grad_norm": 0.9789899587631226,
      "learning_rate": 0.0003575029909417194,
      "loss": 0.5792,
      "step": 3336
    },
    {
      "epoch": 0.01425861199654751,
      "grad_norm": 1.308554768562317,
      "learning_rate": 0.0003574602632028713,
      "loss": 0.7658,
      "step": 3337
    },
    {
      "epoch": 0.014262884879974704,
      "grad_norm": 1.7039380073547363,
      "learning_rate": 0.00035741753546402323,
      "loss": 0.7,
      "step": 3338
    },
    {
      "epoch": 0.014267157763401898,
      "grad_norm": 2.3484885692596436,
      "learning_rate": 0.00035737480772517514,
      "loss": 0.66,
      "step": 3339
    },
    {
      "epoch": 0.014271430646829094,
      "grad_norm": 1.7162470817565918,
      "learning_rate": 0.0003573320799863271,
      "loss": 0.6989,
      "step": 3340
    },
    {
      "epoch": 0.014275703530256287,
      "grad_norm": 1.5288152694702148,
      "learning_rate": 0.00035728935224747907,
      "loss": 0.6349,
      "step": 3341
    },
    {
      "epoch": 0.014279976413683481,
      "grad_norm": 3.302760124206543,
      "learning_rate": 0.000357246624508631,
      "loss": 0.9238,
      "step": 3342
    },
    {
      "epoch": 0.014284249297110677,
      "grad_norm": 3.2891740798950195,
      "learning_rate": 0.00035720389676978295,
      "loss": 1.6098,
      "step": 3343
    },
    {
      "epoch": 0.01428852218053787,
      "grad_norm": 1.4289751052856445,
      "learning_rate": 0.00035716116903093486,
      "loss": 0.5851,
      "step": 3344
    },
    {
      "epoch": 0.014292795063965065,
      "grad_norm": 1.3291038274765015,
      "learning_rate": 0.0003571184412920868,
      "loss": 0.5047,
      "step": 3345
    },
    {
      "epoch": 0.014297067947392259,
      "grad_norm": 0.6769670844078064,
      "learning_rate": 0.00035707571355323873,
      "loss": 0.2586,
      "step": 3346
    },
    {
      "epoch": 0.014301340830819454,
      "grad_norm": 0.7315940260887146,
      "learning_rate": 0.0003570329858143907,
      "loss": 0.2568,
      "step": 3347
    },
    {
      "epoch": 0.014305613714246648,
      "grad_norm": 3.1648926734924316,
      "learning_rate": 0.00035699025807554266,
      "loss": 1.257,
      "step": 3348
    },
    {
      "epoch": 0.014309886597673842,
      "grad_norm": 4.063432693481445,
      "learning_rate": 0.00035694753033669457,
      "loss": 1.0909,
      "step": 3349
    },
    {
      "epoch": 0.014314159481101037,
      "grad_norm": 0.7572095394134521,
      "learning_rate": 0.00035690480259784654,
      "loss": 0.2654,
      "step": 3350
    },
    {
      "epoch": 0.014318432364528231,
      "grad_norm": 3.9747061729431152,
      "learning_rate": 0.00035686207485899845,
      "loss": 1.1754,
      "step": 3351
    },
    {
      "epoch": 0.014322705247955425,
      "grad_norm": 1.2142682075500488,
      "learning_rate": 0.0003568193471201504,
      "loss": 0.4363,
      "step": 3352
    },
    {
      "epoch": 0.014326978131382619,
      "grad_norm": 1.2200955152511597,
      "learning_rate": 0.0003567766193813023,
      "loss": 0.4206,
      "step": 3353
    },
    {
      "epoch": 0.014331251014809815,
      "grad_norm": 1.2769192457199097,
      "learning_rate": 0.0003567338916424543,
      "loss": 0.4899,
      "step": 3354
    },
    {
      "epoch": 0.014335523898237008,
      "grad_norm": 1.3511422872543335,
      "learning_rate": 0.00035669116390360625,
      "loss": 0.6319,
      "step": 3355
    },
    {
      "epoch": 0.014339796781664202,
      "grad_norm": 1.30951988697052,
      "learning_rate": 0.00035664843616475816,
      "loss": 0.4729,
      "step": 3356
    },
    {
      "epoch": 0.014344069665091396,
      "grad_norm": 6.881315231323242,
      "learning_rate": 0.00035660570842591013,
      "loss": 1.7057,
      "step": 3357
    },
    {
      "epoch": 0.014348342548518592,
      "grad_norm": 3.790130138397217,
      "learning_rate": 0.00035656298068706204,
      "loss": 1.0658,
      "step": 3358
    },
    {
      "epoch": 0.014352615431945786,
      "grad_norm": 1.6739310026168823,
      "learning_rate": 0.000356520252948214,
      "loss": 0.6063,
      "step": 3359
    },
    {
      "epoch": 0.01435688831537298,
      "grad_norm": 1.1486834287643433,
      "learning_rate": 0.0003564775252093659,
      "loss": 0.3062,
      "step": 3360
    },
    {
      "epoch": 0.014361161198800175,
      "grad_norm": 3.752948760986328,
      "learning_rate": 0.0003564347974705179,
      "loss": 0.9168,
      "step": 3361
    },
    {
      "epoch": 0.014365434082227369,
      "grad_norm": 3.2117273807525635,
      "learning_rate": 0.00035639206973166985,
      "loss": 0.8704,
      "step": 3362
    },
    {
      "epoch": 0.014369706965654563,
      "grad_norm": 2.5011203289031982,
      "learning_rate": 0.00035634934199282176,
      "loss": 0.8918,
      "step": 3363
    },
    {
      "epoch": 0.014373979849081757,
      "grad_norm": 1.4243123531341553,
      "learning_rate": 0.0003563066142539737,
      "loss": 0.728,
      "step": 3364
    },
    {
      "epoch": 0.014378252732508952,
      "grad_norm": 1.394656777381897,
      "learning_rate": 0.00035626388651512563,
      "loss": 0.5933,
      "step": 3365
    },
    {
      "epoch": 0.014382525615936146,
      "grad_norm": 1.0754141807556152,
      "learning_rate": 0.0003562211587762776,
      "loss": 0.353,
      "step": 3366
    },
    {
      "epoch": 0.01438679849936334,
      "grad_norm": 0.9972798824310303,
      "learning_rate": 0.0003561784310374295,
      "loss": 0.3457,
      "step": 3367
    },
    {
      "epoch": 0.014391071382790535,
      "grad_norm": 1.8194591999053955,
      "learning_rate": 0.0003561357032985815,
      "loss": 0.506,
      "step": 3368
    },
    {
      "epoch": 0.01439534426621773,
      "grad_norm": 0.8918695449829102,
      "learning_rate": 0.00035609297555973344,
      "loss": 0.2885,
      "step": 3369
    },
    {
      "epoch": 0.014399617149644923,
      "grad_norm": 2.977825164794922,
      "learning_rate": 0.00035605024782088535,
      "loss": 0.8327,
      "step": 3370
    },
    {
      "epoch": 0.014403890033072117,
      "grad_norm": 2.2798330783843994,
      "learning_rate": 0.00035600752008203726,
      "loss": 1.134,
      "step": 3371
    },
    {
      "epoch": 0.014408162916499313,
      "grad_norm": 2.9557316303253174,
      "learning_rate": 0.00035596479234318917,
      "loss": 0.7787,
      "step": 3372
    },
    {
      "epoch": 0.014412435799926506,
      "grad_norm": 1.4854025840759277,
      "learning_rate": 0.00035592206460434114,
      "loss": 0.4173,
      "step": 3373
    },
    {
      "epoch": 0.0144167086833537,
      "grad_norm": 4.1664557456970215,
      "learning_rate": 0.00035587933686549305,
      "loss": 1.1714,
      "step": 3374
    },
    {
      "epoch": 0.014420981566780896,
      "grad_norm": 2.359597682952881,
      "learning_rate": 0.000355836609126645,
      "loss": 0.6603,
      "step": 3375
    },
    {
      "epoch": 0.01442525445020809,
      "grad_norm": 4.394975662231445,
      "learning_rate": 0.0003557938813877969,
      "loss": 1.8909,
      "step": 3376
    },
    {
      "epoch": 0.014429527333635284,
      "grad_norm": 0.7799153327941895,
      "learning_rate": 0.0003557511536489489,
      "loss": 0.226,
      "step": 3377
    },
    {
      "epoch": 0.014433800217062477,
      "grad_norm": 2.685279369354248,
      "learning_rate": 0.00035570842591010085,
      "loss": 0.6773,
      "step": 3378
    },
    {
      "epoch": 0.014438073100489673,
      "grad_norm": 1.4596198797225952,
      "learning_rate": 0.00035566569817125276,
      "loss": 0.5547,
      "step": 3379
    },
    {
      "epoch": 0.014442345983916867,
      "grad_norm": 4.56724739074707,
      "learning_rate": 0.0003556229704324047,
      "loss": 1.0082,
      "step": 3380
    },
    {
      "epoch": 0.01444661886734406,
      "grad_norm": 2.2185065746307373,
      "learning_rate": 0.00035558024269355664,
      "loss": 0.4629,
      "step": 3381
    },
    {
      "epoch": 0.014450891750771255,
      "grad_norm": 0.5010581612586975,
      "learning_rate": 0.0003555375149547086,
      "loss": 0.1576,
      "step": 3382
    },
    {
      "epoch": 0.01445516463419845,
      "grad_norm": 4.232100486755371,
      "learning_rate": 0.0003554947872158605,
      "loss": 0.9334,
      "step": 3383
    },
    {
      "epoch": 0.014459437517625644,
      "grad_norm": 2.812021493911743,
      "learning_rate": 0.0003554520594770125,
      "loss": 0.7333,
      "step": 3384
    },
    {
      "epoch": 0.014463710401052838,
      "grad_norm": 0.5634337663650513,
      "learning_rate": 0.00035540933173816444,
      "loss": 0.1643,
      "step": 3385
    },
    {
      "epoch": 0.014467983284480034,
      "grad_norm": 0.47880372405052185,
      "learning_rate": 0.00035536660399931635,
      "loss": 0.1336,
      "step": 3386
    },
    {
      "epoch": 0.014472256167907227,
      "grad_norm": 1.756100058555603,
      "learning_rate": 0.0003553238762604683,
      "loss": 0.7208,
      "step": 3387
    },
    {
      "epoch": 0.014476529051334421,
      "grad_norm": 2.6459925174713135,
      "learning_rate": 0.00035528114852162023,
      "loss": 0.6519,
      "step": 3388
    },
    {
      "epoch": 0.014480801934761615,
      "grad_norm": 2.190117359161377,
      "learning_rate": 0.0003552384207827722,
      "loss": 0.7526,
      "step": 3389
    },
    {
      "epoch": 0.01448507481818881,
      "grad_norm": 2.8502917289733887,
      "learning_rate": 0.0003551956930439241,
      "loss": 0.5957,
      "step": 3390
    },
    {
      "epoch": 0.014489347701616005,
      "grad_norm": 0.9265321493148804,
      "learning_rate": 0.00035515296530507607,
      "loss": 0.1757,
      "step": 3391
    },
    {
      "epoch": 0.014493620585043198,
      "grad_norm": 3.1415345668792725,
      "learning_rate": 0.00035511023756622804,
      "loss": 0.9977,
      "step": 3392
    },
    {
      "epoch": 0.014497893468470394,
      "grad_norm": 1.3600574731826782,
      "learning_rate": 0.00035506750982737995,
      "loss": 0.5415,
      "step": 3393
    },
    {
      "epoch": 0.014502166351897588,
      "grad_norm": 3.5351386070251465,
      "learning_rate": 0.0003550247820885319,
      "loss": 0.6088,
      "step": 3394
    },
    {
      "epoch": 0.014506439235324782,
      "grad_norm": 3.904317855834961,
      "learning_rate": 0.0003549820543496838,
      "loss": 1.0668,
      "step": 3395
    },
    {
      "epoch": 0.014510712118751976,
      "grad_norm": 2.7863433361053467,
      "learning_rate": 0.0003549393266108358,
      "loss": 0.8543,
      "step": 3396
    },
    {
      "epoch": 0.014514985002179171,
      "grad_norm": 3.7013182640075684,
      "learning_rate": 0.0003548965988719877,
      "loss": 0.8917,
      "step": 3397
    },
    {
      "epoch": 0.014519257885606365,
      "grad_norm": 0.5939839482307434,
      "learning_rate": 0.00035485387113313966,
      "loss": 0.0966,
      "step": 3398
    },
    {
      "epoch": 0.014523530769033559,
      "grad_norm": 3.0387628078460693,
      "learning_rate": 0.00035481114339429163,
      "loss": 0.8314,
      "step": 3399
    },
    {
      "epoch": 0.014527803652460754,
      "grad_norm": 1.1977940797805786,
      "learning_rate": 0.00035476841565544354,
      "loss": 0.4501,
      "step": 3400
    },
    {
      "epoch": 0.014532076535887948,
      "grad_norm": 4.598618984222412,
      "learning_rate": 0.0003547256879165955,
      "loss": 0.9516,
      "step": 3401
    },
    {
      "epoch": 0.014536349419315142,
      "grad_norm": 2.2708468437194824,
      "learning_rate": 0.0003546829601777474,
      "loss": 0.7007,
      "step": 3402
    },
    {
      "epoch": 0.014540622302742336,
      "grad_norm": 2.2668986320495605,
      "learning_rate": 0.0003546402324388994,
      "loss": 0.5351,
      "step": 3403
    },
    {
      "epoch": 0.014544895186169532,
      "grad_norm": 3.5265626907348633,
      "learning_rate": 0.00035459750470005124,
      "loss": 1.1961,
      "step": 3404
    },
    {
      "epoch": 0.014549168069596725,
      "grad_norm": 0.6354129314422607,
      "learning_rate": 0.0003545547769612032,
      "loss": 0.1001,
      "step": 3405
    },
    {
      "epoch": 0.01455344095302392,
      "grad_norm": 2.548874855041504,
      "learning_rate": 0.0003545120492223551,
      "loss": 0.8778,
      "step": 3406
    },
    {
      "epoch": 0.014557713836451113,
      "grad_norm": 2.0339770317077637,
      "learning_rate": 0.0003544693214835071,
      "loss": 0.6843,
      "step": 3407
    },
    {
      "epoch": 0.014561986719878309,
      "grad_norm": 2.7289295196533203,
      "learning_rate": 0.00035442659374465904,
      "loss": 0.7959,
      "step": 3408
    },
    {
      "epoch": 0.014566259603305503,
      "grad_norm": 1.3544079065322876,
      "learning_rate": 0.00035438386600581095,
      "loss": 0.3468,
      "step": 3409
    },
    {
      "epoch": 0.014570532486732696,
      "grad_norm": 1.4741970300674438,
      "learning_rate": 0.0003543411382669629,
      "loss": 0.3011,
      "step": 3410
    },
    {
      "epoch": 0.014574805370159892,
      "grad_norm": 1.3537780046463013,
      "learning_rate": 0.00035429841052811483,
      "loss": 0.9024,
      "step": 3411
    },
    {
      "epoch": 0.014579078253587086,
      "grad_norm": 1.462667465209961,
      "learning_rate": 0.0003542556827892668,
      "loss": 0.6307,
      "step": 3412
    },
    {
      "epoch": 0.01458335113701428,
      "grad_norm": 2.2483458518981934,
      "learning_rate": 0.0003542129550504187,
      "loss": 1.1917,
      "step": 3413
    },
    {
      "epoch": 0.014587624020441474,
      "grad_norm": 1.576711893081665,
      "learning_rate": 0.00035417022731157067,
      "loss": 0.55,
      "step": 3414
    },
    {
      "epoch": 0.01459189690386867,
      "grad_norm": 2.6060445308685303,
      "learning_rate": 0.00035412749957272263,
      "loss": 0.6756,
      "step": 3415
    },
    {
      "epoch": 0.014596169787295863,
      "grad_norm": 4.160896301269531,
      "learning_rate": 0.00035408477183387454,
      "loss": 1.7969,
      "step": 3416
    },
    {
      "epoch": 0.014600442670723057,
      "grad_norm": 1.378200888633728,
      "learning_rate": 0.0003540420440950265,
      "loss": 0.5383,
      "step": 3417
    },
    {
      "epoch": 0.014604715554150252,
      "grad_norm": 1.10910964012146,
      "learning_rate": 0.0003539993163561784,
      "loss": 0.4475,
      "step": 3418
    },
    {
      "epoch": 0.014608988437577446,
      "grad_norm": 0.9382457733154297,
      "learning_rate": 0.0003539565886173304,
      "loss": 0.342,
      "step": 3419
    },
    {
      "epoch": 0.01461326132100464,
      "grad_norm": 0.9822912812232971,
      "learning_rate": 0.0003539138608784823,
      "loss": 0.3918,
      "step": 3420
    },
    {
      "epoch": 0.014617534204431834,
      "grad_norm": 1.6980630159378052,
      "learning_rate": 0.00035387113313963426,
      "loss": 0.5212,
      "step": 3421
    },
    {
      "epoch": 0.01462180708785903,
      "grad_norm": 1.1498632431030273,
      "learning_rate": 0.0003538284054007862,
      "loss": 0.4918,
      "step": 3422
    },
    {
      "epoch": 0.014626079971286223,
      "grad_norm": 0.7349570989608765,
      "learning_rate": 0.00035378567766193814,
      "loss": 0.2753,
      "step": 3423
    },
    {
      "epoch": 0.014630352854713417,
      "grad_norm": 0.7167494893074036,
      "learning_rate": 0.0003537429499230901,
      "loss": 0.2453,
      "step": 3424
    },
    {
      "epoch": 0.014634625738140611,
      "grad_norm": 1.567525863647461,
      "learning_rate": 0.000353700222184242,
      "loss": 0.6277,
      "step": 3425
    },
    {
      "epoch": 0.014638898621567807,
      "grad_norm": 3.5753674507141113,
      "learning_rate": 0.000353657494445394,
      "loss": 0.9349,
      "step": 3426
    },
    {
      "epoch": 0.014643171504995,
      "grad_norm": 3.6984198093414307,
      "learning_rate": 0.0003536147667065459,
      "loss": 0.8266,
      "step": 3427
    },
    {
      "epoch": 0.014647444388422194,
      "grad_norm": 1.7274370193481445,
      "learning_rate": 0.00035357203896769785,
      "loss": 0.5005,
      "step": 3428
    },
    {
      "epoch": 0.01465171727184939,
      "grad_norm": 2.8181982040405273,
      "learning_rate": 0.0003535293112288498,
      "loss": 0.8036,
      "step": 3429
    },
    {
      "epoch": 0.014655990155276584,
      "grad_norm": 1.1670398712158203,
      "learning_rate": 0.00035348658349000173,
      "loss": 0.36,
      "step": 3430
    },
    {
      "epoch": 0.014660263038703778,
      "grad_norm": 1.5909111499786377,
      "learning_rate": 0.0003534438557511537,
      "loss": 0.5254,
      "step": 3431
    },
    {
      "epoch": 0.014664535922130972,
      "grad_norm": 1.5301936864852905,
      "learning_rate": 0.0003534011280123056,
      "loss": 0.8509,
      "step": 3432
    },
    {
      "epoch": 0.014668808805558167,
      "grad_norm": 1.8092491626739502,
      "learning_rate": 0.00035335840027345757,
      "loss": 0.56,
      "step": 3433
    },
    {
      "epoch": 0.014673081688985361,
      "grad_norm": 2.116387128829956,
      "learning_rate": 0.0003533156725346095,
      "loss": 0.6265,
      "step": 3434
    },
    {
      "epoch": 0.014677354572412555,
      "grad_norm": 4.04790735244751,
      "learning_rate": 0.00035327294479576144,
      "loss": 1.5226,
      "step": 3435
    },
    {
      "epoch": 0.01468162745583975,
      "grad_norm": 4.675983428955078,
      "learning_rate": 0.00035323021705691335,
      "loss": 1.4073,
      "step": 3436
    },
    {
      "epoch": 0.014685900339266944,
      "grad_norm": 1.4191850423812866,
      "learning_rate": 0.00035318748931806527,
      "loss": 0.3855,
      "step": 3437
    },
    {
      "epoch": 0.014690173222694138,
      "grad_norm": 3.531236410140991,
      "learning_rate": 0.00035314476157921723,
      "loss": 0.7526,
      "step": 3438
    },
    {
      "epoch": 0.014694446106121332,
      "grad_norm": 3.0071160793304443,
      "learning_rate": 0.00035310203384036914,
      "loss": 1.0442,
      "step": 3439
    },
    {
      "epoch": 0.014698718989548528,
      "grad_norm": 2.3175623416900635,
      "learning_rate": 0.0003530593061015211,
      "loss": 0.6711,
      "step": 3440
    },
    {
      "epoch": 0.014702991872975722,
      "grad_norm": 1.0071423053741455,
      "learning_rate": 0.000353016578362673,
      "loss": 0.2797,
      "step": 3441
    },
    {
      "epoch": 0.014707264756402915,
      "grad_norm": 2.79194974899292,
      "learning_rate": 0.000352973850623825,
      "loss": 0.9473,
      "step": 3442
    },
    {
      "epoch": 0.014711537639830111,
      "grad_norm": 0.9029043316841125,
      "learning_rate": 0.0003529311228849769,
      "loss": 0.262,
      "step": 3443
    },
    {
      "epoch": 0.014715810523257305,
      "grad_norm": 33.50009536743164,
      "learning_rate": 0.00035288839514612886,
      "loss": 6.1509,
      "step": 3444
    },
    {
      "epoch": 0.014720083406684499,
      "grad_norm": 2.183670997619629,
      "learning_rate": 0.0003528456674072808,
      "loss": 0.6606,
      "step": 3445
    },
    {
      "epoch": 0.014724356290111693,
      "grad_norm": 2.9993293285369873,
      "learning_rate": 0.00035280293966843273,
      "loss": 0.8791,
      "step": 3446
    },
    {
      "epoch": 0.014728629173538888,
      "grad_norm": 0.9554027318954468,
      "learning_rate": 0.0003527602119295847,
      "loss": 0.2798,
      "step": 3447
    },
    {
      "epoch": 0.014732902056966082,
      "grad_norm": 1.8036831617355347,
      "learning_rate": 0.0003527174841907366,
      "loss": 1.066,
      "step": 3448
    },
    {
      "epoch": 0.014737174940393276,
      "grad_norm": 2.4934422969818115,
      "learning_rate": 0.0003526747564518886,
      "loss": 0.7145,
      "step": 3449
    },
    {
      "epoch": 0.01474144782382047,
      "grad_norm": 0.9410685896873474,
      "learning_rate": 0.0003526320287130405,
      "loss": 0.2432,
      "step": 3450
    },
    {
      "epoch": 0.014745720707247665,
      "grad_norm": 4.1555986404418945,
      "learning_rate": 0.00035258930097419245,
      "loss": 0.8585,
      "step": 3451
    },
    {
      "epoch": 0.01474999359067486,
      "grad_norm": 1.8160802125930786,
      "learning_rate": 0.0003525465732353444,
      "loss": 0.7104,
      "step": 3452
    },
    {
      "epoch": 0.014754266474102053,
      "grad_norm": 0.8593331575393677,
      "learning_rate": 0.0003525038454964963,
      "loss": 0.261,
      "step": 3453
    },
    {
      "epoch": 0.014758539357529249,
      "grad_norm": 2.0966885089874268,
      "learning_rate": 0.0003524611177576483,
      "loss": 0.886,
      "step": 3454
    },
    {
      "epoch": 0.014762812240956442,
      "grad_norm": 2.4793379306793213,
      "learning_rate": 0.0003524183900188002,
      "loss": 0.6326,
      "step": 3455
    },
    {
      "epoch": 0.014767085124383636,
      "grad_norm": 0.7211885452270508,
      "learning_rate": 0.00035237566227995217,
      "loss": 0.2251,
      "step": 3456
    },
    {
      "epoch": 0.01477135800781083,
      "grad_norm": 2.37310791015625,
      "learning_rate": 0.0003523329345411041,
      "loss": 0.7631,
      "step": 3457
    },
    {
      "epoch": 0.014775630891238026,
      "grad_norm": 1.4227174520492554,
      "learning_rate": 0.00035229020680225604,
      "loss": 0.7383,
      "step": 3458
    },
    {
      "epoch": 0.01477990377466522,
      "grad_norm": 1.6091642379760742,
      "learning_rate": 0.000352247479063408,
      "loss": 0.6368,
      "step": 3459
    },
    {
      "epoch": 0.014784176658092413,
      "grad_norm": 1.6895650625228882,
      "learning_rate": 0.0003522047513245599,
      "loss": 0.6795,
      "step": 3460
    },
    {
      "epoch": 0.014788449541519609,
      "grad_norm": 1.1842941045761108,
      "learning_rate": 0.0003521620235857119,
      "loss": 0.4875,
      "step": 3461
    },
    {
      "epoch": 0.014792722424946803,
      "grad_norm": 1.1933811902999878,
      "learning_rate": 0.0003521192958468638,
      "loss": 0.4874,
      "step": 3462
    },
    {
      "epoch": 0.014796995308373997,
      "grad_norm": 0.5117395520210266,
      "learning_rate": 0.00035207656810801576,
      "loss": 0.1508,
      "step": 3463
    },
    {
      "epoch": 0.01480126819180119,
      "grad_norm": 1.5786042213439941,
      "learning_rate": 0.00035203384036916767,
      "loss": 0.6661,
      "step": 3464
    },
    {
      "epoch": 0.014805541075228386,
      "grad_norm": 1.3961710929870605,
      "learning_rate": 0.00035199111263031963,
      "loss": 0.7137,
      "step": 3465
    },
    {
      "epoch": 0.01480981395865558,
      "grad_norm": 1.8369393348693848,
      "learning_rate": 0.0003519483848914716,
      "loss": 1.0617,
      "step": 3466
    },
    {
      "epoch": 0.014814086842082774,
      "grad_norm": 5.4027581214904785,
      "learning_rate": 0.0003519056571526235,
      "loss": 1.8737,
      "step": 3467
    },
    {
      "epoch": 0.014818359725509968,
      "grad_norm": 1.368672251701355,
      "learning_rate": 0.0003518629294137755,
      "loss": 0.5259,
      "step": 3468
    },
    {
      "epoch": 0.014822632608937163,
      "grad_norm": 3.789396047592163,
      "learning_rate": 0.00035182020167492733,
      "loss": 1.6096,
      "step": 3469
    },
    {
      "epoch": 0.014826905492364357,
      "grad_norm": 4.121581554412842,
      "learning_rate": 0.0003517774739360793,
      "loss": 1.2824,
      "step": 3470
    },
    {
      "epoch": 0.014831178375791551,
      "grad_norm": 1.8410558700561523,
      "learning_rate": 0.0003517347461972312,
      "loss": 1.0632,
      "step": 3471
    },
    {
      "epoch": 0.014835451259218747,
      "grad_norm": 2.0496912002563477,
      "learning_rate": 0.00035169201845838317,
      "loss": 0.6309,
      "step": 3472
    },
    {
      "epoch": 0.01483972414264594,
      "grad_norm": 1.0283392667770386,
      "learning_rate": 0.0003516492907195351,
      "loss": 0.4416,
      "step": 3473
    },
    {
      "epoch": 0.014843997026073134,
      "grad_norm": 3.2818734645843506,
      "learning_rate": 0.00035160656298068705,
      "loss": 1.2186,
      "step": 3474
    },
    {
      "epoch": 0.014848269909500328,
      "grad_norm": 0.6496118903160095,
      "learning_rate": 0.000351563835241839,
      "loss": 0.193,
      "step": 3475
    },
    {
      "epoch": 0.014852542792927524,
      "grad_norm": 0.9677483439445496,
      "learning_rate": 0.0003515211075029909,
      "loss": 0.3267,
      "step": 3476
    },
    {
      "epoch": 0.014856815676354718,
      "grad_norm": 1.3862502574920654,
      "learning_rate": 0.0003514783797641429,
      "loss": 0.7082,
      "step": 3477
    },
    {
      "epoch": 0.014861088559781912,
      "grad_norm": 0.9621233344078064,
      "learning_rate": 0.0003514356520252948,
      "loss": 0.9214,
      "step": 3478
    },
    {
      "epoch": 0.014865361443209107,
      "grad_norm": 0.7333513498306274,
      "learning_rate": 0.00035139292428644676,
      "loss": 0.2276,
      "step": 3479
    },
    {
      "epoch": 0.014869634326636301,
      "grad_norm": 2.394132375717163,
      "learning_rate": 0.0003513501965475987,
      "loss": 0.6855,
      "step": 3480
    },
    {
      "epoch": 0.014873907210063495,
      "grad_norm": 0.7283118963241577,
      "learning_rate": 0.00035130746880875064,
      "loss": 0.2109,
      "step": 3481
    },
    {
      "epoch": 0.014878180093490689,
      "grad_norm": 1.1703753471374512,
      "learning_rate": 0.0003512647410699026,
      "loss": 0.4979,
      "step": 3482
    },
    {
      "epoch": 0.014882452976917884,
      "grad_norm": 2.082066774368286,
      "learning_rate": 0.0003512220133310545,
      "loss": 0.3934,
      "step": 3483
    },
    {
      "epoch": 0.014886725860345078,
      "grad_norm": 3.5463244915008545,
      "learning_rate": 0.0003511792855922065,
      "loss": 1.1341,
      "step": 3484
    },
    {
      "epoch": 0.014890998743772272,
      "grad_norm": 2.948981523513794,
      "learning_rate": 0.0003511365578533584,
      "loss": 1.0707,
      "step": 3485
    },
    {
      "epoch": 0.014895271627199468,
      "grad_norm": 3.968026876449585,
      "learning_rate": 0.00035109383011451036,
      "loss": 1.6578,
      "step": 3486
    },
    {
      "epoch": 0.014899544510626661,
      "grad_norm": 0.9579378366470337,
      "learning_rate": 0.00035105110237566227,
      "loss": 0.8453,
      "step": 3487
    },
    {
      "epoch": 0.014903817394053855,
      "grad_norm": 1.3758734464645386,
      "learning_rate": 0.00035100837463681423,
      "loss": 0.6828,
      "step": 3488
    },
    {
      "epoch": 0.014908090277481049,
      "grad_norm": 0.4984586536884308,
      "learning_rate": 0.0003509656468979662,
      "loss": 0.1442,
      "step": 3489
    },
    {
      "epoch": 0.014912363160908245,
      "grad_norm": 1.7276958227157593,
      "learning_rate": 0.0003509229191591181,
      "loss": 0.2791,
      "step": 3490
    },
    {
      "epoch": 0.014916636044335439,
      "grad_norm": 1.4939680099487305,
      "learning_rate": 0.00035088019142027007,
      "loss": 0.4352,
      "step": 3491
    },
    {
      "epoch": 0.014920908927762632,
      "grad_norm": 1.3177820444107056,
      "learning_rate": 0.000350837463681422,
      "loss": 0.6085,
      "step": 3492
    },
    {
      "epoch": 0.014925181811189826,
      "grad_norm": 1.1949224472045898,
      "learning_rate": 0.00035079473594257395,
      "loss": 0.4488,
      "step": 3493
    },
    {
      "epoch": 0.014929454694617022,
      "grad_norm": 1.8459566831588745,
      "learning_rate": 0.00035075200820372586,
      "loss": 0.5332,
      "step": 3494
    },
    {
      "epoch": 0.014933727578044216,
      "grad_norm": 3.6758947372436523,
      "learning_rate": 0.0003507092804648778,
      "loss": 1.7165,
      "step": 3495
    },
    {
      "epoch": 0.01493800046147141,
      "grad_norm": 3.9818413257598877,
      "learning_rate": 0.0003506665527260298,
      "loss": 1.6081,
      "step": 3496
    },
    {
      "epoch": 0.014942273344898605,
      "grad_norm": 4.0606865882873535,
      "learning_rate": 0.0003506238249871817,
      "loss": 1.052,
      "step": 3497
    },
    {
      "epoch": 0.014946546228325799,
      "grad_norm": 1.3398401737213135,
      "learning_rate": 0.00035058109724833366,
      "loss": 0.5509,
      "step": 3498
    },
    {
      "epoch": 0.014950819111752993,
      "grad_norm": 3.494187355041504,
      "learning_rate": 0.0003505383695094856,
      "loss": 1.079,
      "step": 3499
    },
    {
      "epoch": 0.014955091995180187,
      "grad_norm": 1.26456618309021,
      "learning_rate": 0.00035049564177063754,
      "loss": 0.481,
      "step": 3500
    },
    {
      "epoch": 0.014959364878607382,
      "grad_norm": 2.119246482849121,
      "learning_rate": 0.00035045291403178945,
      "loss": 0.5877,
      "step": 3501
    },
    {
      "epoch": 0.014963637762034576,
      "grad_norm": 2.474308490753174,
      "learning_rate": 0.00035041018629294136,
      "loss": 0.5913,
      "step": 3502
    },
    {
      "epoch": 0.01496791064546177,
      "grad_norm": 1.1433683633804321,
      "learning_rate": 0.0003503674585540933,
      "loss": 0.4018,
      "step": 3503
    },
    {
      "epoch": 0.014972183528888966,
      "grad_norm": 2.031327724456787,
      "learning_rate": 0.00035032473081524524,
      "loss": 0.7976,
      "step": 3504
    },
    {
      "epoch": 0.01497645641231616,
      "grad_norm": 1.86063814163208,
      "learning_rate": 0.0003502820030763972,
      "loss": 0.4827,
      "step": 3505
    },
    {
      "epoch": 0.014980729295743353,
      "grad_norm": 1.1673907041549683,
      "learning_rate": 0.0003502392753375491,
      "loss": 0.4564,
      "step": 3506
    },
    {
      "epoch": 0.014985002179170547,
      "grad_norm": 1.185794711112976,
      "learning_rate": 0.0003501965475987011,
      "loss": 0.7423,
      "step": 3507
    },
    {
      "epoch": 0.014989275062597743,
      "grad_norm": 1.845813274383545,
      "learning_rate": 0.000350153819859853,
      "loss": 1.0109,
      "step": 3508
    },
    {
      "epoch": 0.014993547946024937,
      "grad_norm": 1.4764833450317383,
      "learning_rate": 0.00035011109212100495,
      "loss": 0.6823,
      "step": 3509
    },
    {
      "epoch": 0.01499782082945213,
      "grad_norm": 3.7921552658081055,
      "learning_rate": 0.00035006836438215686,
      "loss": 1.4893,
      "step": 3510
    },
    {
      "epoch": 0.015002093712879326,
      "grad_norm": 2.6891138553619385,
      "learning_rate": 0.00035002563664330883,
      "loss": 1.0208,
      "step": 3511
    },
    {
      "epoch": 0.01500636659630652,
      "grad_norm": 4.269336223602295,
      "learning_rate": 0.0003499829089044608,
      "loss": 0.378,
      "step": 3512
    },
    {
      "epoch": 0.015010639479733714,
      "grad_norm": 3.137988805770874,
      "learning_rate": 0.0003499401811656127,
      "loss": 0.9093,
      "step": 3513
    },
    {
      "epoch": 0.015014912363160908,
      "grad_norm": 1.1432634592056274,
      "learning_rate": 0.00034989745342676467,
      "loss": 0.6833,
      "step": 3514
    },
    {
      "epoch": 0.015019185246588103,
      "grad_norm": 1.7292152643203735,
      "learning_rate": 0.0003498547256879166,
      "loss": 0.8081,
      "step": 3515
    },
    {
      "epoch": 0.015023458130015297,
      "grad_norm": 1.0150423049926758,
      "learning_rate": 0.00034981199794906854,
      "loss": 0.3412,
      "step": 3516
    },
    {
      "epoch": 0.015027731013442491,
      "grad_norm": 1.0280253887176514,
      "learning_rate": 0.00034976927021022046,
      "loss": 0.3413,
      "step": 3517
    },
    {
      "epoch": 0.015032003896869685,
      "grad_norm": 3.505967140197754,
      "learning_rate": 0.0003497265424713724,
      "loss": 1.6353,
      "step": 3518
    },
    {
      "epoch": 0.01503627678029688,
      "grad_norm": 3.701545000076294,
      "learning_rate": 0.0003496838147325244,
      "loss": 1.0161,
      "step": 3519
    },
    {
      "epoch": 0.015040549663724074,
      "grad_norm": 2.8295350074768066,
      "learning_rate": 0.0003496410869936763,
      "loss": 0.8009,
      "step": 3520
    },
    {
      "epoch": 0.015044822547151268,
      "grad_norm": 3.308441162109375,
      "learning_rate": 0.00034959835925482826,
      "loss": 1.2102,
      "step": 3521
    },
    {
      "epoch": 0.015049095430578464,
      "grad_norm": 3.5465450286865234,
      "learning_rate": 0.00034955563151598017,
      "loss": 1.2641,
      "step": 3522
    },
    {
      "epoch": 0.015053368314005658,
      "grad_norm": 0.8326442241668701,
      "learning_rate": 0.00034951290377713214,
      "loss": 0.2521,
      "step": 3523
    },
    {
      "epoch": 0.015057641197432851,
      "grad_norm": 0.7358624935150146,
      "learning_rate": 0.00034947017603828405,
      "loss": 0.218,
      "step": 3524
    },
    {
      "epoch": 0.015061914080860045,
      "grad_norm": 0.6720371246337891,
      "learning_rate": 0.000349427448299436,
      "loss": 0.2121,
      "step": 3525
    },
    {
      "epoch": 0.01506618696428724,
      "grad_norm": 2.5469019412994385,
      "learning_rate": 0.000349384720560588,
      "loss": 0.7873,
      "step": 3526
    },
    {
      "epoch": 0.015070459847714435,
      "grad_norm": 3.100234270095825,
      "learning_rate": 0.0003493419928217399,
      "loss": 0.6131,
      "step": 3527
    },
    {
      "epoch": 0.015074732731141629,
      "grad_norm": 4.890516757965088,
      "learning_rate": 0.00034929926508289185,
      "loss": 0.999,
      "step": 3528
    },
    {
      "epoch": 0.015079005614568824,
      "grad_norm": 4.218650817871094,
      "learning_rate": 0.00034925653734404376,
      "loss": 1.0912,
      "step": 3529
    },
    {
      "epoch": 0.015083278497996018,
      "grad_norm": 1.4224023818969727,
      "learning_rate": 0.00034921380960519573,
      "loss": 0.4033,
      "step": 3530
    },
    {
      "epoch": 0.015087551381423212,
      "grad_norm": 2.2866954803466797,
      "learning_rate": 0.00034917108186634764,
      "loss": 0.4396,
      "step": 3531
    },
    {
      "epoch": 0.015091824264850406,
      "grad_norm": 1.2712548971176147,
      "learning_rate": 0.0003491283541274996,
      "loss": 0.5916,
      "step": 3532
    },
    {
      "epoch": 0.015096097148277601,
      "grad_norm": 2.4312429428100586,
      "learning_rate": 0.00034908562638865157,
      "loss": 0.6767,
      "step": 3533
    },
    {
      "epoch": 0.015100370031704795,
      "grad_norm": 2.61828875541687,
      "learning_rate": 0.0003490428986498035,
      "loss": 0.9978,
      "step": 3534
    },
    {
      "epoch": 0.015104642915131989,
      "grad_norm": 0.35442817211151123,
      "learning_rate": 0.0003490001709109554,
      "loss": 0.0969,
      "step": 3535
    },
    {
      "epoch": 0.015108915798559183,
      "grad_norm": 2.587646961212158,
      "learning_rate": 0.0003489574431721073,
      "loss": 0.8987,
      "step": 3536
    },
    {
      "epoch": 0.015113188681986378,
      "grad_norm": 3.215635061264038,
      "learning_rate": 0.00034891471543325927,
      "loss": 0.9174,
      "step": 3537
    },
    {
      "epoch": 0.015117461565413572,
      "grad_norm": 2.421030282974243,
      "learning_rate": 0.0003488719876944112,
      "loss": 1.1231,
      "step": 3538
    },
    {
      "epoch": 0.015121734448840766,
      "grad_norm": 5.700191497802734,
      "learning_rate": 0.00034882925995556314,
      "loss": 1.0378,
      "step": 3539
    },
    {
      "epoch": 0.015126007332267962,
      "grad_norm": 2.1143267154693604,
      "learning_rate": 0.0003487865322167151,
      "loss": 0.8803,
      "step": 3540
    },
    {
      "epoch": 0.015130280215695156,
      "grad_norm": 0.8642880320549011,
      "learning_rate": 0.000348743804477867,
      "loss": 0.4408,
      "step": 3541
    },
    {
      "epoch": 0.01513455309912235,
      "grad_norm": 0.9500637054443359,
      "learning_rate": 0.000348701076739019,
      "loss": 0.1765,
      "step": 3542
    },
    {
      "epoch": 0.015138825982549543,
      "grad_norm": 1.2789756059646606,
      "learning_rate": 0.0003486583490001709,
      "loss": 0.5056,
      "step": 3543
    },
    {
      "epoch": 0.015143098865976739,
      "grad_norm": 1.3516061305999756,
      "learning_rate": 0.00034861562126132286,
      "loss": 0.298,
      "step": 3544
    },
    {
      "epoch": 0.015147371749403933,
      "grad_norm": 2.9910707473754883,
      "learning_rate": 0.00034857289352247477,
      "loss": 0.6572,
      "step": 3545
    },
    {
      "epoch": 0.015151644632831127,
      "grad_norm": 0.6598159074783325,
      "learning_rate": 0.00034853016578362673,
      "loss": 0.1515,
      "step": 3546
    },
    {
      "epoch": 0.015155917516258322,
      "grad_norm": 1.3293455839157104,
      "learning_rate": 0.00034848743804477864,
      "loss": 0.5714,
      "step": 3547
    },
    {
      "epoch": 0.015160190399685516,
      "grad_norm": 0.6645111441612244,
      "learning_rate": 0.0003484447103059306,
      "loss": 0.1597,
      "step": 3548
    },
    {
      "epoch": 0.01516446328311271,
      "grad_norm": 0.6598002314567566,
      "learning_rate": 0.0003484019825670826,
      "loss": 0.1516,
      "step": 3549
    },
    {
      "epoch": 0.015168736166539904,
      "grad_norm": 1.7827454805374146,
      "learning_rate": 0.0003483592548282345,
      "loss": 0.6603,
      "step": 3550
    },
    {
      "epoch": 0.0151730090499671,
      "grad_norm": 0.7247957587242126,
      "learning_rate": 0.00034831652708938645,
      "loss": 0.3833,
      "step": 3551
    },
    {
      "epoch": 0.015177281933394293,
      "grad_norm": 3.1526095867156982,
      "learning_rate": 0.00034827379935053836,
      "loss": 0.9437,
      "step": 3552
    },
    {
      "epoch": 0.015181554816821487,
      "grad_norm": 2.0848445892333984,
      "learning_rate": 0.0003482310716116903,
      "loss": 0.5064,
      "step": 3553
    },
    {
      "epoch": 0.015185827700248683,
      "grad_norm": 1.6690207719802856,
      "learning_rate": 0.00034818834387284224,
      "loss": 0.4184,
      "step": 3554
    },
    {
      "epoch": 0.015190100583675877,
      "grad_norm": 3.8015291690826416,
      "learning_rate": 0.0003481456161339942,
      "loss": 0.8907,
      "step": 3555
    },
    {
      "epoch": 0.01519437346710307,
      "grad_norm": 1.761668086051941,
      "learning_rate": 0.00034810288839514617,
      "loss": 0.596,
      "step": 3556
    },
    {
      "epoch": 0.015198646350530264,
      "grad_norm": 1.4312916994094849,
      "learning_rate": 0.0003480601606562981,
      "loss": 0.7721,
      "step": 3557
    },
    {
      "epoch": 0.01520291923395746,
      "grad_norm": 5.723459720611572,
      "learning_rate": 0.00034801743291745004,
      "loss": 0.961,
      "step": 3558
    },
    {
      "epoch": 0.015207192117384654,
      "grad_norm": 2.2337465286254883,
      "learning_rate": 0.00034797470517860195,
      "loss": 0.8826,
      "step": 3559
    },
    {
      "epoch": 0.015211465000811848,
      "grad_norm": 0.8004159331321716,
      "learning_rate": 0.0003479319774397539,
      "loss": 0.2202,
      "step": 3560
    },
    {
      "epoch": 0.015215737884239041,
      "grad_norm": 0.9275866150856018,
      "learning_rate": 0.00034788924970090583,
      "loss": 0.3667,
      "step": 3561
    },
    {
      "epoch": 0.015220010767666237,
      "grad_norm": 2.782386302947998,
      "learning_rate": 0.0003478465219620578,
      "loss": 0.6748,
      "step": 3562
    },
    {
      "epoch": 0.01522428365109343,
      "grad_norm": 1.9574360847473145,
      "learning_rate": 0.00034780379422320976,
      "loss": 0.728,
      "step": 3563
    },
    {
      "epoch": 0.015228556534520625,
      "grad_norm": 1.941832423210144,
      "learning_rate": 0.00034776106648436167,
      "loss": 0.7284,
      "step": 3564
    },
    {
      "epoch": 0.01523282941794782,
      "grad_norm": 1.5630390644073486,
      "learning_rate": 0.00034771833874551363,
      "loss": 0.4199,
      "step": 3565
    },
    {
      "epoch": 0.015237102301375014,
      "grad_norm": 1.8165615797042847,
      "learning_rate": 0.00034767561100666555,
      "loss": 0.6524,
      "step": 3566
    },
    {
      "epoch": 0.015241375184802208,
      "grad_norm": 1.2960219383239746,
      "learning_rate": 0.0003476328832678175,
      "loss": 0.6923,
      "step": 3567
    },
    {
      "epoch": 0.015245648068229402,
      "grad_norm": 1.6892104148864746,
      "learning_rate": 0.00034759015552896937,
      "loss": 0.5679,
      "step": 3568
    },
    {
      "epoch": 0.015249920951656597,
      "grad_norm": 1.228529691696167,
      "learning_rate": 0.00034754742779012133,
      "loss": 0.3857,
      "step": 3569
    },
    {
      "epoch": 0.015254193835083791,
      "grad_norm": 1.326440453529358,
      "learning_rate": 0.0003475047000512733,
      "loss": 0.3565,
      "step": 3570
    },
    {
      "epoch": 0.015258466718510985,
      "grad_norm": 2.945183277130127,
      "learning_rate": 0.0003474619723124252,
      "loss": 0.6723,
      "step": 3571
    },
    {
      "epoch": 0.01526273960193818,
      "grad_norm": 1.2648378610610962,
      "learning_rate": 0.00034741924457357717,
      "loss": 0.3695,
      "step": 3572
    },
    {
      "epoch": 0.015267012485365375,
      "grad_norm": 1.4983854293823242,
      "learning_rate": 0.0003473765168347291,
      "loss": 0.6951,
      "step": 3573
    },
    {
      "epoch": 0.015271285368792568,
      "grad_norm": 1.4621670246124268,
      "learning_rate": 0.00034733378909588105,
      "loss": 0.6362,
      "step": 3574
    },
    {
      "epoch": 0.015275558252219762,
      "grad_norm": 4.111672401428223,
      "learning_rate": 0.00034729106135703296,
      "loss": 1.4246,
      "step": 3575
    },
    {
      "epoch": 0.015279831135646958,
      "grad_norm": 1.4393800497055054,
      "learning_rate": 0.0003472483336181849,
      "loss": 0.2576,
      "step": 3576
    },
    {
      "epoch": 0.015284104019074152,
      "grad_norm": 3.161895275115967,
      "learning_rate": 0.00034720560587933683,
      "loss": 0.9512,
      "step": 3577
    },
    {
      "epoch": 0.015288376902501346,
      "grad_norm": 3.0047037601470947,
      "learning_rate": 0.0003471628781404888,
      "loss": 0.8341,
      "step": 3578
    },
    {
      "epoch": 0.01529264978592854,
      "grad_norm": 1.7116515636444092,
      "learning_rate": 0.00034712015040164076,
      "loss": 0.421,
      "step": 3579
    },
    {
      "epoch": 0.015296922669355735,
      "grad_norm": 1.4455758333206177,
      "learning_rate": 0.0003470774226627927,
      "loss": 0.6005,
      "step": 3580
    },
    {
      "epoch": 0.015301195552782929,
      "grad_norm": 1.9520230293273926,
      "learning_rate": 0.00034703469492394464,
      "loss": 0.6941,
      "step": 3581
    },
    {
      "epoch": 0.015305468436210123,
      "grad_norm": 2.8776397705078125,
      "learning_rate": 0.00034699196718509655,
      "loss": 0.7054,
      "step": 3582
    },
    {
      "epoch": 0.015309741319637318,
      "grad_norm": 1.3366130590438843,
      "learning_rate": 0.0003469492394462485,
      "loss": 0.517,
      "step": 3583
    },
    {
      "epoch": 0.015314014203064512,
      "grad_norm": 1.7282763719558716,
      "learning_rate": 0.0003469065117074004,
      "loss": 0.477,
      "step": 3584
    },
    {
      "epoch": 0.015318287086491706,
      "grad_norm": 1.4848415851593018,
      "learning_rate": 0.0003468637839685524,
      "loss": 0.3839,
      "step": 3585
    },
    {
      "epoch": 0.0153225599699189,
      "grad_norm": 2.425220489501953,
      "learning_rate": 0.00034682105622970436,
      "loss": 0.7767,
      "step": 3586
    },
    {
      "epoch": 0.015326832853346095,
      "grad_norm": 1.511138677597046,
      "learning_rate": 0.00034677832849085627,
      "loss": 0.5118,
      "step": 3587
    },
    {
      "epoch": 0.01533110573677329,
      "grad_norm": 2.254732608795166,
      "learning_rate": 0.00034673560075200823,
      "loss": 1.0029,
      "step": 3588
    },
    {
      "epoch": 0.015335378620200483,
      "grad_norm": 1.471088171005249,
      "learning_rate": 0.00034669287301316014,
      "loss": 0.5028,
      "step": 3589
    },
    {
      "epoch": 0.015339651503627679,
      "grad_norm": 0.5097065567970276,
      "learning_rate": 0.0003466501452743121,
      "loss": 0.1532,
      "step": 3590
    },
    {
      "epoch": 0.015343924387054873,
      "grad_norm": 2.842938184738159,
      "learning_rate": 0.000346607417535464,
      "loss": 0.803,
      "step": 3591
    },
    {
      "epoch": 0.015348197270482066,
      "grad_norm": 2.405442714691162,
      "learning_rate": 0.000346564689796616,
      "loss": 0.8018,
      "step": 3592
    },
    {
      "epoch": 0.01535247015390926,
      "grad_norm": 1.4571894407272339,
      "learning_rate": 0.00034652196205776795,
      "loss": 0.5011,
      "step": 3593
    },
    {
      "epoch": 0.015356743037336456,
      "grad_norm": 2.5828371047973633,
      "learning_rate": 0.00034647923431891986,
      "loss": 0.4922,
      "step": 3594
    },
    {
      "epoch": 0.01536101592076365,
      "grad_norm": 1.710365891456604,
      "learning_rate": 0.0003464365065800718,
      "loss": 0.5087,
      "step": 3595
    },
    {
      "epoch": 0.015365288804190844,
      "grad_norm": 5.045463562011719,
      "learning_rate": 0.00034639377884122373,
      "loss": 1.7417,
      "step": 3596
    },
    {
      "epoch": 0.01536956168761804,
      "grad_norm": 1.677914023399353,
      "learning_rate": 0.0003463510511023757,
      "loss": 0.5085,
      "step": 3597
    },
    {
      "epoch": 0.015373834571045233,
      "grad_norm": 1.4199830293655396,
      "learning_rate": 0.0003463083233635276,
      "loss": 0.3773,
      "step": 3598
    },
    {
      "epoch": 0.015378107454472427,
      "grad_norm": 2.2585041522979736,
      "learning_rate": 0.0003462655956246796,
      "loss": 0.6965,
      "step": 3599
    },
    {
      "epoch": 0.01538238033789962,
      "grad_norm": 4.7784953117370605,
      "learning_rate": 0.00034622286788583154,
      "loss": 1.581,
      "step": 3600
    },
    {
      "epoch": 0.015386653221326816,
      "grad_norm": 1.3252592086791992,
      "learning_rate": 0.0003461801401469834,
      "loss": 0.2816,
      "step": 3601
    },
    {
      "epoch": 0.01539092610475401,
      "grad_norm": 0.8929089307785034,
      "learning_rate": 0.00034613741240813536,
      "loss": 0.3626,
      "step": 3602
    },
    {
      "epoch": 0.015395198988181204,
      "grad_norm": 2.369081497192383,
      "learning_rate": 0.00034609468466928727,
      "loss": 0.7383,
      "step": 3603
    },
    {
      "epoch": 0.015399471871608398,
      "grad_norm": 1.3342279195785522,
      "learning_rate": 0.00034605195693043924,
      "loss": 0.5098,
      "step": 3604
    },
    {
      "epoch": 0.015403744755035594,
      "grad_norm": 2.4231343269348145,
      "learning_rate": 0.00034600922919159115,
      "loss": 0.6797,
      "step": 3605
    },
    {
      "epoch": 0.015408017638462787,
      "grad_norm": 1.0321656465530396,
      "learning_rate": 0.0003459665014527431,
      "loss": 0.4099,
      "step": 3606
    },
    {
      "epoch": 0.015412290521889981,
      "grad_norm": 2.4198408126831055,
      "learning_rate": 0.0003459237737138951,
      "loss": 0.511,
      "step": 3607
    },
    {
      "epoch": 0.015416563405317177,
      "grad_norm": 1.0060726404190063,
      "learning_rate": 0.000345881045975047,
      "loss": 0.3068,
      "step": 3608
    },
    {
      "epoch": 0.01542083628874437,
      "grad_norm": 1.253479242324829,
      "learning_rate": 0.00034583831823619895,
      "loss": 0.5522,
      "step": 3609
    },
    {
      "epoch": 0.015425109172171565,
      "grad_norm": 1.2081352472305298,
      "learning_rate": 0.00034579559049735086,
      "loss": 0.5115,
      "step": 3610
    },
    {
      "epoch": 0.015429382055598758,
      "grad_norm": 2.2141811847686768,
      "learning_rate": 0.00034575286275850283,
      "loss": 0.7407,
      "step": 3611
    },
    {
      "epoch": 0.015433654939025954,
      "grad_norm": 1.9585500955581665,
      "learning_rate": 0.00034571013501965474,
      "loss": 0.5684,
      "step": 3612
    },
    {
      "epoch": 0.015437927822453148,
      "grad_norm": 0.8868014812469482,
      "learning_rate": 0.0003456674072808067,
      "loss": 0.3289,
      "step": 3613
    },
    {
      "epoch": 0.015442200705880342,
      "grad_norm": 0.737623929977417,
      "learning_rate": 0.0003456246795419586,
      "loss": 0.259,
      "step": 3614
    },
    {
      "epoch": 0.015446473589307537,
      "grad_norm": 2.295705795288086,
      "learning_rate": 0.0003455819518031106,
      "loss": 0.8522,
      "step": 3615
    },
    {
      "epoch": 0.015450746472734731,
      "grad_norm": 1.0491042137145996,
      "learning_rate": 0.00034553922406426255,
      "loss": 0.3221,
      "step": 3616
    },
    {
      "epoch": 0.015455019356161925,
      "grad_norm": 1.213085412979126,
      "learning_rate": 0.00034549649632541446,
      "loss": 0.4453,
      "step": 3617
    },
    {
      "epoch": 0.015459292239589119,
      "grad_norm": 2.8985581398010254,
      "learning_rate": 0.0003454537685865664,
      "loss": 0.7941,
      "step": 3618
    },
    {
      "epoch": 0.015463565123016314,
      "grad_norm": 1.262634038925171,
      "learning_rate": 0.00034541104084771833,
      "loss": 0.4172,
      "step": 3619
    },
    {
      "epoch": 0.015467838006443508,
      "grad_norm": 1.3556511402130127,
      "learning_rate": 0.0003453683131088703,
      "loss": 0.5114,
      "step": 3620
    },
    {
      "epoch": 0.015472110889870702,
      "grad_norm": 0.8262990117073059,
      "learning_rate": 0.0003453255853700222,
      "loss": 0.3439,
      "step": 3621
    },
    {
      "epoch": 0.015476383773297898,
      "grad_norm": 1.3237330913543701,
      "learning_rate": 0.00034528285763117417,
      "loss": 0.4935,
      "step": 3622
    },
    {
      "epoch": 0.015480656656725092,
      "grad_norm": 3.9730396270751953,
      "learning_rate": 0.00034524012989232614,
      "loss": 1.2088,
      "step": 3623
    },
    {
      "epoch": 0.015484929540152285,
      "grad_norm": 0.6632858514785767,
      "learning_rate": 0.00034519740215347805,
      "loss": 0.2549,
      "step": 3624
    },
    {
      "epoch": 0.01548920242357948,
      "grad_norm": 3.6967382431030273,
      "learning_rate": 0.00034515467441463,
      "loss": 1.0263,
      "step": 3625
    },
    {
      "epoch": 0.015493475307006675,
      "grad_norm": 0.6831851601600647,
      "learning_rate": 0.0003451119466757819,
      "loss": 0.2709,
      "step": 3626
    },
    {
      "epoch": 0.015497748190433869,
      "grad_norm": 2.900642156600952,
      "learning_rate": 0.0003450692189369339,
      "loss": 0.6298,
      "step": 3627
    },
    {
      "epoch": 0.015502021073861063,
      "grad_norm": 1.2122427225112915,
      "learning_rate": 0.0003450264911980858,
      "loss": 0.3191,
      "step": 3628
    },
    {
      "epoch": 0.015506293957288256,
      "grad_norm": 5.011335849761963,
      "learning_rate": 0.00034498376345923776,
      "loss": 1.4765,
      "step": 3629
    },
    {
      "epoch": 0.015510566840715452,
      "grad_norm": 3.7145633697509766,
      "learning_rate": 0.00034494103572038973,
      "loss": 1.1359,
      "step": 3630
    },
    {
      "epoch": 0.015514839724142646,
      "grad_norm": 5.615060806274414,
      "learning_rate": 0.00034489830798154164,
      "loss": 1.6923,
      "step": 3631
    },
    {
      "epoch": 0.01551911260756984,
      "grad_norm": 2.2888834476470947,
      "learning_rate": 0.0003448555802426936,
      "loss": 0.6086,
      "step": 3632
    },
    {
      "epoch": 0.015523385490997035,
      "grad_norm": 1.0215240716934204,
      "learning_rate": 0.0003448128525038455,
      "loss": 0.3829,
      "step": 3633
    },
    {
      "epoch": 0.01552765837442423,
      "grad_norm": 1.8227125406265259,
      "learning_rate": 0.0003447701247649974,
      "loss": 0.536,
      "step": 3634
    },
    {
      "epoch": 0.015531931257851423,
      "grad_norm": 0.6187751889228821,
      "learning_rate": 0.00034472739702614934,
      "loss": 0.2388,
      "step": 3635
    },
    {
      "epoch": 0.015536204141278617,
      "grad_norm": 1.9434295892715454,
      "learning_rate": 0.0003446846692873013,
      "loss": 0.4648,
      "step": 3636
    },
    {
      "epoch": 0.015540477024705813,
      "grad_norm": 0.6895425319671631,
      "learning_rate": 0.00034464194154845327,
      "loss": 0.2549,
      "step": 3637
    },
    {
      "epoch": 0.015544749908133006,
      "grad_norm": 1.369865894317627,
      "learning_rate": 0.0003445992138096052,
      "loss": 0.3845,
      "step": 3638
    },
    {
      "epoch": 0.0155490227915602,
      "grad_norm": 1.983259677886963,
      "learning_rate": 0.00034455648607075714,
      "loss": 0.7206,
      "step": 3639
    },
    {
      "epoch": 0.015553295674987396,
      "grad_norm": 1.8923399448394775,
      "learning_rate": 0.00034451375833190905,
      "loss": 0.5862,
      "step": 3640
    },
    {
      "epoch": 0.01555756855841459,
      "grad_norm": 0.43929654359817505,
      "learning_rate": 0.000344471030593061,
      "loss": 0.185,
      "step": 3641
    },
    {
      "epoch": 0.015561841441841784,
      "grad_norm": 1.9097257852554321,
      "learning_rate": 0.00034442830285421293,
      "loss": 0.5521,
      "step": 3642
    },
    {
      "epoch": 0.015566114325268977,
      "grad_norm": 1.1660994291305542,
      "learning_rate": 0.0003443855751153649,
      "loss": 0.2533,
      "step": 3643
    },
    {
      "epoch": 0.015570387208696173,
      "grad_norm": 3.5552432537078857,
      "learning_rate": 0.00034434284737651686,
      "loss": 1.0485,
      "step": 3644
    },
    {
      "epoch": 0.015574660092123367,
      "grad_norm": 1.8335002660751343,
      "learning_rate": 0.00034430011963766877,
      "loss": 0.6282,
      "step": 3645
    },
    {
      "epoch": 0.01557893297555056,
      "grad_norm": 2.3408992290496826,
      "learning_rate": 0.00034425739189882074,
      "loss": 0.5932,
      "step": 3646
    },
    {
      "epoch": 0.015583205858977755,
      "grad_norm": 0.7091997861862183,
      "learning_rate": 0.00034421466415997265,
      "loss": 0.2252,
      "step": 3647
    },
    {
      "epoch": 0.01558747874240495,
      "grad_norm": 4.612784385681152,
      "learning_rate": 0.0003441719364211246,
      "loss": 1.148,
      "step": 3648
    },
    {
      "epoch": 0.015591751625832144,
      "grad_norm": 4.549923896789551,
      "learning_rate": 0.0003441292086822765,
      "loss": 1.1185,
      "step": 3649
    },
    {
      "epoch": 0.015596024509259338,
      "grad_norm": 2.2274417877197266,
      "learning_rate": 0.0003440864809434285,
      "loss": 0.4836,
      "step": 3650
    },
    {
      "epoch": 0.015600297392686533,
      "grad_norm": 1.899397611618042,
      "learning_rate": 0.0003440437532045804,
      "loss": 0.6236,
      "step": 3651
    },
    {
      "epoch": 0.015604570276113727,
      "grad_norm": 2.169522523880005,
      "learning_rate": 0.00034400102546573236,
      "loss": 0.8354,
      "step": 3652
    },
    {
      "epoch": 0.015608843159540921,
      "grad_norm": 2.6621875762939453,
      "learning_rate": 0.00034395829772688433,
      "loss": 0.6832,
      "step": 3653
    },
    {
      "epoch": 0.015613116042968115,
      "grad_norm": 1.7657098770141602,
      "learning_rate": 0.00034391556998803624,
      "loss": 0.6216,
      "step": 3654
    },
    {
      "epoch": 0.01561738892639531,
      "grad_norm": 2.123011350631714,
      "learning_rate": 0.0003438728422491882,
      "loss": 0.6027,
      "step": 3655
    },
    {
      "epoch": 0.015621661809822504,
      "grad_norm": 1.5895006656646729,
      "learning_rate": 0.0003438301145103401,
      "loss": 0.4962,
      "step": 3656
    },
    {
      "epoch": 0.0156259346932497,
      "grad_norm": 3.0976507663726807,
      "learning_rate": 0.0003437873867714921,
      "loss": 1.2134,
      "step": 3657
    },
    {
      "epoch": 0.015630207576676894,
      "grad_norm": 0.4126286506652832,
      "learning_rate": 0.000343744659032644,
      "loss": 0.1291,
      "step": 3658
    },
    {
      "epoch": 0.015634480460104086,
      "grad_norm": 0.7005586624145508,
      "learning_rate": 0.00034370193129379595,
      "loss": 0.348,
      "step": 3659
    },
    {
      "epoch": 0.01563875334353128,
      "grad_norm": 0.31018051505088806,
      "learning_rate": 0.0003436592035549479,
      "loss": 0.1084,
      "step": 3660
    },
    {
      "epoch": 0.015643026226958477,
      "grad_norm": 0.7114949822425842,
      "learning_rate": 0.00034361647581609983,
      "loss": 0.3703,
      "step": 3661
    },
    {
      "epoch": 0.01564729911038567,
      "grad_norm": 1.3877713680267334,
      "learning_rate": 0.0003435737480772518,
      "loss": 0.5506,
      "step": 3662
    },
    {
      "epoch": 0.015651571993812865,
      "grad_norm": 1.6928393840789795,
      "learning_rate": 0.0003435310203384037,
      "loss": 0.2797,
      "step": 3663
    },
    {
      "epoch": 0.01565584487724006,
      "grad_norm": 3.583127975463867,
      "learning_rate": 0.00034348829259955567,
      "loss": 1.2845,
      "step": 3664
    },
    {
      "epoch": 0.015660117760667253,
      "grad_norm": 1.3482658863067627,
      "learning_rate": 0.0003434455648607076,
      "loss": 0.4721,
      "step": 3665
    },
    {
      "epoch": 0.015664390644094448,
      "grad_norm": 1.1454205513000488,
      "learning_rate": 0.00034340283712185955,
      "loss": 0.347,
      "step": 3666
    },
    {
      "epoch": 0.015668663527521644,
      "grad_norm": 0.9928276538848877,
      "learning_rate": 0.00034336010938301146,
      "loss": 0.341,
      "step": 3667
    },
    {
      "epoch": 0.015672936410948836,
      "grad_norm": 3.679008960723877,
      "learning_rate": 0.00034331738164416337,
      "loss": 1.2255,
      "step": 3668
    },
    {
      "epoch": 0.01567720929437603,
      "grad_norm": 1.0267421007156372,
      "learning_rate": 0.00034327465390531533,
      "loss": 0.341,
      "step": 3669
    },
    {
      "epoch": 0.015681482177803224,
      "grad_norm": 4.316529750823975,
      "learning_rate": 0.00034323192616646724,
      "loss": 1.1325,
      "step": 3670
    },
    {
      "epoch": 0.01568575506123042,
      "grad_norm": 3.413963556289673,
      "learning_rate": 0.0003431891984276192,
      "loss": 1.2155,
      "step": 3671
    },
    {
      "epoch": 0.015690027944657615,
      "grad_norm": 0.88351970911026,
      "learning_rate": 0.0003431464706887711,
      "loss": 0.295,
      "step": 3672
    },
    {
      "epoch": 0.015694300828084807,
      "grad_norm": 1.3062740564346313,
      "learning_rate": 0.0003431037429499231,
      "loss": 0.4725,
      "step": 3673
    },
    {
      "epoch": 0.015698573711512002,
      "grad_norm": 4.476534843444824,
      "learning_rate": 0.00034306101521107505,
      "loss": 1.8493,
      "step": 3674
    },
    {
      "epoch": 0.015702846594939198,
      "grad_norm": 2.9692835807800293,
      "learning_rate": 0.00034301828747222696,
      "loss": 1.0888,
      "step": 3675
    },
    {
      "epoch": 0.01570711947836639,
      "grad_norm": 2.479458808898926,
      "learning_rate": 0.0003429755597333789,
      "loss": 0.6829,
      "step": 3676
    },
    {
      "epoch": 0.015711392361793586,
      "grad_norm": 1.1318578720092773,
      "learning_rate": 0.00034293283199453084,
      "loss": 0.4141,
      "step": 3677
    },
    {
      "epoch": 0.01571566524522078,
      "grad_norm": 1.8949618339538574,
      "learning_rate": 0.0003428901042556828,
      "loss": 0.5129,
      "step": 3678
    },
    {
      "epoch": 0.015719938128647973,
      "grad_norm": 1.2196606397628784,
      "learning_rate": 0.0003428473765168347,
      "loss": 0.4162,
      "step": 3679
    },
    {
      "epoch": 0.01572421101207517,
      "grad_norm": 1.2398020029067993,
      "learning_rate": 0.0003428046487779867,
      "loss": 0.3841,
      "step": 3680
    },
    {
      "epoch": 0.01572848389550236,
      "grad_norm": 1.948109745979309,
      "learning_rate": 0.0003427619210391386,
      "loss": 0.4496,
      "step": 3681
    },
    {
      "epoch": 0.015732756778929557,
      "grad_norm": 3.9642417430877686,
      "learning_rate": 0.00034271919330029055,
      "loss": 0.8968,
      "step": 3682
    },
    {
      "epoch": 0.015737029662356752,
      "grad_norm": 1.3241491317749023,
      "learning_rate": 0.0003426764655614425,
      "loss": 0.5706,
      "step": 3683
    },
    {
      "epoch": 0.015741302545783944,
      "grad_norm": 1.8184294700622559,
      "learning_rate": 0.00034263373782259443,
      "loss": 0.4362,
      "step": 3684
    },
    {
      "epoch": 0.01574557542921114,
      "grad_norm": 3.564535140991211,
      "learning_rate": 0.0003425910100837464,
      "loss": 0.9554,
      "step": 3685
    },
    {
      "epoch": 0.015749848312638336,
      "grad_norm": 3.536623001098633,
      "learning_rate": 0.0003425482823448983,
      "loss": 1.2287,
      "step": 3686
    },
    {
      "epoch": 0.015754121196065528,
      "grad_norm": 2.631030321121216,
      "learning_rate": 0.00034250555460605027,
      "loss": 1.3275,
      "step": 3687
    },
    {
      "epoch": 0.015758394079492723,
      "grad_norm": 1.6255015134811401,
      "learning_rate": 0.0003424628268672022,
      "loss": 0.5708,
      "step": 3688
    },
    {
      "epoch": 0.01576266696291992,
      "grad_norm": 0.9350528120994568,
      "learning_rate": 0.00034242009912835414,
      "loss": 0.4607,
      "step": 3689
    },
    {
      "epoch": 0.01576693984634711,
      "grad_norm": 1.1248925924301147,
      "learning_rate": 0.0003423773713895061,
      "loss": 0.4527,
      "step": 3690
    },
    {
      "epoch": 0.015771212729774307,
      "grad_norm": 2.8922042846679688,
      "learning_rate": 0.000342334643650658,
      "loss": 1.5552,
      "step": 3691
    },
    {
      "epoch": 0.015775485613201502,
      "grad_norm": 0.34085577726364136,
      "learning_rate": 0.00034229191591181,
      "loss": 0.1454,
      "step": 3692
    },
    {
      "epoch": 0.015779758496628694,
      "grad_norm": 3.3847248554229736,
      "learning_rate": 0.0003422491881729619,
      "loss": 0.74,
      "step": 3693
    },
    {
      "epoch": 0.01578403138005589,
      "grad_norm": 1.7520235776901245,
      "learning_rate": 0.00034220646043411386,
      "loss": 0.8854,
      "step": 3694
    },
    {
      "epoch": 0.015788304263483082,
      "grad_norm": 1.1459137201309204,
      "learning_rate": 0.00034216373269526577,
      "loss": 0.4621,
      "step": 3695
    },
    {
      "epoch": 0.015792577146910278,
      "grad_norm": 0.5366716384887695,
      "learning_rate": 0.00034212100495641774,
      "loss": 0.2073,
      "step": 3696
    },
    {
      "epoch": 0.015796850030337473,
      "grad_norm": 13.287443161010742,
      "learning_rate": 0.0003420782772175697,
      "loss": 5.8878,
      "step": 3697
    },
    {
      "epoch": 0.015801122913764665,
      "grad_norm": 0.33700039982795715,
      "learning_rate": 0.0003420355494787216,
      "loss": 0.1499,
      "step": 3698
    },
    {
      "epoch": 0.01580539579719186,
      "grad_norm": 1.2609399557113647,
      "learning_rate": 0.0003419928217398735,
      "loss": 0.4644,
      "step": 3699
    },
    {
      "epoch": 0.015809668680619057,
      "grad_norm": 6.795784950256348,
      "learning_rate": 0.00034195009400102543,
      "loss": 2.4603,
      "step": 3700
    },
    {
      "epoch": 0.01581394156404625,
      "grad_norm": 1.2522988319396973,
      "learning_rate": 0.0003419073662621774,
      "loss": 0.595,
      "step": 3701
    },
    {
      "epoch": 0.015818214447473444,
      "grad_norm": 3.501188039779663,
      "learning_rate": 0.0003418646385233293,
      "loss": 1.1438,
      "step": 3702
    },
    {
      "epoch": 0.01582248733090064,
      "grad_norm": 1.3145756721496582,
      "learning_rate": 0.0003418219107844813,
      "loss": 0.6419,
      "step": 3703
    },
    {
      "epoch": 0.015826760214327832,
      "grad_norm": 0.6390284895896912,
      "learning_rate": 0.00034177918304563324,
      "loss": 0.21,
      "step": 3704
    },
    {
      "epoch": 0.015831033097755028,
      "grad_norm": 3.1411397457122803,
      "learning_rate": 0.00034173645530678515,
      "loss": 1.0676,
      "step": 3705
    },
    {
      "epoch": 0.01583530598118222,
      "grad_norm": 26.861635208129883,
      "learning_rate": 0.0003416937275679371,
      "loss": 2.5509,
      "step": 3706
    },
    {
      "epoch": 0.015839578864609415,
      "grad_norm": 6.5101094245910645,
      "learning_rate": 0.000341650999829089,
      "loss": 1.8056,
      "step": 3707
    },
    {
      "epoch": 0.01584385174803661,
      "grad_norm": 1.1797200441360474,
      "learning_rate": 0.000341608272090241,
      "loss": 0.576,
      "step": 3708
    },
    {
      "epoch": 0.015848124631463803,
      "grad_norm": 1.9659054279327393,
      "learning_rate": 0.0003415655443513929,
      "loss": 0.7678,
      "step": 3709
    },
    {
      "epoch": 0.015852397514891,
      "grad_norm": 1.2221028804779053,
      "learning_rate": 0.00034152281661254487,
      "loss": 0.4776,
      "step": 3710
    },
    {
      "epoch": 0.015856670398318194,
      "grad_norm": 1.5004838705062866,
      "learning_rate": 0.00034148008887369683,
      "loss": 0.4619,
      "step": 3711
    },
    {
      "epoch": 0.015860943281745386,
      "grad_norm": 0.3799321949481964,
      "learning_rate": 0.00034143736113484874,
      "loss": 0.1608,
      "step": 3712
    },
    {
      "epoch": 0.015865216165172582,
      "grad_norm": 4.163397789001465,
      "learning_rate": 0.0003413946333960007,
      "loss": 1.5872,
      "step": 3713
    },
    {
      "epoch": 0.015869489048599777,
      "grad_norm": 2.684690475463867,
      "learning_rate": 0.0003413519056571526,
      "loss": 0.937,
      "step": 3714
    },
    {
      "epoch": 0.01587376193202697,
      "grad_norm": 1.308883786201477,
      "learning_rate": 0.0003413091779183046,
      "loss": 0.4495,
      "step": 3715
    },
    {
      "epoch": 0.015878034815454165,
      "grad_norm": 1.5460187196731567,
      "learning_rate": 0.0003412664501794565,
      "loss": 0.7712,
      "step": 3716
    },
    {
      "epoch": 0.01588230769888136,
      "grad_norm": 3.3231241703033447,
      "learning_rate": 0.00034122372244060846,
      "loss": 0.861,
      "step": 3717
    },
    {
      "epoch": 0.015886580582308553,
      "grad_norm": 1.7596994638442993,
      "learning_rate": 0.00034118099470176037,
      "loss": 0.415,
      "step": 3718
    },
    {
      "epoch": 0.01589085346573575,
      "grad_norm": 1.7730633020401,
      "learning_rate": 0.00034113826696291233,
      "loss": 0.4059,
      "step": 3719
    },
    {
      "epoch": 0.01589512634916294,
      "grad_norm": 4.3652496337890625,
      "learning_rate": 0.0003410955392240643,
      "loss": 1.6521,
      "step": 3720
    },
    {
      "epoch": 0.015899399232590136,
      "grad_norm": 1.320691704750061,
      "learning_rate": 0.0003410528114852162,
      "loss": 0.3771,
      "step": 3721
    },
    {
      "epoch": 0.015903672116017332,
      "grad_norm": 0.8162903785705566,
      "learning_rate": 0.0003410100837463682,
      "loss": 0.3573,
      "step": 3722
    },
    {
      "epoch": 0.015907944999444524,
      "grad_norm": 0.921180248260498,
      "learning_rate": 0.0003409673560075201,
      "loss": 0.3186,
      "step": 3723
    },
    {
      "epoch": 0.01591221788287172,
      "grad_norm": 3.57755184173584,
      "learning_rate": 0.00034092462826867205,
      "loss": 1.5752,
      "step": 3724
    },
    {
      "epoch": 0.015916490766298915,
      "grad_norm": 4.90508508682251,
      "learning_rate": 0.00034088190052982396,
      "loss": 2.1753,
      "step": 3725
    },
    {
      "epoch": 0.015920763649726107,
      "grad_norm": 1.326500654220581,
      "learning_rate": 0.0003408391727909759,
      "loss": 0.3378,
      "step": 3726
    },
    {
      "epoch": 0.015925036533153303,
      "grad_norm": 2.099611759185791,
      "learning_rate": 0.0003407964450521279,
      "loss": 0.6807,
      "step": 3727
    },
    {
      "epoch": 0.0159293094165805,
      "grad_norm": 1.129381537437439,
      "learning_rate": 0.0003407537173132798,
      "loss": 0.4729,
      "step": 3728
    },
    {
      "epoch": 0.01593358230000769,
      "grad_norm": 1.641919732093811,
      "learning_rate": 0.00034071098957443177,
      "loss": 0.4175,
      "step": 3729
    },
    {
      "epoch": 0.015937855183434886,
      "grad_norm": 3.734431266784668,
      "learning_rate": 0.0003406682618355837,
      "loss": 1.2819,
      "step": 3730
    },
    {
      "epoch": 0.015942128066862078,
      "grad_norm": 0.7541061639785767,
      "learning_rate": 0.00034062553409673564,
      "loss": 0.2953,
      "step": 3731
    },
    {
      "epoch": 0.015946400950289274,
      "grad_norm": 2.994752883911133,
      "learning_rate": 0.0003405828063578875,
      "loss": 0.8074,
      "step": 3732
    },
    {
      "epoch": 0.01595067383371647,
      "grad_norm": 1.610626220703125,
      "learning_rate": 0.00034054007861903946,
      "loss": 0.625,
      "step": 3733
    },
    {
      "epoch": 0.01595494671714366,
      "grad_norm": 1.1922204494476318,
      "learning_rate": 0.00034049735088019143,
      "loss": 0.2453,
      "step": 3734
    },
    {
      "epoch": 0.015959219600570857,
      "grad_norm": 1.1539384126663208,
      "learning_rate": 0.00034045462314134334,
      "loss": 0.3909,
      "step": 3735
    },
    {
      "epoch": 0.015963492483998053,
      "grad_norm": 3.333513021469116,
      "learning_rate": 0.0003404118954024953,
      "loss": 0.9849,
      "step": 3736
    },
    {
      "epoch": 0.015967765367425245,
      "grad_norm": 2.54754638671875,
      "learning_rate": 0.0003403691676636472,
      "loss": 0.7436,
      "step": 3737
    },
    {
      "epoch": 0.01597203825085244,
      "grad_norm": 1.6585123538970947,
      "learning_rate": 0.0003403264399247992,
      "loss": 0.567,
      "step": 3738
    },
    {
      "epoch": 0.015976311134279636,
      "grad_norm": 1.0896559953689575,
      "learning_rate": 0.0003402837121859511,
      "loss": 0.4154,
      "step": 3739
    },
    {
      "epoch": 0.015980584017706828,
      "grad_norm": 0.9236665964126587,
      "learning_rate": 0.00034024098444710305,
      "loss": 0.3361,
      "step": 3740
    },
    {
      "epoch": 0.015984856901134024,
      "grad_norm": 1.1674777269363403,
      "learning_rate": 0.000340198256708255,
      "loss": 0.214,
      "step": 3741
    },
    {
      "epoch": 0.01598912978456122,
      "grad_norm": 1.8796207904815674,
      "learning_rate": 0.00034015552896940693,
      "loss": 0.7796,
      "step": 3742
    },
    {
      "epoch": 0.01599340266798841,
      "grad_norm": 1.0212364196777344,
      "learning_rate": 0.0003401128012305589,
      "loss": 0.1702,
      "step": 3743
    },
    {
      "epoch": 0.015997675551415607,
      "grad_norm": 0.7851901650428772,
      "learning_rate": 0.0003400700734917108,
      "loss": 0.2572,
      "step": 3744
    },
    {
      "epoch": 0.0160019484348428,
      "grad_norm": 3.2347187995910645,
      "learning_rate": 0.00034002734575286277,
      "loss": 0.8948,
      "step": 3745
    },
    {
      "epoch": 0.016006221318269995,
      "grad_norm": 3.773446798324585,
      "learning_rate": 0.0003399846180140147,
      "loss": 0.9377,
      "step": 3746
    },
    {
      "epoch": 0.01601049420169719,
      "grad_norm": 0.800841212272644,
      "learning_rate": 0.00033994189027516665,
      "loss": 0.1056,
      "step": 3747
    },
    {
      "epoch": 0.016014767085124382,
      "grad_norm": 4.410590648651123,
      "learning_rate": 0.00033989916253631856,
      "loss": 1.0456,
      "step": 3748
    },
    {
      "epoch": 0.016019039968551578,
      "grad_norm": 0.6507395505905151,
      "learning_rate": 0.0003398564347974705,
      "loss": 0.0722,
      "step": 3749
    },
    {
      "epoch": 0.016023312851978774,
      "grad_norm": 1.185517430305481,
      "learning_rate": 0.0003398137070586225,
      "loss": 0.3531,
      "step": 3750
    },
    {
      "epoch": 0.016027585735405966,
      "grad_norm": 3.3974502086639404,
      "learning_rate": 0.0003397709793197744,
      "loss": 1.0846,
      "step": 3751
    },
    {
      "epoch": 0.01603185861883316,
      "grad_norm": 3.541210651397705,
      "learning_rate": 0.00033972825158092636,
      "loss": 1.2926,
      "step": 3752
    },
    {
      "epoch": 0.016036131502260357,
      "grad_norm": 2.1422009468078613,
      "learning_rate": 0.0003396855238420783,
      "loss": 0.7683,
      "step": 3753
    },
    {
      "epoch": 0.01604040438568755,
      "grad_norm": 0.8288891911506653,
      "learning_rate": 0.00033964279610323024,
      "loss": 0.2721,
      "step": 3754
    },
    {
      "epoch": 0.016044677269114745,
      "grad_norm": 3.1160542964935303,
      "learning_rate": 0.00033960006836438215,
      "loss": 0.9963,
      "step": 3755
    },
    {
      "epoch": 0.016048950152541937,
      "grad_norm": 2.6493523120880127,
      "learning_rate": 0.0003395573406255341,
      "loss": 0.6787,
      "step": 3756
    },
    {
      "epoch": 0.016053223035969132,
      "grad_norm": 3.175386428833008,
      "learning_rate": 0.0003395146128866861,
      "loss": 0.531,
      "step": 3757
    },
    {
      "epoch": 0.016057495919396328,
      "grad_norm": 12.4088716506958,
      "learning_rate": 0.000339471885147838,
      "loss": 4.9397,
      "step": 3758
    },
    {
      "epoch": 0.01606176880282352,
      "grad_norm": 1.1402125358581543,
      "learning_rate": 0.00033942915740898996,
      "loss": 0.3706,
      "step": 3759
    },
    {
      "epoch": 0.016066041686250716,
      "grad_norm": 2.455493927001953,
      "learning_rate": 0.00033938642967014187,
      "loss": 0.9419,
      "step": 3760
    },
    {
      "epoch": 0.01607031456967791,
      "grad_norm": 4.53653621673584,
      "learning_rate": 0.00033934370193129383,
      "loss": 1.3231,
      "step": 3761
    },
    {
      "epoch": 0.016074587453105103,
      "grad_norm": 1.4195566177368164,
      "learning_rate": 0.00033930097419244574,
      "loss": 0.4722,
      "step": 3762
    },
    {
      "epoch": 0.0160788603365323,
      "grad_norm": 4.482729434967041,
      "learning_rate": 0.0003392582464535977,
      "loss": 1.2325,
      "step": 3763
    },
    {
      "epoch": 0.016083133219959495,
      "grad_norm": 0.17155930399894714,
      "learning_rate": 0.00033921551871474967,
      "loss": 0.0131,
      "step": 3764
    },
    {
      "epoch": 0.016087406103386687,
      "grad_norm": 1.218196988105774,
      "learning_rate": 0.00033917279097590153,
      "loss": 0.4001,
      "step": 3765
    },
    {
      "epoch": 0.016091678986813882,
      "grad_norm": 3.469588279724121,
      "learning_rate": 0.0003391300632370535,
      "loss": 0.955,
      "step": 3766
    },
    {
      "epoch": 0.016095951870241074,
      "grad_norm": 2.5244271755218506,
      "learning_rate": 0.0003390873354982054,
      "loss": 0.6472,
      "step": 3767
    },
    {
      "epoch": 0.01610022475366827,
      "grad_norm": 1.1560899019241333,
      "learning_rate": 0.00033904460775935737,
      "loss": 0.3196,
      "step": 3768
    },
    {
      "epoch": 0.016104497637095466,
      "grad_norm": 8.866551399230957,
      "learning_rate": 0.0003390018800205093,
      "loss": 3.2669,
      "step": 3769
    },
    {
      "epoch": 0.016108770520522658,
      "grad_norm": 1.2572017908096313,
      "learning_rate": 0.00033895915228166124,
      "loss": 0.382,
      "step": 3770
    },
    {
      "epoch": 0.016113043403949853,
      "grad_norm": 1.027078628540039,
      "learning_rate": 0.0003389164245428132,
      "loss": 0.3528,
      "step": 3771
    },
    {
      "epoch": 0.01611731628737705,
      "grad_norm": 1.2502408027648926,
      "learning_rate": 0.0003388736968039651,
      "loss": 0.3822,
      "step": 3772
    },
    {
      "epoch": 0.01612158917080424,
      "grad_norm": 1.2245078086853027,
      "learning_rate": 0.0003388309690651171,
      "loss": 0.3821,
      "step": 3773
    },
    {
      "epoch": 0.016125862054231437,
      "grad_norm": 2.418140172958374,
      "learning_rate": 0.000338788241326269,
      "loss": 0.5875,
      "step": 3774
    },
    {
      "epoch": 0.016130134937658632,
      "grad_norm": 4.683186054229736,
      "learning_rate": 0.00033874551358742096,
      "loss": 1.7741,
      "step": 3775
    },
    {
      "epoch": 0.016134407821085824,
      "grad_norm": 1.580062985420227,
      "learning_rate": 0.00033870278584857287,
      "loss": 0.4889,
      "step": 3776
    },
    {
      "epoch": 0.01613868070451302,
      "grad_norm": 0.965050995349884,
      "learning_rate": 0.00033866005810972484,
      "loss": 0.3197,
      "step": 3777
    },
    {
      "epoch": 0.016142953587940215,
      "grad_norm": 2.7639710903167725,
      "learning_rate": 0.0003386173303708768,
      "loss": 0.765,
      "step": 3778
    },
    {
      "epoch": 0.016147226471367408,
      "grad_norm": 4.554080009460449,
      "learning_rate": 0.0003385746026320287,
      "loss": 1.5889,
      "step": 3779
    },
    {
      "epoch": 0.016151499354794603,
      "grad_norm": 2.5534427165985107,
      "learning_rate": 0.0003385318748931807,
      "loss": 0.1463,
      "step": 3780
    },
    {
      "epoch": 0.016155772238221795,
      "grad_norm": 0.9417294263839722,
      "learning_rate": 0.0003384891471543326,
      "loss": 0.3357,
      "step": 3781
    },
    {
      "epoch": 0.01616004512164899,
      "grad_norm": 2.7871975898742676,
      "learning_rate": 0.00033844641941548455,
      "loss": 0.4297,
      "step": 3782
    },
    {
      "epoch": 0.016164318005076186,
      "grad_norm": 4.108981609344482,
      "learning_rate": 0.00033840369167663646,
      "loss": 1.2466,
      "step": 3783
    },
    {
      "epoch": 0.01616859088850338,
      "grad_norm": 1.337298035621643,
      "learning_rate": 0.00033836096393778843,
      "loss": 0.433,
      "step": 3784
    },
    {
      "epoch": 0.016172863771930574,
      "grad_norm": 3.812870740890503,
      "learning_rate": 0.00033831823619894034,
      "loss": 0.7091,
      "step": 3785
    },
    {
      "epoch": 0.01617713665535777,
      "grad_norm": 4.0164408683776855,
      "learning_rate": 0.0003382755084600923,
      "loss": 1.117,
      "step": 3786
    },
    {
      "epoch": 0.016181409538784962,
      "grad_norm": 9.81812572479248,
      "learning_rate": 0.00033823278072124427,
      "loss": 3.354,
      "step": 3787
    },
    {
      "epoch": 0.016185682422212157,
      "grad_norm": 0.45443496108055115,
      "learning_rate": 0.0003381900529823962,
      "loss": 0.0429,
      "step": 3788
    },
    {
      "epoch": 0.016189955305639353,
      "grad_norm": 0.49806877970695496,
      "learning_rate": 0.00033814732524354814,
      "loss": 0.0476,
      "step": 3789
    },
    {
      "epoch": 0.016194228189066545,
      "grad_norm": 7.3037872314453125,
      "learning_rate": 0.00033810459750470006,
      "loss": 2.2298,
      "step": 3790
    },
    {
      "epoch": 0.01619850107249374,
      "grad_norm": 1.8899556398391724,
      "learning_rate": 0.000338061869765852,
      "loss": 0.3889,
      "step": 3791
    },
    {
      "epoch": 0.016202773955920933,
      "grad_norm": 0.9666167497634888,
      "learning_rate": 0.00033801914202700393,
      "loss": 0.352,
      "step": 3792
    },
    {
      "epoch": 0.01620704683934813,
      "grad_norm": 0.8389331102371216,
      "learning_rate": 0.0003379764142881559,
      "loss": 0.3265,
      "step": 3793
    },
    {
      "epoch": 0.016211319722775324,
      "grad_norm": 3.2822840213775635,
      "learning_rate": 0.00033793368654930786,
      "loss": 0.5258,
      "step": 3794
    },
    {
      "epoch": 0.016215592606202516,
      "grad_norm": 3.8144664764404297,
      "learning_rate": 0.00033789095881045977,
      "loss": 1.1683,
      "step": 3795
    },
    {
      "epoch": 0.016219865489629712,
      "grad_norm": 2.7371976375579834,
      "learning_rate": 0.00033784823107161174,
      "loss": 0.3485,
      "step": 3796
    },
    {
      "epoch": 0.016224138373056907,
      "grad_norm": 2.5458884239196777,
      "learning_rate": 0.00033780550333276365,
      "loss": 0.6196,
      "step": 3797
    },
    {
      "epoch": 0.0162284112564841,
      "grad_norm": 2.5580713748931885,
      "learning_rate": 0.00033776277559391556,
      "loss": 0.3383,
      "step": 3798
    },
    {
      "epoch": 0.016232684139911295,
      "grad_norm": 2.7384767532348633,
      "learning_rate": 0.00033772004785506747,
      "loss": 0.6735,
      "step": 3799
    },
    {
      "epoch": 0.01623695702333849,
      "grad_norm": 4.401820182800293,
      "learning_rate": 0.00033767732011621943,
      "loss": 0.9823,
      "step": 3800
    },
    {
      "epoch": 0.016241229906765683,
      "grad_norm": 1.0499120950698853,
      "learning_rate": 0.0003376345923773714,
      "loss": 0.382,
      "step": 3801
    },
    {
      "epoch": 0.01624550279019288,
      "grad_norm": 6.583089351654053,
      "learning_rate": 0.0003375918646385233,
      "loss": 1.9306,
      "step": 3802
    },
    {
      "epoch": 0.016249775673620074,
      "grad_norm": 1.517936110496521,
      "learning_rate": 0.0003375491368996753,
      "loss": 0.3194,
      "step": 3803
    },
    {
      "epoch": 0.016254048557047266,
      "grad_norm": 6.358855247497559,
      "learning_rate": 0.0003375064091608272,
      "loss": 2.0109,
      "step": 3804
    },
    {
      "epoch": 0.01625832144047446,
      "grad_norm": 6.007069110870361,
      "learning_rate": 0.00033746368142197915,
      "loss": 1.6871,
      "step": 3805
    },
    {
      "epoch": 0.016262594323901654,
      "grad_norm": 1.8578377962112427,
      "learning_rate": 0.00033742095368313106,
      "loss": 0.8993,
      "step": 3806
    },
    {
      "epoch": 0.01626686720732885,
      "grad_norm": 1.1650660037994385,
      "learning_rate": 0.000337378225944283,
      "loss": 0.4054,
      "step": 3807
    },
    {
      "epoch": 0.016271140090756045,
      "grad_norm": 1.5901424884796143,
      "learning_rate": 0.000337335498205435,
      "loss": 0.4648,
      "step": 3808
    },
    {
      "epoch": 0.016275412974183237,
      "grad_norm": 4.64864444732666,
      "learning_rate": 0.0003372927704665869,
      "loss": 1.5376,
      "step": 3809
    },
    {
      "epoch": 0.016279685857610433,
      "grad_norm": 3.3550403118133545,
      "learning_rate": 0.00033725004272773887,
      "loss": 1.1053,
      "step": 3810
    },
    {
      "epoch": 0.016283958741037628,
      "grad_norm": 2.2795162200927734,
      "learning_rate": 0.0003372073149888908,
      "loss": 0.5043,
      "step": 3811
    },
    {
      "epoch": 0.01628823162446482,
      "grad_norm": 2.364609956741333,
      "learning_rate": 0.00033716458725004274,
      "loss": 0.9007,
      "step": 3812
    },
    {
      "epoch": 0.016292504507892016,
      "grad_norm": 5.233317852020264,
      "learning_rate": 0.00033712185951119465,
      "loss": 1.7167,
      "step": 3813
    },
    {
      "epoch": 0.01629677739131921,
      "grad_norm": 0.6350416541099548,
      "learning_rate": 0.0003370791317723466,
      "loss": 0.2059,
      "step": 3814
    },
    {
      "epoch": 0.016301050274746404,
      "grad_norm": 1.7781184911727905,
      "learning_rate": 0.0003370364040334986,
      "loss": 0.7955,
      "step": 3815
    },
    {
      "epoch": 0.0163053231581736,
      "grad_norm": 4.530871868133545,
      "learning_rate": 0.0003369936762946505,
      "loss": 1.5429,
      "step": 3816
    },
    {
      "epoch": 0.01630959604160079,
      "grad_norm": 2.7388715744018555,
      "learning_rate": 0.00033695094855580246,
      "loss": 0.4193,
      "step": 3817
    },
    {
      "epoch": 0.016313868925027987,
      "grad_norm": 7.030118942260742,
      "learning_rate": 0.00033690822081695437,
      "loss": 2.4181,
      "step": 3818
    },
    {
      "epoch": 0.016318141808455183,
      "grad_norm": 3.3186559677124023,
      "learning_rate": 0.00033686549307810633,
      "loss": 1.0447,
      "step": 3819
    },
    {
      "epoch": 0.016322414691882375,
      "grad_norm": 2.9367239475250244,
      "learning_rate": 0.00033682276533925824,
      "loss": 1.0036,
      "step": 3820
    },
    {
      "epoch": 0.01632668757530957,
      "grad_norm": 2.6693954467773438,
      "learning_rate": 0.0003367800376004102,
      "loss": 0.9576,
      "step": 3821
    },
    {
      "epoch": 0.016330960458736766,
      "grad_norm": 0.5345093011856079,
      "learning_rate": 0.0003367373098615621,
      "loss": 0.1304,
      "step": 3822
    },
    {
      "epoch": 0.016335233342163958,
      "grad_norm": 2.2757740020751953,
      "learning_rate": 0.0003366945821227141,
      "loss": 0.8454,
      "step": 3823
    },
    {
      "epoch": 0.016339506225591154,
      "grad_norm": 1.5443331003189087,
      "learning_rate": 0.00033665185438386605,
      "loss": 0.4746,
      "step": 3824
    },
    {
      "epoch": 0.01634377910901835,
      "grad_norm": 1.4531604051589966,
      "learning_rate": 0.00033660912664501796,
      "loss": 0.449,
      "step": 3825
    },
    {
      "epoch": 0.01634805199244554,
      "grad_norm": 5.977309703826904,
      "learning_rate": 0.0003365663989061699,
      "loss": 1.9085,
      "step": 3826
    },
    {
      "epoch": 0.016352324875872737,
      "grad_norm": 1.7056622505187988,
      "learning_rate": 0.00033652367116732184,
      "loss": 0.5147,
      "step": 3827
    },
    {
      "epoch": 0.016356597759299932,
      "grad_norm": 1.0045911073684692,
      "learning_rate": 0.0003364809434284738,
      "loss": 0.2347,
      "step": 3828
    },
    {
      "epoch": 0.016360870642727125,
      "grad_norm": 2.5294244289398193,
      "learning_rate": 0.0003364382156896257,
      "loss": 0.6999,
      "step": 3829
    },
    {
      "epoch": 0.01636514352615432,
      "grad_norm": 0.8016659617424011,
      "learning_rate": 0.0003363954879507777,
      "loss": 0.1876,
      "step": 3830
    },
    {
      "epoch": 0.016369416409581512,
      "grad_norm": 3.9138882160186768,
      "learning_rate": 0.0003363527602119296,
      "loss": 0.7939,
      "step": 3831
    },
    {
      "epoch": 0.016373689293008708,
      "grad_norm": 1.1874462366104126,
      "learning_rate": 0.0003363100324730815,
      "loss": 0.3658,
      "step": 3832
    },
    {
      "epoch": 0.016377962176435903,
      "grad_norm": 1.839201807975769,
      "learning_rate": 0.00033626730473423346,
      "loss": 0.3241,
      "step": 3833
    },
    {
      "epoch": 0.016382235059863096,
      "grad_norm": 3.207688331604004,
      "learning_rate": 0.0003362245769953854,
      "loss": 0.6715,
      "step": 3834
    },
    {
      "epoch": 0.01638650794329029,
      "grad_norm": 1.7651699781417847,
      "learning_rate": 0.00033618184925653734,
      "loss": 0.2755,
      "step": 3835
    },
    {
      "epoch": 0.016390780826717487,
      "grad_norm": 2.6192636489868164,
      "learning_rate": 0.00033613912151768925,
      "loss": 0.4832,
      "step": 3836
    },
    {
      "epoch": 0.01639505371014468,
      "grad_norm": 1.066999077796936,
      "learning_rate": 0.0003360963937788412,
      "loss": 0.3318,
      "step": 3837
    },
    {
      "epoch": 0.016399326593571874,
      "grad_norm": 1.341170072555542,
      "learning_rate": 0.0003360536660399932,
      "loss": 0.4335,
      "step": 3838
    },
    {
      "epoch": 0.01640359947699907,
      "grad_norm": 3.203418493270874,
      "learning_rate": 0.0003360109383011451,
      "loss": 1.0737,
      "step": 3839
    },
    {
      "epoch": 0.016407872360426262,
      "grad_norm": 2.5318386554718018,
      "learning_rate": 0.00033596821056229706,
      "loss": 1.1043,
      "step": 3840
    },
    {
      "epoch": 0.016412145243853458,
      "grad_norm": 4.470674991607666,
      "learning_rate": 0.00033592548282344897,
      "loss": 1.1747,
      "step": 3841
    },
    {
      "epoch": 0.01641641812728065,
      "grad_norm": 1.7856045961380005,
      "learning_rate": 0.00033588275508460093,
      "loss": 0.409,
      "step": 3842
    },
    {
      "epoch": 0.016420691010707845,
      "grad_norm": 3.8815300464630127,
      "learning_rate": 0.00033584002734575284,
      "loss": 1.3546,
      "step": 3843
    },
    {
      "epoch": 0.01642496389413504,
      "grad_norm": 1.0350561141967773,
      "learning_rate": 0.0003357972996069048,
      "loss": 0.3474,
      "step": 3844
    },
    {
      "epoch": 0.016429236777562233,
      "grad_norm": 0.5727600455284119,
      "learning_rate": 0.00033575457186805677,
      "loss": 0.1807,
      "step": 3845
    },
    {
      "epoch": 0.01643350966098943,
      "grad_norm": 2.7153916358947754,
      "learning_rate": 0.0003357118441292087,
      "loss": 0.5937,
      "step": 3846
    },
    {
      "epoch": 0.016437782544416624,
      "grad_norm": 0.9656012654304504,
      "learning_rate": 0.00033566911639036065,
      "loss": 0.3059,
      "step": 3847
    },
    {
      "epoch": 0.016442055427843816,
      "grad_norm": 1.3272267580032349,
      "learning_rate": 0.00033562638865151256,
      "loss": 0.3915,
      "step": 3848
    },
    {
      "epoch": 0.016446328311271012,
      "grad_norm": 1.6794493198394775,
      "learning_rate": 0.0003355836609126645,
      "loss": 0.5771,
      "step": 3849
    },
    {
      "epoch": 0.016450601194698208,
      "grad_norm": 2.1131973266601562,
      "learning_rate": 0.00033554093317381643,
      "loss": 0.8253,
      "step": 3850
    },
    {
      "epoch": 0.0164548740781254,
      "grad_norm": 0.9391082525253296,
      "learning_rate": 0.0003354982054349684,
      "loss": 0.3061,
      "step": 3851
    },
    {
      "epoch": 0.016459146961552595,
      "grad_norm": 4.368802070617676,
      "learning_rate": 0.0003354554776961203,
      "loss": 1.3445,
      "step": 3852
    },
    {
      "epoch": 0.01646341984497979,
      "grad_norm": 4.987234115600586,
      "learning_rate": 0.0003354127499572723,
      "loss": 1.6475,
      "step": 3853
    },
    {
      "epoch": 0.016467692728406983,
      "grad_norm": 0.5908342599868774,
      "learning_rate": 0.00033537002221842424,
      "loss": 0.1564,
      "step": 3854
    },
    {
      "epoch": 0.01647196561183418,
      "grad_norm": 4.461359024047852,
      "learning_rate": 0.00033532729447957615,
      "loss": 0.8643,
      "step": 3855
    },
    {
      "epoch": 0.01647623849526137,
      "grad_norm": 2.368304967880249,
      "learning_rate": 0.0003352845667407281,
      "loss": 0.9037,
      "step": 3856
    },
    {
      "epoch": 0.016480511378688566,
      "grad_norm": 1.7558560371398926,
      "learning_rate": 0.00033524183900188,
      "loss": 0.6316,
      "step": 3857
    },
    {
      "epoch": 0.016484784262115762,
      "grad_norm": 3.972520351409912,
      "learning_rate": 0.000335199111263032,
      "loss": 1.2495,
      "step": 3858
    },
    {
      "epoch": 0.016489057145542954,
      "grad_norm": 1.8396345376968384,
      "learning_rate": 0.0003351563835241839,
      "loss": 0.6878,
      "step": 3859
    },
    {
      "epoch": 0.01649333002897015,
      "grad_norm": 3.1959731578826904,
      "learning_rate": 0.00033511365578533587,
      "loss": 1.1106,
      "step": 3860
    },
    {
      "epoch": 0.016497602912397345,
      "grad_norm": 2.266906976699829,
      "learning_rate": 0.00033507092804648783,
      "loss": 0.7296,
      "step": 3861
    },
    {
      "epoch": 0.016501875795824537,
      "grad_norm": 1.7862998247146606,
      "learning_rate": 0.00033502820030763974,
      "loss": 0.5943,
      "step": 3862
    },
    {
      "epoch": 0.016506148679251733,
      "grad_norm": 1.4000144004821777,
      "learning_rate": 0.0003349854725687917,
      "loss": 0.302,
      "step": 3863
    },
    {
      "epoch": 0.01651042156267893,
      "grad_norm": 2.094676971435547,
      "learning_rate": 0.00033494274482994356,
      "loss": 0.8876,
      "step": 3864
    },
    {
      "epoch": 0.01651469444610612,
      "grad_norm": 4.0472869873046875,
      "learning_rate": 0.00033490001709109553,
      "loss": 1.1357,
      "step": 3865
    },
    {
      "epoch": 0.016518967329533316,
      "grad_norm": 0.9323806762695312,
      "learning_rate": 0.00033485728935224744,
      "loss": 0.2437,
      "step": 3866
    },
    {
      "epoch": 0.01652324021296051,
      "grad_norm": 1.082578420639038,
      "learning_rate": 0.0003348145616133994,
      "loss": 0.3288,
      "step": 3867
    },
    {
      "epoch": 0.016527513096387704,
      "grad_norm": 2.985840082168579,
      "learning_rate": 0.00033477183387455137,
      "loss": 1.252,
      "step": 3868
    },
    {
      "epoch": 0.0165317859798149,
      "grad_norm": 2.4923572540283203,
      "learning_rate": 0.0003347291061357033,
      "loss": 0.7389,
      "step": 3869
    },
    {
      "epoch": 0.01653605886324209,
      "grad_norm": 2.0594916343688965,
      "learning_rate": 0.00033468637839685525,
      "loss": 0.7337,
      "step": 3870
    },
    {
      "epoch": 0.016540331746669287,
      "grad_norm": 3.3942394256591797,
      "learning_rate": 0.00033464365065800716,
      "loss": 0.907,
      "step": 3871
    },
    {
      "epoch": 0.016544604630096483,
      "grad_norm": 1.996996283531189,
      "learning_rate": 0.0003346009229191591,
      "loss": 0.5187,
      "step": 3872
    },
    {
      "epoch": 0.016548877513523675,
      "grad_norm": 3.3524465560913086,
      "learning_rate": 0.00033455819518031103,
      "loss": 1.0902,
      "step": 3873
    },
    {
      "epoch": 0.01655315039695087,
      "grad_norm": 1.337043046951294,
      "learning_rate": 0.000334515467441463,
      "loss": 0.2641,
      "step": 3874
    },
    {
      "epoch": 0.016557423280378066,
      "grad_norm": 4.42447566986084,
      "learning_rate": 0.00033447273970261496,
      "loss": 1.0734,
      "step": 3875
    },
    {
      "epoch": 0.01656169616380526,
      "grad_norm": 2.7233355045318604,
      "learning_rate": 0.00033443001196376687,
      "loss": 0.4963,
      "step": 3876
    },
    {
      "epoch": 0.016565969047232454,
      "grad_norm": 2.9787144660949707,
      "learning_rate": 0.00033438728422491884,
      "loss": 1.2437,
      "step": 3877
    },
    {
      "epoch": 0.016570241930659646,
      "grad_norm": 3.1927988529205322,
      "learning_rate": 0.00033434455648607075,
      "loss": 0.8017,
      "step": 3878
    },
    {
      "epoch": 0.01657451481408684,
      "grad_norm": 1.4548170566558838,
      "learning_rate": 0.0003343018287472227,
      "loss": 0.4723,
      "step": 3879
    },
    {
      "epoch": 0.016578787697514037,
      "grad_norm": 1.8936909437179565,
      "learning_rate": 0.0003342591010083746,
      "loss": 0.3881,
      "step": 3880
    },
    {
      "epoch": 0.01658306058094123,
      "grad_norm": 1.2628793716430664,
      "learning_rate": 0.0003342163732695266,
      "loss": 0.3167,
      "step": 3881
    },
    {
      "epoch": 0.016587333464368425,
      "grad_norm": 1.2399535179138184,
      "learning_rate": 0.00033417364553067855,
      "loss": 0.6006,
      "step": 3882
    },
    {
      "epoch": 0.01659160634779562,
      "grad_norm": 1.9070796966552734,
      "learning_rate": 0.00033413091779183046,
      "loss": 0.8564,
      "step": 3883
    },
    {
      "epoch": 0.016595879231222813,
      "grad_norm": 2.195533514022827,
      "learning_rate": 0.00033408819005298243,
      "loss": 0.7016,
      "step": 3884
    },
    {
      "epoch": 0.016600152114650008,
      "grad_norm": 3.55206036567688,
      "learning_rate": 0.00033404546231413434,
      "loss": 0.8749,
      "step": 3885
    },
    {
      "epoch": 0.016604424998077204,
      "grad_norm": 1.1346908807754517,
      "learning_rate": 0.0003340027345752863,
      "loss": 0.2945,
      "step": 3886
    },
    {
      "epoch": 0.016608697881504396,
      "grad_norm": 4.754803657531738,
      "learning_rate": 0.0003339600068364382,
      "loss": 1.5319,
      "step": 3887
    },
    {
      "epoch": 0.01661297076493159,
      "grad_norm": 1.0036481618881226,
      "learning_rate": 0.0003339172790975902,
      "loss": 0.2405,
      "step": 3888
    },
    {
      "epoch": 0.016617243648358787,
      "grad_norm": 17.35725212097168,
      "learning_rate": 0.0003338745513587421,
      "loss": 1.0402,
      "step": 3889
    },
    {
      "epoch": 0.01662151653178598,
      "grad_norm": 2.626650810241699,
      "learning_rate": 0.00033383182361989406,
      "loss": 0.5907,
      "step": 3890
    },
    {
      "epoch": 0.016625789415213175,
      "grad_norm": 1.5795097351074219,
      "learning_rate": 0.000333789095881046,
      "loss": 0.55,
      "step": 3891
    },
    {
      "epoch": 0.016630062298640367,
      "grad_norm": 1.5794411897659302,
      "learning_rate": 0.00033374636814219793,
      "loss": 0.4844,
      "step": 3892
    },
    {
      "epoch": 0.016634335182067563,
      "grad_norm": 1.5432182550430298,
      "learning_rate": 0.0003337036404033499,
      "loss": 0.4174,
      "step": 3893
    },
    {
      "epoch": 0.016638608065494758,
      "grad_norm": 1.0037285089492798,
      "learning_rate": 0.0003336609126645018,
      "loss": 0.2441,
      "step": 3894
    },
    {
      "epoch": 0.01664288094892195,
      "grad_norm": 3.1427292823791504,
      "learning_rate": 0.00033361818492565377,
      "loss": 0.6141,
      "step": 3895
    },
    {
      "epoch": 0.016647153832349146,
      "grad_norm": 5.459142208099365,
      "learning_rate": 0.0003335754571868057,
      "loss": 1.8145,
      "step": 3896
    },
    {
      "epoch": 0.01665142671577634,
      "grad_norm": 4.126166343688965,
      "learning_rate": 0.0003335327294479576,
      "loss": 1.6227,
      "step": 3897
    },
    {
      "epoch": 0.016655699599203534,
      "grad_norm": 3.5630853176116943,
      "learning_rate": 0.00033349000170910956,
      "loss": 1.1906,
      "step": 3898
    },
    {
      "epoch": 0.01665997248263073,
      "grad_norm": 2.937055826187134,
      "learning_rate": 0.00033344727397026147,
      "loss": 0.5552,
      "step": 3899
    },
    {
      "epoch": 0.016664245366057925,
      "grad_norm": 2.919893741607666,
      "learning_rate": 0.00033340454623141343,
      "loss": 0.5552,
      "step": 3900
    },
    {
      "epoch": 0.016668518249485117,
      "grad_norm": 3.6664111614227295,
      "learning_rate": 0.00033336181849256535,
      "loss": 1.1694,
      "step": 3901
    },
    {
      "epoch": 0.016672791132912312,
      "grad_norm": 2.021353006362915,
      "learning_rate": 0.0003333190907537173,
      "loss": 0.747,
      "step": 3902
    },
    {
      "epoch": 0.016677064016339505,
      "grad_norm": 3.167336940765381,
      "learning_rate": 0.0003332763630148692,
      "loss": 0.6063,
      "step": 3903
    },
    {
      "epoch": 0.0166813368997667,
      "grad_norm": 0.6106306910514832,
      "learning_rate": 0.0003332336352760212,
      "loss": 0.1274,
      "step": 3904
    },
    {
      "epoch": 0.016685609783193896,
      "grad_norm": 0.5315290093421936,
      "learning_rate": 0.00033319090753717315,
      "loss": 0.1081,
      "step": 3905
    },
    {
      "epoch": 0.016689882666621088,
      "grad_norm": 3.631209135055542,
      "learning_rate": 0.00033314817979832506,
      "loss": 0.9801,
      "step": 3906
    },
    {
      "epoch": 0.016694155550048283,
      "grad_norm": 1.4913768768310547,
      "learning_rate": 0.000333105452059477,
      "loss": 0.4517,
      "step": 3907
    },
    {
      "epoch": 0.01669842843347548,
      "grad_norm": 3.2912960052490234,
      "learning_rate": 0.00033306272432062894,
      "loss": 1.3065,
      "step": 3908
    },
    {
      "epoch": 0.01670270131690267,
      "grad_norm": 2.3512020111083984,
      "learning_rate": 0.0003330199965817809,
      "loss": 0.8285,
      "step": 3909
    },
    {
      "epoch": 0.016706974200329867,
      "grad_norm": 2.23283052444458,
      "learning_rate": 0.0003329772688429328,
      "loss": 0.6586,
      "step": 3910
    },
    {
      "epoch": 0.016711247083757062,
      "grad_norm": 4.523382186889648,
      "learning_rate": 0.0003329345411040848,
      "loss": 1.1679,
      "step": 3911
    },
    {
      "epoch": 0.016715519967184254,
      "grad_norm": 3.001204013824463,
      "learning_rate": 0.00033289181336523674,
      "loss": 1.4088,
      "step": 3912
    },
    {
      "epoch": 0.01671979285061145,
      "grad_norm": 2.1749589443206787,
      "learning_rate": 0.00033284908562638865,
      "loss": 0.3764,
      "step": 3913
    },
    {
      "epoch": 0.016724065734038646,
      "grad_norm": 0.4705268442630768,
      "learning_rate": 0.0003328063578875406,
      "loss": 0.0966,
      "step": 3914
    },
    {
      "epoch": 0.016728338617465838,
      "grad_norm": 1.3590874671936035,
      "learning_rate": 0.00033276363014869253,
      "loss": 0.6263,
      "step": 3915
    },
    {
      "epoch": 0.016732611500893033,
      "grad_norm": 3.851043701171875,
      "learning_rate": 0.0003327209024098445,
      "loss": 1.2971,
      "step": 3916
    },
    {
      "epoch": 0.016736884384320225,
      "grad_norm": 0.4624343812465668,
      "learning_rate": 0.0003326781746709964,
      "loss": 0.0967,
      "step": 3917
    },
    {
      "epoch": 0.01674115726774742,
      "grad_norm": 2.258695363998413,
      "learning_rate": 0.00033263544693214837,
      "loss": 0.8844,
      "step": 3918
    },
    {
      "epoch": 0.016745430151174617,
      "grad_norm": 0.39858677983283997,
      "learning_rate": 0.00033259271919330034,
      "loss": 0.0824,
      "step": 3919
    },
    {
      "epoch": 0.01674970303460181,
      "grad_norm": 3.0726735591888428,
      "learning_rate": 0.00033254999145445225,
      "loss": 0.9007,
      "step": 3920
    },
    {
      "epoch": 0.016753975918029004,
      "grad_norm": 1.701094388961792,
      "learning_rate": 0.0003325072637156042,
      "loss": 0.5296,
      "step": 3921
    },
    {
      "epoch": 0.0167582488014562,
      "grad_norm": 3.689842939376831,
      "learning_rate": 0.0003324645359767561,
      "loss": 1.0225,
      "step": 3922
    },
    {
      "epoch": 0.016762521684883392,
      "grad_norm": 0.5485665202140808,
      "learning_rate": 0.0003324218082379081,
      "loss": 0.1218,
      "step": 3923
    },
    {
      "epoch": 0.016766794568310588,
      "grad_norm": 1.3110445737838745,
      "learning_rate": 0.00033237908049906,
      "loss": 0.6237,
      "step": 3924
    },
    {
      "epoch": 0.016771067451737783,
      "grad_norm": 2.7652084827423096,
      "learning_rate": 0.00033233635276021196,
      "loss": 0.9637,
      "step": 3925
    },
    {
      "epoch": 0.016775340335164975,
      "grad_norm": 3.7125396728515625,
      "learning_rate": 0.0003322936250213639,
      "loss": 1.0822,
      "step": 3926
    },
    {
      "epoch": 0.01677961321859217,
      "grad_norm": 2.0199081897735596,
      "learning_rate": 0.00033225089728251584,
      "loss": 0.7629,
      "step": 3927
    },
    {
      "epoch": 0.016783886102019363,
      "grad_norm": 1.2993714809417725,
      "learning_rate": 0.0003322081695436678,
      "loss": 0.4517,
      "step": 3928
    },
    {
      "epoch": 0.01678815898544656,
      "grad_norm": 2.651376962661743,
      "learning_rate": 0.0003321654418048197,
      "loss": 0.8516,
      "step": 3929
    },
    {
      "epoch": 0.016792431868873754,
      "grad_norm": 0.4684368968009949,
      "learning_rate": 0.0003321227140659716,
      "loss": 0.111,
      "step": 3930
    },
    {
      "epoch": 0.016796704752300946,
      "grad_norm": 1.5882693529129028,
      "learning_rate": 0.00033207998632712354,
      "loss": 0.47,
      "step": 3931
    },
    {
      "epoch": 0.016800977635728142,
      "grad_norm": 2.574012041091919,
      "learning_rate": 0.0003320372585882755,
      "loss": 1.0771,
      "step": 3932
    },
    {
      "epoch": 0.016805250519155338,
      "grad_norm": 2.238969326019287,
      "learning_rate": 0.0003319945308494274,
      "loss": 0.669,
      "step": 3933
    },
    {
      "epoch": 0.01680952340258253,
      "grad_norm": 1.0719869136810303,
      "learning_rate": 0.0003319518031105794,
      "loss": 0.5184,
      "step": 3934
    },
    {
      "epoch": 0.016813796286009725,
      "grad_norm": 2.708735466003418,
      "learning_rate": 0.00033190907537173134,
      "loss": 1.1012,
      "step": 3935
    },
    {
      "epoch": 0.01681806916943692,
      "grad_norm": 1.0015023946762085,
      "learning_rate": 0.00033186634763288325,
      "loss": 0.4877,
      "step": 3936
    },
    {
      "epoch": 0.016822342052864113,
      "grad_norm": 1.7269103527069092,
      "learning_rate": 0.0003318236198940352,
      "loss": 0.5972,
      "step": 3937
    },
    {
      "epoch": 0.01682661493629131,
      "grad_norm": 2.636857748031616,
      "learning_rate": 0.0003317808921551871,
      "loss": 0.6387,
      "step": 3938
    },
    {
      "epoch": 0.016830887819718504,
      "grad_norm": 2.1533336639404297,
      "learning_rate": 0.0003317381644163391,
      "loss": 0.6032,
      "step": 3939
    },
    {
      "epoch": 0.016835160703145696,
      "grad_norm": 2.79250431060791,
      "learning_rate": 0.000331695436677491,
      "loss": 1.191,
      "step": 3940
    },
    {
      "epoch": 0.016839433586572892,
      "grad_norm": 2.1641316413879395,
      "learning_rate": 0.00033165270893864297,
      "loss": 0.6847,
      "step": 3941
    },
    {
      "epoch": 0.016843706470000084,
      "grad_norm": 3.9233641624450684,
      "learning_rate": 0.00033160998119979493,
      "loss": 0.8825,
      "step": 3942
    },
    {
      "epoch": 0.01684797935342728,
      "grad_norm": 1.8477116823196411,
      "learning_rate": 0.00033156725346094684,
      "loss": 0.4649,
      "step": 3943
    },
    {
      "epoch": 0.016852252236854475,
      "grad_norm": 2.019660234451294,
      "learning_rate": 0.0003315245257220988,
      "loss": 0.7587,
      "step": 3944
    },
    {
      "epoch": 0.016856525120281667,
      "grad_norm": 2.150629758834839,
      "learning_rate": 0.0003314817979832507,
      "loss": 0.9887,
      "step": 3945
    },
    {
      "epoch": 0.016860798003708863,
      "grad_norm": 1.0670346021652222,
      "learning_rate": 0.0003314390702444027,
      "loss": 0.409,
      "step": 3946
    },
    {
      "epoch": 0.01686507088713606,
      "grad_norm": 1.6747572422027588,
      "learning_rate": 0.0003313963425055546,
      "loss": 0.7256,
      "step": 3947
    },
    {
      "epoch": 0.01686934377056325,
      "grad_norm": 0.9833766222000122,
      "learning_rate": 0.00033135361476670656,
      "loss": 0.3835,
      "step": 3948
    },
    {
      "epoch": 0.016873616653990446,
      "grad_norm": 3.1798477172851562,
      "learning_rate": 0.0003313108870278585,
      "loss": 0.9899,
      "step": 3949
    },
    {
      "epoch": 0.01687788953741764,
      "grad_norm": 0.8975479602813721,
      "learning_rate": 0.00033126815928901044,
      "loss": 0.3454,
      "step": 3950
    },
    {
      "epoch": 0.016882162420844834,
      "grad_norm": 1.6595878601074219,
      "learning_rate": 0.0003312254315501624,
      "loss": 0.6402,
      "step": 3951
    },
    {
      "epoch": 0.01688643530427203,
      "grad_norm": 1.309258222579956,
      "learning_rate": 0.0003311827038113143,
      "loss": 0.3828,
      "step": 3952
    },
    {
      "epoch": 0.01689070818769922,
      "grad_norm": 0.6533646583557129,
      "learning_rate": 0.0003311399760724663,
      "loss": 0.1958,
      "step": 3953
    },
    {
      "epoch": 0.016894981071126417,
      "grad_norm": 3.2418839931488037,
      "learning_rate": 0.0003310972483336182,
      "loss": 1.1219,
      "step": 3954
    },
    {
      "epoch": 0.016899253954553613,
      "grad_norm": 1.8605060577392578,
      "learning_rate": 0.00033105452059477015,
      "loss": 0.6047,
      "step": 3955
    },
    {
      "epoch": 0.016903526837980805,
      "grad_norm": 2.2538952827453613,
      "learning_rate": 0.00033101179285592206,
      "loss": 0.7612,
      "step": 3956
    },
    {
      "epoch": 0.016907799721408,
      "grad_norm": 2.0980730056762695,
      "learning_rate": 0.00033096906511707403,
      "loss": 0.6505,
      "step": 3957
    },
    {
      "epoch": 0.016912072604835196,
      "grad_norm": 1.9053500890731812,
      "learning_rate": 0.000330926337378226,
      "loss": 0.7402,
      "step": 3958
    },
    {
      "epoch": 0.016916345488262388,
      "grad_norm": 1.8459137678146362,
      "learning_rate": 0.0003308836096393779,
      "loss": 0.5622,
      "step": 3959
    },
    {
      "epoch": 0.016920618371689584,
      "grad_norm": 0.7637109756469727,
      "learning_rate": 0.00033084088190052987,
      "loss": 0.2399,
      "step": 3960
    },
    {
      "epoch": 0.01692489125511678,
      "grad_norm": 1.7992119789123535,
      "learning_rate": 0.0003307981541616818,
      "loss": 0.4898,
      "step": 3961
    },
    {
      "epoch": 0.01692916413854397,
      "grad_norm": 0.6273766756057739,
      "learning_rate": 0.0003307554264228337,
      "loss": 0.2078,
      "step": 3962
    },
    {
      "epoch": 0.016933437021971167,
      "grad_norm": 1.7567343711853027,
      "learning_rate": 0.0003307126986839856,
      "loss": 0.7962,
      "step": 3963
    },
    {
      "epoch": 0.016937709905398363,
      "grad_norm": 5.134040832519531,
      "learning_rate": 0.00033066997094513757,
      "loss": 1.9992,
      "step": 3964
    },
    {
      "epoch": 0.016941982788825555,
      "grad_norm": 4.577861309051514,
      "learning_rate": 0.00033062724320628953,
      "loss": 1.2019,
      "step": 3965
    },
    {
      "epoch": 0.01694625567225275,
      "grad_norm": 0.9445934295654297,
      "learning_rate": 0.00033058451546744144,
      "loss": 0.2437,
      "step": 3966
    },
    {
      "epoch": 0.016950528555679942,
      "grad_norm": 2.4413862228393555,
      "learning_rate": 0.0003305417877285934,
      "loss": 0.9191,
      "step": 3967
    },
    {
      "epoch": 0.016954801439107138,
      "grad_norm": 3.9613447189331055,
      "learning_rate": 0.0003304990599897453,
      "loss": 1.3199,
      "step": 3968
    },
    {
      "epoch": 0.016959074322534334,
      "grad_norm": 3.283590316772461,
      "learning_rate": 0.0003304563322508973,
      "loss": 1.2681,
      "step": 3969
    },
    {
      "epoch": 0.016963347205961526,
      "grad_norm": 4.334395885467529,
      "learning_rate": 0.0003304136045120492,
      "loss": 1.301,
      "step": 3970
    },
    {
      "epoch": 0.01696762008938872,
      "grad_norm": 3.0706441402435303,
      "learning_rate": 0.00033037087677320116,
      "loss": 0.8422,
      "step": 3971
    },
    {
      "epoch": 0.016971892972815917,
      "grad_norm": 0.7539912462234497,
      "learning_rate": 0.0003303281490343531,
      "loss": 0.2707,
      "step": 3972
    },
    {
      "epoch": 0.01697616585624311,
      "grad_norm": 0.8020856976509094,
      "learning_rate": 0.00033028542129550503,
      "loss": 0.2905,
      "step": 3973
    },
    {
      "epoch": 0.016980438739670305,
      "grad_norm": 2.137120246887207,
      "learning_rate": 0.000330242693556657,
      "loss": 0.6176,
      "step": 3974
    },
    {
      "epoch": 0.0169847116230975,
      "grad_norm": 0.7858238816261292,
      "learning_rate": 0.0003301999658178089,
      "loss": 0.2099,
      "step": 3975
    },
    {
      "epoch": 0.016988984506524692,
      "grad_norm": 1.6448702812194824,
      "learning_rate": 0.0003301572380789609,
      "loss": 0.6201,
      "step": 3976
    },
    {
      "epoch": 0.016993257389951888,
      "grad_norm": 2.688152313232422,
      "learning_rate": 0.0003301145103401128,
      "loss": 1.1692,
      "step": 3977
    },
    {
      "epoch": 0.01699753027337908,
      "grad_norm": 2.4187262058258057,
      "learning_rate": 0.00033007178260126475,
      "loss": 0.8649,
      "step": 3978
    },
    {
      "epoch": 0.017001803156806276,
      "grad_norm": 2.9651215076446533,
      "learning_rate": 0.0003300290548624167,
      "loss": 0.6534,
      "step": 3979
    },
    {
      "epoch": 0.01700607604023347,
      "grad_norm": 3.5510683059692383,
      "learning_rate": 0.0003299863271235686,
      "loss": 0.997,
      "step": 3980
    },
    {
      "epoch": 0.017010348923660663,
      "grad_norm": 0.5216924548149109,
      "learning_rate": 0.0003299435993847206,
      "loss": 0.1712,
      "step": 3981
    },
    {
      "epoch": 0.01701462180708786,
      "grad_norm": 0.5056418180465698,
      "learning_rate": 0.0003299008716458725,
      "loss": 0.1685,
      "step": 3982
    },
    {
      "epoch": 0.017018894690515055,
      "grad_norm": 1.9982267618179321,
      "learning_rate": 0.00032985814390702447,
      "loss": 0.7576,
      "step": 3983
    },
    {
      "epoch": 0.017023167573942247,
      "grad_norm": 2.5234954357147217,
      "learning_rate": 0.0003298154161681764,
      "loss": 0.9451,
      "step": 3984
    },
    {
      "epoch": 0.017027440457369442,
      "grad_norm": 0.4442833960056305,
      "learning_rate": 0.00032977268842932834,
      "loss": 0.1354,
      "step": 3985
    },
    {
      "epoch": 0.017031713340796638,
      "grad_norm": 3.5249204635620117,
      "learning_rate": 0.0003297299606904803,
      "loss": 0.8284,
      "step": 3986
    },
    {
      "epoch": 0.01703598622422383,
      "grad_norm": 2.858689308166504,
      "learning_rate": 0.0003296872329516322,
      "loss": 0.494,
      "step": 3987
    },
    {
      "epoch": 0.017040259107651026,
      "grad_norm": 0.6548135280609131,
      "learning_rate": 0.0003296445052127842,
      "loss": 0.1733,
      "step": 3988
    },
    {
      "epoch": 0.017044531991078218,
      "grad_norm": 2.2925217151641846,
      "learning_rate": 0.0003296017774739361,
      "loss": 0.8675,
      "step": 3989
    },
    {
      "epoch": 0.017048804874505413,
      "grad_norm": 3.5094470977783203,
      "learning_rate": 0.00032955904973508806,
      "loss": 0.8669,
      "step": 3990
    },
    {
      "epoch": 0.01705307775793261,
      "grad_norm": 2.6951823234558105,
      "learning_rate": 0.00032951632199623997,
      "loss": 0.836,
      "step": 3991
    },
    {
      "epoch": 0.0170573506413598,
      "grad_norm": 3.813350200653076,
      "learning_rate": 0.00032947359425739193,
      "loss": 1.0085,
      "step": 3992
    },
    {
      "epoch": 0.017061623524786997,
      "grad_norm": 0.6297579407691956,
      "learning_rate": 0.00032943086651854384,
      "loss": 0.1411,
      "step": 3993
    },
    {
      "epoch": 0.017065896408214192,
      "grad_norm": 1.8540773391723633,
      "learning_rate": 0.0003293881387796958,
      "loss": 0.5963,
      "step": 3994
    },
    {
      "epoch": 0.017070169291641384,
      "grad_norm": 1.3577656745910645,
      "learning_rate": 0.0003293454110408477,
      "loss": 0.4961,
      "step": 3995
    },
    {
      "epoch": 0.01707444217506858,
      "grad_norm": 4.517150402069092,
      "learning_rate": 0.00032930268330199963,
      "loss": 1.3318,
      "step": 3996
    },
    {
      "epoch": 0.017078715058495775,
      "grad_norm": 3.627790689468384,
      "learning_rate": 0.0003292599555631516,
      "loss": 1.4037,
      "step": 3997
    },
    {
      "epoch": 0.017082987941922968,
      "grad_norm": 3.2290992736816406,
      "learning_rate": 0.0003292172278243035,
      "loss": 1.1615,
      "step": 3998
    },
    {
      "epoch": 0.017087260825350163,
      "grad_norm": 1.1366411447525024,
      "learning_rate": 0.00032917450008545547,
      "loss": 0.3611,
      "step": 3999
    },
    {
      "epoch": 0.01709153370877736,
      "grad_norm": 1.1233052015304565,
      "learning_rate": 0.0003291317723466074,
      "loss": 0.3601,
      "step": 4000
    },
    {
      "epoch": 0.01709580659220455,
      "grad_norm": 5.198330879211426,
      "learning_rate": 0.00032908904460775935,
      "loss": 1.566,
      "step": 4001
    },
    {
      "epoch": 0.017100079475631746,
      "grad_norm": 2.544344663619995,
      "learning_rate": 0.0003290463168689113,
      "loss": 0.3382,
      "step": 4002
    },
    {
      "epoch": 0.01710435235905894,
      "grad_norm": 2.410994291305542,
      "learning_rate": 0.0003290035891300632,
      "loss": 0.3101,
      "step": 4003
    },
    {
      "epoch": 0.017108625242486134,
      "grad_norm": 2.1782314777374268,
      "learning_rate": 0.0003289608613912152,
      "loss": 0.5831,
      "step": 4004
    },
    {
      "epoch": 0.01711289812591333,
      "grad_norm": 3.966728687286377,
      "learning_rate": 0.0003289181336523671,
      "loss": 1.1396,
      "step": 4005
    },
    {
      "epoch": 0.017117171009340522,
      "grad_norm": 2.9982750415802,
      "learning_rate": 0.00032887540591351906,
      "loss": 1.0629,
      "step": 4006
    },
    {
      "epoch": 0.017121443892767717,
      "grad_norm": 3.2309229373931885,
      "learning_rate": 0.000328832678174671,
      "loss": 1.208,
      "step": 4007
    },
    {
      "epoch": 0.017125716776194913,
      "grad_norm": 2.639772891998291,
      "learning_rate": 0.00032878995043582294,
      "loss": 0.7202,
      "step": 4008
    },
    {
      "epoch": 0.017129989659622105,
      "grad_norm": 1.6629741191864014,
      "learning_rate": 0.0003287472226969749,
      "loss": 0.5182,
      "step": 4009
    },
    {
      "epoch": 0.0171342625430493,
      "grad_norm": 0.37705889344215393,
      "learning_rate": 0.0003287044949581268,
      "loss": 0.1197,
      "step": 4010
    },
    {
      "epoch": 0.017138535426476496,
      "grad_norm": 163.32212829589844,
      "learning_rate": 0.0003286617672192788,
      "loss": 3.4171,
      "step": 4011
    },
    {
      "epoch": 0.01714280830990369,
      "grad_norm": 0.606464684009552,
      "learning_rate": 0.0003286190394804307,
      "loss": 0.2779,
      "step": 4012
    },
    {
      "epoch": 0.017147081193330884,
      "grad_norm": 0.5441408753395081,
      "learning_rate": 0.00032857631174158265,
      "loss": 0.2612,
      "step": 4013
    },
    {
      "epoch": 0.017151354076758076,
      "grad_norm": 0.5487651228904724,
      "learning_rate": 0.00032853358400273457,
      "loss": 0.1737,
      "step": 4014
    },
    {
      "epoch": 0.017155626960185272,
      "grad_norm": 2.777223587036133,
      "learning_rate": 0.00032849085626388653,
      "loss": 1.0064,
      "step": 4015
    },
    {
      "epoch": 0.017159899843612467,
      "grad_norm": 2.18049955368042,
      "learning_rate": 0.0003284481285250385,
      "loss": 0.7911,
      "step": 4016
    },
    {
      "epoch": 0.01716417272703966,
      "grad_norm": 14.883853912353516,
      "learning_rate": 0.0003284054007861904,
      "loss": 1.2817,
      "step": 4017
    },
    {
      "epoch": 0.017168445610466855,
      "grad_norm": 1.9605528116226196,
      "learning_rate": 0.00032836267304734237,
      "loss": 0.76,
      "step": 4018
    },
    {
      "epoch": 0.01717271849389405,
      "grad_norm": 0.47706741094589233,
      "learning_rate": 0.0003283199453084943,
      "loss": 0.2552,
      "step": 4019
    },
    {
      "epoch": 0.017176991377321243,
      "grad_norm": 1.366660714149475,
      "learning_rate": 0.00032827721756964625,
      "loss": 0.3319,
      "step": 4020
    },
    {
      "epoch": 0.01718126426074844,
      "grad_norm": 4.158909797668457,
      "learning_rate": 0.00032823448983079816,
      "loss": 1.4372,
      "step": 4021
    },
    {
      "epoch": 0.017185537144175634,
      "grad_norm": 1.6728776693344116,
      "learning_rate": 0.0003281917620919501,
      "loss": 0.512,
      "step": 4022
    },
    {
      "epoch": 0.017189810027602826,
      "grad_norm": 3.1474130153656006,
      "learning_rate": 0.0003281490343531021,
      "loss": 0.8396,
      "step": 4023
    },
    {
      "epoch": 0.01719408291103002,
      "grad_norm": 3.9452950954437256,
      "learning_rate": 0.000328106306614254,
      "loss": 1.3721,
      "step": 4024
    },
    {
      "epoch": 0.017198355794457217,
      "grad_norm": 1.587702751159668,
      "learning_rate": 0.00032806357887540596,
      "loss": 0.505,
      "step": 4025
    },
    {
      "epoch": 0.01720262867788441,
      "grad_norm": 0.494115948677063,
      "learning_rate": 0.0003280208511365579,
      "loss": 0.2431,
      "step": 4026
    },
    {
      "epoch": 0.017206901561311605,
      "grad_norm": 2.799875020980835,
      "learning_rate": 0.00032797812339770984,
      "loss": 0.7811,
      "step": 4027
    },
    {
      "epoch": 0.017211174444738797,
      "grad_norm": 0.758867621421814,
      "learning_rate": 0.0003279353956588617,
      "loss": 0.2733,
      "step": 4028
    },
    {
      "epoch": 0.017215447328165993,
      "grad_norm": 1.352731704711914,
      "learning_rate": 0.00032789266792001366,
      "loss": 0.2418,
      "step": 4029
    },
    {
      "epoch": 0.01721972021159319,
      "grad_norm": 32.24615478515625,
      "learning_rate": 0.00032784994018116557,
      "loss": 0.9601,
      "step": 4030
    },
    {
      "epoch": 0.01722399309502038,
      "grad_norm": 0.7654522657394409,
      "learning_rate": 0.00032780721244231754,
      "loss": 0.274,
      "step": 4031
    },
    {
      "epoch": 0.017228265978447576,
      "grad_norm": 1.5564972162246704,
      "learning_rate": 0.0003277644847034695,
      "loss": 0.5376,
      "step": 4032
    },
    {
      "epoch": 0.01723253886187477,
      "grad_norm": 2.339900493621826,
      "learning_rate": 0.0003277217569646214,
      "loss": 0.7936,
      "step": 4033
    },
    {
      "epoch": 0.017236811745301964,
      "grad_norm": 2.152618885040283,
      "learning_rate": 0.0003276790292257734,
      "loss": 0.7453,
      "step": 4034
    },
    {
      "epoch": 0.01724108462872916,
      "grad_norm": 0.5903803706169128,
      "learning_rate": 0.0003276363014869253,
      "loss": 0.2239,
      "step": 4035
    },
    {
      "epoch": 0.017245357512156355,
      "grad_norm": 1.2129452228546143,
      "learning_rate": 0.00032759357374807725,
      "loss": 0.1464,
      "step": 4036
    },
    {
      "epoch": 0.017249630395583547,
      "grad_norm": 2.744103193283081,
      "learning_rate": 0.00032755084600922916,
      "loss": 0.5492,
      "step": 4037
    },
    {
      "epoch": 0.017253903279010743,
      "grad_norm": 0.879085898399353,
      "learning_rate": 0.00032750811827038113,
      "loss": 0.243,
      "step": 4038
    },
    {
      "epoch": 0.017258176162437935,
      "grad_norm": 121.51599884033203,
      "learning_rate": 0.0003274653905315331,
      "loss": 0.538,
      "step": 4039
    },
    {
      "epoch": 0.01726244904586513,
      "grad_norm": 2.453213930130005,
      "learning_rate": 0.000327422662792685,
      "loss": 0.6452,
      "step": 4040
    },
    {
      "epoch": 0.017266721929292326,
      "grad_norm": 28.251991271972656,
      "learning_rate": 0.00032737993505383697,
      "loss": 0.4806,
      "step": 4041
    },
    {
      "epoch": 0.017270994812719518,
      "grad_norm": 5.294975280761719,
      "learning_rate": 0.0003273372073149889,
      "loss": 1.8989,
      "step": 4042
    },
    {
      "epoch": 0.017275267696146714,
      "grad_norm": 2.609076738357544,
      "learning_rate": 0.00032729447957614084,
      "loss": 0.8189,
      "step": 4043
    },
    {
      "epoch": 0.01727954057957391,
      "grad_norm": 4.346467971801758,
      "learning_rate": 0.00032725175183729276,
      "loss": 1.3782,
      "step": 4044
    },
    {
      "epoch": 0.0172838134630011,
      "grad_norm": 3.174255132675171,
      "learning_rate": 0.0003272090240984447,
      "loss": 0.8867,
      "step": 4045
    },
    {
      "epoch": 0.017288086346428297,
      "grad_norm": 2.0009946823120117,
      "learning_rate": 0.0003271662963595967,
      "loss": 0.5954,
      "step": 4046
    },
    {
      "epoch": 0.017292359229855492,
      "grad_norm": 2.957169532775879,
      "learning_rate": 0.0003271235686207486,
      "loss": 0.5303,
      "step": 4047
    },
    {
      "epoch": 0.017296632113282685,
      "grad_norm": 5.18798828125,
      "learning_rate": 0.00032708084088190056,
      "loss": 1.8364,
      "step": 4048
    },
    {
      "epoch": 0.01730090499670988,
      "grad_norm": 4.133179664611816,
      "learning_rate": 0.00032703811314305247,
      "loss": 1.1864,
      "step": 4049
    },
    {
      "epoch": 0.017305177880137076,
      "grad_norm": 0.8866106867790222,
      "learning_rate": 0.00032699538540420444,
      "loss": 0.3423,
      "step": 4050
    },
    {
      "epoch": 0.017309450763564268,
      "grad_norm": 0.5988153219223022,
      "learning_rate": 0.00032695265766535635,
      "loss": 0.171,
      "step": 4051
    },
    {
      "epoch": 0.017313723646991463,
      "grad_norm": 1.0475921630859375,
      "learning_rate": 0.0003269099299265083,
      "loss": 0.5273,
      "step": 4052
    },
    {
      "epoch": 0.017317996530418656,
      "grad_norm": 3.5753471851348877,
      "learning_rate": 0.0003268672021876603,
      "loss": 0.73,
      "step": 4053
    },
    {
      "epoch": 0.01732226941384585,
      "grad_norm": 3.6125247478485107,
      "learning_rate": 0.0003268244744488122,
      "loss": 0.8296,
      "step": 4054
    },
    {
      "epoch": 0.017326542297273047,
      "grad_norm": 1.9337865114212036,
      "learning_rate": 0.00032678174670996415,
      "loss": 0.7545,
      "step": 4055
    },
    {
      "epoch": 0.01733081518070024,
      "grad_norm": 2.1350157260894775,
      "learning_rate": 0.00032673901897111606,
      "loss": 0.5903,
      "step": 4056
    },
    {
      "epoch": 0.017335088064127434,
      "grad_norm": 2.2428743839263916,
      "learning_rate": 0.00032669629123226803,
      "loss": 0.6234,
      "step": 4057
    },
    {
      "epoch": 0.01733936094755463,
      "grad_norm": 1.833251714706421,
      "learning_rate": 0.00032665356349341994,
      "loss": 0.5151,
      "step": 4058
    },
    {
      "epoch": 0.017343633830981822,
      "grad_norm": 0.35394904017448425,
      "learning_rate": 0.0003266108357545719,
      "loss": 0.1396,
      "step": 4059
    },
    {
      "epoch": 0.017347906714409018,
      "grad_norm": 0.3546070456504822,
      "learning_rate": 0.0003265681080157238,
      "loss": 0.1399,
      "step": 4060
    },
    {
      "epoch": 0.017352179597836213,
      "grad_norm": 0.5867871642112732,
      "learning_rate": 0.0003265253802768757,
      "loss": 0.3052,
      "step": 4061
    },
    {
      "epoch": 0.017356452481263406,
      "grad_norm": 3.04728364944458,
      "learning_rate": 0.0003264826525380277,
      "loss": 0.6074,
      "step": 4062
    },
    {
      "epoch": 0.0173607253646906,
      "grad_norm": 0.32872259616851807,
      "learning_rate": 0.0003264399247991796,
      "loss": 0.1236,
      "step": 4063
    },
    {
      "epoch": 0.017364998248117793,
      "grad_norm": 2.3021228313446045,
      "learning_rate": 0.00032639719706033157,
      "loss": 0.7389,
      "step": 4064
    },
    {
      "epoch": 0.01736927113154499,
      "grad_norm": 0.381626695394516,
      "learning_rate": 0.0003263544693214835,
      "loss": 0.1336,
      "step": 4065
    },
    {
      "epoch": 0.017373544014972184,
      "grad_norm": 1.0725374221801758,
      "learning_rate": 0.00032631174158263544,
      "loss": 0.5223,
      "step": 4066
    },
    {
      "epoch": 0.017377816898399377,
      "grad_norm": 2.113417387008667,
      "learning_rate": 0.00032626901384378735,
      "loss": 0.5141,
      "step": 4067
    },
    {
      "epoch": 0.017382089781826572,
      "grad_norm": 0.2938401699066162,
      "learning_rate": 0.0003262262861049393,
      "loss": 0.1001,
      "step": 4068
    },
    {
      "epoch": 0.017386362665253768,
      "grad_norm": 3.2241761684417725,
      "learning_rate": 0.0003261835583660913,
      "loss": 0.5781,
      "step": 4069
    },
    {
      "epoch": 0.01739063554868096,
      "grad_norm": 2.0807442665100098,
      "learning_rate": 0.0003261408306272432,
      "loss": 0.4595,
      "step": 4070
    },
    {
      "epoch": 0.017394908432108155,
      "grad_norm": 2.285541534423828,
      "learning_rate": 0.00032609810288839516,
      "loss": 0.6235,
      "step": 4071
    },
    {
      "epoch": 0.01739918131553535,
      "grad_norm": 0.3301706910133362,
      "learning_rate": 0.00032605537514954707,
      "loss": 0.1036,
      "step": 4072
    },
    {
      "epoch": 0.017403454198962543,
      "grad_norm": 2.3777010440826416,
      "learning_rate": 0.00032601264741069903,
      "loss": 0.7963,
      "step": 4073
    },
    {
      "epoch": 0.01740772708238974,
      "grad_norm": 0.8214653730392456,
      "learning_rate": 0.00032596991967185094,
      "loss": 0.347,
      "step": 4074
    },
    {
      "epoch": 0.017411999965816934,
      "grad_norm": 2.2000677585601807,
      "learning_rate": 0.0003259271919330029,
      "loss": 0.5752,
      "step": 4075
    },
    {
      "epoch": 0.017416272849244126,
      "grad_norm": 2.259410858154297,
      "learning_rate": 0.0003258844641941549,
      "loss": 0.5286,
      "step": 4076
    },
    {
      "epoch": 0.017420545732671322,
      "grad_norm": 3.1708827018737793,
      "learning_rate": 0.0003258417364553068,
      "loss": 1.4007,
      "step": 4077
    },
    {
      "epoch": 0.017424818616098514,
      "grad_norm": 4.327641010284424,
      "learning_rate": 0.00032579900871645875,
      "loss": 1.203,
      "step": 4078
    },
    {
      "epoch": 0.01742909149952571,
      "grad_norm": 2.494609832763672,
      "learning_rate": 0.00032575628097761066,
      "loss": 0.7425,
      "step": 4079
    },
    {
      "epoch": 0.017433364382952905,
      "grad_norm": 0.7709693908691406,
      "learning_rate": 0.0003257135532387626,
      "loss": 0.3388,
      "step": 4080
    },
    {
      "epoch": 0.017437637266380097,
      "grad_norm": 2.1654982566833496,
      "learning_rate": 0.00032567082549991454,
      "loss": 0.621,
      "step": 4081
    },
    {
      "epoch": 0.017441910149807293,
      "grad_norm": 3.71970796585083,
      "learning_rate": 0.0003256280977610665,
      "loss": 1.0483,
      "step": 4082
    },
    {
      "epoch": 0.01744618303323449,
      "grad_norm": 0.6794790029525757,
      "learning_rate": 0.00032558537002221847,
      "loss": 0.3321,
      "step": 4083
    },
    {
      "epoch": 0.01745045591666168,
      "grad_norm": 2.6628527641296387,
      "learning_rate": 0.0003255426422833704,
      "loss": 1.042,
      "step": 4084
    },
    {
      "epoch": 0.017454728800088876,
      "grad_norm": 2.0750317573547363,
      "learning_rate": 0.00032549991454452234,
      "loss": 0.6402,
      "step": 4085
    },
    {
      "epoch": 0.017459001683516072,
      "grad_norm": 3.9379496574401855,
      "learning_rate": 0.00032545718680567425,
      "loss": 1.4194,
      "step": 4086
    },
    {
      "epoch": 0.017463274566943264,
      "grad_norm": 2.212829351425171,
      "learning_rate": 0.0003254144590668262,
      "loss": 0.8136,
      "step": 4087
    },
    {
      "epoch": 0.01746754745037046,
      "grad_norm": 0.7724522948265076,
      "learning_rate": 0.00032537173132797813,
      "loss": 0.2172,
      "step": 4088
    },
    {
      "epoch": 0.01747182033379765,
      "grad_norm": 2.4788312911987305,
      "learning_rate": 0.0003253290035891301,
      "loss": 0.9969,
      "step": 4089
    },
    {
      "epoch": 0.017476093217224847,
      "grad_norm": 3.294739246368408,
      "learning_rate": 0.00032528627585028206,
      "loss": 0.9296,
      "step": 4090
    },
    {
      "epoch": 0.017480366100652043,
      "grad_norm": 1.0830422639846802,
      "learning_rate": 0.00032524354811143397,
      "loss": 0.5137,
      "step": 4091
    },
    {
      "epoch": 0.017484638984079235,
      "grad_norm": 1.7887064218521118,
      "learning_rate": 0.00032520082037258593,
      "loss": 0.5717,
      "step": 4092
    },
    {
      "epoch": 0.01748891186750643,
      "grad_norm": 1.2254903316497803,
      "learning_rate": 0.00032515809263373784,
      "loss": 0.3395,
      "step": 4093
    },
    {
      "epoch": 0.017493184750933626,
      "grad_norm": 2.009807586669922,
      "learning_rate": 0.00032511536489488976,
      "loss": 0.4982,
      "step": 4094
    },
    {
      "epoch": 0.01749745763436082,
      "grad_norm": 0.42430636286735535,
      "learning_rate": 0.00032507263715604167,
      "loss": 0.1486,
      "step": 4095
    },
    {
      "epoch": 0.017501730517788014,
      "grad_norm": 2.3414576053619385,
      "learning_rate": 0.00032502990941719363,
      "loss": 0.9992,
      "step": 4096
    },
    {
      "epoch": 0.01750600340121521,
      "grad_norm": 1.8550317287445068,
      "learning_rate": 0.00032498718167834554,
      "loss": 0.6493,
      "step": 4097
    },
    {
      "epoch": 0.0175102762846424,
      "grad_norm": 4.9324564933776855,
      "learning_rate": 0.0003249444539394975,
      "loss": 1.9092,
      "step": 4098
    },
    {
      "epoch": 0.017514549168069597,
      "grad_norm": 1.0761244297027588,
      "learning_rate": 0.00032490172620064947,
      "loss": 0.4635,
      "step": 4099
    },
    {
      "epoch": 0.01751882205149679,
      "grad_norm": 4.029839992523193,
      "learning_rate": 0.0003248589984618014,
      "loss": 1.427,
      "step": 4100
    },
    {
      "epoch": 0.017523094934923985,
      "grad_norm": 1.2044196128845215,
      "learning_rate": 0.00032481627072295335,
      "loss": 0.4002,
      "step": 4101
    },
    {
      "epoch": 0.01752736781835118,
      "grad_norm": 3.4621498584747314,
      "learning_rate": 0.00032477354298410526,
      "loss": 0.7383,
      "step": 4102
    },
    {
      "epoch": 0.017531640701778373,
      "grad_norm": 2.4818363189697266,
      "learning_rate": 0.0003247308152452572,
      "loss": 0.5414,
      "step": 4103
    },
    {
      "epoch": 0.017535913585205568,
      "grad_norm": 4.999886989593506,
      "learning_rate": 0.00032468808750640913,
      "loss": 1.328,
      "step": 4104
    },
    {
      "epoch": 0.017540186468632764,
      "grad_norm": 2.042163372039795,
      "learning_rate": 0.0003246453597675611,
      "loss": 0.7184,
      "step": 4105
    },
    {
      "epoch": 0.017544459352059956,
      "grad_norm": 1.8816312551498413,
      "learning_rate": 0.00032460263202871306,
      "loss": 0.6676,
      "step": 4106
    },
    {
      "epoch": 0.01754873223548715,
      "grad_norm": 23.9405460357666,
      "learning_rate": 0.000324559904289865,
      "loss": 1.3042,
      "step": 4107
    },
    {
      "epoch": 0.017553005118914347,
      "grad_norm": 0.9910088181495667,
      "learning_rate": 0.00032451717655101694,
      "loss": 0.279,
      "step": 4108
    },
    {
      "epoch": 0.01755727800234154,
      "grad_norm": 1.0304843187332153,
      "learning_rate": 0.00032447444881216885,
      "loss": 0.4361,
      "step": 4109
    },
    {
      "epoch": 0.017561550885768735,
      "grad_norm": 2.8694660663604736,
      "learning_rate": 0.0003244317210733208,
      "loss": 0.6042,
      "step": 4110
    },
    {
      "epoch": 0.01756582376919593,
      "grad_norm": 2.4012651443481445,
      "learning_rate": 0.0003243889933344727,
      "loss": 0.9964,
      "step": 4111
    },
    {
      "epoch": 0.017570096652623123,
      "grad_norm": 2.842822790145874,
      "learning_rate": 0.0003243462655956247,
      "loss": 0.5598,
      "step": 4112
    },
    {
      "epoch": 0.017574369536050318,
      "grad_norm": 1.6984084844589233,
      "learning_rate": 0.00032430353785677666,
      "loss": 0.6165,
      "step": 4113
    },
    {
      "epoch": 0.01757864241947751,
      "grad_norm": 0.7462554574012756,
      "learning_rate": 0.00032426081011792857,
      "loss": 0.2713,
      "step": 4114
    },
    {
      "epoch": 0.017582915302904706,
      "grad_norm": 5.782713890075684,
      "learning_rate": 0.00032421808237908053,
      "loss": 2.0488,
      "step": 4115
    },
    {
      "epoch": 0.0175871881863319,
      "grad_norm": 1.0466493368148804,
      "learning_rate": 0.00032417535464023244,
      "loss": 0.3916,
      "step": 4116
    },
    {
      "epoch": 0.017591461069759094,
      "grad_norm": 1.8989648818969727,
      "learning_rate": 0.0003241326269013844,
      "loss": 0.3827,
      "step": 4117
    },
    {
      "epoch": 0.01759573395318629,
      "grad_norm": 0.9869747161865234,
      "learning_rate": 0.0003240898991625363,
      "loss": 0.3653,
      "step": 4118
    },
    {
      "epoch": 0.017600006836613485,
      "grad_norm": 1.8469809293746948,
      "learning_rate": 0.0003240471714236883,
      "loss": 0.506,
      "step": 4119
    },
    {
      "epoch": 0.017604279720040677,
      "grad_norm": 2.1277129650115967,
      "learning_rate": 0.00032400444368484025,
      "loss": 0.511,
      "step": 4120
    },
    {
      "epoch": 0.017608552603467872,
      "grad_norm": 2.3655407428741455,
      "learning_rate": 0.00032396171594599216,
      "loss": 0.4424,
      "step": 4121
    },
    {
      "epoch": 0.017612825486895068,
      "grad_norm": 0.8897787928581238,
      "learning_rate": 0.0003239189882071441,
      "loss": 0.2708,
      "step": 4122
    },
    {
      "epoch": 0.01761709837032226,
      "grad_norm": 1.8252179622650146,
      "learning_rate": 0.00032387626046829603,
      "loss": 0.5398,
      "step": 4123
    },
    {
      "epoch": 0.017621371253749456,
      "grad_norm": 2.0839293003082275,
      "learning_rate": 0.000323833532729448,
      "loss": 0.4639,
      "step": 4124
    },
    {
      "epoch": 0.017625644137176648,
      "grad_norm": 1.4317686557769775,
      "learning_rate": 0.0003237908049905999,
      "loss": 0.3191,
      "step": 4125
    },
    {
      "epoch": 0.017629917020603843,
      "grad_norm": 1.5037206411361694,
      "learning_rate": 0.0003237480772517519,
      "loss": 0.3999,
      "step": 4126
    },
    {
      "epoch": 0.01763418990403104,
      "grad_norm": 1.2635605335235596,
      "learning_rate": 0.00032370534951290373,
      "loss": 0.2643,
      "step": 4127
    },
    {
      "epoch": 0.01763846278745823,
      "grad_norm": 3.1214747428894043,
      "learning_rate": 0.0003236626217740557,
      "loss": 0.5458,
      "step": 4128
    },
    {
      "epoch": 0.017642735670885427,
      "grad_norm": 1.9361904859542847,
      "learning_rate": 0.00032361989403520766,
      "loss": 0.4301,
      "step": 4129
    },
    {
      "epoch": 0.017647008554312622,
      "grad_norm": 0.7521713972091675,
      "learning_rate": 0.00032357716629635957,
      "loss": 0.1771,
      "step": 4130
    },
    {
      "epoch": 0.017651281437739814,
      "grad_norm": 3.0959486961364746,
      "learning_rate": 0.00032353443855751154,
      "loss": 1.1764,
      "step": 4131
    },
    {
      "epoch": 0.01765555432116701,
      "grad_norm": 4.414125919342041,
      "learning_rate": 0.00032349171081866345,
      "loss": 1.5219,
      "step": 4132
    },
    {
      "epoch": 0.017659827204594206,
      "grad_norm": 1.3434242010116577,
      "learning_rate": 0.0003234489830798154,
      "loss": 0.3891,
      "step": 4133
    },
    {
      "epoch": 0.017664100088021398,
      "grad_norm": 1.7893015146255493,
      "learning_rate": 0.0003234062553409673,
      "loss": 0.3757,
      "step": 4134
    },
    {
      "epoch": 0.017668372971448593,
      "grad_norm": 2.594883441925049,
      "learning_rate": 0.0003233635276021193,
      "loss": 1.1391,
      "step": 4135
    },
    {
      "epoch": 0.01767264585487579,
      "grad_norm": 4.833233833312988,
      "learning_rate": 0.00032332079986327125,
      "loss": 1.5727,
      "step": 4136
    },
    {
      "epoch": 0.01767691873830298,
      "grad_norm": 3.244424343109131,
      "learning_rate": 0.00032327807212442316,
      "loss": 1.2668,
      "step": 4137
    },
    {
      "epoch": 0.017681191621730177,
      "grad_norm": 1.3873686790466309,
      "learning_rate": 0.00032323534438557513,
      "loss": 0.3056,
      "step": 4138
    },
    {
      "epoch": 0.01768546450515737,
      "grad_norm": 0.8537372946739197,
      "learning_rate": 0.00032319261664672704,
      "loss": 0.3044,
      "step": 4139
    },
    {
      "epoch": 0.017689737388584564,
      "grad_norm": 2.3996310234069824,
      "learning_rate": 0.000323149888907879,
      "loss": 0.6536,
      "step": 4140
    },
    {
      "epoch": 0.01769401027201176,
      "grad_norm": 0.9366032481193542,
      "learning_rate": 0.0003231071611690309,
      "loss": 0.2879,
      "step": 4141
    },
    {
      "epoch": 0.017698283155438952,
      "grad_norm": 2.3530561923980713,
      "learning_rate": 0.0003230644334301829,
      "loss": 0.598,
      "step": 4142
    },
    {
      "epoch": 0.017702556038866148,
      "grad_norm": 2.004009485244751,
      "learning_rate": 0.00032302170569133485,
      "loss": 0.3892,
      "step": 4143
    },
    {
      "epoch": 0.017706828922293343,
      "grad_norm": 3.274848222732544,
      "learning_rate": 0.00032297897795248676,
      "loss": 1.0976,
      "step": 4144
    },
    {
      "epoch": 0.017711101805720535,
      "grad_norm": 1.2511228322982788,
      "learning_rate": 0.0003229362502136387,
      "loss": 0.37,
      "step": 4145
    },
    {
      "epoch": 0.01771537468914773,
      "grad_norm": 1.0532108545303345,
      "learning_rate": 0.00032289352247479063,
      "loss": 0.264,
      "step": 4146
    },
    {
      "epoch": 0.017719647572574927,
      "grad_norm": 1.2580053806304932,
      "learning_rate": 0.0003228507947359426,
      "loss": 0.3354,
      "step": 4147
    },
    {
      "epoch": 0.01772392045600212,
      "grad_norm": 2.082521915435791,
      "learning_rate": 0.0003228080669970945,
      "loss": 0.9626,
      "step": 4148
    },
    {
      "epoch": 0.017728193339429314,
      "grad_norm": 3.5845210552215576,
      "learning_rate": 0.00032276533925824647,
      "loss": 1.3685,
      "step": 4149
    },
    {
      "epoch": 0.017732466222856506,
      "grad_norm": 0.7497369647026062,
      "learning_rate": 0.00032272261151939844,
      "loss": 0.2695,
      "step": 4150
    },
    {
      "epoch": 0.017736739106283702,
      "grad_norm": 2.6978650093078613,
      "learning_rate": 0.00032267988378055035,
      "loss": 1.0123,
      "step": 4151
    },
    {
      "epoch": 0.017741011989710898,
      "grad_norm": 2.9256997108459473,
      "learning_rate": 0.0003226371560417023,
      "loss": 0.5179,
      "step": 4152
    },
    {
      "epoch": 0.01774528487313809,
      "grad_norm": 0.9986281991004944,
      "learning_rate": 0.0003225944283028542,
      "loss": 0.3174,
      "step": 4153
    },
    {
      "epoch": 0.017749557756565285,
      "grad_norm": 3.150486469268799,
      "learning_rate": 0.0003225517005640062,
      "loss": 1.1403,
      "step": 4154
    },
    {
      "epoch": 0.01775383063999248,
      "grad_norm": 1.3833837509155273,
      "learning_rate": 0.0003225089728251581,
      "loss": 0.4004,
      "step": 4155
    },
    {
      "epoch": 0.017758103523419673,
      "grad_norm": 1.2579563856124878,
      "learning_rate": 0.00032246624508631006,
      "loss": 0.4153,
      "step": 4156
    },
    {
      "epoch": 0.01776237640684687,
      "grad_norm": 3.2607672214508057,
      "learning_rate": 0.00032242351734746203,
      "loss": 0.8989,
      "step": 4157
    },
    {
      "epoch": 0.017766649290274064,
      "grad_norm": 0.43528249859809875,
      "learning_rate": 0.00032238078960861394,
      "loss": 0.1916,
      "step": 4158
    },
    {
      "epoch": 0.017770922173701256,
      "grad_norm": 2.9068431854248047,
      "learning_rate": 0.0003223380618697659,
      "loss": 0.9568,
      "step": 4159
    },
    {
      "epoch": 0.017775195057128452,
      "grad_norm": 3.52974796295166,
      "learning_rate": 0.00032229533413091776,
      "loss": 0.9714,
      "step": 4160
    },
    {
      "epoch": 0.017779467940555647,
      "grad_norm": 2.8437509536743164,
      "learning_rate": 0.0003222526063920697,
      "loss": 0.894,
      "step": 4161
    },
    {
      "epoch": 0.01778374082398284,
      "grad_norm": 2.1949405670166016,
      "learning_rate": 0.00032220987865322164,
      "loss": 0.8589,
      "step": 4162
    },
    {
      "epoch": 0.017788013707410035,
      "grad_norm": 1.4239813089370728,
      "learning_rate": 0.0003221671509143736,
      "loss": 0.3649,
      "step": 4163
    },
    {
      "epoch": 0.017792286590837227,
      "grad_norm": 4.61842155456543,
      "learning_rate": 0.0003221244231755255,
      "loss": 1.4252,
      "step": 4164
    },
    {
      "epoch": 0.017796559474264423,
      "grad_norm": 5.017673492431641,
      "learning_rate": 0.0003220816954366775,
      "loss": 1.7227,
      "step": 4165
    },
    {
      "epoch": 0.01780083235769162,
      "grad_norm": 3.7042598724365234,
      "learning_rate": 0.00032203896769782944,
      "loss": 1.0443,
      "step": 4166
    },
    {
      "epoch": 0.01780510524111881,
      "grad_norm": 1.1279618740081787,
      "learning_rate": 0.00032199623995898135,
      "loss": 0.4005,
      "step": 4167
    },
    {
      "epoch": 0.017809378124546006,
      "grad_norm": 1.3800852298736572,
      "learning_rate": 0.0003219535122201333,
      "loss": 0.3212,
      "step": 4168
    },
    {
      "epoch": 0.017813651007973202,
      "grad_norm": 3.6825568675994873,
      "learning_rate": 0.00032191078448128523,
      "loss": 0.559,
      "step": 4169
    },
    {
      "epoch": 0.017817923891400394,
      "grad_norm": 4.612044334411621,
      "learning_rate": 0.0003218680567424372,
      "loss": 1.2942,
      "step": 4170
    },
    {
      "epoch": 0.01782219677482759,
      "grad_norm": 1.2533729076385498,
      "learning_rate": 0.0003218253290035891,
      "loss": 0.3774,
      "step": 4171
    },
    {
      "epoch": 0.017826469658254785,
      "grad_norm": 1.8893805742263794,
      "learning_rate": 0.00032178260126474107,
      "loss": 0.3647,
      "step": 4172
    },
    {
      "epoch": 0.017830742541681977,
      "grad_norm": 3.8360116481781006,
      "learning_rate": 0.00032173987352589303,
      "loss": 1.1559,
      "step": 4173
    },
    {
      "epoch": 0.017835015425109173,
      "grad_norm": 2.613424777984619,
      "learning_rate": 0.00032169714578704495,
      "loss": 0.6971,
      "step": 4174
    },
    {
      "epoch": 0.017839288308536365,
      "grad_norm": 1.9878920316696167,
      "learning_rate": 0.0003216544180481969,
      "loss": 0.5791,
      "step": 4175
    },
    {
      "epoch": 0.01784356119196356,
      "grad_norm": 1.7805325984954834,
      "learning_rate": 0.0003216116903093488,
      "loss": 0.7859,
      "step": 4176
    },
    {
      "epoch": 0.017847834075390756,
      "grad_norm": 1.064409852027893,
      "learning_rate": 0.0003215689625705008,
      "loss": 0.4007,
      "step": 4177
    },
    {
      "epoch": 0.017852106958817948,
      "grad_norm": 5.574499130249023,
      "learning_rate": 0.0003215262348316527,
      "loss": 1.4479,
      "step": 4178
    },
    {
      "epoch": 0.017856379842245144,
      "grad_norm": 0.4335275888442993,
      "learning_rate": 0.00032148350709280466,
      "loss": 0.2085,
      "step": 4179
    },
    {
      "epoch": 0.01786065272567234,
      "grad_norm": 0.6931473016738892,
      "learning_rate": 0.0003214407793539566,
      "loss": 0.2305,
      "step": 4180
    },
    {
      "epoch": 0.01786492560909953,
      "grad_norm": 0.4500858783721924,
      "learning_rate": 0.00032139805161510854,
      "loss": 0.2109,
      "step": 4181
    },
    {
      "epoch": 0.017869198492526727,
      "grad_norm": 0.5524096488952637,
      "learning_rate": 0.0003213553238762605,
      "loss": 0.2235,
      "step": 4182
    },
    {
      "epoch": 0.017873471375953923,
      "grad_norm": 1.5716478824615479,
      "learning_rate": 0.0003213125961374124,
      "loss": 0.6938,
      "step": 4183
    },
    {
      "epoch": 0.017877744259381115,
      "grad_norm": 1.6877398490905762,
      "learning_rate": 0.0003212698683985644,
      "loss": 0.7403,
      "step": 4184
    },
    {
      "epoch": 0.01788201714280831,
      "grad_norm": 3.094860792160034,
      "learning_rate": 0.0003212271406597163,
      "loss": 1.5408,
      "step": 4185
    },
    {
      "epoch": 0.017886290026235506,
      "grad_norm": 3.502206325531006,
      "learning_rate": 0.00032118441292086825,
      "loss": 1.0035,
      "step": 4186
    },
    {
      "epoch": 0.017890562909662698,
      "grad_norm": 5.7947998046875,
      "learning_rate": 0.0003211416851820202,
      "loss": 3.1795,
      "step": 4187
    },
    {
      "epoch": 0.017894835793089894,
      "grad_norm": 0.9706648588180542,
      "learning_rate": 0.00032109895744317213,
      "loss": 0.4213,
      "step": 4188
    },
    {
      "epoch": 0.017899108676517086,
      "grad_norm": 1.7290524244308472,
      "learning_rate": 0.0003210562297043241,
      "loss": 0.3026,
      "step": 4189
    },
    {
      "epoch": 0.01790338155994428,
      "grad_norm": 2.894235134124756,
      "learning_rate": 0.000321013501965476,
      "loss": 0.8756,
      "step": 4190
    },
    {
      "epoch": 0.017907654443371477,
      "grad_norm": 5.530999660491943,
      "learning_rate": 0.00032097077422662797,
      "loss": 1.295,
      "step": 4191
    },
    {
      "epoch": 0.01791192732679867,
      "grad_norm": 1.7971185445785522,
      "learning_rate": 0.0003209280464877799,
      "loss": 0.5582,
      "step": 4192
    },
    {
      "epoch": 0.017916200210225865,
      "grad_norm": 1.3637018203735352,
      "learning_rate": 0.0003208853187489318,
      "loss": 0.5528,
      "step": 4193
    },
    {
      "epoch": 0.01792047309365306,
      "grad_norm": 0.8766103386878967,
      "learning_rate": 0.0003208425910100837,
      "loss": 0.3422,
      "step": 4194
    },
    {
      "epoch": 0.017924745977080252,
      "grad_norm": 0.49689722061157227,
      "learning_rate": 0.00032079986327123567,
      "loss": 0.2031,
      "step": 4195
    },
    {
      "epoch": 0.017929018860507448,
      "grad_norm": 4.591005802154541,
      "learning_rate": 0.00032075713553238763,
      "loss": 0.9265,
      "step": 4196
    },
    {
      "epoch": 0.017933291743934644,
      "grad_norm": 5.081501007080078,
      "learning_rate": 0.00032071440779353954,
      "loss": 1.18,
      "step": 4197
    },
    {
      "epoch": 0.017937564627361836,
      "grad_norm": 2.979262113571167,
      "learning_rate": 0.0003206716800546915,
      "loss": 0.7387,
      "step": 4198
    },
    {
      "epoch": 0.01794183751078903,
      "grad_norm": 1.9910027980804443,
      "learning_rate": 0.0003206289523158434,
      "loss": 0.6885,
      "step": 4199
    },
    {
      "epoch": 0.017946110394216223,
      "grad_norm": 3.5465645790100098,
      "learning_rate": 0.0003205862245769954,
      "loss": 1.2709,
      "step": 4200
    },
    {
      "epoch": 0.01795038327764342,
      "grad_norm": 0.9682976603507996,
      "learning_rate": 0.0003205434968381473,
      "loss": 0.4415,
      "step": 4201
    },
    {
      "epoch": 0.017954656161070615,
      "grad_norm": 2.9632740020751953,
      "learning_rate": 0.00032050076909929926,
      "loss": 1.4683,
      "step": 4202
    },
    {
      "epoch": 0.017958929044497807,
      "grad_norm": 5.117040634155273,
      "learning_rate": 0.0003204580413604512,
      "loss": 1.4845,
      "step": 4203
    },
    {
      "epoch": 0.017963201927925002,
      "grad_norm": 1.9719552993774414,
      "learning_rate": 0.00032041531362160314,
      "loss": 0.6734,
      "step": 4204
    },
    {
      "epoch": 0.017967474811352198,
      "grad_norm": 1.94733464717865,
      "learning_rate": 0.0003203725858827551,
      "loss": 0.6529,
      "step": 4205
    },
    {
      "epoch": 0.01797174769477939,
      "grad_norm": 5.437829494476318,
      "learning_rate": 0.000320329858143907,
      "loss": 1.0593,
      "step": 4206
    },
    {
      "epoch": 0.017976020578206586,
      "grad_norm": 1.329284906387329,
      "learning_rate": 0.000320287130405059,
      "loss": 0.5366,
      "step": 4207
    },
    {
      "epoch": 0.01798029346163378,
      "grad_norm": 0.5924553871154785,
      "learning_rate": 0.0003202444026662109,
      "loss": 0.2493,
      "step": 4208
    },
    {
      "epoch": 0.017984566345060973,
      "grad_norm": 0.5545107126235962,
      "learning_rate": 0.00032020167492736285,
      "loss": 0.2309,
      "step": 4209
    },
    {
      "epoch": 0.01798883922848817,
      "grad_norm": 4.989425182342529,
      "learning_rate": 0.0003201589471885148,
      "loss": 0.8331,
      "step": 4210
    },
    {
      "epoch": 0.01799311211191536,
      "grad_norm": 1.4364639520645142,
      "learning_rate": 0.0003201162194496667,
      "loss": 0.3267,
      "step": 4211
    },
    {
      "epoch": 0.017997384995342557,
      "grad_norm": 1.0329612493515015,
      "learning_rate": 0.0003200734917108187,
      "loss": 0.2936,
      "step": 4212
    },
    {
      "epoch": 0.018001657878769752,
      "grad_norm": 1.3173543214797974,
      "learning_rate": 0.0003200307639719706,
      "loss": 0.4955,
      "step": 4213
    },
    {
      "epoch": 0.018005930762196944,
      "grad_norm": 3.162841796875,
      "learning_rate": 0.00031998803623312257,
      "loss": 1.1011,
      "step": 4214
    },
    {
      "epoch": 0.01801020364562414,
      "grad_norm": 3.096452474594116,
      "learning_rate": 0.0003199453084942745,
      "loss": 0.9737,
      "step": 4215
    },
    {
      "epoch": 0.018014476529051335,
      "grad_norm": 2.9376275539398193,
      "learning_rate": 0.00031990258075542644,
      "loss": 1.2714,
      "step": 4216
    },
    {
      "epoch": 0.018018749412478528,
      "grad_norm": 3.245222806930542,
      "learning_rate": 0.0003198598530165784,
      "loss": 0.9004,
      "step": 4217
    },
    {
      "epoch": 0.018023022295905723,
      "grad_norm": 2.713345527648926,
      "learning_rate": 0.0003198171252777303,
      "loss": 0.5864,
      "step": 4218
    },
    {
      "epoch": 0.01802729517933292,
      "grad_norm": 2.865248441696167,
      "learning_rate": 0.0003197743975388823,
      "loss": 0.4331,
      "step": 4219
    },
    {
      "epoch": 0.01803156806276011,
      "grad_norm": 1.054249882698059,
      "learning_rate": 0.0003197316698000342,
      "loss": 0.4764,
      "step": 4220
    },
    {
      "epoch": 0.018035840946187306,
      "grad_norm": 0.45096734166145325,
      "learning_rate": 0.00031968894206118616,
      "loss": 0.1976,
      "step": 4221
    },
    {
      "epoch": 0.018040113829614502,
      "grad_norm": 5.691341400146484,
      "learning_rate": 0.00031964621432233807,
      "loss": 2.1267,
      "step": 4222
    },
    {
      "epoch": 0.018044386713041694,
      "grad_norm": 3.0787429809570312,
      "learning_rate": 0.00031960348658349004,
      "loss": 0.9143,
      "step": 4223
    },
    {
      "epoch": 0.01804865959646889,
      "grad_norm": 0.3896207809448242,
      "learning_rate": 0.000319560758844642,
      "loss": 0.1681,
      "step": 4224
    },
    {
      "epoch": 0.018052932479896082,
      "grad_norm": 2.3597850799560547,
      "learning_rate": 0.00031951803110579386,
      "loss": 0.7635,
      "step": 4225
    },
    {
      "epoch": 0.018057205363323277,
      "grad_norm": 0.3899936378002167,
      "learning_rate": 0.0003194753033669458,
      "loss": 0.1612,
      "step": 4226
    },
    {
      "epoch": 0.018061478246750473,
      "grad_norm": 2.915132999420166,
      "learning_rate": 0.00031943257562809773,
      "loss": 1.0462,
      "step": 4227
    },
    {
      "epoch": 0.018065751130177665,
      "grad_norm": 0.49285855889320374,
      "learning_rate": 0.0003193898478892497,
      "loss": 0.2237,
      "step": 4228
    },
    {
      "epoch": 0.01807002401360486,
      "grad_norm": 1.0534807443618774,
      "learning_rate": 0.0003193471201504016,
      "loss": 0.4508,
      "step": 4229
    },
    {
      "epoch": 0.018074296897032056,
      "grad_norm": 1.0942668914794922,
      "learning_rate": 0.0003193043924115536,
      "loss": 0.2765,
      "step": 4230
    },
    {
      "epoch": 0.01807856978045925,
      "grad_norm": 0.5027684569358826,
      "learning_rate": 0.0003192616646727055,
      "loss": 0.2335,
      "step": 4231
    },
    {
      "epoch": 0.018082842663886444,
      "grad_norm": 2.262558698654175,
      "learning_rate": 0.00031921893693385745,
      "loss": 0.6783,
      "step": 4232
    },
    {
      "epoch": 0.01808711554731364,
      "grad_norm": 3.036777973175049,
      "learning_rate": 0.0003191762091950094,
      "loss": 0.7763,
      "step": 4233
    },
    {
      "epoch": 0.018091388430740832,
      "grad_norm": 2.922346353530884,
      "learning_rate": 0.0003191334814561613,
      "loss": 0.7107,
      "step": 4234
    },
    {
      "epoch": 0.018095661314168027,
      "grad_norm": 0.9002669453620911,
      "learning_rate": 0.0003190907537173133,
      "loss": 0.2511,
      "step": 4235
    },
    {
      "epoch": 0.01809993419759522,
      "grad_norm": 0.38864994049072266,
      "learning_rate": 0.0003190480259784652,
      "loss": 0.1757,
      "step": 4236
    },
    {
      "epoch": 0.018104207081022415,
      "grad_norm": 1.7581307888031006,
      "learning_rate": 0.00031900529823961717,
      "loss": 0.4551,
      "step": 4237
    },
    {
      "epoch": 0.01810847996444961,
      "grad_norm": 1.2201861143112183,
      "learning_rate": 0.0003189625705007691,
      "loss": 0.5093,
      "step": 4238
    },
    {
      "epoch": 0.018112752847876803,
      "grad_norm": 2.3101918697357178,
      "learning_rate": 0.00031891984276192104,
      "loss": 0.641,
      "step": 4239
    },
    {
      "epoch": 0.018117025731304,
      "grad_norm": 1.9409817457199097,
      "learning_rate": 0.000318877115023073,
      "loss": 0.6855,
      "step": 4240
    },
    {
      "epoch": 0.018121298614731194,
      "grad_norm": 0.3817281126976013,
      "learning_rate": 0.0003188343872842249,
      "loss": 0.1723,
      "step": 4241
    },
    {
      "epoch": 0.018125571498158386,
      "grad_norm": 1.9213467836380005,
      "learning_rate": 0.0003187916595453769,
      "loss": 0.592,
      "step": 4242
    },
    {
      "epoch": 0.01812984438158558,
      "grad_norm": 1.0552276372909546,
      "learning_rate": 0.0003187489318065288,
      "loss": 0.4137,
      "step": 4243
    },
    {
      "epoch": 0.018134117265012777,
      "grad_norm": 2.96500301361084,
      "learning_rate": 0.00031870620406768076,
      "loss": 0.7898,
      "step": 4244
    },
    {
      "epoch": 0.01813839014843997,
      "grad_norm": 3.597977876663208,
      "learning_rate": 0.00031866347632883267,
      "loss": 1.1013,
      "step": 4245
    },
    {
      "epoch": 0.018142663031867165,
      "grad_norm": 2.0095362663269043,
      "learning_rate": 0.00031862074858998463,
      "loss": 0.5492,
      "step": 4246
    },
    {
      "epoch": 0.01814693591529436,
      "grad_norm": 5.829124927520752,
      "learning_rate": 0.0003185780208511366,
      "loss": 2.1817,
      "step": 4247
    },
    {
      "epoch": 0.018151208798721553,
      "grad_norm": 0.38175007700920105,
      "learning_rate": 0.0003185352931122885,
      "loss": 0.1523,
      "step": 4248
    },
    {
      "epoch": 0.01815548168214875,
      "grad_norm": 1.2205592393875122,
      "learning_rate": 0.0003184925653734405,
      "loss": 0.4985,
      "step": 4249
    },
    {
      "epoch": 0.01815975456557594,
      "grad_norm": 1.9858977794647217,
      "learning_rate": 0.0003184498376345924,
      "loss": 0.6967,
      "step": 4250
    },
    {
      "epoch": 0.018164027449003136,
      "grad_norm": 1.102327585220337,
      "learning_rate": 0.00031840710989574435,
      "loss": 0.4075,
      "step": 4251
    },
    {
      "epoch": 0.01816830033243033,
      "grad_norm": 0.3454526662826538,
      "learning_rate": 0.00031836438215689626,
      "loss": 0.1296,
      "step": 4252
    },
    {
      "epoch": 0.018172573215857524,
      "grad_norm": 1.004317283630371,
      "learning_rate": 0.0003183216544180482,
      "loss": 0.4048,
      "step": 4253
    },
    {
      "epoch": 0.01817684609928472,
      "grad_norm": 2.0916941165924072,
      "learning_rate": 0.0003182789266792002,
      "loss": 0.5496,
      "step": 4254
    },
    {
      "epoch": 0.018181118982711915,
      "grad_norm": 0.7714799642562866,
      "learning_rate": 0.0003182361989403521,
      "loss": 0.1496,
      "step": 4255
    },
    {
      "epoch": 0.018185391866139107,
      "grad_norm": 0.912954568862915,
      "learning_rate": 0.00031819347120150407,
      "loss": 0.3831,
      "step": 4256
    },
    {
      "epoch": 0.018189664749566303,
      "grad_norm": 1.229605793952942,
      "learning_rate": 0.000318150743462656,
      "loss": 0.4851,
      "step": 4257
    },
    {
      "epoch": 0.018193937632993498,
      "grad_norm": 5.5730085372924805,
      "learning_rate": 0.0003181080157238079,
      "loss": 1.2054,
      "step": 4258
    },
    {
      "epoch": 0.01819821051642069,
      "grad_norm": 4.052818775177002,
      "learning_rate": 0.0003180652879849598,
      "loss": 1.3614,
      "step": 4259
    },
    {
      "epoch": 0.018202483399847886,
      "grad_norm": 5.032596111297607,
      "learning_rate": 0.00031802256024611176,
      "loss": 1.5,
      "step": 4260
    },
    {
      "epoch": 0.018206756283275078,
      "grad_norm": 3.4456732273101807,
      "learning_rate": 0.0003179798325072637,
      "loss": 1.3485,
      "step": 4261
    },
    {
      "epoch": 0.018211029166702274,
      "grad_norm": 4.916849613189697,
      "learning_rate": 0.00031793710476841564,
      "loss": 1.4087,
      "step": 4262
    },
    {
      "epoch": 0.01821530205012947,
      "grad_norm": 3.427922248840332,
      "learning_rate": 0.0003178943770295676,
      "loss": 1.3494,
      "step": 4263
    },
    {
      "epoch": 0.01821957493355666,
      "grad_norm": 2.71053409576416,
      "learning_rate": 0.0003178516492907195,
      "loss": 0.578,
      "step": 4264
    },
    {
      "epoch": 0.018223847816983857,
      "grad_norm": 3.280473232269287,
      "learning_rate": 0.0003178089215518715,
      "loss": 1.2948,
      "step": 4265
    },
    {
      "epoch": 0.018228120700411053,
      "grad_norm": 0.8966591954231262,
      "learning_rate": 0.0003177661938130234,
      "loss": 0.363,
      "step": 4266
    },
    {
      "epoch": 0.018232393583838245,
      "grad_norm": 3.368610382080078,
      "learning_rate": 0.00031772346607417535,
      "loss": 0.8906,
      "step": 4267
    },
    {
      "epoch": 0.01823666646726544,
      "grad_norm": 1.9989268779754639,
      "learning_rate": 0.00031768073833532727,
      "loss": 0.4388,
      "step": 4268
    },
    {
      "epoch": 0.018240939350692636,
      "grad_norm": 0.5556154847145081,
      "learning_rate": 0.00031763801059647923,
      "loss": 0.2935,
      "step": 4269
    },
    {
      "epoch": 0.018245212234119828,
      "grad_norm": 2.1732776165008545,
      "learning_rate": 0.0003175952828576312,
      "loss": 0.58,
      "step": 4270
    },
    {
      "epoch": 0.018249485117547024,
      "grad_norm": 1.8313970565795898,
      "learning_rate": 0.0003175525551187831,
      "loss": 0.5854,
      "step": 4271
    },
    {
      "epoch": 0.01825375800097422,
      "grad_norm": 2.849050760269165,
      "learning_rate": 0.00031750982737993507,
      "loss": 0.7334,
      "step": 4272
    },
    {
      "epoch": 0.01825803088440141,
      "grad_norm": 2.795727491378784,
      "learning_rate": 0.000317467099641087,
      "loss": 0.6955,
      "step": 4273
    },
    {
      "epoch": 0.018262303767828607,
      "grad_norm": 0.9858887791633606,
      "learning_rate": 0.00031742437190223895,
      "loss": 0.442,
      "step": 4274
    },
    {
      "epoch": 0.0182665766512558,
      "grad_norm": 4.346449851989746,
      "learning_rate": 0.00031738164416339086,
      "loss": 1.0873,
      "step": 4275
    },
    {
      "epoch": 0.018270849534682995,
      "grad_norm": 0.38486549258232117,
      "learning_rate": 0.0003173389164245428,
      "loss": 0.1708,
      "step": 4276
    },
    {
      "epoch": 0.01827512241811019,
      "grad_norm": 0.9701583981513977,
      "learning_rate": 0.0003172961886856948,
      "loss": 0.3869,
      "step": 4277
    },
    {
      "epoch": 0.018279395301537382,
      "grad_norm": 0.4740673303604126,
      "learning_rate": 0.0003172534609468467,
      "loss": 0.2401,
      "step": 4278
    },
    {
      "epoch": 0.018283668184964578,
      "grad_norm": 1.2491466999053955,
      "learning_rate": 0.00031721073320799866,
      "loss": 0.5823,
      "step": 4279
    },
    {
      "epoch": 0.018287941068391773,
      "grad_norm": 1.7016774415969849,
      "learning_rate": 0.0003171680054691506,
      "loss": 0.7405,
      "step": 4280
    },
    {
      "epoch": 0.018292213951818966,
      "grad_norm": 2.803553581237793,
      "learning_rate": 0.00031712527773030254,
      "loss": 0.4783,
      "step": 4281
    },
    {
      "epoch": 0.01829648683524616,
      "grad_norm": 3.2251970767974854,
      "learning_rate": 0.00031708254999145445,
      "loss": 1.5692,
      "step": 4282
    },
    {
      "epoch": 0.018300759718673357,
      "grad_norm": 3.620421886444092,
      "learning_rate": 0.0003170398222526064,
      "loss": 1.0822,
      "step": 4283
    },
    {
      "epoch": 0.01830503260210055,
      "grad_norm": 1.2632087469100952,
      "learning_rate": 0.0003169970945137584,
      "loss": 0.579,
      "step": 4284
    },
    {
      "epoch": 0.018309305485527744,
      "grad_norm": 2.20945143699646,
      "learning_rate": 0.0003169543667749103,
      "loss": 0.5446,
      "step": 4285
    },
    {
      "epoch": 0.018313578368954937,
      "grad_norm": 2.0429461002349854,
      "learning_rate": 0.00031691163903606225,
      "loss": 0.7909,
      "step": 4286
    },
    {
      "epoch": 0.018317851252382132,
      "grad_norm": 4.098278522491455,
      "learning_rate": 0.00031686891129721417,
      "loss": 1.1544,
      "step": 4287
    },
    {
      "epoch": 0.018322124135809328,
      "grad_norm": 1.7948572635650635,
      "learning_rate": 0.00031682618355836613,
      "loss": 0.6482,
      "step": 4288
    },
    {
      "epoch": 0.01832639701923652,
      "grad_norm": 1.8031026124954224,
      "learning_rate": 0.00031678345581951804,
      "loss": 0.6328,
      "step": 4289
    },
    {
      "epoch": 0.018330669902663715,
      "grad_norm": 0.3936171233654022,
      "learning_rate": 0.00031674072808067,
      "loss": 0.2006,
      "step": 4290
    },
    {
      "epoch": 0.01833494278609091,
      "grad_norm": 2.309600353240967,
      "learning_rate": 0.0003166980003418219,
      "loss": 0.5791,
      "step": 4291
    },
    {
      "epoch": 0.018339215669518103,
      "grad_norm": 1.9049773216247559,
      "learning_rate": 0.00031665527260297383,
      "loss": 0.668,
      "step": 4292
    },
    {
      "epoch": 0.0183434885529453,
      "grad_norm": 2.893519878387451,
      "learning_rate": 0.0003166125448641258,
      "loss": 1.2404,
      "step": 4293
    },
    {
      "epoch": 0.018347761436372494,
      "grad_norm": 5.725131988525391,
      "learning_rate": 0.0003165698171252777,
      "loss": 1.2993,
      "step": 4294
    },
    {
      "epoch": 0.018352034319799686,
      "grad_norm": 1.9575250148773193,
      "learning_rate": 0.00031652708938642967,
      "loss": 0.6034,
      "step": 4295
    },
    {
      "epoch": 0.018356307203226882,
      "grad_norm": 2.163201332092285,
      "learning_rate": 0.0003164843616475816,
      "loss": 0.7909,
      "step": 4296
    },
    {
      "epoch": 0.018360580086654078,
      "grad_norm": 1.2089964151382446,
      "learning_rate": 0.00031644163390873354,
      "loss": 0.4303,
      "step": 4297
    },
    {
      "epoch": 0.01836485297008127,
      "grad_norm": 1.2045518159866333,
      "learning_rate": 0.00031639890616988545,
      "loss": 0.4238,
      "step": 4298
    },
    {
      "epoch": 0.018369125853508465,
      "grad_norm": 1.4561759233474731,
      "learning_rate": 0.0003163561784310374,
      "loss": 0.4071,
      "step": 4299
    },
    {
      "epoch": 0.018373398736935657,
      "grad_norm": 4.434237003326416,
      "learning_rate": 0.0003163134506921894,
      "loss": 1.1084,
      "step": 4300
    },
    {
      "epoch": 0.018377671620362853,
      "grad_norm": 1.1072466373443604,
      "learning_rate": 0.0003162707229533413,
      "loss": 0.4815,
      "step": 4301
    },
    {
      "epoch": 0.01838194450379005,
      "grad_norm": 1.882246494293213,
      "learning_rate": 0.00031622799521449326,
      "loss": 0.6336,
      "step": 4302
    },
    {
      "epoch": 0.01838621738721724,
      "grad_norm": 1.131998896598816,
      "learning_rate": 0.00031618526747564517,
      "loss": 0.4814,
      "step": 4303
    },
    {
      "epoch": 0.018390490270644436,
      "grad_norm": 1.6832939386367798,
      "learning_rate": 0.00031614253973679714,
      "loss": 0.4202,
      "step": 4304
    },
    {
      "epoch": 0.018394763154071632,
      "grad_norm": 3.6448636054992676,
      "learning_rate": 0.00031609981199794905,
      "loss": 1.0761,
      "step": 4305
    },
    {
      "epoch": 0.018399036037498824,
      "grad_norm": 3.7614357471466064,
      "learning_rate": 0.000316057084259101,
      "loss": 1.0045,
      "step": 4306
    },
    {
      "epoch": 0.01840330892092602,
      "grad_norm": 2.4187283515930176,
      "learning_rate": 0.000316014356520253,
      "loss": 0.662,
      "step": 4307
    },
    {
      "epoch": 0.018407581804353215,
      "grad_norm": 2.145331859588623,
      "learning_rate": 0.0003159716287814049,
      "loss": 0.7397,
      "step": 4308
    },
    {
      "epoch": 0.018411854687780407,
      "grad_norm": 2.810826063156128,
      "learning_rate": 0.00031592890104255685,
      "loss": 1.1091,
      "step": 4309
    },
    {
      "epoch": 0.018416127571207603,
      "grad_norm": 2.007953643798828,
      "learning_rate": 0.00031588617330370876,
      "loss": 0.3508,
      "step": 4310
    },
    {
      "epoch": 0.018420400454634795,
      "grad_norm": 1.0741244554519653,
      "learning_rate": 0.00031584344556486073,
      "loss": 0.4003,
      "step": 4311
    },
    {
      "epoch": 0.01842467333806199,
      "grad_norm": 1.3719302415847778,
      "learning_rate": 0.00031580071782601264,
      "loss": 0.473,
      "step": 4312
    },
    {
      "epoch": 0.018428946221489186,
      "grad_norm": 1.5146360397338867,
      "learning_rate": 0.0003157579900871646,
      "loss": 0.4521,
      "step": 4313
    },
    {
      "epoch": 0.01843321910491638,
      "grad_norm": 2.1722676753997803,
      "learning_rate": 0.00031571526234831657,
      "loss": 0.6729,
      "step": 4314
    },
    {
      "epoch": 0.018437491988343574,
      "grad_norm": 1.8076800107955933,
      "learning_rate": 0.0003156725346094685,
      "loss": 0.4601,
      "step": 4315
    },
    {
      "epoch": 0.01844176487177077,
      "grad_norm": 1.693668246269226,
      "learning_rate": 0.00031562980687062044,
      "loss": 0.3433,
      "step": 4316
    },
    {
      "epoch": 0.01844603775519796,
      "grad_norm": 1.966267466545105,
      "learning_rate": 0.00031558707913177236,
      "loss": 0.5714,
      "step": 4317
    },
    {
      "epoch": 0.018450310638625157,
      "grad_norm": 1.1386486291885376,
      "learning_rate": 0.0003155443513929243,
      "loss": 0.3526,
      "step": 4318
    },
    {
      "epoch": 0.018454583522052353,
      "grad_norm": 3.6369974613189697,
      "learning_rate": 0.00031550162365407623,
      "loss": 1.4128,
      "step": 4319
    },
    {
      "epoch": 0.018458856405479545,
      "grad_norm": 6.372410774230957,
      "learning_rate": 0.0003154588959152282,
      "loss": 2.5228,
      "step": 4320
    },
    {
      "epoch": 0.01846312928890674,
      "grad_norm": 1.7926130294799805,
      "learning_rate": 0.00031541616817638016,
      "loss": 0.3925,
      "step": 4321
    },
    {
      "epoch": 0.018467402172333933,
      "grad_norm": 1.0914075374603271,
      "learning_rate": 0.00031537344043753207,
      "loss": 0.3193,
      "step": 4322
    },
    {
      "epoch": 0.018471675055761128,
      "grad_norm": 1.0181158781051636,
      "learning_rate": 0.00031533071269868404,
      "loss": 0.3413,
      "step": 4323
    },
    {
      "epoch": 0.018475947939188324,
      "grad_norm": 0.8476771116256714,
      "learning_rate": 0.0003152879849598359,
      "loss": 0.257,
      "step": 4324
    },
    {
      "epoch": 0.018480220822615516,
      "grad_norm": 2.312059164047241,
      "learning_rate": 0.00031524525722098786,
      "loss": 0.5625,
      "step": 4325
    },
    {
      "epoch": 0.01848449370604271,
      "grad_norm": 3.243063449859619,
      "learning_rate": 0.00031520252948213977,
      "loss": 0.7753,
      "step": 4326
    },
    {
      "epoch": 0.018488766589469907,
      "grad_norm": 3.4491164684295654,
      "learning_rate": 0.00031515980174329173,
      "loss": 1.2351,
      "step": 4327
    },
    {
      "epoch": 0.0184930394728971,
      "grad_norm": 1.6708624362945557,
      "learning_rate": 0.00031511707400444364,
      "loss": 0.2807,
      "step": 4328
    },
    {
      "epoch": 0.018497312356324295,
      "grad_norm": 1.480595588684082,
      "learning_rate": 0.0003150743462655956,
      "loss": 0.4525,
      "step": 4329
    },
    {
      "epoch": 0.01850158523975149,
      "grad_norm": 0.763455867767334,
      "learning_rate": 0.0003150316185267476,
      "loss": 0.2695,
      "step": 4330
    },
    {
      "epoch": 0.018505858123178683,
      "grad_norm": 3.3440158367156982,
      "learning_rate": 0.0003149888907878995,
      "loss": 1.1311,
      "step": 4331
    },
    {
      "epoch": 0.018510131006605878,
      "grad_norm": 4.566093444824219,
      "learning_rate": 0.00031494616304905145,
      "loss": 1.2069,
      "step": 4332
    },
    {
      "epoch": 0.018514403890033074,
      "grad_norm": 1.3972861766815186,
      "learning_rate": 0.00031490343531020336,
      "loss": 0.5392,
      "step": 4333
    },
    {
      "epoch": 0.018518676773460266,
      "grad_norm": 1.9809452295303345,
      "learning_rate": 0.0003148607075713553,
      "loss": 0.3843,
      "step": 4334
    },
    {
      "epoch": 0.01852294965688746,
      "grad_norm": 0.5630205869674683,
      "learning_rate": 0.00031481797983250724,
      "loss": 0.2147,
      "step": 4335
    },
    {
      "epoch": 0.018527222540314654,
      "grad_norm": 1.316245675086975,
      "learning_rate": 0.0003147752520936592,
      "loss": 0.467,
      "step": 4336
    },
    {
      "epoch": 0.01853149542374185,
      "grad_norm": 1.0276238918304443,
      "learning_rate": 0.00031473252435481117,
      "loss": 0.2942,
      "step": 4337
    },
    {
      "epoch": 0.018535768307169045,
      "grad_norm": 1.793691873550415,
      "learning_rate": 0.0003146897966159631,
      "loss": 0.5788,
      "step": 4338
    },
    {
      "epoch": 0.018540041190596237,
      "grad_norm": 4.723329544067383,
      "learning_rate": 0.00031464706887711504,
      "loss": 1.1115,
      "step": 4339
    },
    {
      "epoch": 0.018544314074023432,
      "grad_norm": 0.5514189600944519,
      "learning_rate": 0.00031460434113826695,
      "loss": 0.2063,
      "step": 4340
    },
    {
      "epoch": 0.018548586957450628,
      "grad_norm": 3.2803962230682373,
      "learning_rate": 0.0003145616133994189,
      "loss": 0.9856,
      "step": 4341
    },
    {
      "epoch": 0.01855285984087782,
      "grad_norm": 1.970716953277588,
      "learning_rate": 0.00031451888566057083,
      "loss": 0.3562,
      "step": 4342
    },
    {
      "epoch": 0.018557132724305016,
      "grad_norm": 3.415710687637329,
      "learning_rate": 0.0003144761579217228,
      "loss": 0.8769,
      "step": 4343
    },
    {
      "epoch": 0.01856140560773221,
      "grad_norm": 2.5211732387542725,
      "learning_rate": 0.00031443343018287476,
      "loss": 0.7767,
      "step": 4344
    },
    {
      "epoch": 0.018565678491159403,
      "grad_norm": 2.779597282409668,
      "learning_rate": 0.00031439070244402667,
      "loss": 0.8365,
      "step": 4345
    },
    {
      "epoch": 0.0185699513745866,
      "grad_norm": 0.7811684608459473,
      "learning_rate": 0.00031434797470517863,
      "loss": 0.2963,
      "step": 4346
    },
    {
      "epoch": 0.01857422425801379,
      "grad_norm": 3.476702928543091,
      "learning_rate": 0.00031430524696633054,
      "loss": 1.0536,
      "step": 4347
    },
    {
      "epoch": 0.018578497141440987,
      "grad_norm": 4.473641872406006,
      "learning_rate": 0.0003142625192274825,
      "loss": 0.9719,
      "step": 4348
    },
    {
      "epoch": 0.018582770024868182,
      "grad_norm": 2.6507749557495117,
      "learning_rate": 0.0003142197914886344,
      "loss": 0.9288,
      "step": 4349
    },
    {
      "epoch": 0.018587042908295374,
      "grad_norm": 3.3175485134124756,
      "learning_rate": 0.0003141770637497864,
      "loss": 0.9687,
      "step": 4350
    },
    {
      "epoch": 0.01859131579172257,
      "grad_norm": 4.494172096252441,
      "learning_rate": 0.00031413433601093835,
      "loss": 1.574,
      "step": 4351
    },
    {
      "epoch": 0.018595588675149766,
      "grad_norm": 0.8710716962814331,
      "learning_rate": 0.00031409160827209026,
      "loss": 0.2115,
      "step": 4352
    },
    {
      "epoch": 0.018599861558576958,
      "grad_norm": 5.467491626739502,
      "learning_rate": 0.0003140488805332422,
      "loss": 1.7463,
      "step": 4353
    },
    {
      "epoch": 0.018604134442004153,
      "grad_norm": 3.767072916030884,
      "learning_rate": 0.00031400615279439414,
      "loss": 1.0732,
      "step": 4354
    },
    {
      "epoch": 0.01860840732543135,
      "grad_norm": 3.8316490650177,
      "learning_rate": 0.0003139634250555461,
      "loss": 0.7621,
      "step": 4355
    },
    {
      "epoch": 0.01861268020885854,
      "grad_norm": 2.4563920497894287,
      "learning_rate": 0.000313920697316698,
      "loss": 0.8896,
      "step": 4356
    },
    {
      "epoch": 0.018616953092285737,
      "grad_norm": 0.5856170058250427,
      "learning_rate": 0.0003138779695778499,
      "loss": 0.2513,
      "step": 4357
    },
    {
      "epoch": 0.018621225975712932,
      "grad_norm": 4.732691764831543,
      "learning_rate": 0.0003138352418390019,
      "loss": 1.8931,
      "step": 4358
    },
    {
      "epoch": 0.018625498859140124,
      "grad_norm": 1.361420750617981,
      "learning_rate": 0.0003137925141001538,
      "loss": 0.3951,
      "step": 4359
    },
    {
      "epoch": 0.01862977174256732,
      "grad_norm": 3.501642942428589,
      "learning_rate": 0.00031374978636130576,
      "loss": 0.9745,
      "step": 4360
    },
    {
      "epoch": 0.018634044625994512,
      "grad_norm": 6.727722644805908,
      "learning_rate": 0.0003137070586224577,
      "loss": 2.4048,
      "step": 4361
    },
    {
      "epoch": 0.018638317509421708,
      "grad_norm": 1.2453429698944092,
      "learning_rate": 0.00031366433088360964,
      "loss": 0.3369,
      "step": 4362
    },
    {
      "epoch": 0.018642590392848903,
      "grad_norm": 2.447265148162842,
      "learning_rate": 0.00031362160314476155,
      "loss": 0.6058,
      "step": 4363
    },
    {
      "epoch": 0.018646863276276095,
      "grad_norm": 1.7457586526870728,
      "learning_rate": 0.0003135788754059135,
      "loss": 0.4215,
      "step": 4364
    },
    {
      "epoch": 0.01865113615970329,
      "grad_norm": 7.80764102935791,
      "learning_rate": 0.0003135361476670654,
      "loss": 3.1579,
      "step": 4365
    },
    {
      "epoch": 0.018655409043130487,
      "grad_norm": 0.5065304040908813,
      "learning_rate": 0.0003134934199282174,
      "loss": 0.2234,
      "step": 4366
    },
    {
      "epoch": 0.01865968192655768,
      "grad_norm": 1.4210761785507202,
      "learning_rate": 0.00031345069218936936,
      "loss": 0.354,
      "step": 4367
    },
    {
      "epoch": 0.018663954809984874,
      "grad_norm": 4.30967903137207,
      "learning_rate": 0.00031340796445052127,
      "loss": 1.6667,
      "step": 4368
    },
    {
      "epoch": 0.01866822769341207,
      "grad_norm": 3.7421317100524902,
      "learning_rate": 0.00031336523671167323,
      "loss": 1.459,
      "step": 4369
    },
    {
      "epoch": 0.018672500576839262,
      "grad_norm": 2.0941407680511475,
      "learning_rate": 0.00031332250897282514,
      "loss": 0.7916,
      "step": 4370
    },
    {
      "epoch": 0.018676773460266458,
      "grad_norm": 3.280117988586426,
      "learning_rate": 0.0003132797812339771,
      "loss": 1.3551,
      "step": 4371
    },
    {
      "epoch": 0.01868104634369365,
      "grad_norm": 1.8344413042068481,
      "learning_rate": 0.000313237053495129,
      "loss": 0.6913,
      "step": 4372
    },
    {
      "epoch": 0.018685319227120845,
      "grad_norm": 3.425628423690796,
      "learning_rate": 0.000313194325756281,
      "loss": 0.6534,
      "step": 4373
    },
    {
      "epoch": 0.01868959211054804,
      "grad_norm": 1.889265537261963,
      "learning_rate": 0.00031315159801743295,
      "loss": 0.6534,
      "step": 4374
    },
    {
      "epoch": 0.018693864993975233,
      "grad_norm": 2.746065855026245,
      "learning_rate": 0.00031310887027858486,
      "loss": 0.7943,
      "step": 4375
    },
    {
      "epoch": 0.01869813787740243,
      "grad_norm": 0.6569533348083496,
      "learning_rate": 0.0003130661425397368,
      "loss": 0.2415,
      "step": 4376
    },
    {
      "epoch": 0.018702410760829624,
      "grad_norm": 0.5141692757606506,
      "learning_rate": 0.00031302341480088873,
      "loss": 0.1842,
      "step": 4377
    },
    {
      "epoch": 0.018706683644256816,
      "grad_norm": 3.015594720840454,
      "learning_rate": 0.0003129806870620407,
      "loss": 0.783,
      "step": 4378
    },
    {
      "epoch": 0.018710956527684012,
      "grad_norm": 1.5440787076950073,
      "learning_rate": 0.0003129379593231926,
      "loss": 0.3002,
      "step": 4379
    },
    {
      "epoch": 0.018715229411111207,
      "grad_norm": 0.45206695795059204,
      "learning_rate": 0.0003128952315843446,
      "loss": 0.1612,
      "step": 4380
    },
    {
      "epoch": 0.0187195022945384,
      "grad_norm": 1.590351939201355,
      "learning_rate": 0.00031285250384549654,
      "loss": 0.3041,
      "step": 4381
    },
    {
      "epoch": 0.018723775177965595,
      "grad_norm": 6.184281349182129,
      "learning_rate": 0.00031280977610664845,
      "loss": 1.9872,
      "step": 4382
    },
    {
      "epoch": 0.01872804806139279,
      "grad_norm": 2.630258560180664,
      "learning_rate": 0.0003127670483678004,
      "loss": 0.675,
      "step": 4383
    },
    {
      "epoch": 0.018732320944819983,
      "grad_norm": 0.7133097052574158,
      "learning_rate": 0.0003127243206289523,
      "loss": 0.3261,
      "step": 4384
    },
    {
      "epoch": 0.01873659382824718,
      "grad_norm": 2.0949525833129883,
      "learning_rate": 0.0003126815928901043,
      "loss": 0.6952,
      "step": 4385
    },
    {
      "epoch": 0.01874086671167437,
      "grad_norm": 2.825023889541626,
      "learning_rate": 0.0003126388651512562,
      "loss": 0.6291,
      "step": 4386
    },
    {
      "epoch": 0.018745139595101566,
      "grad_norm": 4.108597278594971,
      "learning_rate": 0.00031259613741240817,
      "loss": 0.9687,
      "step": 4387
    },
    {
      "epoch": 0.018749412478528762,
      "grad_norm": 2.0604026317596436,
      "learning_rate": 0.00031255340967356013,
      "loss": 0.63,
      "step": 4388
    },
    {
      "epoch": 0.018753685361955954,
      "grad_norm": 2.9096858501434326,
      "learning_rate": 0.00031251068193471204,
      "loss": 1.0524,
      "step": 4389
    },
    {
      "epoch": 0.01875795824538315,
      "grad_norm": 1.565906286239624,
      "learning_rate": 0.00031246795419586395,
      "loss": 0.5198,
      "step": 4390
    },
    {
      "epoch": 0.018762231128810345,
      "grad_norm": 1.968581199645996,
      "learning_rate": 0.00031242522645701586,
      "loss": 0.7424,
      "step": 4391
    },
    {
      "epoch": 0.018766504012237537,
      "grad_norm": 1.6953588724136353,
      "learning_rate": 0.00031238249871816783,
      "loss": 0.554,
      "step": 4392
    },
    {
      "epoch": 0.018770776895664733,
      "grad_norm": 2.061338424682617,
      "learning_rate": 0.00031233977097931974,
      "loss": 0.5169,
      "step": 4393
    },
    {
      "epoch": 0.01877504977909193,
      "grad_norm": 2.343075752258301,
      "learning_rate": 0.0003122970432404717,
      "loss": 0.6987,
      "step": 4394
    },
    {
      "epoch": 0.01877932266251912,
      "grad_norm": 1.4612525701522827,
      "learning_rate": 0.00031225431550162367,
      "loss": 0.6508,
      "step": 4395
    },
    {
      "epoch": 0.018783595545946316,
      "grad_norm": 4.0265793800354,
      "learning_rate": 0.0003122115877627756,
      "loss": 1.1803,
      "step": 4396
    },
    {
      "epoch": 0.018787868429373508,
      "grad_norm": 1.4395946264266968,
      "learning_rate": 0.00031216886002392755,
      "loss": 0.247,
      "step": 4397
    },
    {
      "epoch": 0.018792141312800704,
      "grad_norm": 0.5568955540657043,
      "learning_rate": 0.00031212613228507946,
      "loss": 0.1957,
      "step": 4398
    },
    {
      "epoch": 0.0187964141962279,
      "grad_norm": 1.4568114280700684,
      "learning_rate": 0.0003120834045462314,
      "loss": 0.4306,
      "step": 4399
    },
    {
      "epoch": 0.01880068707965509,
      "grad_norm": 2.8679840564727783,
      "learning_rate": 0.00031204067680738333,
      "loss": 0.8616,
      "step": 4400
    },
    {
      "epoch": 0.018804959963082287,
      "grad_norm": 1.5025219917297363,
      "learning_rate": 0.0003119979490685353,
      "loss": 0.2665,
      "step": 4401
    },
    {
      "epoch": 0.018809232846509483,
      "grad_norm": 2.983217477798462,
      "learning_rate": 0.0003119552213296872,
      "loss": 1.0469,
      "step": 4402
    },
    {
      "epoch": 0.018813505729936675,
      "grad_norm": 0.6675621271133423,
      "learning_rate": 0.00031191249359083917,
      "loss": 0.2892,
      "step": 4403
    },
    {
      "epoch": 0.01881777861336387,
      "grad_norm": 2.8492932319641113,
      "learning_rate": 0.00031186976585199114,
      "loss": 0.894,
      "step": 4404
    },
    {
      "epoch": 0.018822051496791066,
      "grad_norm": 3.8556272983551025,
      "learning_rate": 0.00031182703811314305,
      "loss": 0.9732,
      "step": 4405
    },
    {
      "epoch": 0.018826324380218258,
      "grad_norm": 3.9752750396728516,
      "learning_rate": 0.000311784310374295,
      "loss": 1.5419,
      "step": 4406
    },
    {
      "epoch": 0.018830597263645454,
      "grad_norm": 3.9681947231292725,
      "learning_rate": 0.0003117415826354469,
      "loss": 1.4458,
      "step": 4407
    },
    {
      "epoch": 0.01883487014707265,
      "grad_norm": 2.1961874961853027,
      "learning_rate": 0.0003116988548965989,
      "loss": 0.5316,
      "step": 4408
    },
    {
      "epoch": 0.01883914303049984,
      "grad_norm": 1.6127054691314697,
      "learning_rate": 0.0003116561271577508,
      "loss": 0.3777,
      "step": 4409
    },
    {
      "epoch": 0.018843415913927037,
      "grad_norm": 3.6465823650360107,
      "learning_rate": 0.00031161339941890276,
      "loss": 1.0554,
      "step": 4410
    },
    {
      "epoch": 0.01884768879735423,
      "grad_norm": 1.295425534248352,
      "learning_rate": 0.00031157067168005473,
      "loss": 0.2152,
      "step": 4411
    },
    {
      "epoch": 0.018851961680781425,
      "grad_norm": 1.6858913898468018,
      "learning_rate": 0.00031152794394120664,
      "loss": 0.4291,
      "step": 4412
    },
    {
      "epoch": 0.01885623456420862,
      "grad_norm": 0.6447777152061462,
      "learning_rate": 0.0003114852162023586,
      "loss": 0.2241,
      "step": 4413
    },
    {
      "epoch": 0.018860507447635812,
      "grad_norm": 1.2130560874938965,
      "learning_rate": 0.0003114424884635105,
      "loss": 0.3826,
      "step": 4414
    },
    {
      "epoch": 0.018864780331063008,
      "grad_norm": 1.2890552282333374,
      "learning_rate": 0.0003113997607246625,
      "loss": 0.3987,
      "step": 4415
    },
    {
      "epoch": 0.018869053214490204,
      "grad_norm": 3.564561128616333,
      "learning_rate": 0.0003113570329858144,
      "loss": 0.9636,
      "step": 4416
    },
    {
      "epoch": 0.018873326097917396,
      "grad_norm": 0.7949464917182922,
      "learning_rate": 0.00031131430524696636,
      "loss": 0.3051,
      "step": 4417
    },
    {
      "epoch": 0.01887759898134459,
      "grad_norm": 2.852409839630127,
      "learning_rate": 0.0003112715775081183,
      "loss": 0.7952,
      "step": 4418
    },
    {
      "epoch": 0.018881871864771787,
      "grad_norm": 1.1859952211380005,
      "learning_rate": 0.00031122884976927023,
      "loss": 0.1854,
      "step": 4419
    },
    {
      "epoch": 0.01888614474819898,
      "grad_norm": 1.298917531967163,
      "learning_rate": 0.0003111861220304222,
      "loss": 0.2451,
      "step": 4420
    },
    {
      "epoch": 0.018890417631626175,
      "grad_norm": 3.2417781352996826,
      "learning_rate": 0.0003111433942915741,
      "loss": 1.04,
      "step": 4421
    },
    {
      "epoch": 0.018894690515053367,
      "grad_norm": 1.1747264862060547,
      "learning_rate": 0.00031110066655272607,
      "loss": 0.3192,
      "step": 4422
    },
    {
      "epoch": 0.018898963398480562,
      "grad_norm": 1.8988163471221924,
      "learning_rate": 0.00031105793881387793,
      "loss": 0.5513,
      "step": 4423
    },
    {
      "epoch": 0.018903236281907758,
      "grad_norm": 6.850679874420166,
      "learning_rate": 0.0003110152110750299,
      "loss": 2.3967,
      "step": 4424
    },
    {
      "epoch": 0.01890750916533495,
      "grad_norm": 4.30838680267334,
      "learning_rate": 0.00031097248333618186,
      "loss": 1.9118,
      "step": 4425
    },
    {
      "epoch": 0.018911782048762146,
      "grad_norm": 2.534280300140381,
      "learning_rate": 0.00031092975559733377,
      "loss": 0.8341,
      "step": 4426
    },
    {
      "epoch": 0.01891605493218934,
      "grad_norm": 1.8871265649795532,
      "learning_rate": 0.00031088702785848573,
      "loss": 0.5055,
      "step": 4427
    },
    {
      "epoch": 0.018920327815616533,
      "grad_norm": 1.198651671409607,
      "learning_rate": 0.00031084430011963765,
      "loss": 0.3052,
      "step": 4428
    },
    {
      "epoch": 0.01892460069904373,
      "grad_norm": 2.7216293811798096,
      "learning_rate": 0.0003108015723807896,
      "loss": 0.5868,
      "step": 4429
    },
    {
      "epoch": 0.018928873582470924,
      "grad_norm": 6.6299943923950195,
      "learning_rate": 0.0003107588446419415,
      "loss": 2.2916,
      "step": 4430
    },
    {
      "epoch": 0.018933146465898117,
      "grad_norm": 3.1452441215515137,
      "learning_rate": 0.0003107161169030935,
      "loss": 1.1518,
      "step": 4431
    },
    {
      "epoch": 0.018937419349325312,
      "grad_norm": 4.966402530670166,
      "learning_rate": 0.0003106733891642454,
      "loss": 1.1097,
      "step": 4432
    },
    {
      "epoch": 0.018941692232752504,
      "grad_norm": 0.8714284896850586,
      "learning_rate": 0.00031063066142539736,
      "loss": 0.2055,
      "step": 4433
    },
    {
      "epoch": 0.0189459651161797,
      "grad_norm": 2.8817875385284424,
      "learning_rate": 0.0003105879336865493,
      "loss": 1.0174,
      "step": 4434
    },
    {
      "epoch": 0.018950237999606896,
      "grad_norm": 2.0464529991149902,
      "learning_rate": 0.00031054520594770124,
      "loss": 0.7576,
      "step": 4435
    },
    {
      "epoch": 0.018954510883034088,
      "grad_norm": 4.162163734436035,
      "learning_rate": 0.0003105024782088532,
      "loss": 1.6911,
      "step": 4436
    },
    {
      "epoch": 0.018958783766461283,
      "grad_norm": 4.816715240478516,
      "learning_rate": 0.0003104597504700051,
      "loss": 1.3062,
      "step": 4437
    },
    {
      "epoch": 0.01896305664988848,
      "grad_norm": 2.2666704654693604,
      "learning_rate": 0.0003104170227311571,
      "loss": 0.7634,
      "step": 4438
    },
    {
      "epoch": 0.01896732953331567,
      "grad_norm": 3.8645856380462646,
      "learning_rate": 0.000310374294992309,
      "loss": 1.0686,
      "step": 4439
    },
    {
      "epoch": 0.018971602416742867,
      "grad_norm": 1.2022475004196167,
      "learning_rate": 0.00031033156725346095,
      "loss": 0.1564,
      "step": 4440
    },
    {
      "epoch": 0.018975875300170062,
      "grad_norm": 2.3111326694488525,
      "learning_rate": 0.0003102888395146129,
      "loss": 0.6655,
      "step": 4441
    },
    {
      "epoch": 0.018980148183597254,
      "grad_norm": 3.820268154144287,
      "learning_rate": 0.00031024611177576483,
      "loss": 0.9687,
      "step": 4442
    },
    {
      "epoch": 0.01898442106702445,
      "grad_norm": 0.3891761302947998,
      "learning_rate": 0.0003102033840369168,
      "loss": 0.091,
      "step": 4443
    },
    {
      "epoch": 0.018988693950451645,
      "grad_norm": 3.5969300270080566,
      "learning_rate": 0.0003101606562980687,
      "loss": 0.7727,
      "step": 4444
    },
    {
      "epoch": 0.018992966833878838,
      "grad_norm": 3.0018510818481445,
      "learning_rate": 0.00031011792855922067,
      "loss": 0.9098,
      "step": 4445
    },
    {
      "epoch": 0.018997239717306033,
      "grad_norm": 1.431021809577942,
      "learning_rate": 0.0003100752008203726,
      "loss": 0.6495,
      "step": 4446
    },
    {
      "epoch": 0.019001512600733225,
      "grad_norm": 1.4705885648727417,
      "learning_rate": 0.00031003247308152455,
      "loss": 0.2748,
      "step": 4447
    },
    {
      "epoch": 0.01900578548416042,
      "grad_norm": 0.3840979039669037,
      "learning_rate": 0.0003099897453426765,
      "loss": 0.0769,
      "step": 4448
    },
    {
      "epoch": 0.019010058367587616,
      "grad_norm": 0.3182859420776367,
      "learning_rate": 0.0003099470176038284,
      "loss": 0.0631,
      "step": 4449
    },
    {
      "epoch": 0.01901433125101481,
      "grad_norm": 2.104156494140625,
      "learning_rate": 0.0003099042898649804,
      "loss": 0.6714,
      "step": 4450
    },
    {
      "epoch": 0.019018604134442004,
      "grad_norm": 3.757096290588379,
      "learning_rate": 0.0003098615621261323,
      "loss": 1.6054,
      "step": 4451
    },
    {
      "epoch": 0.0190228770178692,
      "grad_norm": 3.868359088897705,
      "learning_rate": 0.00030981883438728426,
      "loss": 1.3256,
      "step": 4452
    },
    {
      "epoch": 0.019027149901296392,
      "grad_norm": 3.5674195289611816,
      "learning_rate": 0.00030977610664843617,
      "loss": 1.2695,
      "step": 4453
    },
    {
      "epoch": 0.019031422784723587,
      "grad_norm": 1.9329167604446411,
      "learning_rate": 0.00030973337890958814,
      "loss": 0.5658,
      "step": 4454
    },
    {
      "epoch": 0.019035695668150783,
      "grad_norm": 0.24950142204761505,
      "learning_rate": 0.00030969065117074005,
      "loss": 0.0376,
      "step": 4455
    },
    {
      "epoch": 0.019039968551577975,
      "grad_norm": 3.378467559814453,
      "learning_rate": 0.00030964792343189196,
      "loss": 1.2412,
      "step": 4456
    },
    {
      "epoch": 0.01904424143500517,
      "grad_norm": 5.229193687438965,
      "learning_rate": 0.0003096051956930439,
      "loss": 1.6285,
      "step": 4457
    },
    {
      "epoch": 0.019048514318432363,
      "grad_norm": 1.9026107788085938,
      "learning_rate": 0.00030956246795419583,
      "loss": 0.6836,
      "step": 4458
    },
    {
      "epoch": 0.01905278720185956,
      "grad_norm": 4.156215667724609,
      "learning_rate": 0.0003095197402153478,
      "loss": 1.4667,
      "step": 4459
    },
    {
      "epoch": 0.019057060085286754,
      "grad_norm": 3.0563344955444336,
      "learning_rate": 0.0003094770124764997,
      "loss": 0.6549,
      "step": 4460
    },
    {
      "epoch": 0.019061332968713946,
      "grad_norm": 3.079956531524658,
      "learning_rate": 0.0003094342847376517,
      "loss": 0.8027,
      "step": 4461
    },
    {
      "epoch": 0.01906560585214114,
      "grad_norm": 4.7130818367004395,
      "learning_rate": 0.00030939155699880364,
      "loss": 1.2865,
      "step": 4462
    },
    {
      "epoch": 0.019069878735568337,
      "grad_norm": 3.049070358276367,
      "learning_rate": 0.00030934882925995555,
      "loss": 0.8677,
      "step": 4463
    },
    {
      "epoch": 0.01907415161899553,
      "grad_norm": 2.712871551513672,
      "learning_rate": 0.0003093061015211075,
      "loss": 0.5932,
      "step": 4464
    },
    {
      "epoch": 0.019078424502422725,
      "grad_norm": 4.725494384765625,
      "learning_rate": 0.0003092633737822594,
      "loss": 1.1238,
      "step": 4465
    },
    {
      "epoch": 0.01908269738584992,
      "grad_norm": 2.661450147628784,
      "learning_rate": 0.0003092206460434114,
      "loss": 1.0614,
      "step": 4466
    },
    {
      "epoch": 0.019086970269277113,
      "grad_norm": 1.9343985319137573,
      "learning_rate": 0.0003091779183045633,
      "loss": 0.6266,
      "step": 4467
    },
    {
      "epoch": 0.01909124315270431,
      "grad_norm": 3.7062668800354004,
      "learning_rate": 0.00030913519056571527,
      "loss": 0.9047,
      "step": 4468
    },
    {
      "epoch": 0.019095516036131504,
      "grad_norm": 1.1979304552078247,
      "learning_rate": 0.0003090924628268672,
      "loss": 0.5542,
      "step": 4469
    },
    {
      "epoch": 0.019099788919558696,
      "grad_norm": 2.287014961242676,
      "learning_rate": 0.00030904973508801914,
      "loss": 0.9478,
      "step": 4470
    },
    {
      "epoch": 0.01910406180298589,
      "grad_norm": 2.6673872470855713,
      "learning_rate": 0.0003090070073491711,
      "loss": 0.9291,
      "step": 4471
    },
    {
      "epoch": 0.019108334686413084,
      "grad_norm": 2.697735071182251,
      "learning_rate": 0.000308964279610323,
      "loss": 0.8754,
      "step": 4472
    },
    {
      "epoch": 0.01911260756984028,
      "grad_norm": 2.910830020904541,
      "learning_rate": 0.000308921551871475,
      "loss": 0.9039,
      "step": 4473
    },
    {
      "epoch": 0.019116880453267475,
      "grad_norm": 1.642507791519165,
      "learning_rate": 0.0003088788241326269,
      "loss": 0.5255,
      "step": 4474
    },
    {
      "epoch": 0.019121153336694667,
      "grad_norm": 1.1406185626983643,
      "learning_rate": 0.00030883609639377886,
      "loss": 0.4771,
      "step": 4475
    },
    {
      "epoch": 0.019125426220121863,
      "grad_norm": 2.806788206100464,
      "learning_rate": 0.00030879336865493077,
      "loss": 0.6119,
      "step": 4476
    },
    {
      "epoch": 0.019129699103549058,
      "grad_norm": 1.231824278831482,
      "learning_rate": 0.00030875064091608274,
      "loss": 0.5695,
      "step": 4477
    },
    {
      "epoch": 0.01913397198697625,
      "grad_norm": 1.8431463241577148,
      "learning_rate": 0.0003087079131772347,
      "loss": 0.5765,
      "step": 4478
    },
    {
      "epoch": 0.019138244870403446,
      "grad_norm": 2.267564535140991,
      "learning_rate": 0.0003086651854383866,
      "loss": 0.6141,
      "step": 4479
    },
    {
      "epoch": 0.01914251775383064,
      "grad_norm": 1.676928997039795,
      "learning_rate": 0.0003086224576995386,
      "loss": 0.4199,
      "step": 4480
    },
    {
      "epoch": 0.019146790637257834,
      "grad_norm": 2.2836055755615234,
      "learning_rate": 0.0003085797299606905,
      "loss": 0.7836,
      "step": 4481
    },
    {
      "epoch": 0.01915106352068503,
      "grad_norm": 1.6978636980056763,
      "learning_rate": 0.00030853700222184245,
      "loss": 0.4478,
      "step": 4482
    },
    {
      "epoch": 0.01915533640411222,
      "grad_norm": 1.0273282527923584,
      "learning_rate": 0.00030849427448299436,
      "loss": 0.4739,
      "step": 4483
    },
    {
      "epoch": 0.019159609287539417,
      "grad_norm": 1.1170568466186523,
      "learning_rate": 0.0003084515467441463,
      "loss": 0.2282,
      "step": 4484
    },
    {
      "epoch": 0.019163882170966613,
      "grad_norm": 1.658435344696045,
      "learning_rate": 0.0003084088190052983,
      "loss": 0.3707,
      "step": 4485
    },
    {
      "epoch": 0.019168155054393805,
      "grad_norm": 4.444296836853027,
      "learning_rate": 0.0003083660912664502,
      "loss": 0.9128,
      "step": 4486
    },
    {
      "epoch": 0.019172427937821,
      "grad_norm": 2.5057179927825928,
      "learning_rate": 0.00030832336352760217,
      "loss": 0.9866,
      "step": 4487
    },
    {
      "epoch": 0.019176700821248196,
      "grad_norm": 1.3786616325378418,
      "learning_rate": 0.000308280635788754,
      "loss": 0.3394,
      "step": 4488
    },
    {
      "epoch": 0.019180973704675388,
      "grad_norm": 1.548100471496582,
      "learning_rate": 0.000308237908049906,
      "loss": 0.2834,
      "step": 4489
    },
    {
      "epoch": 0.019185246588102584,
      "grad_norm": 0.563675045967102,
      "learning_rate": 0.0003081951803110579,
      "loss": 0.1501,
      "step": 4490
    },
    {
      "epoch": 0.01918951947152978,
      "grad_norm": 1.0572623014450073,
      "learning_rate": 0.00030815245257220986,
      "loss": 0.4531,
      "step": 4491
    },
    {
      "epoch": 0.01919379235495697,
      "grad_norm": 1.090116262435913,
      "learning_rate": 0.00030810972483336183,
      "loss": 0.2396,
      "step": 4492
    },
    {
      "epoch": 0.019198065238384167,
      "grad_norm": 2.022303342819214,
      "learning_rate": 0.00030806699709451374,
      "loss": 0.5961,
      "step": 4493
    },
    {
      "epoch": 0.019202338121811362,
      "grad_norm": 2.034745454788208,
      "learning_rate": 0.0003080242693556657,
      "loss": 0.5585,
      "step": 4494
    },
    {
      "epoch": 0.019206611005238555,
      "grad_norm": 0.6519643068313599,
      "learning_rate": 0.0003079815416168176,
      "loss": 0.3181,
      "step": 4495
    },
    {
      "epoch": 0.01921088388866575,
      "grad_norm": 1.8734490871429443,
      "learning_rate": 0.0003079388138779696,
      "loss": 0.4828,
      "step": 4496
    },
    {
      "epoch": 0.019215156772092942,
      "grad_norm": 1.49514639377594,
      "learning_rate": 0.0003078960861391215,
      "loss": 0.4764,
      "step": 4497
    },
    {
      "epoch": 0.019219429655520138,
      "grad_norm": 2.3677191734313965,
      "learning_rate": 0.00030785335840027346,
      "loss": 1.0437,
      "step": 4498
    },
    {
      "epoch": 0.019223702538947333,
      "grad_norm": 1.0723700523376465,
      "learning_rate": 0.0003078106306614254,
      "loss": 0.2275,
      "step": 4499
    },
    {
      "epoch": 0.019227975422374526,
      "grad_norm": 1.8681050539016724,
      "learning_rate": 0.00030776790292257733,
      "loss": 0.5104,
      "step": 4500
    },
    {
      "epoch": 0.01923224830580172,
      "grad_norm": 0.8099208474159241,
      "learning_rate": 0.0003077251751837293,
      "loss": 0.3434,
      "step": 4501
    },
    {
      "epoch": 0.019236521189228917,
      "grad_norm": 5.190368175506592,
      "learning_rate": 0.0003076824474448812,
      "loss": 1.3282,
      "step": 4502
    },
    {
      "epoch": 0.01924079407265611,
      "grad_norm": 2.3128437995910645,
      "learning_rate": 0.0003076397197060332,
      "loss": 0.9187,
      "step": 4503
    },
    {
      "epoch": 0.019245066956083304,
      "grad_norm": 4.695009231567383,
      "learning_rate": 0.0003075969919671851,
      "loss": 1.3961,
      "step": 4504
    },
    {
      "epoch": 0.0192493398395105,
      "grad_norm": 5.26858377456665,
      "learning_rate": 0.00030755426422833705,
      "loss": 0.9453,
      "step": 4505
    },
    {
      "epoch": 0.019253612722937692,
      "grad_norm": 2.3211004734039307,
      "learning_rate": 0.00030751153648948896,
      "loss": 0.8483,
      "step": 4506
    },
    {
      "epoch": 0.019257885606364888,
      "grad_norm": 0.6258264183998108,
      "learning_rate": 0.0003074688087506409,
      "loss": 0.2771,
      "step": 4507
    },
    {
      "epoch": 0.01926215848979208,
      "grad_norm": 3.1147470474243164,
      "learning_rate": 0.0003074260810117929,
      "loss": 0.9436,
      "step": 4508
    },
    {
      "epoch": 0.019266431373219275,
      "grad_norm": 2.6187667846679688,
      "learning_rate": 0.0003073833532729448,
      "loss": 0.953,
      "step": 4509
    },
    {
      "epoch": 0.01927070425664647,
      "grad_norm": 1.096258282661438,
      "learning_rate": 0.00030734062553409677,
      "loss": 0.3337,
      "step": 4510
    },
    {
      "epoch": 0.019274977140073663,
      "grad_norm": 4.778893947601318,
      "learning_rate": 0.0003072978977952487,
      "loss": 1.3898,
      "step": 4511
    },
    {
      "epoch": 0.01927925002350086,
      "grad_norm": 2.0830931663513184,
      "learning_rate": 0.00030725517005640064,
      "loss": 0.5664,
      "step": 4512
    },
    {
      "epoch": 0.019283522906928054,
      "grad_norm": 1.9019966125488281,
      "learning_rate": 0.00030721244231755255,
      "loss": 0.5504,
      "step": 4513
    },
    {
      "epoch": 0.019287795790355246,
      "grad_norm": 3.0445733070373535,
      "learning_rate": 0.0003071697145787045,
      "loss": 0.8477,
      "step": 4514
    },
    {
      "epoch": 0.019292068673782442,
      "grad_norm": 1.8722816705703735,
      "learning_rate": 0.0003071269868398565,
      "loss": 0.4442,
      "step": 4515
    },
    {
      "epoch": 0.019296341557209638,
      "grad_norm": 4.196455001831055,
      "learning_rate": 0.0003070842591010084,
      "loss": 1.0806,
      "step": 4516
    },
    {
      "epoch": 0.01930061444063683,
      "grad_norm": 2.478132724761963,
      "learning_rate": 0.00030704153136216036,
      "loss": 0.7756,
      "step": 4517
    },
    {
      "epoch": 0.019304887324064025,
      "grad_norm": 2.0971932411193848,
      "learning_rate": 0.00030699880362331227,
      "loss": 0.4895,
      "step": 4518
    },
    {
      "epoch": 0.01930916020749122,
      "grad_norm": 29.870662689208984,
      "learning_rate": 0.00030695607588446423,
      "loss": 1.3062,
      "step": 4519
    },
    {
      "epoch": 0.019313433090918413,
      "grad_norm": 1.7293587923049927,
      "learning_rate": 0.00030691334814561614,
      "loss": 0.4045,
      "step": 4520
    },
    {
      "epoch": 0.01931770597434561,
      "grad_norm": 0.923790454864502,
      "learning_rate": 0.00030687062040676805,
      "loss": 0.2903,
      "step": 4521
    },
    {
      "epoch": 0.0193219788577728,
      "grad_norm": 1.0696018934249878,
      "learning_rate": 0.00030682789266792,
      "loss": 0.3064,
      "step": 4522
    },
    {
      "epoch": 0.019326251741199996,
      "grad_norm": 4.397649765014648,
      "learning_rate": 0.00030678516492907193,
      "loss": 1.3484,
      "step": 4523
    },
    {
      "epoch": 0.019330524624627192,
      "grad_norm": 4.747227668762207,
      "learning_rate": 0.0003067424371902239,
      "loss": 1.8517,
      "step": 4524
    },
    {
      "epoch": 0.019334797508054384,
      "grad_norm": 2.2837698459625244,
      "learning_rate": 0.0003066997094513758,
      "loss": 0.4385,
      "step": 4525
    },
    {
      "epoch": 0.01933907039148158,
      "grad_norm": 0.9041438698768616,
      "learning_rate": 0.00030665698171252777,
      "loss": 0.2063,
      "step": 4526
    },
    {
      "epoch": 0.019343343274908775,
      "grad_norm": 3.0954129695892334,
      "learning_rate": 0.0003066142539736797,
      "loss": 1.5832,
      "step": 4527
    },
    {
      "epoch": 0.019347616158335967,
      "grad_norm": 3.176300525665283,
      "learning_rate": 0.00030657152623483165,
      "loss": 0.7874,
      "step": 4528
    },
    {
      "epoch": 0.019351889041763163,
      "grad_norm": 0.964904248714447,
      "learning_rate": 0.0003065287984959836,
      "loss": 0.2136,
      "step": 4529
    },
    {
      "epoch": 0.01935616192519036,
      "grad_norm": 4.128872871398926,
      "learning_rate": 0.0003064860707571355,
      "loss": 1.0721,
      "step": 4530
    },
    {
      "epoch": 0.01936043480861755,
      "grad_norm": 0.777081310749054,
      "learning_rate": 0.0003064433430182875,
      "loss": 0.2387,
      "step": 4531
    },
    {
      "epoch": 0.019364707692044746,
      "grad_norm": 1.9836692810058594,
      "learning_rate": 0.0003064006152794394,
      "loss": 0.5285,
      "step": 4532
    },
    {
      "epoch": 0.01936898057547194,
      "grad_norm": 1.1540460586547852,
      "learning_rate": 0.00030635788754059136,
      "loss": 0.2654,
      "step": 4533
    },
    {
      "epoch": 0.019373253458899134,
      "grad_norm": 3.124417781829834,
      "learning_rate": 0.0003063151598017433,
      "loss": 1.5119,
      "step": 4534
    },
    {
      "epoch": 0.01937752634232633,
      "grad_norm": 0.561276376247406,
      "learning_rate": 0.00030627243206289524,
      "loss": 0.1766,
      "step": 4535
    },
    {
      "epoch": 0.01938179922575352,
      "grad_norm": 0.962041437625885,
      "learning_rate": 0.00030622970432404715,
      "loss": 0.2031,
      "step": 4536
    },
    {
      "epoch": 0.019386072109180717,
      "grad_norm": 1.0378613471984863,
      "learning_rate": 0.0003061869765851991,
      "loss": 0.2517,
      "step": 4537
    },
    {
      "epoch": 0.019390344992607913,
      "grad_norm": 4.736783981323242,
      "learning_rate": 0.0003061442488463511,
      "loss": 1.4754,
      "step": 4538
    },
    {
      "epoch": 0.019394617876035105,
      "grad_norm": 2.6347105503082275,
      "learning_rate": 0.000306101521107503,
      "loss": 1.0782,
      "step": 4539
    },
    {
      "epoch": 0.0193988907594623,
      "grad_norm": 2.742558479309082,
      "learning_rate": 0.00030605879336865495,
      "loss": 0.8451,
      "step": 4540
    },
    {
      "epoch": 0.019403163642889496,
      "grad_norm": 1.8477264642715454,
      "learning_rate": 0.00030601606562980687,
      "loss": 0.4501,
      "step": 4541
    },
    {
      "epoch": 0.01940743652631669,
      "grad_norm": 2.9548280239105225,
      "learning_rate": 0.00030597333789095883,
      "loss": 0.7528,
      "step": 4542
    },
    {
      "epoch": 0.019411709409743884,
      "grad_norm": 2.1003198623657227,
      "learning_rate": 0.00030593061015211074,
      "loss": 0.5412,
      "step": 4543
    },
    {
      "epoch": 0.019415982293171076,
      "grad_norm": 4.676516056060791,
      "learning_rate": 0.0003058878824132627,
      "loss": 1.2012,
      "step": 4544
    },
    {
      "epoch": 0.01942025517659827,
      "grad_norm": 0.5630630850791931,
      "learning_rate": 0.00030584515467441467,
      "loss": 0.1709,
      "step": 4545
    },
    {
      "epoch": 0.019424528060025467,
      "grad_norm": 4.683176517486572,
      "learning_rate": 0.0003058024269355666,
      "loss": 0.9669,
      "step": 4546
    },
    {
      "epoch": 0.01942880094345266,
      "grad_norm": 3.111067771911621,
      "learning_rate": 0.00030575969919671855,
      "loss": 1.3269,
      "step": 4547
    },
    {
      "epoch": 0.019433073826879855,
      "grad_norm": 0.513774573802948,
      "learning_rate": 0.00030571697145787046,
      "loss": 0.1542,
      "step": 4548
    },
    {
      "epoch": 0.01943734671030705,
      "grad_norm": 2.9557292461395264,
      "learning_rate": 0.0003056742437190224,
      "loss": 1.2508,
      "step": 4549
    },
    {
      "epoch": 0.019441619593734243,
      "grad_norm": 4.573948860168457,
      "learning_rate": 0.00030563151598017433,
      "loss": 1.1276,
      "step": 4550
    },
    {
      "epoch": 0.019445892477161438,
      "grad_norm": 4.388671875,
      "learning_rate": 0.0003055887882413263,
      "loss": 0.8132,
      "step": 4551
    },
    {
      "epoch": 0.019450165360588634,
      "grad_norm": 2.1623077392578125,
      "learning_rate": 0.00030554606050247826,
      "loss": 0.5896,
      "step": 4552
    },
    {
      "epoch": 0.019454438244015826,
      "grad_norm": 2.874298334121704,
      "learning_rate": 0.0003055033327636302,
      "loss": 1.0712,
      "step": 4553
    },
    {
      "epoch": 0.01945871112744302,
      "grad_norm": 0.44607970118522644,
      "learning_rate": 0.0003054606050247821,
      "loss": 0.1297,
      "step": 4554
    },
    {
      "epoch": 0.019462984010870217,
      "grad_norm": 1.472243070602417,
      "learning_rate": 0.000305417877285934,
      "loss": 0.3332,
      "step": 4555
    },
    {
      "epoch": 0.01946725689429741,
      "grad_norm": 5.700464725494385,
      "learning_rate": 0.00030537514954708596,
      "loss": 2.1465,
      "step": 4556
    },
    {
      "epoch": 0.019471529777724605,
      "grad_norm": 1.6753941774368286,
      "learning_rate": 0.00030533242180823787,
      "loss": 0.4538,
      "step": 4557
    },
    {
      "epoch": 0.019475802661151797,
      "grad_norm": 0.7303038239479065,
      "learning_rate": 0.00030528969406938984,
      "loss": 0.2712,
      "step": 4558
    },
    {
      "epoch": 0.019480075544578992,
      "grad_norm": 3.639556407928467,
      "learning_rate": 0.0003052469663305418,
      "loss": 1.0414,
      "step": 4559
    },
    {
      "epoch": 0.019484348428006188,
      "grad_norm": 4.726082801818848,
      "learning_rate": 0.0003052042385916937,
      "loss": 1.1107,
      "step": 4560
    },
    {
      "epoch": 0.01948862131143338,
      "grad_norm": 2.3228225708007812,
      "learning_rate": 0.0003051615108528457,
      "loss": 0.6488,
      "step": 4561
    },
    {
      "epoch": 0.019492894194860576,
      "grad_norm": 0.7038305401802063,
      "learning_rate": 0.0003051187831139976,
      "loss": 0.2744,
      "step": 4562
    },
    {
      "epoch": 0.01949716707828777,
      "grad_norm": 0.7859911918640137,
      "learning_rate": 0.00030507605537514955,
      "loss": 0.1566,
      "step": 4563
    },
    {
      "epoch": 0.019501439961714963,
      "grad_norm": 1.31950044631958,
      "learning_rate": 0.00030503332763630146,
      "loss": 0.3982,
      "step": 4564
    },
    {
      "epoch": 0.01950571284514216,
      "grad_norm": 0.6607264876365662,
      "learning_rate": 0.00030499059989745343,
      "loss": 0.2592,
      "step": 4565
    },
    {
      "epoch": 0.019509985728569355,
      "grad_norm": 4.401561737060547,
      "learning_rate": 0.0003049478721586054,
      "loss": 1.0039,
      "step": 4566
    },
    {
      "epoch": 0.019514258611996547,
      "grad_norm": 3.2824888229370117,
      "learning_rate": 0.0003049051444197573,
      "loss": 1.5037,
      "step": 4567
    },
    {
      "epoch": 0.019518531495423742,
      "grad_norm": 2.615719795227051,
      "learning_rate": 0.00030486241668090927,
      "loss": 0.5346,
      "step": 4568
    },
    {
      "epoch": 0.019522804378850935,
      "grad_norm": 4.228556156158447,
      "learning_rate": 0.0003048196889420612,
      "loss": 1.0182,
      "step": 4569
    },
    {
      "epoch": 0.01952707726227813,
      "grad_norm": 2.0038414001464844,
      "learning_rate": 0.00030477696120321314,
      "loss": 0.7493,
      "step": 4570
    },
    {
      "epoch": 0.019531350145705326,
      "grad_norm": 1.7447251081466675,
      "learning_rate": 0.00030473423346436505,
      "loss": 0.5256,
      "step": 4571
    },
    {
      "epoch": 0.019535623029132518,
      "grad_norm": 2.6480460166931152,
      "learning_rate": 0.000304691505725517,
      "loss": 1.0189,
      "step": 4572
    },
    {
      "epoch": 0.019539895912559713,
      "grad_norm": 4.381050109863281,
      "learning_rate": 0.00030464877798666893,
      "loss": 1.3936,
      "step": 4573
    },
    {
      "epoch": 0.01954416879598691,
      "grad_norm": 2.055839776992798,
      "learning_rate": 0.0003046060502478209,
      "loss": 0.4641,
      "step": 4574
    },
    {
      "epoch": 0.0195484416794141,
      "grad_norm": 2.741142749786377,
      "learning_rate": 0.00030456332250897286,
      "loss": 1.3699,
      "step": 4575
    },
    {
      "epoch": 0.019552714562841297,
      "grad_norm": 4.075748443603516,
      "learning_rate": 0.00030452059477012477,
      "loss": 0.9409,
      "step": 4576
    },
    {
      "epoch": 0.019556987446268492,
      "grad_norm": 4.576905250549316,
      "learning_rate": 0.00030447786703127674,
      "loss": 1.3173,
      "step": 4577
    },
    {
      "epoch": 0.019561260329695684,
      "grad_norm": 4.000234603881836,
      "learning_rate": 0.00030443513929242865,
      "loss": 0.8197,
      "step": 4578
    },
    {
      "epoch": 0.01956553321312288,
      "grad_norm": 3.8210060596466064,
      "learning_rate": 0.0003043924115535806,
      "loss": 0.7482,
      "step": 4579
    },
    {
      "epoch": 0.019569806096550076,
      "grad_norm": 0.6046057343482971,
      "learning_rate": 0.0003043496838147325,
      "loss": 0.1975,
      "step": 4580
    },
    {
      "epoch": 0.019574078979977268,
      "grad_norm": 1.73733651638031,
      "learning_rate": 0.0003043069560758845,
      "loss": 0.4912,
      "step": 4581
    },
    {
      "epoch": 0.019578351863404463,
      "grad_norm": 0.5911709070205688,
      "learning_rate": 0.00030426422833703645,
      "loss": 0.209,
      "step": 4582
    },
    {
      "epoch": 0.019582624746831655,
      "grad_norm": 2.1204946041107178,
      "learning_rate": 0.00030422150059818836,
      "loss": 0.4857,
      "step": 4583
    },
    {
      "epoch": 0.01958689763025885,
      "grad_norm": 1.691197156906128,
      "learning_rate": 0.00030417877285934033,
      "loss": 0.4895,
      "step": 4584
    },
    {
      "epoch": 0.019591170513686047,
      "grad_norm": 2.872979164123535,
      "learning_rate": 0.00030413604512049224,
      "loss": 1.0528,
      "step": 4585
    },
    {
      "epoch": 0.01959544339711324,
      "grad_norm": 3.430760145187378,
      "learning_rate": 0.0003040933173816442,
      "loss": 1.5023,
      "step": 4586
    },
    {
      "epoch": 0.019599716280540434,
      "grad_norm": 1.4130741357803345,
      "learning_rate": 0.00030405058964279606,
      "loss": 0.3995,
      "step": 4587
    },
    {
      "epoch": 0.01960398916396763,
      "grad_norm": 0.9356618523597717,
      "learning_rate": 0.000304007861903948,
      "loss": 0.2092,
      "step": 4588
    },
    {
      "epoch": 0.019608262047394822,
      "grad_norm": 2.7281739711761475,
      "learning_rate": 0.0003039651341651,
      "loss": 0.9121,
      "step": 4589
    },
    {
      "epoch": 0.019612534930822018,
      "grad_norm": 1.963030219078064,
      "learning_rate": 0.0003039224064262519,
      "loss": 0.3841,
      "step": 4590
    },
    {
      "epoch": 0.019616807814249213,
      "grad_norm": 3.9236183166503906,
      "learning_rate": 0.00030387967868740387,
      "loss": 0.8769,
      "step": 4591
    },
    {
      "epoch": 0.019621080697676405,
      "grad_norm": 0.814247727394104,
      "learning_rate": 0.0003038369509485558,
      "loss": 0.1831,
      "step": 4592
    },
    {
      "epoch": 0.0196253535811036,
      "grad_norm": 1.2152643203735352,
      "learning_rate": 0.00030379422320970774,
      "loss": 0.3686,
      "step": 4593
    },
    {
      "epoch": 0.019629626464530793,
      "grad_norm": 1.4408077001571655,
      "learning_rate": 0.00030375149547085965,
      "loss": 0.2523,
      "step": 4594
    },
    {
      "epoch": 0.01963389934795799,
      "grad_norm": 2.2676398754119873,
      "learning_rate": 0.0003037087677320116,
      "loss": 0.6864,
      "step": 4595
    },
    {
      "epoch": 0.019638172231385184,
      "grad_norm": 1.721380591392517,
      "learning_rate": 0.0003036660399931636,
      "loss": 0.5374,
      "step": 4596
    },
    {
      "epoch": 0.019642445114812376,
      "grad_norm": 0.8835619688034058,
      "learning_rate": 0.0003036233122543155,
      "loss": 0.3259,
      "step": 4597
    },
    {
      "epoch": 0.019646717998239572,
      "grad_norm": 3.761871099472046,
      "learning_rate": 0.00030358058451546746,
      "loss": 1.3882,
      "step": 4598
    },
    {
      "epoch": 0.019650990881666767,
      "grad_norm": 3.1476564407348633,
      "learning_rate": 0.00030353785677661937,
      "loss": 0.6403,
      "step": 4599
    },
    {
      "epoch": 0.01965526376509396,
      "grad_norm": 3.146853446960449,
      "learning_rate": 0.00030349512903777133,
      "loss": 0.6402,
      "step": 4600
    },
    {
      "epoch": 0.019659536648521155,
      "grad_norm": 2.1516005992889404,
      "learning_rate": 0.00030345240129892324,
      "loss": 1.1387,
      "step": 4601
    },
    {
      "epoch": 0.01966380953194835,
      "grad_norm": 3.4973535537719727,
      "learning_rate": 0.0003034096735600752,
      "loss": 0.9769,
      "step": 4602
    },
    {
      "epoch": 0.019668082415375543,
      "grad_norm": 2.1207308769226074,
      "learning_rate": 0.0003033669458212272,
      "loss": 0.5846,
      "step": 4603
    },
    {
      "epoch": 0.01967235529880274,
      "grad_norm": 0.9725799560546875,
      "learning_rate": 0.0003033242180823791,
      "loss": 0.2632,
      "step": 4604
    },
    {
      "epoch": 0.019676628182229934,
      "grad_norm": 5.087978363037109,
      "learning_rate": 0.00030328149034353105,
      "loss": 1.5125,
      "step": 4605
    },
    {
      "epoch": 0.019680901065657126,
      "grad_norm": 3.0014402866363525,
      "learning_rate": 0.00030323876260468296,
      "loss": 0.6296,
      "step": 4606
    },
    {
      "epoch": 0.019685173949084322,
      "grad_norm": 1.7316209077835083,
      "learning_rate": 0.0003031960348658349,
      "loss": 0.4233,
      "step": 4607
    },
    {
      "epoch": 0.019689446832511514,
      "grad_norm": 1.6829906702041626,
      "learning_rate": 0.00030315330712698684,
      "loss": 0.3893,
      "step": 4608
    },
    {
      "epoch": 0.01969371971593871,
      "grad_norm": 0.9869527816772461,
      "learning_rate": 0.0003031105793881388,
      "loss": 0.2519,
      "step": 4609
    },
    {
      "epoch": 0.019697992599365905,
      "grad_norm": 3.907909631729126,
      "learning_rate": 0.0003030678516492907,
      "loss": 0.8748,
      "step": 4610
    },
    {
      "epoch": 0.019702265482793097,
      "grad_norm": 1.3740041255950928,
      "learning_rate": 0.0003030251239104427,
      "loss": 0.2167,
      "step": 4611
    },
    {
      "epoch": 0.019706538366220293,
      "grad_norm": 1.7262343168258667,
      "learning_rate": 0.00030298239617159464,
      "loss": 0.651,
      "step": 4612
    },
    {
      "epoch": 0.01971081124964749,
      "grad_norm": 2.7437286376953125,
      "learning_rate": 0.00030293966843274655,
      "loss": 0.9155,
      "step": 4613
    },
    {
      "epoch": 0.01971508413307468,
      "grad_norm": 2.2866501808166504,
      "learning_rate": 0.0003028969406938985,
      "loss": 1.1947,
      "step": 4614
    },
    {
      "epoch": 0.019719357016501876,
      "grad_norm": 1.2038602828979492,
      "learning_rate": 0.00030285421295505043,
      "loss": 0.1947,
      "step": 4615
    },
    {
      "epoch": 0.01972362989992907,
      "grad_norm": 3.9280762672424316,
      "learning_rate": 0.0003028114852162024,
      "loss": 0.8495,
      "step": 4616
    },
    {
      "epoch": 0.019727902783356264,
      "grad_norm": 2.321643590927124,
      "learning_rate": 0.0003027687574773543,
      "loss": 0.7332,
      "step": 4617
    },
    {
      "epoch": 0.01973217566678346,
      "grad_norm": 3.7480735778808594,
      "learning_rate": 0.00030272602973850627,
      "loss": 1.1276,
      "step": 4618
    },
    {
      "epoch": 0.01973644855021065,
      "grad_norm": 4.657224655151367,
      "learning_rate": 0.00030268330199965823,
      "loss": 1.2602,
      "step": 4619
    },
    {
      "epoch": 0.019740721433637847,
      "grad_norm": 2.1141183376312256,
      "learning_rate": 0.0003026405742608101,
      "loss": 0.6497,
      "step": 4620
    },
    {
      "epoch": 0.019744994317065043,
      "grad_norm": 1.2185240983963013,
      "learning_rate": 0.00030259784652196206,
      "loss": 0.4075,
      "step": 4621
    },
    {
      "epoch": 0.019749267200492235,
      "grad_norm": 3.1427853107452393,
      "learning_rate": 0.00030255511878311397,
      "loss": 0.7659,
      "step": 4622
    },
    {
      "epoch": 0.01975354008391943,
      "grad_norm": 0.9733952879905701,
      "learning_rate": 0.00030251239104426593,
      "loss": 0.2157,
      "step": 4623
    },
    {
      "epoch": 0.019757812967346626,
      "grad_norm": 1.1047104597091675,
      "learning_rate": 0.00030246966330541784,
      "loss": 0.3922,
      "step": 4624
    },
    {
      "epoch": 0.019762085850773818,
      "grad_norm": 1.6812493801116943,
      "learning_rate": 0.0003024269355665698,
      "loss": 0.526,
      "step": 4625
    },
    {
      "epoch": 0.019766358734201014,
      "grad_norm": 3.873239517211914,
      "learning_rate": 0.00030238420782772177,
      "loss": 0.8393,
      "step": 4626
    },
    {
      "epoch": 0.01977063161762821,
      "grad_norm": 3.985907793045044,
      "learning_rate": 0.0003023414800888737,
      "loss": 1.381,
      "step": 4627
    },
    {
      "epoch": 0.0197749045010554,
      "grad_norm": 3.845552921295166,
      "learning_rate": 0.00030229875235002565,
      "loss": 0.8392,
      "step": 4628
    },
    {
      "epoch": 0.019779177384482597,
      "grad_norm": 3.654438018798828,
      "learning_rate": 0.00030225602461117756,
      "loss": 0.7413,
      "step": 4629
    },
    {
      "epoch": 0.019783450267909793,
      "grad_norm": 3.6216413974761963,
      "learning_rate": 0.0003022132968723295,
      "loss": 0.7402,
      "step": 4630
    },
    {
      "epoch": 0.019787723151336985,
      "grad_norm": 2.883302927017212,
      "learning_rate": 0.00030217056913348143,
      "loss": 0.5857,
      "step": 4631
    },
    {
      "epoch": 0.01979199603476418,
      "grad_norm": 5.004777431488037,
      "learning_rate": 0.0003021278413946334,
      "loss": 1.1085,
      "step": 4632
    },
    {
      "epoch": 0.019796268918191372,
      "grad_norm": 1.4636930227279663,
      "learning_rate": 0.00030208511365578536,
      "loss": 0.3592,
      "step": 4633
    },
    {
      "epoch": 0.019800541801618568,
      "grad_norm": 3.0881662368774414,
      "learning_rate": 0.0003020423859169373,
      "loss": 1.5593,
      "step": 4634
    },
    {
      "epoch": 0.019804814685045764,
      "grad_norm": 3.3257405757904053,
      "learning_rate": 0.00030199965817808924,
      "loss": 1.0526,
      "step": 4635
    },
    {
      "epoch": 0.019809087568472956,
      "grad_norm": 2.438140392303467,
      "learning_rate": 0.00030195693043924115,
      "loss": 0.4737,
      "step": 4636
    },
    {
      "epoch": 0.01981336045190015,
      "grad_norm": 2.39726185798645,
      "learning_rate": 0.0003019142027003931,
      "loss": 0.9374,
      "step": 4637
    },
    {
      "epoch": 0.019817633335327347,
      "grad_norm": 41.91028594970703,
      "learning_rate": 0.000301871474961545,
      "loss": 1.0617,
      "step": 4638
    },
    {
      "epoch": 0.01982190621875454,
      "grad_norm": 2.4449613094329834,
      "learning_rate": 0.000301828747222697,
      "loss": 0.4739,
      "step": 4639
    },
    {
      "epoch": 0.019826179102181735,
      "grad_norm": 2.9315319061279297,
      "learning_rate": 0.0003017860194838489,
      "loss": 1.1174,
      "step": 4640
    },
    {
      "epoch": 0.01983045198560893,
      "grad_norm": 2.718222141265869,
      "learning_rate": 0.00030174329174500087,
      "loss": 0.4876,
      "step": 4641
    },
    {
      "epoch": 0.019834724869036122,
      "grad_norm": 2.28653621673584,
      "learning_rate": 0.00030170056400615283,
      "loss": 0.9011,
      "step": 4642
    },
    {
      "epoch": 0.019838997752463318,
      "grad_norm": 1.7412323951721191,
      "learning_rate": 0.00030165783626730474,
      "loss": 0.3942,
      "step": 4643
    },
    {
      "epoch": 0.01984327063589051,
      "grad_norm": 4.5586395263671875,
      "learning_rate": 0.0003016151085284567,
      "loss": 0.9886,
      "step": 4644
    },
    {
      "epoch": 0.019847543519317706,
      "grad_norm": 5.169331073760986,
      "learning_rate": 0.0003015723807896086,
      "loss": 1.0603,
      "step": 4645
    },
    {
      "epoch": 0.0198518164027449,
      "grad_norm": 2.011441469192505,
      "learning_rate": 0.0003015296530507606,
      "loss": 0.3816,
      "step": 4646
    },
    {
      "epoch": 0.019856089286172093,
      "grad_norm": 1.8605403900146484,
      "learning_rate": 0.0003014869253119125,
      "loss": 0.4588,
      "step": 4647
    },
    {
      "epoch": 0.01986036216959929,
      "grad_norm": 4.770453453063965,
      "learning_rate": 0.00030144419757306446,
      "loss": 2.1233,
      "step": 4648
    },
    {
      "epoch": 0.019864635053026485,
      "grad_norm": 1.6431738138198853,
      "learning_rate": 0.0003014014698342164,
      "loss": 0.3268,
      "step": 4649
    },
    {
      "epoch": 0.019868907936453677,
      "grad_norm": 3.4876034259796143,
      "learning_rate": 0.00030135874209536833,
      "loss": 0.9331,
      "step": 4650
    },
    {
      "epoch": 0.019873180819880872,
      "grad_norm": 3.1381189823150635,
      "learning_rate": 0.0003013160143565203,
      "loss": 0.6522,
      "step": 4651
    },
    {
      "epoch": 0.019877453703308068,
      "grad_norm": 2.8892080783843994,
      "learning_rate": 0.0003012732866176722,
      "loss": 0.854,
      "step": 4652
    },
    {
      "epoch": 0.01988172658673526,
      "grad_norm": 1.5063211917877197,
      "learning_rate": 0.0003012305588788241,
      "loss": 0.2882,
      "step": 4653
    },
    {
      "epoch": 0.019885999470162456,
      "grad_norm": 2.95931077003479,
      "learning_rate": 0.00030118783113997603,
      "loss": 0.6812,
      "step": 4654
    },
    {
      "epoch": 0.019890272353589648,
      "grad_norm": 4.25416898727417,
      "learning_rate": 0.000301145103401128,
      "loss": 1.4608,
      "step": 4655
    },
    {
      "epoch": 0.019894545237016843,
      "grad_norm": 4.7843098640441895,
      "learning_rate": 0.00030110237566227996,
      "loss": 1.3323,
      "step": 4656
    },
    {
      "epoch": 0.01989881812044404,
      "grad_norm": 4.519199371337891,
      "learning_rate": 0.00030105964792343187,
      "loss": 0.714,
      "step": 4657
    },
    {
      "epoch": 0.01990309100387123,
      "grad_norm": 1.3946878910064697,
      "learning_rate": 0.00030101692018458384,
      "loss": 0.2535,
      "step": 4658
    },
    {
      "epoch": 0.019907363887298427,
      "grad_norm": 3.609131097793579,
      "learning_rate": 0.00030097419244573575,
      "loss": 0.869,
      "step": 4659
    },
    {
      "epoch": 0.019911636770725622,
      "grad_norm": 1.5060299634933472,
      "learning_rate": 0.0003009314647068877,
      "loss": 0.3156,
      "step": 4660
    },
    {
      "epoch": 0.019915909654152814,
      "grad_norm": 2.1725845336914062,
      "learning_rate": 0.0003008887369680396,
      "loss": 0.3458,
      "step": 4661
    },
    {
      "epoch": 0.01992018253758001,
      "grad_norm": 3.386340856552124,
      "learning_rate": 0.0003008460092291916,
      "loss": 0.6237,
      "step": 4662
    },
    {
      "epoch": 0.019924455421007205,
      "grad_norm": 2.085524797439575,
      "learning_rate": 0.00030080328149034355,
      "loss": 0.3202,
      "step": 4663
    },
    {
      "epoch": 0.019928728304434398,
      "grad_norm": 3.528269052505493,
      "learning_rate": 0.00030076055375149546,
      "loss": 1.5673,
      "step": 4664
    },
    {
      "epoch": 0.019933001187861593,
      "grad_norm": 1.0730875730514526,
      "learning_rate": 0.00030071782601264743,
      "loss": 0.2767,
      "step": 4665
    },
    {
      "epoch": 0.01993727407128879,
      "grad_norm": 3.49269437789917,
      "learning_rate": 0.00030067509827379934,
      "loss": 0.8281,
      "step": 4666
    },
    {
      "epoch": 0.01994154695471598,
      "grad_norm": 5.0291876792907715,
      "learning_rate": 0.0003006323705349513,
      "loss": 1.1505,
      "step": 4667
    },
    {
      "epoch": 0.019945819838143176,
      "grad_norm": 4.077733039855957,
      "learning_rate": 0.0003005896427961032,
      "loss": 1.0254,
      "step": 4668
    },
    {
      "epoch": 0.01995009272157037,
      "grad_norm": 2.8560054302215576,
      "learning_rate": 0.0003005469150572552,
      "loss": 0.6498,
      "step": 4669
    },
    {
      "epoch": 0.019954365604997564,
      "grad_norm": 2.853626251220703,
      "learning_rate": 0.00030050418731840715,
      "loss": 0.6497,
      "step": 4670
    },
    {
      "epoch": 0.01995863848842476,
      "grad_norm": 5.505063056945801,
      "learning_rate": 0.00030046145957955906,
      "loss": 0.8735,
      "step": 4671
    },
    {
      "epoch": 0.019962911371851952,
      "grad_norm": 4.056975364685059,
      "learning_rate": 0.000300418731840711,
      "loss": 0.9116,
      "step": 4672
    },
    {
      "epoch": 0.019967184255279147,
      "grad_norm": 3.7165212631225586,
      "learning_rate": 0.00030037600410186293,
      "loss": 0.6777,
      "step": 4673
    },
    {
      "epoch": 0.019971457138706343,
      "grad_norm": 5.195243835449219,
      "learning_rate": 0.0003003332763630149,
      "loss": 2.9604,
      "step": 4674
    },
    {
      "epoch": 0.019975730022133535,
      "grad_norm": 2.425708532333374,
      "learning_rate": 0.0003002905486241668,
      "loss": 0.6718,
      "step": 4675
    },
    {
      "epoch": 0.01998000290556073,
      "grad_norm": 3.111670970916748,
      "learning_rate": 0.00030024782088531877,
      "loss": 1.3248,
      "step": 4676
    },
    {
      "epoch": 0.019984275788987926,
      "grad_norm": 2.007891893386841,
      "learning_rate": 0.0003002050931464707,
      "loss": 0.3248,
      "step": 4677
    },
    {
      "epoch": 0.01998854867241512,
      "grad_norm": 0.8782044649124146,
      "learning_rate": 0.00030016236540762265,
      "loss": 0.1524,
      "step": 4678
    },
    {
      "epoch": 0.019992821555842314,
      "grad_norm": 2.8535666465759277,
      "learning_rate": 0.0003001196376687746,
      "loss": 0.7917,
      "step": 4679
    },
    {
      "epoch": 0.019997094439269506,
      "grad_norm": 2.978522777557373,
      "learning_rate": 0.0003000769099299265,
      "loss": 0.8088,
      "step": 4680
    },
    {
      "epoch": 0.020001367322696702,
      "grad_norm": 2.372291326522827,
      "learning_rate": 0.0003000341821910785,
      "loss": 0.5913,
      "step": 4681
    },
    {
      "epoch": 0.020005640206123897,
      "grad_norm": 2.912386655807495,
      "learning_rate": 0.0002999914544522304,
      "loss": 1.4038,
      "step": 4682
    },
    {
      "epoch": 0.02000991308955109,
      "grad_norm": 3.437739133834839,
      "learning_rate": 0.00029994872671338236,
      "loss": 0.6646,
      "step": 4683
    },
    {
      "epoch": 0.020014185972978285,
      "grad_norm": 3.8397881984710693,
      "learning_rate": 0.0002999059989745343,
      "loss": 1.0391,
      "step": 4684
    },
    {
      "epoch": 0.02001845885640548,
      "grad_norm": 1.085431456565857,
      "learning_rate": 0.00029986327123568624,
      "loss": 0.3056,
      "step": 4685
    },
    {
      "epoch": 0.020022731739832673,
      "grad_norm": 3.8158493041992188,
      "learning_rate": 0.00029982054349683815,
      "loss": 1.8463,
      "step": 4686
    },
    {
      "epoch": 0.02002700462325987,
      "grad_norm": 2.0643150806427,
      "learning_rate": 0.00029977781575799006,
      "loss": 0.4089,
      "step": 4687
    },
    {
      "epoch": 0.020031277506687064,
      "grad_norm": 2.021253824234009,
      "learning_rate": 0.000299735088019142,
      "loss": 0.4669,
      "step": 4688
    },
    {
      "epoch": 0.020035550390114256,
      "grad_norm": 1.345834493637085,
      "learning_rate": 0.00029969236028029394,
      "loss": 0.2114,
      "step": 4689
    },
    {
      "epoch": 0.02003982327354145,
      "grad_norm": 1.948042392730713,
      "learning_rate": 0.0002996496325414459,
      "loss": 0.4563,
      "step": 4690
    },
    {
      "epoch": 0.020044096156968647,
      "grad_norm": 3.2761518955230713,
      "learning_rate": 0.0002996069048025978,
      "loss": 0.5841,
      "step": 4691
    },
    {
      "epoch": 0.02004836904039584,
      "grad_norm": 3.5610337257385254,
      "learning_rate": 0.0002995641770637498,
      "loss": 0.7632,
      "step": 4692
    },
    {
      "epoch": 0.020052641923823035,
      "grad_norm": 3.4180750846862793,
      "learning_rate": 0.00029952144932490174,
      "loss": 0.6527,
      "step": 4693
    },
    {
      "epoch": 0.020056914807250227,
      "grad_norm": 5.465874671936035,
      "learning_rate": 0.00029947872158605365,
      "loss": 0.8892,
      "step": 4694
    },
    {
      "epoch": 0.020061187690677423,
      "grad_norm": 1.3110263347625732,
      "learning_rate": 0.0002994359938472056,
      "loss": 0.2349,
      "step": 4695
    },
    {
      "epoch": 0.020065460574104618,
      "grad_norm": 2.9376559257507324,
      "learning_rate": 0.00029939326610835753,
      "loss": 0.5372,
      "step": 4696
    },
    {
      "epoch": 0.02006973345753181,
      "grad_norm": 2.533137798309326,
      "learning_rate": 0.0002993505383695095,
      "loss": 0.4792,
      "step": 4697
    },
    {
      "epoch": 0.020074006340959006,
      "grad_norm": 2.413025140762329,
      "learning_rate": 0.0002993078106306614,
      "loss": 0.4461,
      "step": 4698
    },
    {
      "epoch": 0.0200782792243862,
      "grad_norm": 2.047602891921997,
      "learning_rate": 0.00029926508289181337,
      "loss": 0.3652,
      "step": 4699
    },
    {
      "epoch": 0.020082552107813394,
      "grad_norm": 3.772016763687134,
      "learning_rate": 0.00029922235515296533,
      "loss": 1.5255,
      "step": 4700
    },
    {
      "epoch": 0.02008682499124059,
      "grad_norm": 2.8409125804901123,
      "learning_rate": 0.00029917962741411725,
      "loss": 0.7742,
      "step": 4701
    },
    {
      "epoch": 0.020091097874667785,
      "grad_norm": 2.9136223793029785,
      "learning_rate": 0.0002991368996752692,
      "loss": 0.6356,
      "step": 4702
    },
    {
      "epoch": 0.020095370758094977,
      "grad_norm": 2.1735990047454834,
      "learning_rate": 0.0002990941719364211,
      "loss": 0.6783,
      "step": 4703
    },
    {
      "epoch": 0.020099643641522173,
      "grad_norm": 1.7225459814071655,
      "learning_rate": 0.0002990514441975731,
      "loss": 0.3066,
      "step": 4704
    },
    {
      "epoch": 0.020103916524949365,
      "grad_norm": 1.5458523035049438,
      "learning_rate": 0.000299008716458725,
      "loss": 0.3048,
      "step": 4705
    },
    {
      "epoch": 0.02010818940837656,
      "grad_norm": 2.0377163887023926,
      "learning_rate": 0.00029896598871987696,
      "loss": 0.6658,
      "step": 4706
    },
    {
      "epoch": 0.020112462291803756,
      "grad_norm": 3.2050986289978027,
      "learning_rate": 0.00029892326098102887,
      "loss": 1.373,
      "step": 4707
    },
    {
      "epoch": 0.020116735175230948,
      "grad_norm": 2.00894832611084,
      "learning_rate": 0.00029888053324218084,
      "loss": 0.489,
      "step": 4708
    },
    {
      "epoch": 0.020121008058658144,
      "grad_norm": 3.3365485668182373,
      "learning_rate": 0.0002988378055033328,
      "loss": 2.3649,
      "step": 4709
    },
    {
      "epoch": 0.02012528094208534,
      "grad_norm": 2.9647819995880127,
      "learning_rate": 0.0002987950777644847,
      "loss": 1.2296,
      "step": 4710
    },
    {
      "epoch": 0.02012955382551253,
      "grad_norm": 1.5374982357025146,
      "learning_rate": 0.0002987523500256367,
      "loss": 0.3217,
      "step": 4711
    },
    {
      "epoch": 0.020133826708939727,
      "grad_norm": 1.9261008501052856,
      "learning_rate": 0.0002987096222867886,
      "loss": 0.4758,
      "step": 4712
    },
    {
      "epoch": 0.020138099592366922,
      "grad_norm": 2.368927240371704,
      "learning_rate": 0.00029866689454794055,
      "loss": 0.6259,
      "step": 4713
    },
    {
      "epoch": 0.020142372475794115,
      "grad_norm": 6.759260177612305,
      "learning_rate": 0.00029862416680909246,
      "loss": 1.8044,
      "step": 4714
    },
    {
      "epoch": 0.02014664535922131,
      "grad_norm": 1.455140471458435,
      "learning_rate": 0.00029858143907024443,
      "loss": 1.17,
      "step": 4715
    },
    {
      "epoch": 0.020150918242648506,
      "grad_norm": 6.56983757019043,
      "learning_rate": 0.0002985387113313964,
      "loss": 1.7217,
      "step": 4716
    },
    {
      "epoch": 0.020155191126075698,
      "grad_norm": 2.4102468490600586,
      "learning_rate": 0.0002984959835925483,
      "loss": 0.6302,
      "step": 4717
    },
    {
      "epoch": 0.020159464009502893,
      "grad_norm": 1.0185054540634155,
      "learning_rate": 0.0002984532558537002,
      "loss": 0.2093,
      "step": 4718
    },
    {
      "epoch": 0.020163736892930086,
      "grad_norm": 1.4107099771499634,
      "learning_rate": 0.0002984105281148521,
      "loss": 1.0784,
      "step": 4719
    },
    {
      "epoch": 0.02016800977635728,
      "grad_norm": 2.5550897121429443,
      "learning_rate": 0.0002983678003760041,
      "loss": 0.9713,
      "step": 4720
    },
    {
      "epoch": 0.020172282659784477,
      "grad_norm": 2.725200653076172,
      "learning_rate": 0.000298325072637156,
      "loss": 1.287,
      "step": 4721
    },
    {
      "epoch": 0.02017655554321167,
      "grad_norm": 2.5664234161376953,
      "learning_rate": 0.00029828234489830797,
      "loss": 0.6583,
      "step": 4722
    },
    {
      "epoch": 0.020180828426638864,
      "grad_norm": 1.4711899757385254,
      "learning_rate": 0.00029823961715945993,
      "loss": 1.0333,
      "step": 4723
    },
    {
      "epoch": 0.02018510131006606,
      "grad_norm": 3.2852272987365723,
      "learning_rate": 0.00029819688942061184,
      "loss": 1.197,
      "step": 4724
    },
    {
      "epoch": 0.020189374193493252,
      "grad_norm": 2.705551862716675,
      "learning_rate": 0.0002981541616817638,
      "loss": 0.7097,
      "step": 4725
    },
    {
      "epoch": 0.020193647076920448,
      "grad_norm": 4.262691974639893,
      "learning_rate": 0.0002981114339429157,
      "loss": 1.4754,
      "step": 4726
    },
    {
      "epoch": 0.020197919960347643,
      "grad_norm": 2.260629892349243,
      "learning_rate": 0.0002980687062040677,
      "loss": 0.4909,
      "step": 4727
    },
    {
      "epoch": 0.020202192843774835,
      "grad_norm": 0.730216920375824,
      "learning_rate": 0.0002980259784652196,
      "loss": 0.1233,
      "step": 4728
    },
    {
      "epoch": 0.02020646572720203,
      "grad_norm": 1.7040899991989136,
      "learning_rate": 0.00029798325072637156,
      "loss": 0.3367,
      "step": 4729
    },
    {
      "epoch": 0.020210738610629223,
      "grad_norm": 1.4884846210479736,
      "learning_rate": 0.0002979405229875235,
      "loss": 0.9745,
      "step": 4730
    },
    {
      "epoch": 0.02021501149405642,
      "grad_norm": 1.796439528465271,
      "learning_rate": 0.00029789779524867543,
      "loss": 0.5372,
      "step": 4731
    },
    {
      "epoch": 0.020219284377483614,
      "grad_norm": 2.891369581222534,
      "learning_rate": 0.0002978550675098274,
      "loss": 0.6826,
      "step": 4732
    },
    {
      "epoch": 0.020223557260910806,
      "grad_norm": 2.7526872158050537,
      "learning_rate": 0.0002978123397709793,
      "loss": 0.7022,
      "step": 4733
    },
    {
      "epoch": 0.020227830144338002,
      "grad_norm": 3.0282421112060547,
      "learning_rate": 0.0002977696120321313,
      "loss": 1.0253,
      "step": 4734
    },
    {
      "epoch": 0.020232103027765198,
      "grad_norm": 2.3796775341033936,
      "learning_rate": 0.0002977268842932832,
      "loss": 0.7072,
      "step": 4735
    },
    {
      "epoch": 0.02023637591119239,
      "grad_norm": 1.8965575695037842,
      "learning_rate": 0.00029768415655443515,
      "loss": 0.5402,
      "step": 4736
    },
    {
      "epoch": 0.020240648794619585,
      "grad_norm": 1.4366769790649414,
      "learning_rate": 0.0002976414288155871,
      "loss": 0.9808,
      "step": 4737
    },
    {
      "epoch": 0.02024492167804678,
      "grad_norm": 1.2282283306121826,
      "learning_rate": 0.000297598701076739,
      "loss": 0.44,
      "step": 4738
    },
    {
      "epoch": 0.020249194561473973,
      "grad_norm": 1.5137875080108643,
      "learning_rate": 0.000297555973337891,
      "loss": 0.4559,
      "step": 4739
    },
    {
      "epoch": 0.02025346744490117,
      "grad_norm": 1.312247395515442,
      "learning_rate": 0.0002975132455990429,
      "loss": 0.4844,
      "step": 4740
    },
    {
      "epoch": 0.020257740328328364,
      "grad_norm": 1.0067026615142822,
      "learning_rate": 0.00029747051786019487,
      "loss": 0.2671,
      "step": 4741
    },
    {
      "epoch": 0.020262013211755556,
      "grad_norm": 1.4688773155212402,
      "learning_rate": 0.0002974277901213468,
      "loss": 0.5429,
      "step": 4742
    },
    {
      "epoch": 0.020266286095182752,
      "grad_norm": 1.464117169380188,
      "learning_rate": 0.00029738506238249874,
      "loss": 0.5277,
      "step": 4743
    },
    {
      "epoch": 0.020270558978609944,
      "grad_norm": 1.4702916145324707,
      "learning_rate": 0.00029734233464365065,
      "loss": 0.545,
      "step": 4744
    },
    {
      "epoch": 0.02027483186203714,
      "grad_norm": 2.728609561920166,
      "learning_rate": 0.0002972996069048026,
      "loss": 0.9158,
      "step": 4745
    },
    {
      "epoch": 0.020279104745464335,
      "grad_norm": 2.6274502277374268,
      "learning_rate": 0.0002972568791659546,
      "loss": 0.7975,
      "step": 4746
    },
    {
      "epoch": 0.020283377628891527,
      "grad_norm": 1.2598942518234253,
      "learning_rate": 0.0002972141514271065,
      "loss": 0.4345,
      "step": 4747
    },
    {
      "epoch": 0.020287650512318723,
      "grad_norm": 3.891235113143921,
      "learning_rate": 0.00029717142368825846,
      "loss": 1.9364,
      "step": 4748
    },
    {
      "epoch": 0.02029192339574592,
      "grad_norm": 1.6116875410079956,
      "learning_rate": 0.00029712869594941037,
      "loss": 0.9384,
      "step": 4749
    },
    {
      "epoch": 0.02029619627917311,
      "grad_norm": 1.09018874168396,
      "learning_rate": 0.00029708596821056234,
      "loss": 0.3205,
      "step": 4750
    },
    {
      "epoch": 0.020300469162600306,
      "grad_norm": 2.708279848098755,
      "learning_rate": 0.0002970432404717142,
      "loss": 0.7561,
      "step": 4751
    },
    {
      "epoch": 0.020304742046027502,
      "grad_norm": 2.6921610832214355,
      "learning_rate": 0.00029700051273286616,
      "loss": 0.9676,
      "step": 4752
    },
    {
      "epoch": 0.020309014929454694,
      "grad_norm": 1.3894357681274414,
      "learning_rate": 0.0002969577849940181,
      "loss": 0.3909,
      "step": 4753
    },
    {
      "epoch": 0.02031328781288189,
      "grad_norm": 1.8643046617507935,
      "learning_rate": 0.00029691505725517003,
      "loss": 0.7028,
      "step": 4754
    },
    {
      "epoch": 0.02031756069630908,
      "grad_norm": 2.0028836727142334,
      "learning_rate": 0.000296872329516322,
      "loss": 0.5031,
      "step": 4755
    },
    {
      "epoch": 0.020321833579736277,
      "grad_norm": 1.1456449031829834,
      "learning_rate": 0.0002968296017774739,
      "loss": 0.4197,
      "step": 4756
    },
    {
      "epoch": 0.020326106463163473,
      "grad_norm": 2.5339863300323486,
      "learning_rate": 0.0002967868740386259,
      "loss": 0.5249,
      "step": 4757
    },
    {
      "epoch": 0.020330379346590665,
      "grad_norm": 2.4048514366149902,
      "learning_rate": 0.0002967441462997778,
      "loss": 0.633,
      "step": 4758
    },
    {
      "epoch": 0.02033465223001786,
      "grad_norm": 2.663543701171875,
      "learning_rate": 0.00029670141856092975,
      "loss": 1.3618,
      "step": 4759
    },
    {
      "epoch": 0.020338925113445056,
      "grad_norm": 2.663602352142334,
      "learning_rate": 0.0002966586908220817,
      "loss": 1.3722,
      "step": 4760
    },
    {
      "epoch": 0.02034319799687225,
      "grad_norm": 1.1329858303070068,
      "learning_rate": 0.0002966159630832336,
      "loss": 0.3507,
      "step": 4761
    },
    {
      "epoch": 0.020347470880299444,
      "grad_norm": 3.7614431381225586,
      "learning_rate": 0.0002965732353443856,
      "loss": 1.3471,
      "step": 4762
    },
    {
      "epoch": 0.02035174376372664,
      "grad_norm": 3.0349810123443604,
      "learning_rate": 0.0002965305076055375,
      "loss": 0.7592,
      "step": 4763
    },
    {
      "epoch": 0.02035601664715383,
      "grad_norm": 1.1066817045211792,
      "learning_rate": 0.00029648777986668946,
      "loss": 0.3503,
      "step": 4764
    },
    {
      "epoch": 0.020360289530581027,
      "grad_norm": 2.998581886291504,
      "learning_rate": 0.0002964450521278414,
      "loss": 0.7348,
      "step": 4765
    },
    {
      "epoch": 0.02036456241400822,
      "grad_norm": 3.9061965942382812,
      "learning_rate": 0.00029640232438899334,
      "loss": 1.2565,
      "step": 4766
    },
    {
      "epoch": 0.020368835297435415,
      "grad_norm": 2.967343330383301,
      "learning_rate": 0.0002963595966501453,
      "loss": 0.7165,
      "step": 4767
    },
    {
      "epoch": 0.02037310818086261,
      "grad_norm": 2.8621573448181152,
      "learning_rate": 0.0002963168689112972,
      "loss": 0.6871,
      "step": 4768
    },
    {
      "epoch": 0.020377381064289803,
      "grad_norm": 0.7208709120750427,
      "learning_rate": 0.0002962741411724492,
      "loss": 0.3063,
      "step": 4769
    },
    {
      "epoch": 0.020381653947716998,
      "grad_norm": 3.3647384643554688,
      "learning_rate": 0.0002962314134336011,
      "loss": 1.1673,
      "step": 4770
    },
    {
      "epoch": 0.020385926831144194,
      "grad_norm": 3.2350351810455322,
      "learning_rate": 0.00029618868569475306,
      "loss": 0.7698,
      "step": 4771
    },
    {
      "epoch": 0.020390199714571386,
      "grad_norm": 3.515669107437134,
      "learning_rate": 0.00029614595795590497,
      "loss": 1.1043,
      "step": 4772
    },
    {
      "epoch": 0.02039447259799858,
      "grad_norm": 1.2297194004058838,
      "learning_rate": 0.00029610323021705693,
      "loss": 0.3983,
      "step": 4773
    },
    {
      "epoch": 0.020398745481425777,
      "grad_norm": 2.8831658363342285,
      "learning_rate": 0.0002960605024782089,
      "loss": 0.7828,
      "step": 4774
    },
    {
      "epoch": 0.02040301836485297,
      "grad_norm": 1.0626883506774902,
      "learning_rate": 0.0002960177747393608,
      "loss": 0.3347,
      "step": 4775
    },
    {
      "epoch": 0.020407291248280165,
      "grad_norm": 0.6357920169830322,
      "learning_rate": 0.0002959750470005128,
      "loss": 0.2871,
      "step": 4776
    },
    {
      "epoch": 0.02041156413170736,
      "grad_norm": 1.0361809730529785,
      "learning_rate": 0.0002959323192616647,
      "loss": 0.365,
      "step": 4777
    },
    {
      "epoch": 0.020415837015134553,
      "grad_norm": 1.0943098068237305,
      "learning_rate": 0.00029588959152281665,
      "loss": 0.3647,
      "step": 4778
    },
    {
      "epoch": 0.020420109898561748,
      "grad_norm": 0.9622637033462524,
      "learning_rate": 0.00029584686378396856,
      "loss": 0.3045,
      "step": 4779
    },
    {
      "epoch": 0.02042438278198894,
      "grad_norm": 1.6326202154159546,
      "learning_rate": 0.0002958041360451205,
      "loss": 0.9857,
      "step": 4780
    },
    {
      "epoch": 0.020428655665416136,
      "grad_norm": 1.7626628875732422,
      "learning_rate": 0.00029576140830627244,
      "loss": 0.5629,
      "step": 4781
    },
    {
      "epoch": 0.02043292854884333,
      "grad_norm": 4.022639274597168,
      "learning_rate": 0.0002957186805674244,
      "loss": 1.2186,
      "step": 4782
    },
    {
      "epoch": 0.020437201432270524,
      "grad_norm": 1.5712225437164307,
      "learning_rate": 0.00029567595282857637,
      "loss": 0.4593,
      "step": 4783
    },
    {
      "epoch": 0.02044147431569772,
      "grad_norm": 3.3430697917938232,
      "learning_rate": 0.0002956332250897282,
      "loss": 0.5973,
      "step": 4784
    },
    {
      "epoch": 0.020445747199124915,
      "grad_norm": 1.7567906379699707,
      "learning_rate": 0.0002955904973508802,
      "loss": 0.4555,
      "step": 4785
    },
    {
      "epoch": 0.020450020082552107,
      "grad_norm": 1.6888608932495117,
      "learning_rate": 0.0002955477696120321,
      "loss": 0.4606,
      "step": 4786
    },
    {
      "epoch": 0.020454292965979302,
      "grad_norm": 2.6715402603149414,
      "learning_rate": 0.00029550504187318406,
      "loss": 0.9922,
      "step": 4787
    },
    {
      "epoch": 0.020458565849406498,
      "grad_norm": 1.7024961709976196,
      "learning_rate": 0.000295462314134336,
      "loss": 0.8658,
      "step": 4788
    },
    {
      "epoch": 0.02046283873283369,
      "grad_norm": 3.962404727935791,
      "learning_rate": 0.00029541958639548794,
      "loss": 1.0403,
      "step": 4789
    },
    {
      "epoch": 0.020467111616260886,
      "grad_norm": 0.8259895443916321,
      "learning_rate": 0.0002953768586566399,
      "loss": 0.3049,
      "step": 4790
    },
    {
      "epoch": 0.020471384499688078,
      "grad_norm": 1.9529550075531006,
      "learning_rate": 0.0002953341309177918,
      "loss": 0.718,
      "step": 4791
    },
    {
      "epoch": 0.020475657383115273,
      "grad_norm": 2.203367233276367,
      "learning_rate": 0.0002952914031789438,
      "loss": 1.0117,
      "step": 4792
    },
    {
      "epoch": 0.02047993026654247,
      "grad_norm": 2.998046875,
      "learning_rate": 0.0002952486754400957,
      "loss": 1.1487,
      "step": 4793
    },
    {
      "epoch": 0.02048420314996966,
      "grad_norm": 1.0639499425888062,
      "learning_rate": 0.00029520594770124765,
      "loss": 0.3188,
      "step": 4794
    },
    {
      "epoch": 0.020488476033396857,
      "grad_norm": 1.3757414817810059,
      "learning_rate": 0.00029516321996239957,
      "loss": 0.5431,
      "step": 4795
    },
    {
      "epoch": 0.020492748916824052,
      "grad_norm": 2.5538487434387207,
      "learning_rate": 0.00029512049222355153,
      "loss": 0.7171,
      "step": 4796
    },
    {
      "epoch": 0.020497021800251244,
      "grad_norm": 0.9915019273757935,
      "learning_rate": 0.0002950777644847035,
      "loss": 0.319,
      "step": 4797
    },
    {
      "epoch": 0.02050129468367844,
      "grad_norm": 1.331629753112793,
      "learning_rate": 0.0002950350367458554,
      "loss": 0.5161,
      "step": 4798
    },
    {
      "epoch": 0.020505567567105636,
      "grad_norm": 0.7902365326881409,
      "learning_rate": 0.00029499230900700737,
      "loss": 0.2876,
      "step": 4799
    },
    {
      "epoch": 0.020509840450532828,
      "grad_norm": 0.9330162405967712,
      "learning_rate": 0.0002949495812681593,
      "loss": 0.2934,
      "step": 4800
    },
    {
      "epoch": 0.020514113333960023,
      "grad_norm": 3.5876083374023438,
      "learning_rate": 0.00029490685352931125,
      "loss": 0.7,
      "step": 4801
    },
    {
      "epoch": 0.02051838621738722,
      "grad_norm": 3.3381524085998535,
      "learning_rate": 0.00029486412579046316,
      "loss": 0.8828,
      "step": 4802
    },
    {
      "epoch": 0.02052265910081441,
      "grad_norm": 1.5337823629379272,
      "learning_rate": 0.0002948213980516151,
      "loss": 0.9593,
      "step": 4803
    },
    {
      "epoch": 0.020526931984241607,
      "grad_norm": 0.6796988844871521,
      "learning_rate": 0.0002947786703127671,
      "loss": 0.253,
      "step": 4804
    },
    {
      "epoch": 0.0205312048676688,
      "grad_norm": 1.2147421836853027,
      "learning_rate": 0.000294735942573919,
      "loss": 0.3864,
      "step": 4805
    },
    {
      "epoch": 0.020535477751095994,
      "grad_norm": 2.2263009548187256,
      "learning_rate": 0.00029469321483507096,
      "loss": 0.5525,
      "step": 4806
    },
    {
      "epoch": 0.02053975063452319,
      "grad_norm": 2.675973892211914,
      "learning_rate": 0.0002946504870962229,
      "loss": 1.0453,
      "step": 4807
    },
    {
      "epoch": 0.020544023517950382,
      "grad_norm": 0.4769000709056854,
      "learning_rate": 0.00029460775935737484,
      "loss": 0.1855,
      "step": 4808
    },
    {
      "epoch": 0.020548296401377578,
      "grad_norm": 0.9014683365821838,
      "learning_rate": 0.00029456503161852675,
      "loss": 0.2875,
      "step": 4809
    },
    {
      "epoch": 0.020552569284804773,
      "grad_norm": 3.447495222091675,
      "learning_rate": 0.0002945223038796787,
      "loss": 1.3364,
      "step": 4810
    },
    {
      "epoch": 0.020556842168231965,
      "grad_norm": 1.795243740081787,
      "learning_rate": 0.0002944795761408306,
      "loss": 0.6746,
      "step": 4811
    },
    {
      "epoch": 0.02056111505165916,
      "grad_norm": 1.7818644046783447,
      "learning_rate": 0.0002944368484019826,
      "loss": 0.5832,
      "step": 4812
    },
    {
      "epoch": 0.020565387935086357,
      "grad_norm": 4.883720874786377,
      "learning_rate": 0.00029439412066313455,
      "loss": 1.4344,
      "step": 4813
    },
    {
      "epoch": 0.02056966081851355,
      "grad_norm": 4.828447341918945,
      "learning_rate": 0.00029435139292428647,
      "loss": 1.3808,
      "step": 4814
    },
    {
      "epoch": 0.020573933701940744,
      "grad_norm": 1.7411260604858398,
      "learning_rate": 0.00029430866518543843,
      "loss": 0.9815,
      "step": 4815
    },
    {
      "epoch": 0.020578206585367936,
      "grad_norm": 1.6596760749816895,
      "learning_rate": 0.00029426593744659034,
      "loss": 0.6285,
      "step": 4816
    },
    {
      "epoch": 0.020582479468795132,
      "grad_norm": 2.613785982131958,
      "learning_rate": 0.00029422320970774225,
      "loss": 0.9575,
      "step": 4817
    },
    {
      "epoch": 0.020586752352222328,
      "grad_norm": 3.4328434467315674,
      "learning_rate": 0.00029418048196889416,
      "loss": 1.5561,
      "step": 4818
    },
    {
      "epoch": 0.02059102523564952,
      "grad_norm": 3.10274338722229,
      "learning_rate": 0.00029413775423004613,
      "loss": 1.4819,
      "step": 4819
    },
    {
      "epoch": 0.020595298119076715,
      "grad_norm": 0.5296342968940735,
      "learning_rate": 0.0002940950264911981,
      "loss": 0.2369,
      "step": 4820
    },
    {
      "epoch": 0.02059957100250391,
      "grad_norm": 1.6153700351715088,
      "learning_rate": 0.00029405229875235,
      "loss": 0.513,
      "step": 4821
    },
    {
      "epoch": 0.020603843885931103,
      "grad_norm": 0.5327675938606262,
      "learning_rate": 0.00029400957101350197,
      "loss": 0.184,
      "step": 4822
    },
    {
      "epoch": 0.0206081167693583,
      "grad_norm": 1.439222812652588,
      "learning_rate": 0.0002939668432746539,
      "loss": 0.5217,
      "step": 4823
    },
    {
      "epoch": 0.020612389652785494,
      "grad_norm": 1.1029388904571533,
      "learning_rate": 0.00029392411553580584,
      "loss": 0.3526,
      "step": 4824
    },
    {
      "epoch": 0.020616662536212686,
      "grad_norm": 1.1318129301071167,
      "learning_rate": 0.00029388138779695775,
      "loss": 0.3983,
      "step": 4825
    },
    {
      "epoch": 0.020620935419639882,
      "grad_norm": 0.59721839427948,
      "learning_rate": 0.0002938386600581097,
      "loss": 0.3096,
      "step": 4826
    },
    {
      "epoch": 0.020625208303067077,
      "grad_norm": 0.7531629204750061,
      "learning_rate": 0.0002937959323192617,
      "loss": 0.2654,
      "step": 4827
    },
    {
      "epoch": 0.02062948118649427,
      "grad_norm": 2.9448914527893066,
      "learning_rate": 0.0002937532045804136,
      "loss": 0.8954,
      "step": 4828
    },
    {
      "epoch": 0.020633754069921465,
      "grad_norm": 2.273627758026123,
      "learning_rate": 0.00029371047684156556,
      "loss": 0.6522,
      "step": 4829
    },
    {
      "epoch": 0.020638026953348657,
      "grad_norm": 1.400224208831787,
      "learning_rate": 0.00029366774910271747,
      "loss": 0.5413,
      "step": 4830
    },
    {
      "epoch": 0.020642299836775853,
      "grad_norm": 2.9755795001983643,
      "learning_rate": 0.00029362502136386944,
      "loss": 0.7427,
      "step": 4831
    },
    {
      "epoch": 0.02064657272020305,
      "grad_norm": 2.913733959197998,
      "learning_rate": 0.00029358229362502135,
      "loss": 1.1332,
      "step": 4832
    },
    {
      "epoch": 0.02065084560363024,
      "grad_norm": 3.213331460952759,
      "learning_rate": 0.0002935395658861733,
      "loss": 1.0361,
      "step": 4833
    },
    {
      "epoch": 0.020655118487057436,
      "grad_norm": 1.653410792350769,
      "learning_rate": 0.0002934968381473253,
      "loss": 0.6722,
      "step": 4834
    },
    {
      "epoch": 0.02065939137048463,
      "grad_norm": 0.9396642446517944,
      "learning_rate": 0.0002934541104084772,
      "loss": 0.384,
      "step": 4835
    },
    {
      "epoch": 0.020663664253911824,
      "grad_norm": 1.629757285118103,
      "learning_rate": 0.00029341138266962915,
      "loss": 1.0107,
      "step": 4836
    },
    {
      "epoch": 0.02066793713733902,
      "grad_norm": 1.8691726922988892,
      "learning_rate": 0.00029336865493078106,
      "loss": 1.0529,
      "step": 4837
    },
    {
      "epoch": 0.020672210020766215,
      "grad_norm": 2.5554285049438477,
      "learning_rate": 0.00029332592719193303,
      "loss": 1.0172,
      "step": 4838
    },
    {
      "epoch": 0.020676482904193407,
      "grad_norm": 2.858333110809326,
      "learning_rate": 0.00029328319945308494,
      "loss": 0.8832,
      "step": 4839
    },
    {
      "epoch": 0.020680755787620603,
      "grad_norm": 0.5409292578697205,
      "learning_rate": 0.0002932404717142369,
      "loss": 0.2176,
      "step": 4840
    },
    {
      "epoch": 0.020685028671047795,
      "grad_norm": 1.2101643085479736,
      "learning_rate": 0.00029319774397538887,
      "loss": 0.4746,
      "step": 4841
    },
    {
      "epoch": 0.02068930155447499,
      "grad_norm": 0.854804277420044,
      "learning_rate": 0.0002931550162365408,
      "loss": 0.3381,
      "step": 4842
    },
    {
      "epoch": 0.020693574437902186,
      "grad_norm": 0.37755030393600464,
      "learning_rate": 0.00029311228849769274,
      "loss": 0.1349,
      "step": 4843
    },
    {
      "epoch": 0.020697847321329378,
      "grad_norm": 1.370483160018921,
      "learning_rate": 0.00029306956075884465,
      "loss": 0.3982,
      "step": 4844
    },
    {
      "epoch": 0.020702120204756574,
      "grad_norm": 2.401517391204834,
      "learning_rate": 0.0002930268330199966,
      "loss": 0.9308,
      "step": 4845
    },
    {
      "epoch": 0.02070639308818377,
      "grad_norm": 1.3763550519943237,
      "learning_rate": 0.00029298410528114853,
      "loss": 0.4189,
      "step": 4846
    },
    {
      "epoch": 0.02071066597161096,
      "grad_norm": 2.4161581993103027,
      "learning_rate": 0.0002929413775423005,
      "loss": 0.7392,
      "step": 4847
    },
    {
      "epoch": 0.020714938855038157,
      "grad_norm": 0.8827409148216248,
      "learning_rate": 0.0002928986498034524,
      "loss": 0.2964,
      "step": 4848
    },
    {
      "epoch": 0.020719211738465353,
      "grad_norm": 1.276032567024231,
      "learning_rate": 0.00029285592206460437,
      "loss": 0.4399,
      "step": 4849
    },
    {
      "epoch": 0.020723484621892545,
      "grad_norm": 0.9714011549949646,
      "learning_rate": 0.0002928131943257563,
      "loss": 0.367,
      "step": 4850
    },
    {
      "epoch": 0.02072775750531974,
      "grad_norm": 1.5016412734985352,
      "learning_rate": 0.0002927704665869082,
      "loss": 0.4995,
      "step": 4851
    },
    {
      "epoch": 0.020732030388746936,
      "grad_norm": 0.29572808742523193,
      "learning_rate": 0.00029272773884806016,
      "loss": 0.0664,
      "step": 4852
    },
    {
      "epoch": 0.020736303272174128,
      "grad_norm": 2.936389684677124,
      "learning_rate": 0.00029268501110921207,
      "loss": 1.5376,
      "step": 4853
    },
    {
      "epoch": 0.020740576155601324,
      "grad_norm": 1.5263746976852417,
      "learning_rate": 0.00029264228337036403,
      "loss": 0.9186,
      "step": 4854
    },
    {
      "epoch": 0.020744849039028516,
      "grad_norm": 2.7723166942596436,
      "learning_rate": 0.00029259955563151594,
      "loss": 1.3486,
      "step": 4855
    },
    {
      "epoch": 0.02074912192245571,
      "grad_norm": 1.5339934825897217,
      "learning_rate": 0.0002925568278926679,
      "loss": 0.9186,
      "step": 4856
    },
    {
      "epoch": 0.020753394805882907,
      "grad_norm": 2.37827205657959,
      "learning_rate": 0.0002925141001538199,
      "loss": 0.4471,
      "step": 4857
    },
    {
      "epoch": 0.0207576676893101,
      "grad_norm": 2.5341267585754395,
      "learning_rate": 0.0002924713724149718,
      "loss": 0.7878,
      "step": 4858
    },
    {
      "epoch": 0.020761940572737295,
      "grad_norm": 4.035839557647705,
      "learning_rate": 0.00029242864467612375,
      "loss": 2.0865,
      "step": 4859
    },
    {
      "epoch": 0.02076621345616449,
      "grad_norm": 3.1215872764587402,
      "learning_rate": 0.00029238591693727566,
      "loss": 0.9659,
      "step": 4860
    },
    {
      "epoch": 0.020770486339591682,
      "grad_norm": 3.3464837074279785,
      "learning_rate": 0.0002923431891984276,
      "loss": 0.8368,
      "step": 4861
    },
    {
      "epoch": 0.020774759223018878,
      "grad_norm": 0.9465720057487488,
      "learning_rate": 0.00029230046145957954,
      "loss": 0.3939,
      "step": 4862
    },
    {
      "epoch": 0.020779032106446074,
      "grad_norm": 2.4514312744140625,
      "learning_rate": 0.0002922577337207315,
      "loss": 0.7032,
      "step": 4863
    },
    {
      "epoch": 0.020783304989873266,
      "grad_norm": 2.472811222076416,
      "learning_rate": 0.00029221500598188347,
      "loss": 0.7335,
      "step": 4864
    },
    {
      "epoch": 0.02078757787330046,
      "grad_norm": 1.4226045608520508,
      "learning_rate": 0.0002921722782430354,
      "loss": 0.8029,
      "step": 4865
    },
    {
      "epoch": 0.020791850756727653,
      "grad_norm": 2.3148114681243896,
      "learning_rate": 0.00029212955050418734,
      "loss": 0.9075,
      "step": 4866
    },
    {
      "epoch": 0.02079612364015485,
      "grad_norm": 3.181797504425049,
      "learning_rate": 0.00029208682276533925,
      "loss": 1.1106,
      "step": 4867
    },
    {
      "epoch": 0.020800396523582045,
      "grad_norm": 1.2951959371566772,
      "learning_rate": 0.0002920440950264912,
      "loss": 0.4798,
      "step": 4868
    },
    {
      "epoch": 0.020804669407009237,
      "grad_norm": 2.1008362770080566,
      "learning_rate": 0.00029200136728764313,
      "loss": 0.7555,
      "step": 4869
    },
    {
      "epoch": 0.020808942290436432,
      "grad_norm": 0.30888718366622925,
      "learning_rate": 0.0002919586395487951,
      "loss": 0.0806,
      "step": 4870
    },
    {
      "epoch": 0.020813215173863628,
      "grad_norm": 1.364760160446167,
      "learning_rate": 0.00029191591180994706,
      "loss": 0.9064,
      "step": 4871
    },
    {
      "epoch": 0.02081748805729082,
      "grad_norm": 1.6561425924301147,
      "learning_rate": 0.00029187318407109897,
      "loss": 0.6778,
      "step": 4872
    },
    {
      "epoch": 0.020821760940718016,
      "grad_norm": 2.4508655071258545,
      "learning_rate": 0.00029183045633225093,
      "loss": 0.7029,
      "step": 4873
    },
    {
      "epoch": 0.02082603382414521,
      "grad_norm": 0.4738420844078064,
      "learning_rate": 0.00029178772859340284,
      "loss": 0.1594,
      "step": 4874
    },
    {
      "epoch": 0.020830306707572403,
      "grad_norm": 0.6445268392562866,
      "learning_rate": 0.0002917450008545548,
      "loss": 0.2881,
      "step": 4875
    },
    {
      "epoch": 0.0208345795909996,
      "grad_norm": 0.8874511122703552,
      "learning_rate": 0.0002917022731157067,
      "loss": 0.4003,
      "step": 4876
    },
    {
      "epoch": 0.02083885247442679,
      "grad_norm": 1.5484638214111328,
      "learning_rate": 0.0002916595453768587,
      "loss": 0.7908,
      "step": 4877
    },
    {
      "epoch": 0.020843125357853987,
      "grad_norm": 1.6026513576507568,
      "learning_rate": 0.00029161681763801065,
      "loss": 0.8046,
      "step": 4878
    },
    {
      "epoch": 0.020847398241281182,
      "grad_norm": 2.1857001781463623,
      "learning_rate": 0.00029157408989916256,
      "loss": 0.5331,
      "step": 4879
    },
    {
      "epoch": 0.020851671124708374,
      "grad_norm": 3.550808906555176,
      "learning_rate": 0.0002915313621603145,
      "loss": 1.2748,
      "step": 4880
    },
    {
      "epoch": 0.02085594400813557,
      "grad_norm": 2.423091173171997,
      "learning_rate": 0.00029148863442146644,
      "loss": 0.6796,
      "step": 4881
    },
    {
      "epoch": 0.020860216891562765,
      "grad_norm": 0.6794295907020569,
      "learning_rate": 0.0002914459066826184,
      "loss": 0.2875,
      "step": 4882
    },
    {
      "epoch": 0.020864489774989958,
      "grad_norm": 1.5690162181854248,
      "learning_rate": 0.00029140317894377026,
      "loss": 0.7263,
      "step": 4883
    },
    {
      "epoch": 0.020868762658417153,
      "grad_norm": 2.238746166229248,
      "learning_rate": 0.0002913604512049222,
      "loss": 0.6347,
      "step": 4884
    },
    {
      "epoch": 0.02087303554184435,
      "grad_norm": 1.626615285873413,
      "learning_rate": 0.00029131772346607413,
      "loss": 0.7219,
      "step": 4885
    },
    {
      "epoch": 0.02087730842527154,
      "grad_norm": 2.632944107055664,
      "learning_rate": 0.0002912749957272261,
      "loss": 1.126,
      "step": 4886
    },
    {
      "epoch": 0.020881581308698736,
      "grad_norm": 0.5527125597000122,
      "learning_rate": 0.00029123226798837806,
      "loss": 0.2002,
      "step": 4887
    },
    {
      "epoch": 0.020885854192125932,
      "grad_norm": 3.8053297996520996,
      "learning_rate": 0.00029118954024953,
      "loss": 0.9377,
      "step": 4888
    },
    {
      "epoch": 0.020890127075553124,
      "grad_norm": 1.6744232177734375,
      "learning_rate": 0.00029114681251068194,
      "loss": 0.6749,
      "step": 4889
    },
    {
      "epoch": 0.02089439995898032,
      "grad_norm": 0.5419356226921082,
      "learning_rate": 0.00029110408477183385,
      "loss": 0.1916,
      "step": 4890
    },
    {
      "epoch": 0.020898672842407512,
      "grad_norm": 2.4900248050689697,
      "learning_rate": 0.0002910613570329858,
      "loss": 0.7818,
      "step": 4891
    },
    {
      "epoch": 0.020902945725834707,
      "grad_norm": 0.6650251746177673,
      "learning_rate": 0.0002910186292941377,
      "loss": 0.2154,
      "step": 4892
    },
    {
      "epoch": 0.020907218609261903,
      "grad_norm": 1.4869465827941895,
      "learning_rate": 0.0002909759015552897,
      "loss": 0.5617,
      "step": 4893
    },
    {
      "epoch": 0.020911491492689095,
      "grad_norm": 1.8208550214767456,
      "learning_rate": 0.00029093317381644166,
      "loss": 0.4856,
      "step": 4894
    },
    {
      "epoch": 0.02091576437611629,
      "grad_norm": 2.6478867530822754,
      "learning_rate": 0.00029089044607759357,
      "loss": 0.7063,
      "step": 4895
    },
    {
      "epoch": 0.020920037259543486,
      "grad_norm": 2.6254141330718994,
      "learning_rate": 0.00029084771833874553,
      "loss": 0.9732,
      "step": 4896
    },
    {
      "epoch": 0.02092431014297068,
      "grad_norm": 2.6228814125061035,
      "learning_rate": 0.00029080499059989744,
      "loss": 0.5292,
      "step": 4897
    },
    {
      "epoch": 0.020928583026397874,
      "grad_norm": 3.1133272647857666,
      "learning_rate": 0.0002907622628610494,
      "loss": 0.9624,
      "step": 4898
    },
    {
      "epoch": 0.02093285590982507,
      "grad_norm": 2.4847450256347656,
      "learning_rate": 0.0002907195351222013,
      "loss": 0.7567,
      "step": 4899
    },
    {
      "epoch": 0.020937128793252262,
      "grad_norm": 1.515001893043518,
      "learning_rate": 0.0002906768073833533,
      "loss": 0.3509,
      "step": 4900
    },
    {
      "epoch": 0.020941401676679457,
      "grad_norm": 2.427395820617676,
      "learning_rate": 0.00029063407964450525,
      "loss": 0.6143,
      "step": 4901
    },
    {
      "epoch": 0.02094567456010665,
      "grad_norm": 2.4799323081970215,
      "learning_rate": 0.00029059135190565716,
      "loss": 0.6421,
      "step": 4902
    },
    {
      "epoch": 0.020949947443533845,
      "grad_norm": 1.240065097808838,
      "learning_rate": 0.0002905486241668091,
      "loss": 0.4188,
      "step": 4903
    },
    {
      "epoch": 0.02095422032696104,
      "grad_norm": 2.2029905319213867,
      "learning_rate": 0.00029050589642796103,
      "loss": 0.6901,
      "step": 4904
    },
    {
      "epoch": 0.020958493210388233,
      "grad_norm": 3.9181408882141113,
      "learning_rate": 0.000290463168689113,
      "loss": 1.1716,
      "step": 4905
    },
    {
      "epoch": 0.02096276609381543,
      "grad_norm": 2.75167179107666,
      "learning_rate": 0.0002904204409502649,
      "loss": 0.6484,
      "step": 4906
    },
    {
      "epoch": 0.020967038977242624,
      "grad_norm": 4.674277305603027,
      "learning_rate": 0.0002903777132114169,
      "loss": 1.8616,
      "step": 4907
    },
    {
      "epoch": 0.020971311860669816,
      "grad_norm": 1.0174837112426758,
      "learning_rate": 0.00029033498547256884,
      "loss": 0.2235,
      "step": 4908
    },
    {
      "epoch": 0.02097558474409701,
      "grad_norm": 2.413522243499756,
      "learning_rate": 0.00029029225773372075,
      "loss": 0.9684,
      "step": 4909
    },
    {
      "epoch": 0.020979857627524207,
      "grad_norm": 1.9016679525375366,
      "learning_rate": 0.0002902495299948727,
      "loss": 0.2721,
      "step": 4910
    },
    {
      "epoch": 0.0209841305109514,
      "grad_norm": 3.1356894969940186,
      "learning_rate": 0.0002902068022560246,
      "loss": 0.8766,
      "step": 4911
    },
    {
      "epoch": 0.020988403394378595,
      "grad_norm": 2.126260995864868,
      "learning_rate": 0.0002901640745171766,
      "loss": 0.3831,
      "step": 4912
    },
    {
      "epoch": 0.02099267627780579,
      "grad_norm": 4.514726161956787,
      "learning_rate": 0.0002901213467783285,
      "loss": 1.6235,
      "step": 4913
    },
    {
      "epoch": 0.020996949161232983,
      "grad_norm": 2.5967190265655518,
      "learning_rate": 0.00029007861903948047,
      "loss": 0.3061,
      "step": 4914
    },
    {
      "epoch": 0.02100122204466018,
      "grad_norm": 100.32125091552734,
      "learning_rate": 0.0002900358913006324,
      "loss": 0.7406,
      "step": 4915
    },
    {
      "epoch": 0.02100549492808737,
      "grad_norm": 4.147606372833252,
      "learning_rate": 0.0002899931635617843,
      "loss": 1.033,
      "step": 4916
    },
    {
      "epoch": 0.021009767811514566,
      "grad_norm": 5.557517051696777,
      "learning_rate": 0.00028995043582293625,
      "loss": 1.0376,
      "step": 4917
    },
    {
      "epoch": 0.02101404069494176,
      "grad_norm": 4.771697044372559,
      "learning_rate": 0.00028990770808408816,
      "loss": 1.0793,
      "step": 4918
    },
    {
      "epoch": 0.021018313578368954,
      "grad_norm": 3.0669119358062744,
      "learning_rate": 0.00028986498034524013,
      "loss": 0.5299,
      "step": 4919
    },
    {
      "epoch": 0.02102258646179615,
      "grad_norm": 7.808586120605469,
      "learning_rate": 0.00028982225260639204,
      "loss": 1.6639,
      "step": 4920
    },
    {
      "epoch": 0.021026859345223345,
      "grad_norm": 4.048057556152344,
      "learning_rate": 0.000289779524867544,
      "loss": 0.7798,
      "step": 4921
    },
    {
      "epoch": 0.021031132228650537,
      "grad_norm": 44.6035041809082,
      "learning_rate": 0.0002897367971286959,
      "loss": 0.9274,
      "step": 4922
    },
    {
      "epoch": 0.021035405112077733,
      "grad_norm": 44.35900115966797,
      "learning_rate": 0.0002896940693898479,
      "loss": 2.0131,
      "step": 4923
    },
    {
      "epoch": 0.021039677995504928,
      "grad_norm": 4.701767444610596,
      "learning_rate": 0.00028965134165099984,
      "loss": 0.5046,
      "step": 4924
    },
    {
      "epoch": 0.02104395087893212,
      "grad_norm": 5.521541118621826,
      "learning_rate": 0.00028960861391215176,
      "loss": 1.5023,
      "step": 4925
    },
    {
      "epoch": 0.021048223762359316,
      "grad_norm": 2.1330513954162598,
      "learning_rate": 0.0002895658861733037,
      "loss": 0.442,
      "step": 4926
    },
    {
      "epoch": 0.021052496645786508,
      "grad_norm": 2.1048872470855713,
      "learning_rate": 0.00028952315843445563,
      "loss": 0.4778,
      "step": 4927
    },
    {
      "epoch": 0.021056769529213704,
      "grad_norm": 3.596824884414673,
      "learning_rate": 0.0002894804306956076,
      "loss": 0.8602,
      "step": 4928
    },
    {
      "epoch": 0.0210610424126409,
      "grad_norm": 4.814186096191406,
      "learning_rate": 0.0002894377029567595,
      "loss": 1.4537,
      "step": 4929
    },
    {
      "epoch": 0.02106531529606809,
      "grad_norm": 2.621858596801758,
      "learning_rate": 0.00028939497521791147,
      "loss": 0.9443,
      "step": 4930
    },
    {
      "epoch": 0.021069588179495287,
      "grad_norm": 2.1566553115844727,
      "learning_rate": 0.00028935224747906344,
      "loss": 0.7505,
      "step": 4931
    },
    {
      "epoch": 0.021073861062922482,
      "grad_norm": 0.5109860897064209,
      "learning_rate": 0.00028930951974021535,
      "loss": 0.1843,
      "step": 4932
    },
    {
      "epoch": 0.021078133946349675,
      "grad_norm": 1.0024964809417725,
      "learning_rate": 0.0002892667920013673,
      "loss": 0.278,
      "step": 4933
    },
    {
      "epoch": 0.02108240682977687,
      "grad_norm": 4.27151346206665,
      "learning_rate": 0.0002892240642625192,
      "loss": 1.1045,
      "step": 4934
    },
    {
      "epoch": 0.021086679713204066,
      "grad_norm": 0.5783336758613586,
      "learning_rate": 0.0002891813365236712,
      "loss": 0.165,
      "step": 4935
    },
    {
      "epoch": 0.021090952596631258,
      "grad_norm": 2.664881944656372,
      "learning_rate": 0.0002891386087848231,
      "loss": 0.9535,
      "step": 4936
    },
    {
      "epoch": 0.021095225480058453,
      "grad_norm": 5.5026350021362305,
      "learning_rate": 0.00028909588104597506,
      "loss": 1.4355,
      "step": 4937
    },
    {
      "epoch": 0.02109949836348565,
      "grad_norm": 6.054020404815674,
      "learning_rate": 0.00028905315330712703,
      "loss": 3.1205,
      "step": 4938
    },
    {
      "epoch": 0.02110377124691284,
      "grad_norm": 1.801958680152893,
      "learning_rate": 0.00028901042556827894,
      "loss": 0.4544,
      "step": 4939
    },
    {
      "epoch": 0.021108044130340037,
      "grad_norm": 2.0806753635406494,
      "learning_rate": 0.0002889676978294309,
      "loss": 0.4771,
      "step": 4940
    },
    {
      "epoch": 0.02111231701376723,
      "grad_norm": 2.2687883377075195,
      "learning_rate": 0.0002889249700905828,
      "loss": 0.8226,
      "step": 4941
    },
    {
      "epoch": 0.021116589897194425,
      "grad_norm": 215.94020080566406,
      "learning_rate": 0.0002888822423517348,
      "loss": 2.3504,
      "step": 4942
    },
    {
      "epoch": 0.02112086278062162,
      "grad_norm": 2.0461833477020264,
      "learning_rate": 0.0002888395146128867,
      "loss": 0.726,
      "step": 4943
    },
    {
      "epoch": 0.021125135664048812,
      "grad_norm": 2.0469839572906494,
      "learning_rate": 0.00028879678687403866,
      "loss": 0.7636,
      "step": 4944
    },
    {
      "epoch": 0.021129408547476008,
      "grad_norm": 4.922983169555664,
      "learning_rate": 0.0002887540591351906,
      "loss": 1.5341,
      "step": 4945
    },
    {
      "epoch": 0.021133681430903203,
      "grad_norm": 1.8402332067489624,
      "learning_rate": 0.00028871133139634253,
      "loss": 0.6523,
      "step": 4946
    },
    {
      "epoch": 0.021137954314330396,
      "grad_norm": 4.341061592102051,
      "learning_rate": 0.0002886686036574945,
      "loss": 1.214,
      "step": 4947
    },
    {
      "epoch": 0.02114222719775759,
      "grad_norm": 3.2423105239868164,
      "learning_rate": 0.0002886258759186464,
      "loss": 1.3969,
      "step": 4948
    },
    {
      "epoch": 0.021146500081184787,
      "grad_norm": 5.456667423248291,
      "learning_rate": 0.0002885831481797983,
      "loss": 1.0699,
      "step": 4949
    },
    {
      "epoch": 0.02115077296461198,
      "grad_norm": 4.988929748535156,
      "learning_rate": 0.00028854042044095023,
      "loss": 0.9747,
      "step": 4950
    },
    {
      "epoch": 0.021155045848039174,
      "grad_norm": 1.7499604225158691,
      "learning_rate": 0.0002884976927021022,
      "loss": 0.4512,
      "step": 4951
    },
    {
      "epoch": 0.021159318731466367,
      "grad_norm": 3.394974946975708,
      "learning_rate": 0.0002884549649632541,
      "loss": 0.9454,
      "step": 4952
    },
    {
      "epoch": 0.021163591614893562,
      "grad_norm": 4.601411819458008,
      "learning_rate": 0.00028841223722440607,
      "loss": 1.1654,
      "step": 4953
    },
    {
      "epoch": 0.021167864498320758,
      "grad_norm": 1.3221510648727417,
      "learning_rate": 0.00028836950948555803,
      "loss": 0.2564,
      "step": 4954
    },
    {
      "epoch": 0.02117213738174795,
      "grad_norm": 3.4450807571411133,
      "learning_rate": 0.00028832678174670995,
      "loss": 1.1521,
      "step": 4955
    },
    {
      "epoch": 0.021176410265175145,
      "grad_norm": 2.278386354446411,
      "learning_rate": 0.0002882840540078619,
      "loss": 0.7399,
      "step": 4956
    },
    {
      "epoch": 0.02118068314860234,
      "grad_norm": 3.504335641860962,
      "learning_rate": 0.0002882413262690138,
      "loss": 0.7567,
      "step": 4957
    },
    {
      "epoch": 0.021184956032029533,
      "grad_norm": 2.4012813568115234,
      "learning_rate": 0.0002881985985301658,
      "loss": 0.6508,
      "step": 4958
    },
    {
      "epoch": 0.02118922891545673,
      "grad_norm": 3.321110486984253,
      "learning_rate": 0.0002881558707913177,
      "loss": 0.9132,
      "step": 4959
    },
    {
      "epoch": 0.021193501798883924,
      "grad_norm": 2.4738359451293945,
      "learning_rate": 0.00028811314305246966,
      "loss": 0.7898,
      "step": 4960
    },
    {
      "epoch": 0.021197774682311116,
      "grad_norm": 3.4401657581329346,
      "learning_rate": 0.0002880704153136216,
      "loss": 0.8211,
      "step": 4961
    },
    {
      "epoch": 0.021202047565738312,
      "grad_norm": 2.9248342514038086,
      "learning_rate": 0.00028802768757477354,
      "loss": 0.8942,
      "step": 4962
    },
    {
      "epoch": 0.021206320449165508,
      "grad_norm": 2.653895854949951,
      "learning_rate": 0.0002879849598359255,
      "loss": 0.9115,
      "step": 4963
    },
    {
      "epoch": 0.0212105933325927,
      "grad_norm": 4.348575115203857,
      "learning_rate": 0.0002879422320970774,
      "loss": 0.919,
      "step": 4964
    },
    {
      "epoch": 0.021214866216019895,
      "grad_norm": 1.479904055595398,
      "learning_rate": 0.0002878995043582294,
      "loss": 0.4327,
      "step": 4965
    },
    {
      "epoch": 0.021219139099447087,
      "grad_norm": 1.304783582687378,
      "learning_rate": 0.0002878567766193813,
      "loss": 0.306,
      "step": 4966
    },
    {
      "epoch": 0.021223411982874283,
      "grad_norm": 2.6607627868652344,
      "learning_rate": 0.00028781404888053325,
      "loss": 0.6807,
      "step": 4967
    },
    {
      "epoch": 0.02122768486630148,
      "grad_norm": 1.2672054767608643,
      "learning_rate": 0.0002877713211416852,
      "loss": 0.2791,
      "step": 4968
    },
    {
      "epoch": 0.02123195774972867,
      "grad_norm": 6.377174377441406,
      "learning_rate": 0.00028772859340283713,
      "loss": 3.1525,
      "step": 4969
    },
    {
      "epoch": 0.021236230633155866,
      "grad_norm": 5.000580787658691,
      "learning_rate": 0.0002876858656639891,
      "loss": 2.0738,
      "step": 4970
    },
    {
      "epoch": 0.021240503516583062,
      "grad_norm": 3.417438507080078,
      "learning_rate": 0.000287643137925141,
      "loss": 1.32,
      "step": 4971
    },
    {
      "epoch": 0.021244776400010254,
      "grad_norm": 1.1030479669570923,
      "learning_rate": 0.00028760041018629297,
      "loss": 0.3274,
      "step": 4972
    },
    {
      "epoch": 0.02124904928343745,
      "grad_norm": 3.2207155227661133,
      "learning_rate": 0.0002875576824474449,
      "loss": 0.7997,
      "step": 4973
    },
    {
      "epoch": 0.021253322166864645,
      "grad_norm": 1.1603344678878784,
      "learning_rate": 0.00028751495470859685,
      "loss": 0.3082,
      "step": 4974
    },
    {
      "epoch": 0.021257595050291837,
      "grad_norm": 1.8988286256790161,
      "learning_rate": 0.0002874722269697488,
      "loss": 0.4941,
      "step": 4975
    },
    {
      "epoch": 0.021261867933719033,
      "grad_norm": 0.8203104734420776,
      "learning_rate": 0.0002874294992309007,
      "loss": 0.2393,
      "step": 4976
    },
    {
      "epoch": 0.021266140817146225,
      "grad_norm": 3.0121326446533203,
      "learning_rate": 0.0002873867714920527,
      "loss": 0.6688,
      "step": 4977
    },
    {
      "epoch": 0.02127041370057342,
      "grad_norm": 3.0170302391052246,
      "learning_rate": 0.0002873440437532046,
      "loss": 1.0496,
      "step": 4978
    },
    {
      "epoch": 0.021274686584000616,
      "grad_norm": 3.3569753170013428,
      "learning_rate": 0.00028730131601435656,
      "loss": 0.7513,
      "step": 4979
    },
    {
      "epoch": 0.02127895946742781,
      "grad_norm": 2.238436698913574,
      "learning_rate": 0.00028725858827550847,
      "loss": 0.5231,
      "step": 4980
    },
    {
      "epoch": 0.021283232350855004,
      "grad_norm": 2.71447491645813,
      "learning_rate": 0.0002872158605366604,
      "loss": 0.9232,
      "step": 4981
    },
    {
      "epoch": 0.0212875052342822,
      "grad_norm": 2.544163465499878,
      "learning_rate": 0.0002871731327978123,
      "loss": 0.903,
      "step": 4982
    },
    {
      "epoch": 0.02129177811770939,
      "grad_norm": 3.476637125015259,
      "learning_rate": 0.00028713040505896426,
      "loss": 1.0557,
      "step": 4983
    },
    {
      "epoch": 0.021296051001136587,
      "grad_norm": 2.1771464347839355,
      "learning_rate": 0.0002870876773201162,
      "loss": 0.5346,
      "step": 4984
    },
    {
      "epoch": 0.021300323884563783,
      "grad_norm": 2.536973476409912,
      "learning_rate": 0.00028704494958126813,
      "loss": 0.7823,
      "step": 4985
    },
    {
      "epoch": 0.021304596767990975,
      "grad_norm": 1.6687384843826294,
      "learning_rate": 0.0002870022218424201,
      "loss": 0.3296,
      "step": 4986
    },
    {
      "epoch": 0.02130886965141817,
      "grad_norm": 2.61676287651062,
      "learning_rate": 0.000286959494103572,
      "loss": 0.7795,
      "step": 4987
    },
    {
      "epoch": 0.021313142534845363,
      "grad_norm": 5.273221969604492,
      "learning_rate": 0.000286916766364724,
      "loss": 2.23,
      "step": 4988
    },
    {
      "epoch": 0.021317415418272558,
      "grad_norm": 1.611491084098816,
      "learning_rate": 0.0002868740386258759,
      "loss": 0.3173,
      "step": 4989
    },
    {
      "epoch": 0.021321688301699754,
      "grad_norm": 0.9878186583518982,
      "learning_rate": 0.00028683131088702785,
      "loss": 0.1963,
      "step": 4990
    },
    {
      "epoch": 0.021325961185126946,
      "grad_norm": 1.5658009052276611,
      "learning_rate": 0.0002867885831481798,
      "loss": 0.2972,
      "step": 4991
    },
    {
      "epoch": 0.02133023406855414,
      "grad_norm": 4.483652591705322,
      "learning_rate": 0.0002867458554093317,
      "loss": 1.188,
      "step": 4992
    },
    {
      "epoch": 0.021334506951981337,
      "grad_norm": 1.5532411336898804,
      "learning_rate": 0.0002867031276704837,
      "loss": 0.5055,
      "step": 4993
    },
    {
      "epoch": 0.02133877983540853,
      "grad_norm": 2.8490729331970215,
      "learning_rate": 0.0002866603999316356,
      "loss": 0.6055,
      "step": 4994
    },
    {
      "epoch": 0.021343052718835725,
      "grad_norm": 5.0160017013549805,
      "learning_rate": 0.00028661767219278757,
      "loss": 1.3171,
      "step": 4995
    },
    {
      "epoch": 0.02134732560226292,
      "grad_norm": 1.8209648132324219,
      "learning_rate": 0.0002865749444539395,
      "loss": 0.5779,
      "step": 4996
    },
    {
      "epoch": 0.021351598485690113,
      "grad_norm": 1.2117278575897217,
      "learning_rate": 0.00028653221671509144,
      "loss": 0.4062,
      "step": 4997
    },
    {
      "epoch": 0.021355871369117308,
      "grad_norm": 2.3480710983276367,
      "learning_rate": 0.0002864894889762434,
      "loss": 0.6345,
      "step": 4998
    },
    {
      "epoch": 0.021360144252544504,
      "grad_norm": 1.8110737800598145,
      "learning_rate": 0.0002864467612373953,
      "loss": 0.3792,
      "step": 4999
    },
    {
      "epoch": 0.021364417135971696,
      "grad_norm": 4.518646717071533,
      "learning_rate": 0.0002864040334985473,
      "loss": 1.3482,
      "step": 5000
    },
    {
      "epoch": 0.02136869001939889,
      "grad_norm": 4.393507480621338,
      "learning_rate": 0.0002863613057596992,
      "loss": 1.0049,
      "step": 5001
    },
    {
      "epoch": 0.021372962902826084,
      "grad_norm": 0.8401092886924744,
      "learning_rate": 0.00028631857802085116,
      "loss": 0.3091,
      "step": 5002
    },
    {
      "epoch": 0.02137723578625328,
      "grad_norm": 3.665069341659546,
      "learning_rate": 0.00028627585028200307,
      "loss": 1.2571,
      "step": 5003
    },
    {
      "epoch": 0.021381508669680475,
      "grad_norm": 2.204521894454956,
      "learning_rate": 0.00028623312254315503,
      "loss": 1.2026,
      "step": 5004
    },
    {
      "epoch": 0.021385781553107667,
      "grad_norm": 1.634628415107727,
      "learning_rate": 0.000286190394804307,
      "loss": 0.6622,
      "step": 5005
    },
    {
      "epoch": 0.021390054436534862,
      "grad_norm": 1.7428349256515503,
      "learning_rate": 0.0002861476670654589,
      "loss": 0.4613,
      "step": 5006
    },
    {
      "epoch": 0.021394327319962058,
      "grad_norm": 3.2224133014678955,
      "learning_rate": 0.0002861049393266109,
      "loss": 1.0792,
      "step": 5007
    },
    {
      "epoch": 0.02139860020338925,
      "grad_norm": 4.528894901275635,
      "learning_rate": 0.0002860622115877628,
      "loss": 1.079,
      "step": 5008
    },
    {
      "epoch": 0.021402873086816446,
      "grad_norm": 2.2460145950317383,
      "learning_rate": 0.00028601948384891475,
      "loss": 0.8498,
      "step": 5009
    },
    {
      "epoch": 0.02140714597024364,
      "grad_norm": 2.8021161556243896,
      "learning_rate": 0.00028597675611006666,
      "loss": 0.8205,
      "step": 5010
    },
    {
      "epoch": 0.021411418853670833,
      "grad_norm": 3.425119400024414,
      "learning_rate": 0.0002859340283712186,
      "loss": 1.264,
      "step": 5011
    },
    {
      "epoch": 0.02141569173709803,
      "grad_norm": 3.277216911315918,
      "learning_rate": 0.0002858913006323706,
      "loss": 0.8204,
      "step": 5012
    },
    {
      "epoch": 0.02141996462052522,
      "grad_norm": 1.8018156290054321,
      "learning_rate": 0.0002858485728935225,
      "loss": 0.5979,
      "step": 5013
    },
    {
      "epoch": 0.021424237503952417,
      "grad_norm": 1.7347431182861328,
      "learning_rate": 0.0002858058451546744,
      "loss": 0.5641,
      "step": 5014
    },
    {
      "epoch": 0.021428510387379612,
      "grad_norm": 1.4820106029510498,
      "learning_rate": 0.0002857631174158263,
      "loss": 0.421,
      "step": 5015
    },
    {
      "epoch": 0.021432783270806804,
      "grad_norm": 1.1459356546401978,
      "learning_rate": 0.0002857203896769783,
      "loss": 0.3392,
      "step": 5016
    },
    {
      "epoch": 0.021437056154234,
      "grad_norm": 5.214770793914795,
      "learning_rate": 0.0002856776619381302,
      "loss": 1.4635,
      "step": 5017
    },
    {
      "epoch": 0.021441329037661196,
      "grad_norm": 1.0029454231262207,
      "learning_rate": 0.00028563493419928216,
      "loss": 0.335,
      "step": 5018
    },
    {
      "epoch": 0.021445601921088388,
      "grad_norm": 2.1382861137390137,
      "learning_rate": 0.0002855922064604341,
      "loss": 0.5725,
      "step": 5019
    },
    {
      "epoch": 0.021449874804515583,
      "grad_norm": 1.3257815837860107,
      "learning_rate": 0.00028554947872158604,
      "loss": 0.2718,
      "step": 5020
    },
    {
      "epoch": 0.02145414768794278,
      "grad_norm": 21.151607513427734,
      "learning_rate": 0.000285506750982738,
      "loss": 1.2181,
      "step": 5021
    },
    {
      "epoch": 0.02145842057136997,
      "grad_norm": 4.8628926277160645,
      "learning_rate": 0.0002854640232438899,
      "loss": 0.247,
      "step": 5022
    },
    {
      "epoch": 0.021462693454797167,
      "grad_norm": 6.98976469039917,
      "learning_rate": 0.0002854212955050419,
      "loss": 0.8795,
      "step": 5023
    },
    {
      "epoch": 0.021466966338224362,
      "grad_norm": 10.826004981994629,
      "learning_rate": 0.0002853785677661938,
      "loss": 1.7701,
      "step": 5024
    },
    {
      "epoch": 0.021471239221651554,
      "grad_norm": 1.0660268068313599,
      "learning_rate": 0.00028533584002734576,
      "loss": 0.1771,
      "step": 5025
    },
    {
      "epoch": 0.02147551210507875,
      "grad_norm": 2.0437395572662354,
      "learning_rate": 0.00028529311228849767,
      "loss": 0.2561,
      "step": 5026
    },
    {
      "epoch": 0.021479784988505942,
      "grad_norm": 4.993447303771973,
      "learning_rate": 0.00028525038454964963,
      "loss": 1.3876,
      "step": 5027
    },
    {
      "epoch": 0.021484057871933138,
      "grad_norm": 3.9130563735961914,
      "learning_rate": 0.0002852076568108016,
      "loss": 0.7656,
      "step": 5028
    },
    {
      "epoch": 0.021488330755360333,
      "grad_norm": 2.021549701690674,
      "learning_rate": 0.0002851649290719535,
      "loss": 0.2515,
      "step": 5029
    },
    {
      "epoch": 0.021492603638787525,
      "grad_norm": 5.342154502868652,
      "learning_rate": 0.0002851222013331055,
      "loss": 0.8664,
      "step": 5030
    },
    {
      "epoch": 0.02149687652221472,
      "grad_norm": 4.907790660858154,
      "learning_rate": 0.0002850794735942574,
      "loss": 1.5524,
      "step": 5031
    },
    {
      "epoch": 0.021501149405641917,
      "grad_norm": 0.6597476601600647,
      "learning_rate": 0.00028503674585540935,
      "loss": 0.0822,
      "step": 5032
    },
    {
      "epoch": 0.02150542228906911,
      "grad_norm": 3.2928833961486816,
      "learning_rate": 0.00028499401811656126,
      "loss": 0.6057,
      "step": 5033
    },
    {
      "epoch": 0.021509695172496304,
      "grad_norm": 5.275197982788086,
      "learning_rate": 0.0002849512903777132,
      "loss": 1.2847,
      "step": 5034
    },
    {
      "epoch": 0.0215139680559235,
      "grad_norm": 1.8831419944763184,
      "learning_rate": 0.0002849085626388652,
      "loss": 0.2313,
      "step": 5035
    },
    {
      "epoch": 0.021518240939350692,
      "grad_norm": 5.145633697509766,
      "learning_rate": 0.0002848658349000171,
      "loss": 0.8017,
      "step": 5036
    },
    {
      "epoch": 0.021522513822777888,
      "grad_norm": 0.4530506134033203,
      "learning_rate": 0.00028482310716116906,
      "loss": 0.0444,
      "step": 5037
    },
    {
      "epoch": 0.02152678670620508,
      "grad_norm": 4.319029808044434,
      "learning_rate": 0.000284780379422321,
      "loss": 0.8075,
      "step": 5038
    },
    {
      "epoch": 0.021531059589632275,
      "grad_norm": 2.430983066558838,
      "learning_rate": 0.00028473765168347294,
      "loss": 0.6427,
      "step": 5039
    },
    {
      "epoch": 0.02153533247305947,
      "grad_norm": 3.8922150135040283,
      "learning_rate": 0.00028469492394462485,
      "loss": 1.0929,
      "step": 5040
    },
    {
      "epoch": 0.021539605356486663,
      "grad_norm": 3.3702962398529053,
      "learning_rate": 0.0002846521962057768,
      "loss": 0.4763,
      "step": 5041
    },
    {
      "epoch": 0.02154387823991386,
      "grad_norm": 2.0798230171203613,
      "learning_rate": 0.0002846094684669288,
      "loss": 0.5954,
      "step": 5042
    },
    {
      "epoch": 0.021548151123341054,
      "grad_norm": 3.6912641525268555,
      "learning_rate": 0.0002845667407280807,
      "loss": 0.984,
      "step": 5043
    },
    {
      "epoch": 0.021552424006768246,
      "grad_norm": 3.3275578022003174,
      "learning_rate": 0.00028452401298923266,
      "loss": 1.016,
      "step": 5044
    },
    {
      "epoch": 0.021556696890195442,
      "grad_norm": 1.142090082168579,
      "learning_rate": 0.00028448128525038457,
      "loss": 0.4257,
      "step": 5045
    },
    {
      "epoch": 0.021560969773622637,
      "grad_norm": 3.0518417358398438,
      "learning_rate": 0.00028443855751153653,
      "loss": 0.7113,
      "step": 5046
    },
    {
      "epoch": 0.02156524265704983,
      "grad_norm": 2.020317554473877,
      "learning_rate": 0.0002843958297726884,
      "loss": 0.7689,
      "step": 5047
    },
    {
      "epoch": 0.021569515540477025,
      "grad_norm": 5.532323360443115,
      "learning_rate": 0.00028435310203384035,
      "loss": 1.1688,
      "step": 5048
    },
    {
      "epoch": 0.02157378842390422,
      "grad_norm": 3.0213286876678467,
      "learning_rate": 0.00028431037429499226,
      "loss": 0.8103,
      "step": 5049
    },
    {
      "epoch": 0.021578061307331413,
      "grad_norm": 0.888170599937439,
      "learning_rate": 0.00028426764655614423,
      "loss": 0.2642,
      "step": 5050
    },
    {
      "epoch": 0.02158233419075861,
      "grad_norm": 1.5603135824203491,
      "learning_rate": 0.0002842249188172962,
      "loss": 0.4572,
      "step": 5051
    },
    {
      "epoch": 0.0215866070741858,
      "grad_norm": 3.0029313564300537,
      "learning_rate": 0.0002841821910784481,
      "loss": 0.8498,
      "step": 5052
    },
    {
      "epoch": 0.021590879957612996,
      "grad_norm": 3.4796392917633057,
      "learning_rate": 0.00028413946333960007,
      "loss": 0.8164,
      "step": 5053
    },
    {
      "epoch": 0.021595152841040192,
      "grad_norm": 4.878078937530518,
      "learning_rate": 0.000284096735600752,
      "loss": 1.4969,
      "step": 5054
    },
    {
      "epoch": 0.021599425724467384,
      "grad_norm": 1.9458622932434082,
      "learning_rate": 0.00028405400786190395,
      "loss": 0.6691,
      "step": 5055
    },
    {
      "epoch": 0.02160369860789458,
      "grad_norm": 2.1889381408691406,
      "learning_rate": 0.00028401128012305586,
      "loss": 0.6527,
      "step": 5056
    },
    {
      "epoch": 0.021607971491321775,
      "grad_norm": 2.7951583862304688,
      "learning_rate": 0.0002839685523842078,
      "loss": 1.0346,
      "step": 5057
    },
    {
      "epoch": 0.021612244374748967,
      "grad_norm": 4.184629917144775,
      "learning_rate": 0.0002839258246453598,
      "loss": 1.0463,
      "step": 5058
    },
    {
      "epoch": 0.021616517258176163,
      "grad_norm": 2.1515390872955322,
      "learning_rate": 0.0002838830969065117,
      "loss": 0.6541,
      "step": 5059
    },
    {
      "epoch": 0.02162079014160336,
      "grad_norm": 1.6112754344940186,
      "learning_rate": 0.00028384036916766366,
      "loss": 0.4035,
      "step": 5060
    },
    {
      "epoch": 0.02162506302503055,
      "grad_norm": 2.784895896911621,
      "learning_rate": 0.0002837976414288156,
      "loss": 0.9454,
      "step": 5061
    },
    {
      "epoch": 0.021629335908457746,
      "grad_norm": 4.49472188949585,
      "learning_rate": 0.00028375491368996754,
      "loss": 1.2912,
      "step": 5062
    },
    {
      "epoch": 0.021633608791884938,
      "grad_norm": 1.4690117835998535,
      "learning_rate": 0.00028371218595111945,
      "loss": 0.3523,
      "step": 5063
    },
    {
      "epoch": 0.021637881675312134,
      "grad_norm": 3.031071424484253,
      "learning_rate": 0.0002836694582122714,
      "loss": 0.8407,
      "step": 5064
    },
    {
      "epoch": 0.02164215455873933,
      "grad_norm": 1.783519983291626,
      "learning_rate": 0.0002836267304734234,
      "loss": 0.5858,
      "step": 5065
    },
    {
      "epoch": 0.02164642744216652,
      "grad_norm": 1.9475911855697632,
      "learning_rate": 0.0002835840027345753,
      "loss": 0.4885,
      "step": 5066
    },
    {
      "epoch": 0.021650700325593717,
      "grad_norm": 0.5837293863296509,
      "learning_rate": 0.00028354127499572725,
      "loss": 0.1603,
      "step": 5067
    },
    {
      "epoch": 0.021654973209020913,
      "grad_norm": 2.1792075634002686,
      "learning_rate": 0.00028349854725687917,
      "loss": 0.5136,
      "step": 5068
    },
    {
      "epoch": 0.021659246092448105,
      "grad_norm": 1.920276165008545,
      "learning_rate": 0.00028345581951803113,
      "loss": 0.4891,
      "step": 5069
    },
    {
      "epoch": 0.0216635189758753,
      "grad_norm": 1.1559616327285767,
      "learning_rate": 0.00028341309177918304,
      "loss": 0.2274,
      "step": 5070
    },
    {
      "epoch": 0.021667791859302496,
      "grad_norm": 3.297158718109131,
      "learning_rate": 0.000283370364040335,
      "loss": 0.7625,
      "step": 5071
    },
    {
      "epoch": 0.021672064742729688,
      "grad_norm": 1.4216519594192505,
      "learning_rate": 0.00028332763630148697,
      "loss": 0.6791,
      "step": 5072
    },
    {
      "epoch": 0.021676337626156884,
      "grad_norm": 4.2809600830078125,
      "learning_rate": 0.0002832849085626389,
      "loss": 0.8733,
      "step": 5073
    },
    {
      "epoch": 0.02168061050958408,
      "grad_norm": 2.1033287048339844,
      "learning_rate": 0.00028324218082379085,
      "loss": 0.5243,
      "step": 5074
    },
    {
      "epoch": 0.02168488339301127,
      "grad_norm": 3.307528257369995,
      "learning_rate": 0.00028319945308494276,
      "loss": 1.0901,
      "step": 5075
    },
    {
      "epoch": 0.021689156276438467,
      "grad_norm": 2.4210116863250732,
      "learning_rate": 0.0002831567253460947,
      "loss": 1.1046,
      "step": 5076
    },
    {
      "epoch": 0.02169342915986566,
      "grad_norm": 2.798436164855957,
      "learning_rate": 0.00028311399760724663,
      "loss": 0.7424,
      "step": 5077
    },
    {
      "epoch": 0.021697702043292855,
      "grad_norm": 2.2322967052459717,
      "learning_rate": 0.0002830712698683986,
      "loss": 0.5613,
      "step": 5078
    },
    {
      "epoch": 0.02170197492672005,
      "grad_norm": 2.302718162536621,
      "learning_rate": 0.00028302854212955056,
      "loss": 1.05,
      "step": 5079
    },
    {
      "epoch": 0.021706247810147242,
      "grad_norm": 2.2444145679473877,
      "learning_rate": 0.0002829858143907024,
      "loss": 1.0499,
      "step": 5080
    },
    {
      "epoch": 0.021710520693574438,
      "grad_norm": 3.596442222595215,
      "learning_rate": 0.0002829430866518544,
      "loss": 0.9485,
      "step": 5081
    },
    {
      "epoch": 0.021714793577001634,
      "grad_norm": 3.69156813621521,
      "learning_rate": 0.0002829003589130063,
      "loss": 0.7581,
      "step": 5082
    },
    {
      "epoch": 0.021719066460428826,
      "grad_norm": 0.6656892895698547,
      "learning_rate": 0.00028285763117415826,
      "loss": 0.1302,
      "step": 5083
    },
    {
      "epoch": 0.02172333934385602,
      "grad_norm": 0.9526717066764832,
      "learning_rate": 0.00028281490343531017,
      "loss": 0.37,
      "step": 5084
    },
    {
      "epoch": 0.021727612227283217,
      "grad_norm": 1.1843262910842896,
      "learning_rate": 0.00028277217569646214,
      "loss": 0.569,
      "step": 5085
    },
    {
      "epoch": 0.02173188511071041,
      "grad_norm": 3.4613964557647705,
      "learning_rate": 0.00028272944795761405,
      "loss": 0.815,
      "step": 5086
    },
    {
      "epoch": 0.021736157994137605,
      "grad_norm": 2.8747217655181885,
      "learning_rate": 0.000282686720218766,
      "loss": 0.8942,
      "step": 5087
    },
    {
      "epoch": 0.021740430877564797,
      "grad_norm": 1.0797057151794434,
      "learning_rate": 0.000282643992479918,
      "loss": 0.3641,
      "step": 5088
    },
    {
      "epoch": 0.021744703760991992,
      "grad_norm": 3.594487428665161,
      "learning_rate": 0.0002826012647410699,
      "loss": 1.486,
      "step": 5089
    },
    {
      "epoch": 0.021748976644419188,
      "grad_norm": 2.524094820022583,
      "learning_rate": 0.00028255853700222185,
      "loss": 0.5666,
      "step": 5090
    },
    {
      "epoch": 0.02175324952784638,
      "grad_norm": 1.8177070617675781,
      "learning_rate": 0.00028251580926337376,
      "loss": 0.445,
      "step": 5091
    },
    {
      "epoch": 0.021757522411273576,
      "grad_norm": 2.9240846633911133,
      "learning_rate": 0.00028247308152452573,
      "loss": 0.7168,
      "step": 5092
    },
    {
      "epoch": 0.02176179529470077,
      "grad_norm": 5.098639965057373,
      "learning_rate": 0.00028243035378567764,
      "loss": 1.3126,
      "step": 5093
    },
    {
      "epoch": 0.021766068178127963,
      "grad_norm": 1.4805859327316284,
      "learning_rate": 0.0002823876260468296,
      "loss": 0.5573,
      "step": 5094
    },
    {
      "epoch": 0.02177034106155516,
      "grad_norm": 2.731402635574341,
      "learning_rate": 0.00028234489830798157,
      "loss": 0.7227,
      "step": 5095
    },
    {
      "epoch": 0.021774613944982354,
      "grad_norm": 1.1511297225952148,
      "learning_rate": 0.0002823021705691335,
      "loss": 0.5369,
      "step": 5096
    },
    {
      "epoch": 0.021778886828409547,
      "grad_norm": 2.3717870712280273,
      "learning_rate": 0.00028225944283028544,
      "loss": 0.5212,
      "step": 5097
    },
    {
      "epoch": 0.021783159711836742,
      "grad_norm": 1.1697907447814941,
      "learning_rate": 0.00028221671509143735,
      "loss": 0.5157,
      "step": 5098
    },
    {
      "epoch": 0.021787432595263934,
      "grad_norm": 0.6589415669441223,
      "learning_rate": 0.0002821739873525893,
      "loss": 0.183,
      "step": 5099
    },
    {
      "epoch": 0.02179170547869113,
      "grad_norm": 3.3729257583618164,
      "learning_rate": 0.00028213125961374123,
      "loss": 1.0249,
      "step": 5100
    },
    {
      "epoch": 0.021795978362118325,
      "grad_norm": 1.8343497514724731,
      "learning_rate": 0.0002820885318748932,
      "loss": 0.3793,
      "step": 5101
    },
    {
      "epoch": 0.021800251245545518,
      "grad_norm": 2.4523890018463135,
      "learning_rate": 0.00028204580413604516,
      "loss": 0.7453,
      "step": 5102
    },
    {
      "epoch": 0.021804524128972713,
      "grad_norm": 2.6185173988342285,
      "learning_rate": 0.00028200307639719707,
      "loss": 0.572,
      "step": 5103
    },
    {
      "epoch": 0.02180879701239991,
      "grad_norm": 1.4820020198822021,
      "learning_rate": 0.00028196034865834904,
      "loss": 0.5974,
      "step": 5104
    },
    {
      "epoch": 0.0218130698958271,
      "grad_norm": 1.7454423904418945,
      "learning_rate": 0.00028191762091950095,
      "loss": 0.8983,
      "step": 5105
    },
    {
      "epoch": 0.021817342779254296,
      "grad_norm": 1.5706367492675781,
      "learning_rate": 0.0002818748931806529,
      "loss": 0.3618,
      "step": 5106
    },
    {
      "epoch": 0.021821615662681492,
      "grad_norm": 1.7507085800170898,
      "learning_rate": 0.0002818321654418048,
      "loss": 0.9,
      "step": 5107
    },
    {
      "epoch": 0.021825888546108684,
      "grad_norm": 1.2265896797180176,
      "learning_rate": 0.0002817894377029568,
      "loss": 0.4635,
      "step": 5108
    },
    {
      "epoch": 0.02183016142953588,
      "grad_norm": 1.732133150100708,
      "learning_rate": 0.00028174670996410875,
      "loss": 0.3058,
      "step": 5109
    },
    {
      "epoch": 0.021834434312963075,
      "grad_norm": 0.7954158782958984,
      "learning_rate": 0.00028170398222526066,
      "loss": 0.2712,
      "step": 5110
    },
    {
      "epoch": 0.021838707196390268,
      "grad_norm": 1.942586064338684,
      "learning_rate": 0.00028166125448641263,
      "loss": 0.5081,
      "step": 5111
    },
    {
      "epoch": 0.021842980079817463,
      "grad_norm": 1.6963531970977783,
      "learning_rate": 0.00028161852674756454,
      "loss": 0.4925,
      "step": 5112
    },
    {
      "epoch": 0.021847252963244655,
      "grad_norm": 3.932708978652954,
      "learning_rate": 0.00028157579900871645,
      "loss": 0.958,
      "step": 5113
    },
    {
      "epoch": 0.02185152584667185,
      "grad_norm": 3.991162061691284,
      "learning_rate": 0.00028153307126986836,
      "loss": 0.9813,
      "step": 5114
    },
    {
      "epoch": 0.021855798730099046,
      "grad_norm": 1.4671169519424438,
      "learning_rate": 0.0002814903435310203,
      "loss": 0.3189,
      "step": 5115
    },
    {
      "epoch": 0.02186007161352624,
      "grad_norm": 2.015444755554199,
      "learning_rate": 0.00028144761579217224,
      "loss": 0.4742,
      "step": 5116
    },
    {
      "epoch": 0.021864344496953434,
      "grad_norm": 1.5019769668579102,
      "learning_rate": 0.0002814048880533242,
      "loss": 0.2887,
      "step": 5117
    },
    {
      "epoch": 0.02186861738038063,
      "grad_norm": 0.6298646926879883,
      "learning_rate": 0.00028136216031447617,
      "loss": 0.2146,
      "step": 5118
    },
    {
      "epoch": 0.021872890263807822,
      "grad_norm": 1.306882381439209,
      "learning_rate": 0.0002813194325756281,
      "loss": 0.4631,
      "step": 5119
    },
    {
      "epoch": 0.021877163147235017,
      "grad_norm": 1.9594662189483643,
      "learning_rate": 0.00028127670483678004,
      "loss": 0.4924,
      "step": 5120
    },
    {
      "epoch": 0.021881436030662213,
      "grad_norm": 1.4318512678146362,
      "learning_rate": 0.00028123397709793195,
      "loss": 0.2821,
      "step": 5121
    },
    {
      "epoch": 0.021885708914089405,
      "grad_norm": 0.7603127956390381,
      "learning_rate": 0.0002811912493590839,
      "loss": 0.2376,
      "step": 5122
    },
    {
      "epoch": 0.0218899817975166,
      "grad_norm": 5.319621562957764,
      "learning_rate": 0.00028114852162023583,
      "loss": 1.6397,
      "step": 5123
    },
    {
      "epoch": 0.021894254680943793,
      "grad_norm": 1.6302741765975952,
      "learning_rate": 0.0002811057938813878,
      "loss": 0.4146,
      "step": 5124
    },
    {
      "epoch": 0.02189852756437099,
      "grad_norm": 1.5953692197799683,
      "learning_rate": 0.00028106306614253976,
      "loss": 0.6428,
      "step": 5125
    },
    {
      "epoch": 0.021902800447798184,
      "grad_norm": 4.909322261810303,
      "learning_rate": 0.00028102033840369167,
      "loss": 1.3426,
      "step": 5126
    },
    {
      "epoch": 0.021907073331225376,
      "grad_norm": 2.474081039428711,
      "learning_rate": 0.00028097761066484363,
      "loss": 0.7643,
      "step": 5127
    },
    {
      "epoch": 0.02191134621465257,
      "grad_norm": 1.1111904382705688,
      "learning_rate": 0.00028093488292599554,
      "loss": 0.3095,
      "step": 5128
    },
    {
      "epoch": 0.021915619098079767,
      "grad_norm": 1.1064749956130981,
      "learning_rate": 0.0002808921551871475,
      "loss": 0.3265,
      "step": 5129
    },
    {
      "epoch": 0.02191989198150696,
      "grad_norm": 4.796290874481201,
      "learning_rate": 0.0002808494274482994,
      "loss": 1.1592,
      "step": 5130
    },
    {
      "epoch": 0.021924164864934155,
      "grad_norm": 3.785975933074951,
      "learning_rate": 0.0002808066997094514,
      "loss": 1.5497,
      "step": 5131
    },
    {
      "epoch": 0.02192843774836135,
      "grad_norm": 0.8136601448059082,
      "learning_rate": 0.00028076397197060335,
      "loss": 0.253,
      "step": 5132
    },
    {
      "epoch": 0.021932710631788543,
      "grad_norm": 1.9752174615859985,
      "learning_rate": 0.00028072124423175526,
      "loss": 0.5542,
      "step": 5133
    },
    {
      "epoch": 0.02193698351521574,
      "grad_norm": 1.285200595855713,
      "learning_rate": 0.0002806785164929072,
      "loss": 0.4286,
      "step": 5134
    },
    {
      "epoch": 0.021941256398642934,
      "grad_norm": 1.8047229051589966,
      "learning_rate": 0.00028063578875405914,
      "loss": 0.4885,
      "step": 5135
    },
    {
      "epoch": 0.021945529282070126,
      "grad_norm": 0.6625280380249023,
      "learning_rate": 0.0002805930610152111,
      "loss": 0.2411,
      "step": 5136
    },
    {
      "epoch": 0.02194980216549732,
      "grad_norm": 1.7468982934951782,
      "learning_rate": 0.000280550333276363,
      "loss": 0.4152,
      "step": 5137
    },
    {
      "epoch": 0.021954075048924514,
      "grad_norm": 1.0707881450653076,
      "learning_rate": 0.000280507605537515,
      "loss": 0.2638,
      "step": 5138
    },
    {
      "epoch": 0.02195834793235171,
      "grad_norm": 1.214411973953247,
      "learning_rate": 0.00028046487779866694,
      "loss": 0.3605,
      "step": 5139
    },
    {
      "epoch": 0.021962620815778905,
      "grad_norm": 1.6126011610031128,
      "learning_rate": 0.00028042215005981885,
      "loss": 0.6089,
      "step": 5140
    },
    {
      "epoch": 0.021966893699206097,
      "grad_norm": 1.1418497562408447,
      "learning_rate": 0.0002803794223209708,
      "loss": 0.3446,
      "step": 5141
    },
    {
      "epoch": 0.021971166582633293,
      "grad_norm": 1.9829741716384888,
      "learning_rate": 0.00028033669458212273,
      "loss": 0.5372,
      "step": 5142
    },
    {
      "epoch": 0.021975439466060488,
      "grad_norm": 2.1597557067871094,
      "learning_rate": 0.0002802939668432747,
      "loss": 0.985,
      "step": 5143
    },
    {
      "epoch": 0.02197971234948768,
      "grad_norm": 0.7370429039001465,
      "learning_rate": 0.0002802512391044266,
      "loss": 0.2374,
      "step": 5144
    },
    {
      "epoch": 0.021983985232914876,
      "grad_norm": 5.502527713775635,
      "learning_rate": 0.00028020851136557857,
      "loss": 1.6303,
      "step": 5145
    },
    {
      "epoch": 0.02198825811634207,
      "grad_norm": 3.5096209049224854,
      "learning_rate": 0.0002801657836267305,
      "loss": 0.8399,
      "step": 5146
    },
    {
      "epoch": 0.021992530999769264,
      "grad_norm": 3.088355302810669,
      "learning_rate": 0.0002801230558878824,
      "loss": 1.1942,
      "step": 5147
    },
    {
      "epoch": 0.02199680388319646,
      "grad_norm": 1.8815745115280151,
      "learning_rate": 0.00028008032814903436,
      "loss": 0.3446,
      "step": 5148
    },
    {
      "epoch": 0.02200107676662365,
      "grad_norm": 2.2080459594726562,
      "learning_rate": 0.00028003760041018627,
      "loss": 0.5205,
      "step": 5149
    },
    {
      "epoch": 0.022005349650050847,
      "grad_norm": 2.0636978149414062,
      "learning_rate": 0.00027999487267133823,
      "loss": 0.7309,
      "step": 5150
    },
    {
      "epoch": 0.022009622533478043,
      "grad_norm": 2.1251206398010254,
      "learning_rate": 0.00027995214493249014,
      "loss": 0.5359,
      "step": 5151
    },
    {
      "epoch": 0.022013895416905235,
      "grad_norm": 2.129972457885742,
      "learning_rate": 0.0002799094171936421,
      "loss": 0.5187,
      "step": 5152
    },
    {
      "epoch": 0.02201816830033243,
      "grad_norm": 1.2516531944274902,
      "learning_rate": 0.000279866689454794,
      "loss": 0.3586,
      "step": 5153
    },
    {
      "epoch": 0.022022441183759626,
      "grad_norm": 4.689372539520264,
      "learning_rate": 0.000279823961715946,
      "loss": 2.1042,
      "step": 5154
    },
    {
      "epoch": 0.022026714067186818,
      "grad_norm": 2.6854300498962402,
      "learning_rate": 0.00027978123397709795,
      "loss": 0.7779,
      "step": 5155
    },
    {
      "epoch": 0.022030986950614014,
      "grad_norm": 2.2606842517852783,
      "learning_rate": 0.00027973850623824986,
      "loss": 0.5441,
      "step": 5156
    },
    {
      "epoch": 0.02203525983404121,
      "grad_norm": 1.941927433013916,
      "learning_rate": 0.0002796957784994018,
      "loss": 0.6557,
      "step": 5157
    },
    {
      "epoch": 0.0220395327174684,
      "grad_norm": 1.5423253774642944,
      "learning_rate": 0.00027965305076055373,
      "loss": 0.3051,
      "step": 5158
    },
    {
      "epoch": 0.022043805600895597,
      "grad_norm": 3.075801372528076,
      "learning_rate": 0.0002796103230217057,
      "loss": 1.0305,
      "step": 5159
    },
    {
      "epoch": 0.022048078484322792,
      "grad_norm": 2.0435404777526855,
      "learning_rate": 0.0002795675952828576,
      "loss": 0.4749,
      "step": 5160
    },
    {
      "epoch": 0.022052351367749985,
      "grad_norm": 0.5971378087997437,
      "learning_rate": 0.0002795248675440096,
      "loss": 0.222,
      "step": 5161
    },
    {
      "epoch": 0.02205662425117718,
      "grad_norm": 2.304095983505249,
      "learning_rate": 0.00027948213980516154,
      "loss": 0.7298,
      "step": 5162
    },
    {
      "epoch": 0.022060897134604372,
      "grad_norm": 4.610841274261475,
      "learning_rate": 0.00027943941206631345,
      "loss": 1.3316,
      "step": 5163
    },
    {
      "epoch": 0.022065170018031568,
      "grad_norm": 4.668686866760254,
      "learning_rate": 0.0002793966843274654,
      "loss": 0.9848,
      "step": 5164
    },
    {
      "epoch": 0.022069442901458763,
      "grad_norm": 2.068352699279785,
      "learning_rate": 0.0002793539565886173,
      "loss": 0.9476,
      "step": 5165
    },
    {
      "epoch": 0.022073715784885956,
      "grad_norm": 2.484940767288208,
      "learning_rate": 0.0002793112288497693,
      "loss": 0.9749,
      "step": 5166
    },
    {
      "epoch": 0.02207798866831315,
      "grad_norm": 0.7072017192840576,
      "learning_rate": 0.0002792685011109212,
      "loss": 0.2376,
      "step": 5167
    },
    {
      "epoch": 0.022082261551740347,
      "grad_norm": 3.0231642723083496,
      "learning_rate": 0.00027922577337207317,
      "loss": 1.5106,
      "step": 5168
    },
    {
      "epoch": 0.02208653443516754,
      "grad_norm": 2.298731565475464,
      "learning_rate": 0.00027918304563322513,
      "loss": 0.6682,
      "step": 5169
    },
    {
      "epoch": 0.022090807318594734,
      "grad_norm": 4.710660457611084,
      "learning_rate": 0.00027914031789437704,
      "loss": 1.276,
      "step": 5170
    },
    {
      "epoch": 0.02209508020202193,
      "grad_norm": 0.5306355357170105,
      "learning_rate": 0.000279097590155529,
      "loss": 0.2062,
      "step": 5171
    },
    {
      "epoch": 0.022099353085449122,
      "grad_norm": 4.5939812660217285,
      "learning_rate": 0.0002790548624166809,
      "loss": 1.2069,
      "step": 5172
    },
    {
      "epoch": 0.022103625968876318,
      "grad_norm": 3.0345170497894287,
      "learning_rate": 0.0002790121346778329,
      "loss": 1.3163,
      "step": 5173
    },
    {
      "epoch": 0.02210789885230351,
      "grad_norm": 4.062643527984619,
      "learning_rate": 0.0002789694069389848,
      "loss": 1.7968,
      "step": 5174
    },
    {
      "epoch": 0.022112171735730705,
      "grad_norm": 1.9418537616729736,
      "learning_rate": 0.00027892667920013676,
      "loss": 0.6241,
      "step": 5175
    },
    {
      "epoch": 0.0221164446191579,
      "grad_norm": 0.7657918334007263,
      "learning_rate": 0.0002788839514612887,
      "loss": 0.2532,
      "step": 5176
    },
    {
      "epoch": 0.022120717502585093,
      "grad_norm": 2.6374871730804443,
      "learning_rate": 0.00027884122372244063,
      "loss": 0.8869,
      "step": 5177
    },
    {
      "epoch": 0.02212499038601229,
      "grad_norm": 0.5495319366455078,
      "learning_rate": 0.0002787984959835926,
      "loss": 0.2149,
      "step": 5178
    },
    {
      "epoch": 0.022129263269439484,
      "grad_norm": 1.6228344440460205,
      "learning_rate": 0.00027875576824474446,
      "loss": 0.4506,
      "step": 5179
    },
    {
      "epoch": 0.022133536152866676,
      "grad_norm": 1.936415195465088,
      "learning_rate": 0.0002787130405058964,
      "loss": 0.5352,
      "step": 5180
    },
    {
      "epoch": 0.022137809036293872,
      "grad_norm": 0.5209551453590393,
      "learning_rate": 0.00027867031276704833,
      "loss": 0.2313,
      "step": 5181
    },
    {
      "epoch": 0.022142081919721068,
      "grad_norm": 3.3336353302001953,
      "learning_rate": 0.0002786275850282003,
      "loss": 1.3288,
      "step": 5182
    },
    {
      "epoch": 0.02214635480314826,
      "grad_norm": 3.7187657356262207,
      "learning_rate": 0.0002785848572893522,
      "loss": 1.0635,
      "step": 5183
    },
    {
      "epoch": 0.022150627686575455,
      "grad_norm": 2.924145221710205,
      "learning_rate": 0.00027854212955050417,
      "loss": 0.7863,
      "step": 5184
    },
    {
      "epoch": 0.022154900570002647,
      "grad_norm": 2.046929359436035,
      "learning_rate": 0.00027849940181165614,
      "loss": 0.8119,
      "step": 5185
    },
    {
      "epoch": 0.022159173453429843,
      "grad_norm": 3.8421523571014404,
      "learning_rate": 0.00027845667407280805,
      "loss": 1.3536,
      "step": 5186
    },
    {
      "epoch": 0.02216344633685704,
      "grad_norm": 0.43821078538894653,
      "learning_rate": 0.00027841394633396,
      "loss": 0.1704,
      "step": 5187
    },
    {
      "epoch": 0.02216771922028423,
      "grad_norm": 2.0524842739105225,
      "learning_rate": 0.0002783712185951119,
      "loss": 0.6702,
      "step": 5188
    },
    {
      "epoch": 0.022171992103711426,
      "grad_norm": 0.7661091685295105,
      "learning_rate": 0.0002783284908562639,
      "loss": 0.2865,
      "step": 5189
    },
    {
      "epoch": 0.022176264987138622,
      "grad_norm": 0.47411638498306274,
      "learning_rate": 0.0002782857631174158,
      "loss": 0.2274,
      "step": 5190
    },
    {
      "epoch": 0.022180537870565814,
      "grad_norm": 2.088973045349121,
      "learning_rate": 0.00027824303537856776,
      "loss": 0.8286,
      "step": 5191
    },
    {
      "epoch": 0.02218481075399301,
      "grad_norm": 2.3954856395721436,
      "learning_rate": 0.00027820030763971973,
      "loss": 0.7757,
      "step": 5192
    },
    {
      "epoch": 0.022189083637420205,
      "grad_norm": 0.45874783396720886,
      "learning_rate": 0.00027815757990087164,
      "loss": 0.1914,
      "step": 5193
    },
    {
      "epoch": 0.022193356520847397,
      "grad_norm": 1.619455099105835,
      "learning_rate": 0.0002781148521620236,
      "loss": 0.4641,
      "step": 5194
    },
    {
      "epoch": 0.022197629404274593,
      "grad_norm": 1.134335994720459,
      "learning_rate": 0.0002780721244231755,
      "loss": 0.3351,
      "step": 5195
    },
    {
      "epoch": 0.02220190228770179,
      "grad_norm": 2.1101746559143066,
      "learning_rate": 0.0002780293966843275,
      "loss": 0.575,
      "step": 5196
    },
    {
      "epoch": 0.02220617517112898,
      "grad_norm": 1.9001420736312866,
      "learning_rate": 0.0002779866689454794,
      "loss": 0.4827,
      "step": 5197
    },
    {
      "epoch": 0.022210448054556176,
      "grad_norm": 1.3684955835342407,
      "learning_rate": 0.00027794394120663136,
      "loss": 0.4529,
      "step": 5198
    },
    {
      "epoch": 0.02221472093798337,
      "grad_norm": 2.5813794136047363,
      "learning_rate": 0.0002779012134677833,
      "loss": 0.9697,
      "step": 5199
    },
    {
      "epoch": 0.022218993821410564,
      "grad_norm": 1.8993566036224365,
      "learning_rate": 0.00027785848572893523,
      "loss": 0.4589,
      "step": 5200
    },
    {
      "epoch": 0.02222326670483776,
      "grad_norm": 2.420379638671875,
      "learning_rate": 0.0002778157579900872,
      "loss": 0.8354,
      "step": 5201
    },
    {
      "epoch": 0.02222753958826495,
      "grad_norm": 1.3988887071609497,
      "learning_rate": 0.0002777730302512391,
      "loss": 0.8185,
      "step": 5202
    },
    {
      "epoch": 0.022231812471692147,
      "grad_norm": 2.1177046298980713,
      "learning_rate": 0.00027773030251239107,
      "loss": 1.2017,
      "step": 5203
    },
    {
      "epoch": 0.022236085355119343,
      "grad_norm": 1.8755240440368652,
      "learning_rate": 0.000277687574773543,
      "loss": 0.5032,
      "step": 5204
    },
    {
      "epoch": 0.022240358238546535,
      "grad_norm": 2.8436789512634277,
      "learning_rate": 0.00027764484703469495,
      "loss": 0.931,
      "step": 5205
    },
    {
      "epoch": 0.02224463112197373,
      "grad_norm": 4.760545253753662,
      "learning_rate": 0.0002776021192958469,
      "loss": 2.4139,
      "step": 5206
    },
    {
      "epoch": 0.022248904005400926,
      "grad_norm": 1.6429083347320557,
      "learning_rate": 0.0002775593915569988,
      "loss": 0.4591,
      "step": 5207
    },
    {
      "epoch": 0.022253176888828118,
      "grad_norm": 1.7945150136947632,
      "learning_rate": 0.0002775166638181508,
      "loss": 0.5401,
      "step": 5208
    },
    {
      "epoch": 0.022257449772255314,
      "grad_norm": 1.772254228591919,
      "learning_rate": 0.0002774739360793027,
      "loss": 0.5353,
      "step": 5209
    },
    {
      "epoch": 0.022261722655682506,
      "grad_norm": 3.4223341941833496,
      "learning_rate": 0.00027743120834045466,
      "loss": 1.0932,
      "step": 5210
    },
    {
      "epoch": 0.0222659955391097,
      "grad_norm": 3.044261932373047,
      "learning_rate": 0.0002773884806016066,
      "loss": 1.3357,
      "step": 5211
    },
    {
      "epoch": 0.022270268422536897,
      "grad_norm": 1.2580327987670898,
      "learning_rate": 0.0002773457528627585,
      "loss": 0.7145,
      "step": 5212
    },
    {
      "epoch": 0.02227454130596409,
      "grad_norm": 2.8098719120025635,
      "learning_rate": 0.00027730302512391045,
      "loss": 0.6516,
      "step": 5213
    },
    {
      "epoch": 0.022278814189391285,
      "grad_norm": 3.2623345851898193,
      "learning_rate": 0.00027726029738506236,
      "loss": 0.777,
      "step": 5214
    },
    {
      "epoch": 0.02228308707281848,
      "grad_norm": 2.625246286392212,
      "learning_rate": 0.0002772175696462143,
      "loss": 0.8847,
      "step": 5215
    },
    {
      "epoch": 0.022287359956245673,
      "grad_norm": 2.2835192680358887,
      "learning_rate": 0.00027717484190736624,
      "loss": 0.6805,
      "step": 5216
    },
    {
      "epoch": 0.022291632839672868,
      "grad_norm": 1.1482527256011963,
      "learning_rate": 0.0002771321141685182,
      "loss": 0.3402,
      "step": 5217
    },
    {
      "epoch": 0.022295905723100064,
      "grad_norm": 2.7489867210388184,
      "learning_rate": 0.0002770893864296701,
      "loss": 0.862,
      "step": 5218
    },
    {
      "epoch": 0.022300178606527256,
      "grad_norm": 3.549962282180786,
      "learning_rate": 0.0002770466586908221,
      "loss": 1.1283,
      "step": 5219
    },
    {
      "epoch": 0.02230445148995445,
      "grad_norm": 1.6293041706085205,
      "learning_rate": 0.000277003930951974,
      "loss": 0.4974,
      "step": 5220
    },
    {
      "epoch": 0.022308724373381647,
      "grad_norm": 1.3422859907150269,
      "learning_rate": 0.00027696120321312595,
      "loss": 0.4068,
      "step": 5221
    },
    {
      "epoch": 0.02231299725680884,
      "grad_norm": 1.0733566284179688,
      "learning_rate": 0.0002769184754742779,
      "loss": 0.4412,
      "step": 5222
    },
    {
      "epoch": 0.022317270140236035,
      "grad_norm": 0.5821141004562378,
      "learning_rate": 0.00027687574773542983,
      "loss": 0.2318,
      "step": 5223
    },
    {
      "epoch": 0.022321543023663227,
      "grad_norm": 1.4034854173660278,
      "learning_rate": 0.0002768330199965818,
      "loss": 0.6818,
      "step": 5224
    },
    {
      "epoch": 0.022325815907090422,
      "grad_norm": 3.208230972290039,
      "learning_rate": 0.0002767902922577337,
      "loss": 1.6873,
      "step": 5225
    },
    {
      "epoch": 0.022330088790517618,
      "grad_norm": 4.1130266189575195,
      "learning_rate": 0.00027674756451888567,
      "loss": 1.0906,
      "step": 5226
    },
    {
      "epoch": 0.02233436167394481,
      "grad_norm": 1.8402801752090454,
      "learning_rate": 0.0002767048367800376,
      "loss": 1.1405,
      "step": 5227
    },
    {
      "epoch": 0.022338634557372006,
      "grad_norm": 1.7807097434997559,
      "learning_rate": 0.00027666210904118955,
      "loss": 0.909,
      "step": 5228
    },
    {
      "epoch": 0.0223429074407992,
      "grad_norm": 3.0332605838775635,
      "learning_rate": 0.0002766193813023415,
      "loss": 1.2006,
      "step": 5229
    },
    {
      "epoch": 0.022347180324226393,
      "grad_norm": 4.393460750579834,
      "learning_rate": 0.0002765766535634934,
      "loss": 2.0526,
      "step": 5230
    },
    {
      "epoch": 0.02235145320765359,
      "grad_norm": 4.700431823730469,
      "learning_rate": 0.0002765339258246454,
      "loss": 1.4247,
      "step": 5231
    },
    {
      "epoch": 0.022355726091080785,
      "grad_norm": 4.296188831329346,
      "learning_rate": 0.0002764911980857973,
      "loss": 1.4686,
      "step": 5232
    },
    {
      "epoch": 0.022359998974507977,
      "grad_norm": 1.898174524307251,
      "learning_rate": 0.00027644847034694926,
      "loss": 0.5385,
      "step": 5233
    },
    {
      "epoch": 0.022364271857935172,
      "grad_norm": 4.675201416015625,
      "learning_rate": 0.00027640574260810117,
      "loss": 1.2699,
      "step": 5234
    },
    {
      "epoch": 0.022368544741362364,
      "grad_norm": 1.742558479309082,
      "learning_rate": 0.00027636301486925314,
      "loss": 0.8674,
      "step": 5235
    },
    {
      "epoch": 0.02237281762478956,
      "grad_norm": 1.6958956718444824,
      "learning_rate": 0.0002763202871304051,
      "loss": 0.5593,
      "step": 5236
    },
    {
      "epoch": 0.022377090508216756,
      "grad_norm": 3.417862892150879,
      "learning_rate": 0.000276277559391557,
      "loss": 1.2161,
      "step": 5237
    },
    {
      "epoch": 0.022381363391643948,
      "grad_norm": 1.6890262365341187,
      "learning_rate": 0.000276234831652709,
      "loss": 0.9951,
      "step": 5238
    },
    {
      "epoch": 0.022385636275071143,
      "grad_norm": 1.1619313955307007,
      "learning_rate": 0.0002761921039138609,
      "loss": 0.3834,
      "step": 5239
    },
    {
      "epoch": 0.02238990915849834,
      "grad_norm": 1.9094451665878296,
      "learning_rate": 0.00027614937617501285,
      "loss": 0.712,
      "step": 5240
    },
    {
      "epoch": 0.02239418204192553,
      "grad_norm": 1.6136412620544434,
      "learning_rate": 0.00027610664843616476,
      "loss": 0.6036,
      "step": 5241
    },
    {
      "epoch": 0.022398454925352727,
      "grad_norm": 1.859566569328308,
      "learning_rate": 0.00027606392069731673,
      "loss": 0.6695,
      "step": 5242
    },
    {
      "epoch": 0.022402727808779922,
      "grad_norm": 1.6039146184921265,
      "learning_rate": 0.0002760211929584687,
      "loss": 0.4467,
      "step": 5243
    },
    {
      "epoch": 0.022407000692207114,
      "grad_norm": 1.3550655841827393,
      "learning_rate": 0.00027597846521962055,
      "loss": 0.6458,
      "step": 5244
    },
    {
      "epoch": 0.02241127357563431,
      "grad_norm": 1.1274688243865967,
      "learning_rate": 0.0002759357374807725,
      "loss": 0.3682,
      "step": 5245
    },
    {
      "epoch": 0.022415546459061506,
      "grad_norm": 1.3091974258422852,
      "learning_rate": 0.0002758930097419244,
      "loss": 0.4613,
      "step": 5246
    },
    {
      "epoch": 0.022419819342488698,
      "grad_norm": 1.585633397102356,
      "learning_rate": 0.0002758502820030764,
      "loss": 0.4105,
      "step": 5247
    },
    {
      "epoch": 0.022424092225915893,
      "grad_norm": 1.9948630332946777,
      "learning_rate": 0.0002758075542642283,
      "loss": 0.7009,
      "step": 5248
    },
    {
      "epoch": 0.022428365109343085,
      "grad_norm": 1.8686522245407104,
      "learning_rate": 0.00027576482652538027,
      "loss": 0.4911,
      "step": 5249
    },
    {
      "epoch": 0.02243263799277028,
      "grad_norm": 1.185556411743164,
      "learning_rate": 0.00027572209878653223,
      "loss": 0.3916,
      "step": 5250
    },
    {
      "epoch": 0.022436910876197477,
      "grad_norm": 1.500935435295105,
      "learning_rate": 0.00027567937104768414,
      "loss": 0.788,
      "step": 5251
    },
    {
      "epoch": 0.02244118375962467,
      "grad_norm": 1.1597257852554321,
      "learning_rate": 0.0002756366433088361,
      "loss": 0.3907,
      "step": 5252
    },
    {
      "epoch": 0.022445456643051864,
      "grad_norm": 2.8994476795196533,
      "learning_rate": 0.000275593915569988,
      "loss": 1.2689,
      "step": 5253
    },
    {
      "epoch": 0.02244972952647906,
      "grad_norm": 1.4662052392959595,
      "learning_rate": 0.00027555118783114,
      "loss": 0.7644,
      "step": 5254
    },
    {
      "epoch": 0.022454002409906252,
      "grad_norm": 0.9908211827278137,
      "learning_rate": 0.0002755084600922919,
      "loss": 0.4014,
      "step": 5255
    },
    {
      "epoch": 0.022458275293333448,
      "grad_norm": 1.6702213287353516,
      "learning_rate": 0.00027546573235344386,
      "loss": 0.602,
      "step": 5256
    },
    {
      "epoch": 0.022462548176760643,
      "grad_norm": 4.427383899688721,
      "learning_rate": 0.00027542300461459577,
      "loss": 2.041,
      "step": 5257
    },
    {
      "epoch": 0.022466821060187835,
      "grad_norm": 4.442209243774414,
      "learning_rate": 0.00027538027687574773,
      "loss": 2.0313,
      "step": 5258
    },
    {
      "epoch": 0.02247109394361503,
      "grad_norm": 1.262339472770691,
      "learning_rate": 0.0002753375491368997,
      "loss": 0.6268,
      "step": 5259
    },
    {
      "epoch": 0.022475366827042223,
      "grad_norm": 1.23491370677948,
      "learning_rate": 0.0002752948213980516,
      "loss": 0.4452,
      "step": 5260
    },
    {
      "epoch": 0.02247963971046942,
      "grad_norm": 0.7301327586174011,
      "learning_rate": 0.0002752520936592036,
      "loss": 0.2296,
      "step": 5261
    },
    {
      "epoch": 0.022483912593896614,
      "grad_norm": 2.0955851078033447,
      "learning_rate": 0.0002752093659203555,
      "loss": 0.8423,
      "step": 5262
    },
    {
      "epoch": 0.022488185477323806,
      "grad_norm": 0.8349853754043579,
      "learning_rate": 0.00027516663818150745,
      "loss": 0.2948,
      "step": 5263
    },
    {
      "epoch": 0.022492458360751002,
      "grad_norm": 1.2337121963500977,
      "learning_rate": 0.00027512391044265936,
      "loss": 0.7567,
      "step": 5264
    },
    {
      "epoch": 0.022496731244178197,
      "grad_norm": 1.2419538497924805,
      "learning_rate": 0.0002750811827038113,
      "loss": 0.7576,
      "step": 5265
    },
    {
      "epoch": 0.02250100412760539,
      "grad_norm": 3.4205474853515625,
      "learning_rate": 0.0002750384549649633,
      "loss": 1.3618,
      "step": 5266
    },
    {
      "epoch": 0.022505277011032585,
      "grad_norm": 4.082455635070801,
      "learning_rate": 0.0002749957272261152,
      "loss": 1.2865,
      "step": 5267
    },
    {
      "epoch": 0.02250954989445978,
      "grad_norm": 2.244096279144287,
      "learning_rate": 0.00027495299948726717,
      "loss": 0.7985,
      "step": 5268
    },
    {
      "epoch": 0.022513822777886973,
      "grad_norm": 1.064751148223877,
      "learning_rate": 0.0002749102717484191,
      "loss": 0.3406,
      "step": 5269
    },
    {
      "epoch": 0.02251809566131417,
      "grad_norm": 2.37870717048645,
      "learning_rate": 0.00027486754400957104,
      "loss": 0.8612,
      "step": 5270
    },
    {
      "epoch": 0.022522368544741364,
      "grad_norm": 2.1672611236572266,
      "learning_rate": 0.00027482481627072295,
      "loss": 0.7346,
      "step": 5271
    },
    {
      "epoch": 0.022526641428168556,
      "grad_norm": 0.5562596917152405,
      "learning_rate": 0.0002747820885318749,
      "loss": 0.2092,
      "step": 5272
    },
    {
      "epoch": 0.022530914311595752,
      "grad_norm": 1.0965619087219238,
      "learning_rate": 0.0002747393607930269,
      "loss": 0.3901,
      "step": 5273
    },
    {
      "epoch": 0.022535187195022944,
      "grad_norm": 1.3320001363754272,
      "learning_rate": 0.0002746966330541788,
      "loss": 0.7099,
      "step": 5274
    },
    {
      "epoch": 0.02253946007845014,
      "grad_norm": 2.894400119781494,
      "learning_rate": 0.00027465390531533076,
      "loss": 1.1851,
      "step": 5275
    },
    {
      "epoch": 0.022543732961877335,
      "grad_norm": 0.4580908417701721,
      "learning_rate": 0.00027461117757648267,
      "loss": 0.157,
      "step": 5276
    },
    {
      "epoch": 0.022548005845304527,
      "grad_norm": 1.5410462617874146,
      "learning_rate": 0.0002745684498376346,
      "loss": 0.5138,
      "step": 5277
    },
    {
      "epoch": 0.022552278728731723,
      "grad_norm": 1.907686471939087,
      "learning_rate": 0.0002745257220987865,
      "loss": 0.565,
      "step": 5278
    },
    {
      "epoch": 0.02255655161215892,
      "grad_norm": 0.9701628684997559,
      "learning_rate": 0.00027448299435993846,
      "loss": 0.4384,
      "step": 5279
    },
    {
      "epoch": 0.02256082449558611,
      "grad_norm": 1.3885283470153809,
      "learning_rate": 0.0002744402666210904,
      "loss": 0.6617,
      "step": 5280
    },
    {
      "epoch": 0.022565097379013306,
      "grad_norm": 0.8513922095298767,
      "learning_rate": 0.00027439753888224233,
      "loss": 0.304,
      "step": 5281
    },
    {
      "epoch": 0.0225693702624405,
      "grad_norm": 1.856197476387024,
      "learning_rate": 0.0002743548111433943,
      "loss": 0.7051,
      "step": 5282
    },
    {
      "epoch": 0.022573643145867694,
      "grad_norm": 2.1979358196258545,
      "learning_rate": 0.0002743120834045462,
      "loss": 0.7336,
      "step": 5283
    },
    {
      "epoch": 0.02257791602929489,
      "grad_norm": 1.51809823513031,
      "learning_rate": 0.00027426935566569817,
      "loss": 0.5288,
      "step": 5284
    },
    {
      "epoch": 0.02258218891272208,
      "grad_norm": 2.1750545501708984,
      "learning_rate": 0.0002742266279268501,
      "loss": 0.7087,
      "step": 5285
    },
    {
      "epoch": 0.022586461796149277,
      "grad_norm": 0.7811111211776733,
      "learning_rate": 0.00027418390018800205,
      "loss": 0.3418,
      "step": 5286
    },
    {
      "epoch": 0.022590734679576473,
      "grad_norm": 0.6649925708770752,
      "learning_rate": 0.00027414117244915396,
      "loss": 0.3077,
      "step": 5287
    },
    {
      "epoch": 0.022595007563003665,
      "grad_norm": 1.7006914615631104,
      "learning_rate": 0.0002740984447103059,
      "loss": 0.5562,
      "step": 5288
    },
    {
      "epoch": 0.02259928044643086,
      "grad_norm": 4.536141872406006,
      "learning_rate": 0.0002740557169714579,
      "loss": 1.1915,
      "step": 5289
    },
    {
      "epoch": 0.022603553329858056,
      "grad_norm": 1.9491698741912842,
      "learning_rate": 0.0002740129892326098,
      "loss": 0.7065,
      "step": 5290
    },
    {
      "epoch": 0.022607826213285248,
      "grad_norm": 2.708436965942383,
      "learning_rate": 0.00027397026149376176,
      "loss": 0.9171,
      "step": 5291
    },
    {
      "epoch": 0.022612099096712444,
      "grad_norm": 1.9021258354187012,
      "learning_rate": 0.0002739275337549137,
      "loss": 0.722,
      "step": 5292
    },
    {
      "epoch": 0.02261637198013964,
      "grad_norm": 1.3811122179031372,
      "learning_rate": 0.00027388480601606564,
      "loss": 0.601,
      "step": 5293
    },
    {
      "epoch": 0.02262064486356683,
      "grad_norm": 3.788978099822998,
      "learning_rate": 0.00027384207827721755,
      "loss": 1.0157,
      "step": 5294
    },
    {
      "epoch": 0.022624917746994027,
      "grad_norm": 1.991003394126892,
      "learning_rate": 0.0002737993505383695,
      "loss": 0.6592,
      "step": 5295
    },
    {
      "epoch": 0.02262919063042122,
      "grad_norm": 1.7152259349822998,
      "learning_rate": 0.0002737566227995215,
      "loss": 0.5019,
      "step": 5296
    },
    {
      "epoch": 0.022633463513848415,
      "grad_norm": 2.526888847351074,
      "learning_rate": 0.0002737138950606734,
      "loss": 0.8683,
      "step": 5297
    },
    {
      "epoch": 0.02263773639727561,
      "grad_norm": 2.4749372005462646,
      "learning_rate": 0.00027367116732182536,
      "loss": 1.0045,
      "step": 5298
    },
    {
      "epoch": 0.022642009280702802,
      "grad_norm": 2.772085666656494,
      "learning_rate": 0.00027362843958297727,
      "loss": 1.3724,
      "step": 5299
    },
    {
      "epoch": 0.022646282164129998,
      "grad_norm": 3.718564987182617,
      "learning_rate": 0.00027358571184412923,
      "loss": 0.8794,
      "step": 5300
    },
    {
      "epoch": 0.022650555047557194,
      "grad_norm": 0.5538026690483093,
      "learning_rate": 0.00027354298410528114,
      "loss": 0.2409,
      "step": 5301
    },
    {
      "epoch": 0.022654827930984386,
      "grad_norm": 0.5187076330184937,
      "learning_rate": 0.0002735002563664331,
      "loss": 0.2303,
      "step": 5302
    },
    {
      "epoch": 0.02265910081441158,
      "grad_norm": 4.563176155090332,
      "learning_rate": 0.0002734575286275851,
      "loss": 0.9014,
      "step": 5303
    },
    {
      "epoch": 0.022663373697838777,
      "grad_norm": 1.3606666326522827,
      "learning_rate": 0.000273414800888737,
      "loss": 0.6808,
      "step": 5304
    },
    {
      "epoch": 0.02266764658126597,
      "grad_norm": 1.7466816902160645,
      "learning_rate": 0.00027337207314988895,
      "loss": 0.4111,
      "step": 5305
    },
    {
      "epoch": 0.022671919464693165,
      "grad_norm": 1.3358241319656372,
      "learning_rate": 0.00027332934541104086,
      "loss": 0.5077,
      "step": 5306
    },
    {
      "epoch": 0.02267619234812036,
      "grad_norm": 1.7759486436843872,
      "learning_rate": 0.0002732866176721928,
      "loss": 0.6509,
      "step": 5307
    },
    {
      "epoch": 0.022680465231547552,
      "grad_norm": 0.4839808940887451,
      "learning_rate": 0.00027324388993334474,
      "loss": 0.222,
      "step": 5308
    },
    {
      "epoch": 0.022684738114974748,
      "grad_norm": 1.9560602903366089,
      "learning_rate": 0.0002732011621944967,
      "loss": 0.6766,
      "step": 5309
    },
    {
      "epoch": 0.02268901099840194,
      "grad_norm": 1.9454108476638794,
      "learning_rate": 0.0002731584344556486,
      "loss": 0.6367,
      "step": 5310
    },
    {
      "epoch": 0.022693283881829136,
      "grad_norm": 1.4029399156570435,
      "learning_rate": 0.0002731157067168005,
      "loss": 0.5068,
      "step": 5311
    },
    {
      "epoch": 0.02269755676525633,
      "grad_norm": 2.977670192718506,
      "learning_rate": 0.0002730729789779525,
      "loss": 1.0197,
      "step": 5312
    },
    {
      "epoch": 0.022701829648683523,
      "grad_norm": 2.331054925918579,
      "learning_rate": 0.0002730302512391044,
      "loss": 0.5664,
      "step": 5313
    },
    {
      "epoch": 0.02270610253211072,
      "grad_norm": 1.8677960634231567,
      "learning_rate": 0.00027298752350025636,
      "loss": 0.6008,
      "step": 5314
    },
    {
      "epoch": 0.022710375415537915,
      "grad_norm": 2.2698185443878174,
      "learning_rate": 0.00027294479576140827,
      "loss": 0.5283,
      "step": 5315
    },
    {
      "epoch": 0.022714648298965107,
      "grad_norm": 4.3908162117004395,
      "learning_rate": 0.00027290206802256024,
      "loss": 0.6292,
      "step": 5316
    },
    {
      "epoch": 0.022718921182392302,
      "grad_norm": 1.657251238822937,
      "learning_rate": 0.0002728593402837122,
      "loss": 0.4291,
      "step": 5317
    },
    {
      "epoch": 0.022723194065819498,
      "grad_norm": 1.2601863145828247,
      "learning_rate": 0.0002728166125448641,
      "loss": 0.4698,
      "step": 5318
    },
    {
      "epoch": 0.02272746694924669,
      "grad_norm": 1.3913103342056274,
      "learning_rate": 0.0002727738848060161,
      "loss": 0.4292,
      "step": 5319
    },
    {
      "epoch": 0.022731739832673886,
      "grad_norm": 0.5314759016036987,
      "learning_rate": 0.000272731157067168,
      "loss": 0.2399,
      "step": 5320
    },
    {
      "epoch": 0.022736012716101078,
      "grad_norm": 3.038944721221924,
      "learning_rate": 0.00027268842932831995,
      "loss": 1.2497,
      "step": 5321
    },
    {
      "epoch": 0.022740285599528273,
      "grad_norm": 1.1185511350631714,
      "learning_rate": 0.00027264570158947186,
      "loss": 0.3979,
      "step": 5322
    },
    {
      "epoch": 0.02274455848295547,
      "grad_norm": 3.728469133377075,
      "learning_rate": 0.00027260297385062383,
      "loss": 1.1237,
      "step": 5323
    },
    {
      "epoch": 0.02274883136638266,
      "grad_norm": 0.9141468405723572,
      "learning_rate": 0.00027256024611177574,
      "loss": 0.3355,
      "step": 5324
    },
    {
      "epoch": 0.022753104249809857,
      "grad_norm": 1.1281203031539917,
      "learning_rate": 0.0002725175183729277,
      "loss": 0.3817,
      "step": 5325
    },
    {
      "epoch": 0.022757377133237052,
      "grad_norm": 2.4378955364227295,
      "learning_rate": 0.00027247479063407967,
      "loss": 0.8295,
      "step": 5326
    },
    {
      "epoch": 0.022761650016664244,
      "grad_norm": 4.683827877044678,
      "learning_rate": 0.0002724320628952316,
      "loss": 1.4243,
      "step": 5327
    },
    {
      "epoch": 0.02276592290009144,
      "grad_norm": 2.088759183883667,
      "learning_rate": 0.00027238933515638355,
      "loss": 0.383,
      "step": 5328
    },
    {
      "epoch": 0.022770195783518635,
      "grad_norm": 1.0070409774780273,
      "learning_rate": 0.00027234660741753546,
      "loss": 0.3342,
      "step": 5329
    },
    {
      "epoch": 0.022774468666945828,
      "grad_norm": 2.3355212211608887,
      "learning_rate": 0.0002723038796786874,
      "loss": 0.8274,
      "step": 5330
    },
    {
      "epoch": 0.022778741550373023,
      "grad_norm": 4.097424030303955,
      "learning_rate": 0.00027226115193983933,
      "loss": 1.4075,
      "step": 5331
    },
    {
      "epoch": 0.02278301443380022,
      "grad_norm": 3.432343006134033,
      "learning_rate": 0.0002722184242009913,
      "loss": 1.3264,
      "step": 5332
    },
    {
      "epoch": 0.02278728731722741,
      "grad_norm": 0.5547700524330139,
      "learning_rate": 0.00027217569646214326,
      "loss": 0.2109,
      "step": 5333
    },
    {
      "epoch": 0.022791560200654606,
      "grad_norm": 2.2410359382629395,
      "learning_rate": 0.0002721329687232952,
      "loss": 0.6542,
      "step": 5334
    },
    {
      "epoch": 0.0227958330840818,
      "grad_norm": 2.619040012359619,
      "learning_rate": 0.00027209024098444714,
      "loss": 1.5048,
      "step": 5335
    },
    {
      "epoch": 0.022800105967508994,
      "grad_norm": 1.2837343215942383,
      "learning_rate": 0.00027204751324559905,
      "loss": 0.3207,
      "step": 5336
    },
    {
      "epoch": 0.02280437885093619,
      "grad_norm": 1.0515331029891968,
      "learning_rate": 0.000272004785506751,
      "loss": 0.3823,
      "step": 5337
    },
    {
      "epoch": 0.022808651734363382,
      "grad_norm": 0.9459619522094727,
      "learning_rate": 0.0002719620577679029,
      "loss": 0.349,
      "step": 5338
    },
    {
      "epoch": 0.022812924617790577,
      "grad_norm": 1.289058804512024,
      "learning_rate": 0.0002719193300290549,
      "loss": 0.3228,
      "step": 5339
    },
    {
      "epoch": 0.022817197501217773,
      "grad_norm": 0.9684017300605774,
      "learning_rate": 0.00027187660229020685,
      "loss": 0.382,
      "step": 5340
    },
    {
      "epoch": 0.022821470384644965,
      "grad_norm": 2.5993547439575195,
      "learning_rate": 0.00027183387455135877,
      "loss": 0.9619,
      "step": 5341
    },
    {
      "epoch": 0.02282574326807216,
      "grad_norm": 0.7507789731025696,
      "learning_rate": 0.00027179114681251073,
      "loss": 0.2278,
      "step": 5342
    },
    {
      "epoch": 0.022830016151499356,
      "grad_norm": 1.8099695444107056,
      "learning_rate": 0.0002717484190736626,
      "loss": 0.9261,
      "step": 5343
    },
    {
      "epoch": 0.02283428903492655,
      "grad_norm": 1.0464138984680176,
      "learning_rate": 0.00027170569133481455,
      "loss": 0.382,
      "step": 5344
    },
    {
      "epoch": 0.022838561918353744,
      "grad_norm": 0.9289910793304443,
      "learning_rate": 0.00027166296359596646,
      "loss": 0.3339,
      "step": 5345
    },
    {
      "epoch": 0.022842834801780936,
      "grad_norm": 1.2114065885543823,
      "learning_rate": 0.00027162023585711843,
      "loss": 0.2759,
      "step": 5346
    },
    {
      "epoch": 0.02284710768520813,
      "grad_norm": 1.2137082815170288,
      "learning_rate": 0.0002715775081182704,
      "loss": 0.2715,
      "step": 5347
    },
    {
      "epoch": 0.022851380568635327,
      "grad_norm": 0.7511981129646301,
      "learning_rate": 0.0002715347803794223,
      "loss": 0.3234,
      "step": 5348
    },
    {
      "epoch": 0.02285565345206252,
      "grad_norm": 4.780261993408203,
      "learning_rate": 0.00027149205264057427,
      "loss": 1.3605,
      "step": 5349
    },
    {
      "epoch": 0.022859926335489715,
      "grad_norm": 4.075405120849609,
      "learning_rate": 0.0002714493249017262,
      "loss": 1.32,
      "step": 5350
    },
    {
      "epoch": 0.02286419921891691,
      "grad_norm": 1.5666165351867676,
      "learning_rate": 0.00027140659716287814,
      "loss": 0.509,
      "step": 5351
    },
    {
      "epoch": 0.022868472102344103,
      "grad_norm": 1.2176522016525269,
      "learning_rate": 0.00027136386942403005,
      "loss": 0.4443,
      "step": 5352
    },
    {
      "epoch": 0.0228727449857713,
      "grad_norm": 4.022064208984375,
      "learning_rate": 0.000271321141685182,
      "loss": 1.1895,
      "step": 5353
    },
    {
      "epoch": 0.022877017869198494,
      "grad_norm": 1.9308815002441406,
      "learning_rate": 0.000271278413946334,
      "loss": 0.6276,
      "step": 5354
    },
    {
      "epoch": 0.022881290752625686,
      "grad_norm": 2.02980637550354,
      "learning_rate": 0.0002712356862074859,
      "loss": 0.414,
      "step": 5355
    },
    {
      "epoch": 0.02288556363605288,
      "grad_norm": 1.2677356004714966,
      "learning_rate": 0.00027119295846863786,
      "loss": 0.5103,
      "step": 5356
    },
    {
      "epoch": 0.022889836519480077,
      "grad_norm": 1.1969265937805176,
      "learning_rate": 0.00027115023072978977,
      "loss": 0.417,
      "step": 5357
    },
    {
      "epoch": 0.02289410940290727,
      "grad_norm": 0.7052171230316162,
      "learning_rate": 0.00027110750299094174,
      "loss": 0.2512,
      "step": 5358
    },
    {
      "epoch": 0.022898382286334465,
      "grad_norm": 2.5500247478485107,
      "learning_rate": 0.00027106477525209365,
      "loss": 1.0548,
      "step": 5359
    },
    {
      "epoch": 0.022902655169761657,
      "grad_norm": 2.5159952640533447,
      "learning_rate": 0.0002710220475132456,
      "loss": 1.0551,
      "step": 5360
    },
    {
      "epoch": 0.022906928053188853,
      "grad_norm": 3.198246479034424,
      "learning_rate": 0.0002709793197743975,
      "loss": 1.1635,
      "step": 5361
    },
    {
      "epoch": 0.022911200936616048,
      "grad_norm": 3.0998167991638184,
      "learning_rate": 0.0002709365920355495,
      "loss": 1.5766,
      "step": 5362
    },
    {
      "epoch": 0.02291547382004324,
      "grad_norm": 1.0329539775848389,
      "learning_rate": 0.00027089386429670145,
      "loss": 0.2017,
      "step": 5363
    },
    {
      "epoch": 0.022919746703470436,
      "grad_norm": 2.0118916034698486,
      "learning_rate": 0.00027085113655785336,
      "loss": 0.8717,
      "step": 5364
    },
    {
      "epoch": 0.02292401958689763,
      "grad_norm": 2.179286479949951,
      "learning_rate": 0.00027080840881900533,
      "loss": 0.5848,
      "step": 5365
    },
    {
      "epoch": 0.022928292470324824,
      "grad_norm": 1.5415871143341064,
      "learning_rate": 0.00027076568108015724,
      "loss": 0.3972,
      "step": 5366
    },
    {
      "epoch": 0.02293256535375202,
      "grad_norm": 3.6013708114624023,
      "learning_rate": 0.0002707229533413092,
      "loss": 0.8413,
      "step": 5367
    },
    {
      "epoch": 0.022936838237179215,
      "grad_norm": 1.3095996379852295,
      "learning_rate": 0.0002706802256024611,
      "loss": 0.3348,
      "step": 5368
    },
    {
      "epoch": 0.022941111120606407,
      "grad_norm": 0.9673678278923035,
      "learning_rate": 0.0002706374978636131,
      "loss": 0.4385,
      "step": 5369
    },
    {
      "epoch": 0.022945384004033603,
      "grad_norm": 3.6110968589782715,
      "learning_rate": 0.00027059477012476504,
      "loss": 1.3994,
      "step": 5370
    },
    {
      "epoch": 0.022949656887460795,
      "grad_norm": 0.8451308608055115,
      "learning_rate": 0.00027055204238591695,
      "loss": 0.282,
      "step": 5371
    },
    {
      "epoch": 0.02295392977088799,
      "grad_norm": 2.658473491668701,
      "learning_rate": 0.0002705093146470689,
      "loss": 0.9031,
      "step": 5372
    },
    {
      "epoch": 0.022958202654315186,
      "grad_norm": 0.8490443229675293,
      "learning_rate": 0.00027046658690822083,
      "loss": 0.3892,
      "step": 5373
    },
    {
      "epoch": 0.022962475537742378,
      "grad_norm": 2.918874979019165,
      "learning_rate": 0.0002704238591693728,
      "loss": 0.9021,
      "step": 5374
    },
    {
      "epoch": 0.022966748421169574,
      "grad_norm": 1.6716405153274536,
      "learning_rate": 0.0002703811314305247,
      "loss": 0.7002,
      "step": 5375
    },
    {
      "epoch": 0.02297102130459677,
      "grad_norm": 1.3391355276107788,
      "learning_rate": 0.0002703384036916766,
      "loss": 0.3697,
      "step": 5376
    },
    {
      "epoch": 0.02297529418802396,
      "grad_norm": 4.395381927490234,
      "learning_rate": 0.0002702956759528286,
      "loss": 1.0903,
      "step": 5377
    },
    {
      "epoch": 0.022979567071451157,
      "grad_norm": 1.270194172859192,
      "learning_rate": 0.0002702529482139805,
      "loss": 0.3346,
      "step": 5378
    },
    {
      "epoch": 0.022983839954878352,
      "grad_norm": 0.76854008436203,
      "learning_rate": 0.00027021022047513246,
      "loss": 0.3638,
      "step": 5379
    },
    {
      "epoch": 0.022988112838305545,
      "grad_norm": 1.7416818141937256,
      "learning_rate": 0.00027016749273628437,
      "loss": 0.7079,
      "step": 5380
    },
    {
      "epoch": 0.02299238572173274,
      "grad_norm": 0.6516565084457397,
      "learning_rate": 0.00027012476499743633,
      "loss": 0.2428,
      "step": 5381
    },
    {
      "epoch": 0.022996658605159936,
      "grad_norm": 2.339289665222168,
      "learning_rate": 0.00027008203725858824,
      "loss": 0.6726,
      "step": 5382
    },
    {
      "epoch": 0.023000931488587128,
      "grad_norm": 1.0344843864440918,
      "learning_rate": 0.0002700393095197402,
      "loss": 0.3895,
      "step": 5383
    },
    {
      "epoch": 0.023005204372014323,
      "grad_norm": 2.2769782543182373,
      "learning_rate": 0.0002699965817808922,
      "loss": 0.9458,
      "step": 5384
    },
    {
      "epoch": 0.023009477255441516,
      "grad_norm": 0.6843300461769104,
      "learning_rate": 0.0002699538540420441,
      "loss": 0.2564,
      "step": 5385
    },
    {
      "epoch": 0.02301375013886871,
      "grad_norm": 1.3803927898406982,
      "learning_rate": 0.00026991112630319605,
      "loss": 0.3411,
      "step": 5386
    },
    {
      "epoch": 0.023018023022295907,
      "grad_norm": 1.3296027183532715,
      "learning_rate": 0.00026986839856434796,
      "loss": 0.4927,
      "step": 5387
    },
    {
      "epoch": 0.0230222959057231,
      "grad_norm": 1.3253836631774902,
      "learning_rate": 0.0002698256708254999,
      "loss": 0.4923,
      "step": 5388
    },
    {
      "epoch": 0.023026568789150294,
      "grad_norm": 1.2537710666656494,
      "learning_rate": 0.00026978294308665184,
      "loss": 0.2832,
      "step": 5389
    },
    {
      "epoch": 0.02303084167257749,
      "grad_norm": 3.782182216644287,
      "learning_rate": 0.0002697402153478038,
      "loss": 0.8812,
      "step": 5390
    },
    {
      "epoch": 0.023035114556004682,
      "grad_norm": 0.6760072112083435,
      "learning_rate": 0.0002696974876089557,
      "loss": 0.2881,
      "step": 5391
    },
    {
      "epoch": 0.023039387439431878,
      "grad_norm": 1.6402133703231812,
      "learning_rate": 0.0002696547598701077,
      "loss": 0.419,
      "step": 5392
    },
    {
      "epoch": 0.023043660322859073,
      "grad_norm": 1.3844852447509766,
      "learning_rate": 0.00026961203213125964,
      "loss": 0.5038,
      "step": 5393
    },
    {
      "epoch": 0.023047933206286265,
      "grad_norm": 0.401943176984787,
      "learning_rate": 0.00026956930439241155,
      "loss": 0.1455,
      "step": 5394
    },
    {
      "epoch": 0.02305220608971346,
      "grad_norm": 2.883204936981201,
      "learning_rate": 0.0002695265766535635,
      "loss": 0.9208,
      "step": 5395
    },
    {
      "epoch": 0.023056478973140653,
      "grad_norm": 1.2001245021820068,
      "learning_rate": 0.00026948384891471543,
      "loss": 0.232,
      "step": 5396
    },
    {
      "epoch": 0.02306075185656785,
      "grad_norm": 2.4512135982513428,
      "learning_rate": 0.0002694411211758674,
      "loss": 0.806,
      "step": 5397
    },
    {
      "epoch": 0.023065024739995044,
      "grad_norm": 0.8072501420974731,
      "learning_rate": 0.0002693983934370193,
      "loss": 0.3355,
      "step": 5398
    },
    {
      "epoch": 0.023069297623422236,
      "grad_norm": 0.7144790887832642,
      "learning_rate": 0.00026935566569817127,
      "loss": 0.2791,
      "step": 5399
    },
    {
      "epoch": 0.023073570506849432,
      "grad_norm": 0.9864429831504822,
      "learning_rate": 0.00026931293795932323,
      "loss": 0.3981,
      "step": 5400
    },
    {
      "epoch": 0.023077843390276628,
      "grad_norm": 0.9667622447013855,
      "learning_rate": 0.00026927021022047514,
      "loss": 0.3817,
      "step": 5401
    },
    {
      "epoch": 0.02308211627370382,
      "grad_norm": 1.0254675149917603,
      "learning_rate": 0.0002692274824816271,
      "loss": 0.1765,
      "step": 5402
    },
    {
      "epoch": 0.023086389157131015,
      "grad_norm": 0.7707894444465637,
      "learning_rate": 0.000269184754742779,
      "loss": 0.304,
      "step": 5403
    },
    {
      "epoch": 0.02309066204055821,
      "grad_norm": 0.8865957260131836,
      "learning_rate": 0.000269142027003931,
      "loss": 0.3335,
      "step": 5404
    },
    {
      "epoch": 0.023094934923985403,
      "grad_norm": 1.993119716644287,
      "learning_rate": 0.0002690992992650829,
      "loss": 0.6263,
      "step": 5405
    },
    {
      "epoch": 0.0230992078074126,
      "grad_norm": 4.106899261474609,
      "learning_rate": 0.00026905657152623486,
      "loss": 1.617,
      "step": 5406
    },
    {
      "epoch": 0.02310348069083979,
      "grad_norm": 0.9855079650878906,
      "learning_rate": 0.0002690138437873868,
      "loss": 0.4148,
      "step": 5407
    },
    {
      "epoch": 0.023107753574266986,
      "grad_norm": 2.888753652572632,
      "learning_rate": 0.00026897111604853874,
      "loss": 0.8924,
      "step": 5408
    },
    {
      "epoch": 0.023112026457694182,
      "grad_norm": 0.8706482648849487,
      "learning_rate": 0.00026892838830969065,
      "loss": 0.3334,
      "step": 5409
    },
    {
      "epoch": 0.023116299341121374,
      "grad_norm": 2.721301794052124,
      "learning_rate": 0.00026888566057084256,
      "loss": 0.8902,
      "step": 5410
    },
    {
      "epoch": 0.02312057222454857,
      "grad_norm": 2.247941255569458,
      "learning_rate": 0.0002688429328319945,
      "loss": 0.5374,
      "step": 5411
    },
    {
      "epoch": 0.023124845107975765,
      "grad_norm": 0.6931151151657104,
      "learning_rate": 0.00026880020509314643,
      "loss": 0.3242,
      "step": 5412
    },
    {
      "epoch": 0.023129117991402957,
      "grad_norm": 3.7747578620910645,
      "learning_rate": 0.0002687574773542984,
      "loss": 1.1373,
      "step": 5413
    },
    {
      "epoch": 0.023133390874830153,
      "grad_norm": 1.7175370454788208,
      "learning_rate": 0.00026871474961545036,
      "loss": 0.7629,
      "step": 5414
    },
    {
      "epoch": 0.02313766375825735,
      "grad_norm": 3.7149112224578857,
      "learning_rate": 0.0002686720218766023,
      "loss": 1.8388,
      "step": 5415
    },
    {
      "epoch": 0.02314193664168454,
      "grad_norm": 3.063584327697754,
      "learning_rate": 0.00026862929413775424,
      "loss": 0.913,
      "step": 5416
    },
    {
      "epoch": 0.023146209525111736,
      "grad_norm": 2.1247918605804443,
      "learning_rate": 0.00026858656639890615,
      "loss": 0.5962,
      "step": 5417
    },
    {
      "epoch": 0.023150482408538932,
      "grad_norm": 0.6080315709114075,
      "learning_rate": 0.0002685438386600581,
      "loss": 0.3065,
      "step": 5418
    },
    {
      "epoch": 0.023154755291966124,
      "grad_norm": 3.412612199783325,
      "learning_rate": 0.00026850111092121,
      "loss": 1.3176,
      "step": 5419
    },
    {
      "epoch": 0.02315902817539332,
      "grad_norm": 1.4821233749389648,
      "learning_rate": 0.000268458383182362,
      "loss": 0.3421,
      "step": 5420
    },
    {
      "epoch": 0.02316330105882051,
      "grad_norm": 4.171220302581787,
      "learning_rate": 0.00026841565544351396,
      "loss": 0.9863,
      "step": 5421
    },
    {
      "epoch": 0.023167573942247707,
      "grad_norm": 0.8266634941101074,
      "learning_rate": 0.00026837292770466587,
      "loss": 0.3483,
      "step": 5422
    },
    {
      "epoch": 0.023171846825674903,
      "grad_norm": 3.9442172050476074,
      "learning_rate": 0.00026833019996581783,
      "loss": 1.2116,
      "step": 5423
    },
    {
      "epoch": 0.023176119709102095,
      "grad_norm": 2.9605352878570557,
      "learning_rate": 0.00026828747222696974,
      "loss": 1.1282,
      "step": 5424
    },
    {
      "epoch": 0.02318039259252929,
      "grad_norm": 0.7416931390762329,
      "learning_rate": 0.0002682447444881217,
      "loss": 0.3191,
      "step": 5425
    },
    {
      "epoch": 0.023184665475956486,
      "grad_norm": 2.05012583732605,
      "learning_rate": 0.0002682020167492736,
      "loss": 0.6366,
      "step": 5426
    },
    {
      "epoch": 0.02318893835938368,
      "grad_norm": 0.996843695640564,
      "learning_rate": 0.0002681592890104256,
      "loss": 0.3495,
      "step": 5427
    },
    {
      "epoch": 0.023193211242810874,
      "grad_norm": 3.0034241676330566,
      "learning_rate": 0.0002681165612715775,
      "loss": 0.7227,
      "step": 5428
    },
    {
      "epoch": 0.02319748412623807,
      "grad_norm": 2.643547534942627,
      "learning_rate": 0.00026807383353272946,
      "loss": 1.0568,
      "step": 5429
    },
    {
      "epoch": 0.02320175700966526,
      "grad_norm": 1.1097209453582764,
      "learning_rate": 0.0002680311057938814,
      "loss": 0.4194,
      "step": 5430
    },
    {
      "epoch": 0.023206029893092457,
      "grad_norm": 1.6608595848083496,
      "learning_rate": 0.00026798837805503333,
      "loss": 0.5494,
      "step": 5431
    },
    {
      "epoch": 0.02321030277651965,
      "grad_norm": 1.986700415611267,
      "learning_rate": 0.0002679456503161853,
      "loss": 0.4733,
      "step": 5432
    },
    {
      "epoch": 0.023214575659946845,
      "grad_norm": 1.3965197801589966,
      "learning_rate": 0.0002679029225773372,
      "loss": 0.7308,
      "step": 5433
    },
    {
      "epoch": 0.02321884854337404,
      "grad_norm": 4.693535327911377,
      "learning_rate": 0.0002678601948384892,
      "loss": 1.3474,
      "step": 5434
    },
    {
      "epoch": 0.023223121426801233,
      "grad_norm": 3.1822707653045654,
      "learning_rate": 0.0002678174670996411,
      "loss": 1.0846,
      "step": 5435
    },
    {
      "epoch": 0.023227394310228428,
      "grad_norm": 1.7032198905944824,
      "learning_rate": 0.00026777473936079305,
      "loss": 0.3358,
      "step": 5436
    },
    {
      "epoch": 0.023231667193655624,
      "grad_norm": 1.9003981351852417,
      "learning_rate": 0.000267732011621945,
      "loss": 0.4034,
      "step": 5437
    },
    {
      "epoch": 0.023235940077082816,
      "grad_norm": 1.057794213294983,
      "learning_rate": 0.0002676892838830969,
      "loss": 0.3824,
      "step": 5438
    },
    {
      "epoch": 0.02324021296051001,
      "grad_norm": 0.8101839423179626,
      "learning_rate": 0.0002676465561442489,
      "loss": 0.2808,
      "step": 5439
    },
    {
      "epoch": 0.023244485843937207,
      "grad_norm": 1.6254905462265015,
      "learning_rate": 0.0002676038284054008,
      "loss": 0.6318,
      "step": 5440
    },
    {
      "epoch": 0.0232487587273644,
      "grad_norm": 2.0356225967407227,
      "learning_rate": 0.00026756110066655277,
      "loss": 0.5174,
      "step": 5441
    },
    {
      "epoch": 0.023253031610791595,
      "grad_norm": 2.9896717071533203,
      "learning_rate": 0.0002675183729277046,
      "loss": 0.532,
      "step": 5442
    },
    {
      "epoch": 0.02325730449421879,
      "grad_norm": 2.050637722015381,
      "learning_rate": 0.0002674756451888566,
      "loss": 0.7982,
      "step": 5443
    },
    {
      "epoch": 0.023261577377645982,
      "grad_norm": 3.1702632904052734,
      "learning_rate": 0.00026743291745000855,
      "loss": 1.0194,
      "step": 5444
    },
    {
      "epoch": 0.023265850261073178,
      "grad_norm": 0.6679240465164185,
      "learning_rate": 0.00026739018971116046,
      "loss": 0.312,
      "step": 5445
    },
    {
      "epoch": 0.02327012314450037,
      "grad_norm": 2.4125900268554688,
      "learning_rate": 0.00026734746197231243,
      "loss": 0.844,
      "step": 5446
    },
    {
      "epoch": 0.023274396027927566,
      "grad_norm": 1.9788930416107178,
      "learning_rate": 0.00026730473423346434,
      "loss": 0.575,
      "step": 5447
    },
    {
      "epoch": 0.02327866891135476,
      "grad_norm": 3.714966297149658,
      "learning_rate": 0.0002672620064946163,
      "loss": 0.936,
      "step": 5448
    },
    {
      "epoch": 0.023282941794781953,
      "grad_norm": 2.040705442428589,
      "learning_rate": 0.0002672192787557682,
      "loss": 0.4396,
      "step": 5449
    },
    {
      "epoch": 0.02328721467820915,
      "grad_norm": 2.0944273471832275,
      "learning_rate": 0.0002671765510169202,
      "loss": 0.8523,
      "step": 5450
    },
    {
      "epoch": 0.023291487561636345,
      "grad_norm": 1.816726803779602,
      "learning_rate": 0.00026713382327807214,
      "loss": 0.6477,
      "step": 5451
    },
    {
      "epoch": 0.023295760445063537,
      "grad_norm": 1.8115673065185547,
      "learning_rate": 0.00026709109553922406,
      "loss": 0.5975,
      "step": 5452
    },
    {
      "epoch": 0.023300033328490732,
      "grad_norm": 1.0649362802505493,
      "learning_rate": 0.000267048367800376,
      "loss": 0.3814,
      "step": 5453
    },
    {
      "epoch": 0.023304306211917928,
      "grad_norm": 1.9481769800186157,
      "learning_rate": 0.00026700564006152793,
      "loss": 0.5617,
      "step": 5454
    },
    {
      "epoch": 0.02330857909534512,
      "grad_norm": 2.1327147483825684,
      "learning_rate": 0.0002669629123226799,
      "loss": 0.7755,
      "step": 5455
    },
    {
      "epoch": 0.023312851978772316,
      "grad_norm": 0.9793769717216492,
      "learning_rate": 0.0002669201845838318,
      "loss": 0.3642,
      "step": 5456
    },
    {
      "epoch": 0.023317124862199508,
      "grad_norm": 3.2620670795440674,
      "learning_rate": 0.00026687745684498377,
      "loss": 0.7373,
      "step": 5457
    },
    {
      "epoch": 0.023321397745626703,
      "grad_norm": 2.6316354274749756,
      "learning_rate": 0.00026683472910613574,
      "loss": 0.839,
      "step": 5458
    },
    {
      "epoch": 0.0233256706290539,
      "grad_norm": 2.1917879581451416,
      "learning_rate": 0.00026679200136728765,
      "loss": 1.3163,
      "step": 5459
    },
    {
      "epoch": 0.02332994351248109,
      "grad_norm": 3.669201135635376,
      "learning_rate": 0.0002667492736284396,
      "loss": 0.9038,
      "step": 5460
    },
    {
      "epoch": 0.023334216395908287,
      "grad_norm": 2.1265385150909424,
      "learning_rate": 0.0002667065458895915,
      "loss": 0.5713,
      "step": 5461
    },
    {
      "epoch": 0.023338489279335482,
      "grad_norm": 1.8510969877243042,
      "learning_rate": 0.0002666638181507435,
      "loss": 0.743,
      "step": 5462
    },
    {
      "epoch": 0.023342762162762674,
      "grad_norm": 1.028460144996643,
      "learning_rate": 0.0002666210904118954,
      "loss": 0.3817,
      "step": 5463
    },
    {
      "epoch": 0.02334703504618987,
      "grad_norm": 2.443652868270874,
      "learning_rate": 0.00026657836267304736,
      "loss": 0.9793,
      "step": 5464
    },
    {
      "epoch": 0.023351307929617066,
      "grad_norm": 2.9525129795074463,
      "learning_rate": 0.0002665356349341993,
      "loss": 0.7322,
      "step": 5465
    },
    {
      "epoch": 0.023355580813044258,
      "grad_norm": 2.0679423809051514,
      "learning_rate": 0.00026649290719535124,
      "loss": 0.5784,
      "step": 5466
    },
    {
      "epoch": 0.023359853696471453,
      "grad_norm": 1.3628565073013306,
      "learning_rate": 0.0002664501794565032,
      "loss": 0.6931,
      "step": 5467
    },
    {
      "epoch": 0.02336412657989865,
      "grad_norm": 1.0273998975753784,
      "learning_rate": 0.0002664074517176551,
      "loss": 0.3667,
      "step": 5468
    },
    {
      "epoch": 0.02336839946332584,
      "grad_norm": 1.8251922130584717,
      "learning_rate": 0.0002663647239788071,
      "loss": 0.6285,
      "step": 5469
    },
    {
      "epoch": 0.023372672346753037,
      "grad_norm": 2.0425617694854736,
      "learning_rate": 0.000266321996239959,
      "loss": 0.7637,
      "step": 5470
    },
    {
      "epoch": 0.02337694523018023,
      "grad_norm": 1.4040826559066772,
      "learning_rate": 0.00026627926850111096,
      "loss": 0.4644,
      "step": 5471
    },
    {
      "epoch": 0.023381218113607424,
      "grad_norm": 1.8854399919509888,
      "learning_rate": 0.00026623654076226287,
      "loss": 0.3115,
      "step": 5472
    },
    {
      "epoch": 0.02338549099703462,
      "grad_norm": 2.653475046157837,
      "learning_rate": 0.00026619381302341483,
      "loss": 0.8346,
      "step": 5473
    },
    {
      "epoch": 0.023389763880461812,
      "grad_norm": 4.237457275390625,
      "learning_rate": 0.00026615108528456674,
      "loss": 1.3303,
      "step": 5474
    },
    {
      "epoch": 0.023394036763889008,
      "grad_norm": 1.8021893501281738,
      "learning_rate": 0.00026610835754571865,
      "loss": 0.4815,
      "step": 5475
    },
    {
      "epoch": 0.023398309647316203,
      "grad_norm": 3.7099862098693848,
      "learning_rate": 0.0002660656298068706,
      "loss": 1.9107,
      "step": 5476
    },
    {
      "epoch": 0.023402582530743395,
      "grad_norm": 3.686796188354492,
      "learning_rate": 0.00026602290206802253,
      "loss": 1.8713,
      "step": 5477
    },
    {
      "epoch": 0.02340685541417059,
      "grad_norm": 1.7771610021591187,
      "learning_rate": 0.0002659801743291745,
      "loss": 0.2503,
      "step": 5478
    },
    {
      "epoch": 0.023411128297597786,
      "grad_norm": 1.329698920249939,
      "learning_rate": 0.0002659374465903264,
      "loss": 0.6241,
      "step": 5479
    },
    {
      "epoch": 0.02341540118102498,
      "grad_norm": 1.741553783416748,
      "learning_rate": 0.00026589471885147837,
      "loss": 0.6269,
      "step": 5480
    },
    {
      "epoch": 0.023419674064452174,
      "grad_norm": 1.3974014520645142,
      "learning_rate": 0.00026585199111263033,
      "loss": 0.5464,
      "step": 5481
    },
    {
      "epoch": 0.023423946947879366,
      "grad_norm": 1.7307288646697998,
      "learning_rate": 0.00026580926337378224,
      "loss": 0.5469,
      "step": 5482
    },
    {
      "epoch": 0.023428219831306562,
      "grad_norm": 3.1214115619659424,
      "learning_rate": 0.0002657665356349342,
      "loss": 0.5962,
      "step": 5483
    },
    {
      "epoch": 0.023432492714733757,
      "grad_norm": 1.0848698616027832,
      "learning_rate": 0.0002657238078960861,
      "loss": 0.5034,
      "step": 5484
    },
    {
      "epoch": 0.02343676559816095,
      "grad_norm": 1.4370707273483276,
      "learning_rate": 0.0002656810801572381,
      "loss": 0.6765,
      "step": 5485
    },
    {
      "epoch": 0.023441038481588145,
      "grad_norm": 0.9962893724441528,
      "learning_rate": 0.00026563835241839,
      "loss": 0.3523,
      "step": 5486
    },
    {
      "epoch": 0.02344531136501534,
      "grad_norm": 1.3947854042053223,
      "learning_rate": 0.00026559562467954196,
      "loss": 0.5065,
      "step": 5487
    },
    {
      "epoch": 0.023449584248442533,
      "grad_norm": 0.8710182905197144,
      "learning_rate": 0.0002655528969406939,
      "loss": 0.3705,
      "step": 5488
    },
    {
      "epoch": 0.02345385713186973,
      "grad_norm": 0.8718884587287903,
      "learning_rate": 0.00026551016920184584,
      "loss": 0.371,
      "step": 5489
    },
    {
      "epoch": 0.023458130015296924,
      "grad_norm": 0.8695992827415466,
      "learning_rate": 0.0002654674414629978,
      "loss": 0.3062,
      "step": 5490
    },
    {
      "epoch": 0.023462402898724116,
      "grad_norm": 1.1076527833938599,
      "learning_rate": 0.0002654247137241497,
      "loss": 0.3771,
      "step": 5491
    },
    {
      "epoch": 0.023466675782151312,
      "grad_norm": 1.3874667882919312,
      "learning_rate": 0.0002653819859853017,
      "loss": 0.6289,
      "step": 5492
    },
    {
      "epoch": 0.023470948665578507,
      "grad_norm": 1.4467853307724,
      "learning_rate": 0.0002653392582464536,
      "loss": 0.3809,
      "step": 5493
    },
    {
      "epoch": 0.0234752215490057,
      "grad_norm": 1.7074781656265259,
      "learning_rate": 0.00026529653050760555,
      "loss": 0.6286,
      "step": 5494
    },
    {
      "epoch": 0.023479494432432895,
      "grad_norm": 1.7034916877746582,
      "learning_rate": 0.00026525380276875746,
      "loss": 0.6286,
      "step": 5495
    },
    {
      "epoch": 0.023483767315860087,
      "grad_norm": 1.3556151390075684,
      "learning_rate": 0.00026521107502990943,
      "loss": 0.139,
      "step": 5496
    },
    {
      "epoch": 0.023488040199287283,
      "grad_norm": 1.3596723079681396,
      "learning_rate": 0.0002651683472910614,
      "loss": 0.6202,
      "step": 5497
    },
    {
      "epoch": 0.02349231308271448,
      "grad_norm": 0.8830792903900146,
      "learning_rate": 0.0002651256195522133,
      "loss": 0.3335,
      "step": 5498
    },
    {
      "epoch": 0.02349658596614167,
      "grad_norm": 1.3439195156097412,
      "learning_rate": 0.00026508289181336527,
      "loss": 0.5869,
      "step": 5499
    },
    {
      "epoch": 0.023500858849568866,
      "grad_norm": 3.6081244945526123,
      "learning_rate": 0.0002650401640745172,
      "loss": 1.7061,
      "step": 5500
    },
    {
      "epoch": 0.02350513173299606,
      "grad_norm": 0.9954474568367004,
      "learning_rate": 0.00026499743633566915,
      "loss": 0.4149,
      "step": 5501
    },
    {
      "epoch": 0.023509404616423254,
      "grad_norm": 2.0125184059143066,
      "learning_rate": 0.00026495470859682106,
      "loss": 1.2328,
      "step": 5502
    },
    {
      "epoch": 0.02351367749985045,
      "grad_norm": 0.9244332313537598,
      "learning_rate": 0.000264911980857973,
      "loss": 0.3817,
      "step": 5503
    },
    {
      "epoch": 0.023517950383277645,
      "grad_norm": 2.8297979831695557,
      "learning_rate": 0.000264869253119125,
      "loss": 0.5542,
      "step": 5504
    },
    {
      "epoch": 0.023522223266704837,
      "grad_norm": 0.9354360699653625,
      "learning_rate": 0.0002648265253802769,
      "loss": 0.3646,
      "step": 5505
    },
    {
      "epoch": 0.023526496150132033,
      "grad_norm": 4.280975341796875,
      "learning_rate": 0.00026478379764142886,
      "loss": 1.1576,
      "step": 5506
    },
    {
      "epoch": 0.023530769033559225,
      "grad_norm": 2.6978237628936768,
      "learning_rate": 0.0002647410699025807,
      "loss": 0.5071,
      "step": 5507
    },
    {
      "epoch": 0.02353504191698642,
      "grad_norm": 0.6629400253295898,
      "learning_rate": 0.0002646983421637327,
      "loss": 0.2567,
      "step": 5508
    },
    {
      "epoch": 0.023539314800413616,
      "grad_norm": 4.022591590881348,
      "learning_rate": 0.0002646556144248846,
      "loss": 1.364,
      "step": 5509
    },
    {
      "epoch": 0.023543587683840808,
      "grad_norm": 3.685955286026001,
      "learning_rate": 0.00026461288668603656,
      "loss": 1.1446,
      "step": 5510
    },
    {
      "epoch": 0.023547860567268004,
      "grad_norm": 1.2199697494506836,
      "learning_rate": 0.0002645701589471885,
      "loss": 0.7692,
      "step": 5511
    },
    {
      "epoch": 0.0235521334506952,
      "grad_norm": 2.177516222000122,
      "learning_rate": 0.00026452743120834043,
      "loss": 0.7369,
      "step": 5512
    },
    {
      "epoch": 0.02355640633412239,
      "grad_norm": 1.548019289970398,
      "learning_rate": 0.0002644847034694924,
      "loss": 0.585,
      "step": 5513
    },
    {
      "epoch": 0.023560679217549587,
      "grad_norm": 3.3330790996551514,
      "learning_rate": 0.0002644419757306443,
      "loss": 0.7368,
      "step": 5514
    },
    {
      "epoch": 0.023564952100976783,
      "grad_norm": 0.43381813168525696,
      "learning_rate": 0.0002643992479917963,
      "loss": 0.2151,
      "step": 5515
    },
    {
      "epoch": 0.023569224984403975,
      "grad_norm": 1.6392326354980469,
      "learning_rate": 0.0002643565202529482,
      "loss": 0.6916,
      "step": 5516
    },
    {
      "epoch": 0.02357349786783117,
      "grad_norm": 2.074139356613159,
      "learning_rate": 0.00026431379251410015,
      "loss": 0.6328,
      "step": 5517
    },
    {
      "epoch": 0.023577770751258362,
      "grad_norm": 0.5156311392784119,
      "learning_rate": 0.0002642710647752521,
      "loss": 0.2416,
      "step": 5518
    },
    {
      "epoch": 0.023582043634685558,
      "grad_norm": 1.8481143712997437,
      "learning_rate": 0.000264228337036404,
      "loss": 0.7202,
      "step": 5519
    },
    {
      "epoch": 0.023586316518112754,
      "grad_norm": 0.4774000346660614,
      "learning_rate": 0.000264185609297556,
      "loss": 0.225,
      "step": 5520
    },
    {
      "epoch": 0.023590589401539946,
      "grad_norm": 1.5142427682876587,
      "learning_rate": 0.0002641428815587079,
      "loss": 0.5854,
      "step": 5521
    },
    {
      "epoch": 0.02359486228496714,
      "grad_norm": 1.7403379678726196,
      "learning_rate": 0.00026410015381985987,
      "loss": 0.6799,
      "step": 5522
    },
    {
      "epoch": 0.023599135168394337,
      "grad_norm": 3.1744019985198975,
      "learning_rate": 0.0002640574260810118,
      "loss": 0.9351,
      "step": 5523
    },
    {
      "epoch": 0.02360340805182153,
      "grad_norm": 2.5224809646606445,
      "learning_rate": 0.00026401469834216374,
      "loss": 0.9405,
      "step": 5524
    },
    {
      "epoch": 0.023607680935248725,
      "grad_norm": 0.5769997835159302,
      "learning_rate": 0.0002639719706033157,
      "loss": 0.2709,
      "step": 5525
    },
    {
      "epoch": 0.02361195381867592,
      "grad_norm": 1.1782208681106567,
      "learning_rate": 0.0002639292428644676,
      "loss": 0.5166,
      "step": 5526
    },
    {
      "epoch": 0.023616226702103112,
      "grad_norm": 1.4017664194107056,
      "learning_rate": 0.0002638865151256196,
      "loss": 0.5509,
      "step": 5527
    },
    {
      "epoch": 0.023620499585530308,
      "grad_norm": 1.8057109117507935,
      "learning_rate": 0.0002638437873867715,
      "loss": 0.4316,
      "step": 5528
    },
    {
      "epoch": 0.023624772468957504,
      "grad_norm": 1.461374282836914,
      "learning_rate": 0.00026380105964792346,
      "loss": 0.5508,
      "step": 5529
    },
    {
      "epoch": 0.023629045352384696,
      "grad_norm": 1.1595028638839722,
      "learning_rate": 0.00026375833190907537,
      "loss": 0.4838,
      "step": 5530
    },
    {
      "epoch": 0.02363331823581189,
      "grad_norm": 1.7548606395721436,
      "learning_rate": 0.00026371560417022733,
      "loss": 0.6888,
      "step": 5531
    },
    {
      "epoch": 0.023637591119239083,
      "grad_norm": 1.3881205320358276,
      "learning_rate": 0.00026367287643137925,
      "loss": 0.5037,
      "step": 5532
    },
    {
      "epoch": 0.02364186400266628,
      "grad_norm": 4.132630825042725,
      "learning_rate": 0.0002636301486925312,
      "loss": 1.2192,
      "step": 5533
    },
    {
      "epoch": 0.023646136886093475,
      "grad_norm": 1.188330054283142,
      "learning_rate": 0.0002635874209536832,
      "loss": 0.461,
      "step": 5534
    },
    {
      "epoch": 0.023650409769520667,
      "grad_norm": 1.0912315845489502,
      "learning_rate": 0.0002635446932148351,
      "loss": 0.4192,
      "step": 5535
    },
    {
      "epoch": 0.023654682652947862,
      "grad_norm": 3.383277654647827,
      "learning_rate": 0.00026350196547598705,
      "loss": 1.674,
      "step": 5536
    },
    {
      "epoch": 0.023658955536375058,
      "grad_norm": 4.547978401184082,
      "learning_rate": 0.00026345923773713896,
      "loss": 1.2776,
      "step": 5537
    },
    {
      "epoch": 0.02366322841980225,
      "grad_norm": 1.7885525226593018,
      "learning_rate": 0.0002634165099982909,
      "loss": 0.6808,
      "step": 5538
    },
    {
      "epoch": 0.023667501303229446,
      "grad_norm": 1.834912896156311,
      "learning_rate": 0.00026337378225944284,
      "loss": 0.471,
      "step": 5539
    },
    {
      "epoch": 0.02367177418665664,
      "grad_norm": 3.8600399494171143,
      "learning_rate": 0.00026333105452059475,
      "loss": 1.7562,
      "step": 5540
    },
    {
      "epoch": 0.023676047070083833,
      "grad_norm": 3.712125539779663,
      "learning_rate": 0.0002632883267817467,
      "loss": 1.0351,
      "step": 5541
    },
    {
      "epoch": 0.02368031995351103,
      "grad_norm": 4.371644020080566,
      "learning_rate": 0.0002632455990428986,
      "loss": 1.1908,
      "step": 5542
    },
    {
      "epoch": 0.02368459283693822,
      "grad_norm": 3.0524120330810547,
      "learning_rate": 0.0002632028713040506,
      "loss": 1.0314,
      "step": 5543
    },
    {
      "epoch": 0.023688865720365417,
      "grad_norm": 1.9375337362289429,
      "learning_rate": 0.0002631601435652025,
      "loss": 0.3978,
      "step": 5544
    },
    {
      "epoch": 0.023693138603792612,
      "grad_norm": 0.4794096350669861,
      "learning_rate": 0.00026311741582635446,
      "loss": 0.1972,
      "step": 5545
    },
    {
      "epoch": 0.023697411487219804,
      "grad_norm": 2.4992129802703857,
      "learning_rate": 0.0002630746880875064,
      "loss": 1.1094,
      "step": 5546
    },
    {
      "epoch": 0.023701684370647,
      "grad_norm": 0.6502757668495178,
      "learning_rate": 0.00026303196034865834,
      "loss": 0.2581,
      "step": 5547
    },
    {
      "epoch": 0.023705957254074195,
      "grad_norm": 1.2491320371627808,
      "learning_rate": 0.0002629892326098103,
      "loss": 0.4603,
      "step": 5548
    },
    {
      "epoch": 0.023710230137501388,
      "grad_norm": 1.3523805141448975,
      "learning_rate": 0.0002629465048709622,
      "loss": 0.5198,
      "step": 5549
    },
    {
      "epoch": 0.023714503020928583,
      "grad_norm": 0.3699425756931305,
      "learning_rate": 0.0002629037771321142,
      "loss": 0.1528,
      "step": 5550
    },
    {
      "epoch": 0.02371877590435578,
      "grad_norm": 1.9076346158981323,
      "learning_rate": 0.0002628610493932661,
      "loss": 0.6739,
      "step": 5551
    },
    {
      "epoch": 0.02372304878778297,
      "grad_norm": 0.357700377702713,
      "learning_rate": 0.00026281832165441806,
      "loss": 0.1637,
      "step": 5552
    },
    {
      "epoch": 0.023727321671210166,
      "grad_norm": 0.5603259205818176,
      "learning_rate": 0.00026277559391556997,
      "loss": 0.2124,
      "step": 5553
    },
    {
      "epoch": 0.023731594554637362,
      "grad_norm": 1.8819580078125,
      "learning_rate": 0.00026273286617672193,
      "loss": 0.3274,
      "step": 5554
    },
    {
      "epoch": 0.023735867438064554,
      "grad_norm": 1.4448026418685913,
      "learning_rate": 0.0002626901384378739,
      "loss": 0.3836,
      "step": 5555
    },
    {
      "epoch": 0.02374014032149175,
      "grad_norm": 1.8099714517593384,
      "learning_rate": 0.0002626474106990258,
      "loss": 0.5957,
      "step": 5556
    },
    {
      "epoch": 0.023744413204918942,
      "grad_norm": 1.4067187309265137,
      "learning_rate": 0.00026260468296017777,
      "loss": 0.5388,
      "step": 5557
    },
    {
      "epoch": 0.023748686088346137,
      "grad_norm": 2.0993943214416504,
      "learning_rate": 0.0002625619552213297,
      "loss": 0.6523,
      "step": 5558
    },
    {
      "epoch": 0.023752958971773333,
      "grad_norm": 0.5694209337234497,
      "learning_rate": 0.00026251922748248165,
      "loss": 0.1999,
      "step": 5559
    },
    {
      "epoch": 0.023757231855200525,
      "grad_norm": 1.395916223526001,
      "learning_rate": 0.00026247649974363356,
      "loss": 0.8966,
      "step": 5560
    },
    {
      "epoch": 0.02376150473862772,
      "grad_norm": 0.7390958666801453,
      "learning_rate": 0.0002624337720047855,
      "loss": 0.3627,
      "step": 5561
    },
    {
      "epoch": 0.023765777622054916,
      "grad_norm": 2.3566651344299316,
      "learning_rate": 0.0002623910442659375,
      "loss": 0.6338,
      "step": 5562
    },
    {
      "epoch": 0.02377005050548211,
      "grad_norm": 1.3517286777496338,
      "learning_rate": 0.0002623483165270894,
      "loss": 0.4745,
      "step": 5563
    },
    {
      "epoch": 0.023774323388909304,
      "grad_norm": 2.463487386703491,
      "learning_rate": 0.00026230558878824136,
      "loss": 0.8324,
      "step": 5564
    },
    {
      "epoch": 0.0237785962723365,
      "grad_norm": 1.709586501121521,
      "learning_rate": 0.0002622628610493933,
      "loss": 0.6593,
      "step": 5565
    },
    {
      "epoch": 0.023782869155763692,
      "grad_norm": 3.567107915878296,
      "learning_rate": 0.00026222013331054524,
      "loss": 1.4092,
      "step": 5566
    },
    {
      "epoch": 0.023787142039190887,
      "grad_norm": 0.5682569742202759,
      "learning_rate": 0.00026217740557169715,
      "loss": 0.1885,
      "step": 5567
    },
    {
      "epoch": 0.02379141492261808,
      "grad_norm": 0.6507622003555298,
      "learning_rate": 0.0002621346778328491,
      "loss": 0.3441,
      "step": 5568
    },
    {
      "epoch": 0.023795687806045275,
      "grad_norm": 3.129181146621704,
      "learning_rate": 0.000262091950094001,
      "loss": 0.867,
      "step": 5569
    },
    {
      "epoch": 0.02379996068947247,
      "grad_norm": 1.6707128286361694,
      "learning_rate": 0.000262049222355153,
      "loss": 0.6758,
      "step": 5570
    },
    {
      "epoch": 0.023804233572899663,
      "grad_norm": 2.176332712173462,
      "learning_rate": 0.00026200649461630496,
      "loss": 0.7873,
      "step": 5571
    },
    {
      "epoch": 0.02380850645632686,
      "grad_norm": 1.5472102165222168,
      "learning_rate": 0.00026196376687745687,
      "loss": 0.6104,
      "step": 5572
    },
    {
      "epoch": 0.023812779339754054,
      "grad_norm": 3.1032941341400146,
      "learning_rate": 0.0002619210391386088,
      "loss": 1.0745,
      "step": 5573
    },
    {
      "epoch": 0.023817052223181246,
      "grad_norm": 1.0466145277023315,
      "learning_rate": 0.0002618783113997607,
      "loss": 0.4133,
      "step": 5574
    },
    {
      "epoch": 0.02382132510660844,
      "grad_norm": 1.0624178647994995,
      "learning_rate": 0.00026183558366091265,
      "loss": 0.4254,
      "step": 5575
    },
    {
      "epoch": 0.023825597990035637,
      "grad_norm": 3.8368449211120605,
      "learning_rate": 0.00026179285592206456,
      "loss": 1.7733,
      "step": 5576
    },
    {
      "epoch": 0.02382987087346283,
      "grad_norm": 2.765519380569458,
      "learning_rate": 0.00026175012818321653,
      "loss": 1.0558,
      "step": 5577
    },
    {
      "epoch": 0.023834143756890025,
      "grad_norm": 1.3125098943710327,
      "learning_rate": 0.0002617074004443685,
      "loss": 0.3327,
      "step": 5578
    },
    {
      "epoch": 0.02383841664031722,
      "grad_norm": 1.690489649772644,
      "learning_rate": 0.0002616646727055204,
      "loss": 0.4575,
      "step": 5579
    },
    {
      "epoch": 0.023842689523744413,
      "grad_norm": 1.3491355180740356,
      "learning_rate": 0.00026162194496667237,
      "loss": 0.4963,
      "step": 5580
    },
    {
      "epoch": 0.023846962407171608,
      "grad_norm": 1.3673219680786133,
      "learning_rate": 0.0002615792172278243,
      "loss": 0.496,
      "step": 5581
    },
    {
      "epoch": 0.0238512352905988,
      "grad_norm": 2.0724024772644043,
      "learning_rate": 0.00026153648948897625,
      "loss": 0.6827,
      "step": 5582
    },
    {
      "epoch": 0.023855508174025996,
      "grad_norm": 1.2610492706298828,
      "learning_rate": 0.00026149376175012816,
      "loss": 0.4669,
      "step": 5583
    },
    {
      "epoch": 0.02385978105745319,
      "grad_norm": 3.044611930847168,
      "learning_rate": 0.0002614510340112801,
      "loss": 1.5432,
      "step": 5584
    },
    {
      "epoch": 0.023864053940880384,
      "grad_norm": 0.5755990147590637,
      "learning_rate": 0.0002614083062724321,
      "loss": 0.1773,
      "step": 5585
    },
    {
      "epoch": 0.02386832682430758,
      "grad_norm": 3.6313953399658203,
      "learning_rate": 0.000261365578533584,
      "loss": 1.0805,
      "step": 5586
    },
    {
      "epoch": 0.023872599707734775,
      "grad_norm": 3.6447300910949707,
      "learning_rate": 0.00026132285079473596,
      "loss": 1.092,
      "step": 5587
    },
    {
      "epoch": 0.023876872591161967,
      "grad_norm": 3.212791681289673,
      "learning_rate": 0.00026128012305588787,
      "loss": 1.3897,
      "step": 5588
    },
    {
      "epoch": 0.023881145474589163,
      "grad_norm": 5.1580424308776855,
      "learning_rate": 0.00026123739531703984,
      "loss": 2.5188,
      "step": 5589
    },
    {
      "epoch": 0.023885418358016358,
      "grad_norm": 3.063728094100952,
      "learning_rate": 0.00026119466757819175,
      "loss": 1.3964,
      "step": 5590
    },
    {
      "epoch": 0.02388969124144355,
      "grad_norm": 0.7515437006950378,
      "learning_rate": 0.0002611519398393437,
      "loss": 0.2343,
      "step": 5591
    },
    {
      "epoch": 0.023893964124870746,
      "grad_norm": 3.3613247871398926,
      "learning_rate": 0.0002611092121004957,
      "loss": 1.1522,
      "step": 5592
    },
    {
      "epoch": 0.023898237008297938,
      "grad_norm": 0.2738999128341675,
      "learning_rate": 0.0002610664843616476,
      "loss": 0.0941,
      "step": 5593
    },
    {
      "epoch": 0.023902509891725134,
      "grad_norm": 1.0576486587524414,
      "learning_rate": 0.00026102375662279955,
      "loss": 0.3848,
      "step": 5594
    },
    {
      "epoch": 0.02390678277515233,
      "grad_norm": 0.7871668934822083,
      "learning_rate": 0.00026098102888395146,
      "loss": 0.4669,
      "step": 5595
    },
    {
      "epoch": 0.02391105565857952,
      "grad_norm": 3.3474531173706055,
      "learning_rate": 0.00026093830114510343,
      "loss": 0.8994,
      "step": 5596
    },
    {
      "epoch": 0.023915328542006717,
      "grad_norm": 0.7823370099067688,
      "learning_rate": 0.00026089557340625534,
      "loss": 0.4511,
      "step": 5597
    },
    {
      "epoch": 0.023919601425433912,
      "grad_norm": 1.3106952905654907,
      "learning_rate": 0.0002608528456674073,
      "loss": 0.4458,
      "step": 5598
    },
    {
      "epoch": 0.023923874308861105,
      "grad_norm": 1.0799990892410278,
      "learning_rate": 0.0002608101179285592,
      "loss": 0.3936,
      "step": 5599
    },
    {
      "epoch": 0.0239281471922883,
      "grad_norm": 3.373579978942871,
      "learning_rate": 0.0002607673901897112,
      "loss": 1.5498,
      "step": 5600
    },
    {
      "epoch": 0.023932420075715496,
      "grad_norm": 1.8874205350875854,
      "learning_rate": 0.00026072466245086315,
      "loss": 0.5432,
      "step": 5601
    },
    {
      "epoch": 0.023936692959142688,
      "grad_norm": 0.7678603529930115,
      "learning_rate": 0.00026068193471201506,
      "loss": 0.405,
      "step": 5602
    },
    {
      "epoch": 0.023940965842569883,
      "grad_norm": 2.45011568069458,
      "learning_rate": 0.000260639206973167,
      "loss": 0.8446,
      "step": 5603
    },
    {
      "epoch": 0.02394523872599708,
      "grad_norm": 0.5802210569381714,
      "learning_rate": 0.00026059647923431893,
      "loss": 0.2065,
      "step": 5604
    },
    {
      "epoch": 0.02394951160942427,
      "grad_norm": 2.3950791358947754,
      "learning_rate": 0.0002605537514954709,
      "loss": 1.0471,
      "step": 5605
    },
    {
      "epoch": 0.023953784492851467,
      "grad_norm": 1.7918561697006226,
      "learning_rate": 0.00026051102375662275,
      "loss": 0.4919,
      "step": 5606
    },
    {
      "epoch": 0.02395805737627866,
      "grad_norm": 1.0085209608078003,
      "learning_rate": 0.0002604682960177747,
      "loss": 0.3766,
      "step": 5607
    },
    {
      "epoch": 0.023962330259705854,
      "grad_norm": 1.1645931005477905,
      "learning_rate": 0.0002604255682789267,
      "loss": 0.4492,
      "step": 5608
    },
    {
      "epoch": 0.02396660314313305,
      "grad_norm": 0.723240852355957,
      "learning_rate": 0.0002603828405400786,
      "loss": 0.3844,
      "step": 5609
    },
    {
      "epoch": 0.023970876026560242,
      "grad_norm": 1.3221663236618042,
      "learning_rate": 0.00026034011280123056,
      "loss": 0.9239,
      "step": 5610
    },
    {
      "epoch": 0.023975148909987438,
      "grad_norm": 0.5609074234962463,
      "learning_rate": 0.00026029738506238247,
      "loss": 0.1955,
      "step": 5611
    },
    {
      "epoch": 0.023979421793414633,
      "grad_norm": 1.6350171566009521,
      "learning_rate": 0.00026025465732353444,
      "loss": 0.3988,
      "step": 5612
    },
    {
      "epoch": 0.023983694676841825,
      "grad_norm": 0.7181392908096313,
      "learning_rate": 0.00026021192958468635,
      "loss": 0.3845,
      "step": 5613
    },
    {
      "epoch": 0.02398796756026902,
      "grad_norm": 1.3340929746627808,
      "learning_rate": 0.0002601692018458383,
      "loss": 0.9163,
      "step": 5614
    },
    {
      "epoch": 0.023992240443696217,
      "grad_norm": 0.5671340227127075,
      "learning_rate": 0.0002601264741069903,
      "loss": 0.2062,
      "step": 5615
    },
    {
      "epoch": 0.02399651332712341,
      "grad_norm": 3.7694787979125977,
      "learning_rate": 0.0002600837463681422,
      "loss": 1.6662,
      "step": 5616
    },
    {
      "epoch": 0.024000786210550604,
      "grad_norm": 0.5864117741584778,
      "learning_rate": 0.00026004101862929415,
      "loss": 0.324,
      "step": 5617
    },
    {
      "epoch": 0.024005059093977796,
      "grad_norm": 2.3717405796051025,
      "learning_rate": 0.00025999829089044606,
      "loss": 0.9307,
      "step": 5618
    },
    {
      "epoch": 0.024009331977404992,
      "grad_norm": 1.1129776239395142,
      "learning_rate": 0.00025995556315159803,
      "loss": 0.43,
      "step": 5619
    },
    {
      "epoch": 0.024013604860832188,
      "grad_norm": 1.0331194400787354,
      "learning_rate": 0.00025991283541274994,
      "loss": 0.3715,
      "step": 5620
    },
    {
      "epoch": 0.02401787774425938,
      "grad_norm": 1.7681221961975098,
      "learning_rate": 0.0002598701076739019,
      "loss": 0.7733,
      "step": 5621
    },
    {
      "epoch": 0.024022150627686575,
      "grad_norm": 4.205497741699219,
      "learning_rate": 0.00025982737993505387,
      "loss": 1.0485,
      "step": 5622
    },
    {
      "epoch": 0.02402642351111377,
      "grad_norm": 0.6605757474899292,
      "learning_rate": 0.0002597846521962058,
      "loss": 0.342,
      "step": 5623
    },
    {
      "epoch": 0.024030696394540963,
      "grad_norm": 0.5401126742362976,
      "learning_rate": 0.00025974192445735774,
      "loss": 0.2958,
      "step": 5624
    },
    {
      "epoch": 0.02403496927796816,
      "grad_norm": 1.1282687187194824,
      "learning_rate": 0.00025969919671850965,
      "loss": 0.4298,
      "step": 5625
    },
    {
      "epoch": 0.024039242161395354,
      "grad_norm": 3.768486976623535,
      "learning_rate": 0.0002596564689796616,
      "loss": 0.9769,
      "step": 5626
    },
    {
      "epoch": 0.024043515044822546,
      "grad_norm": 2.5085384845733643,
      "learning_rate": 0.00025961374124081353,
      "loss": 0.93,
      "step": 5627
    },
    {
      "epoch": 0.024047787928249742,
      "grad_norm": 2.1684422492980957,
      "learning_rate": 0.0002595710135019655,
      "loss": 1.3736,
      "step": 5628
    },
    {
      "epoch": 0.024052060811676934,
      "grad_norm": 0.6205859184265137,
      "learning_rate": 0.00025952828576311746,
      "loss": 0.2427,
      "step": 5629
    },
    {
      "epoch": 0.02405633369510413,
      "grad_norm": 1.2475401163101196,
      "learning_rate": 0.00025948555802426937,
      "loss": 0.3697,
      "step": 5630
    },
    {
      "epoch": 0.024060606578531325,
      "grad_norm": 1.5535892248153687,
      "learning_rate": 0.00025944283028542134,
      "loss": 0.5252,
      "step": 5631
    },
    {
      "epoch": 0.024064879461958517,
      "grad_norm": 1.97889244556427,
      "learning_rate": 0.00025940010254657325,
      "loss": 0.5505,
      "step": 5632
    },
    {
      "epoch": 0.024069152345385713,
      "grad_norm": 1.1868053674697876,
      "learning_rate": 0.0002593573748077252,
      "loss": 0.4753,
      "step": 5633
    },
    {
      "epoch": 0.02407342522881291,
      "grad_norm": 3.8841567039489746,
      "learning_rate": 0.0002593146470688771,
      "loss": 1.2465,
      "step": 5634
    },
    {
      "epoch": 0.0240776981122401,
      "grad_norm": 4.203524112701416,
      "learning_rate": 0.0002592719193300291,
      "loss": 2.5368,
      "step": 5635
    },
    {
      "epoch": 0.024081970995667296,
      "grad_norm": 1.7590702772140503,
      "learning_rate": 0.000259229191591181,
      "loss": 0.7915,
      "step": 5636
    },
    {
      "epoch": 0.024086243879094492,
      "grad_norm": 0.5929696559906006,
      "learning_rate": 0.00025918646385233296,
      "loss": 0.2883,
      "step": 5637
    },
    {
      "epoch": 0.024090516762521684,
      "grad_norm": 4.526533126831055,
      "learning_rate": 0.00025914373611348493,
      "loss": 1.9218,
      "step": 5638
    },
    {
      "epoch": 0.02409478964594888,
      "grad_norm": 2.3801207542419434,
      "learning_rate": 0.0002591010083746368,
      "loss": 0.7535,
      "step": 5639
    },
    {
      "epoch": 0.024099062529376075,
      "grad_norm": 0.4818264842033386,
      "learning_rate": 0.00025905828063578875,
      "loss": 0.2422,
      "step": 5640
    },
    {
      "epoch": 0.024103335412803267,
      "grad_norm": 3.3105885982513428,
      "learning_rate": 0.00025901555289694066,
      "loss": 1.2057,
      "step": 5641
    },
    {
      "epoch": 0.024107608296230463,
      "grad_norm": 0.41089990735054016,
      "learning_rate": 0.0002589728251580926,
      "loss": 0.1982,
      "step": 5642
    },
    {
      "epoch": 0.024111881179657655,
      "grad_norm": 1.053909420967102,
      "learning_rate": 0.00025893009741924454,
      "loss": 0.4772,
      "step": 5643
    },
    {
      "epoch": 0.02411615406308485,
      "grad_norm": 1.6372126340866089,
      "learning_rate": 0.0002588873696803965,
      "loss": 0.3744,
      "step": 5644
    },
    {
      "epoch": 0.024120426946512046,
      "grad_norm": 0.4058555066585541,
      "learning_rate": 0.00025884464194154847,
      "loss": 0.1812,
      "step": 5645
    },
    {
      "epoch": 0.02412469982993924,
      "grad_norm": 1.62986159324646,
      "learning_rate": 0.0002588019142027004,
      "loss": 0.6913,
      "step": 5646
    },
    {
      "epoch": 0.024128972713366434,
      "grad_norm": 1.2514086961746216,
      "learning_rate": 0.00025875918646385234,
      "loss": 0.5089,
      "step": 5647
    },
    {
      "epoch": 0.02413324559679363,
      "grad_norm": 0.6745350360870361,
      "learning_rate": 0.00025871645872500425,
      "loss": 0.252,
      "step": 5648
    },
    {
      "epoch": 0.02413751848022082,
      "grad_norm": 1.0427911281585693,
      "learning_rate": 0.0002586737309861562,
      "loss": 0.4808,
      "step": 5649
    },
    {
      "epoch": 0.024141791363648017,
      "grad_norm": 0.5941592454910278,
      "learning_rate": 0.00025863100324730813,
      "loss": 0.2186,
      "step": 5650
    },
    {
      "epoch": 0.024146064247075213,
      "grad_norm": 1.196441411972046,
      "learning_rate": 0.0002585882755084601,
      "loss": 0.5717,
      "step": 5651
    },
    {
      "epoch": 0.024150337130502405,
      "grad_norm": 3.002558469772339,
      "learning_rate": 0.00025854554776961206,
      "loss": 1.1565,
      "step": 5652
    },
    {
      "epoch": 0.0241546100139296,
      "grad_norm": 1.5417531728744507,
      "learning_rate": 0.00025850282003076397,
      "loss": 0.3146,
      "step": 5653
    },
    {
      "epoch": 0.024158882897356793,
      "grad_norm": 2.2878878116607666,
      "learning_rate": 0.00025846009229191593,
      "loss": 0.8081,
      "step": 5654
    },
    {
      "epoch": 0.024163155780783988,
      "grad_norm": 1.5300381183624268,
      "learning_rate": 0.00025841736455306784,
      "loss": 0.3034,
      "step": 5655
    },
    {
      "epoch": 0.024167428664211184,
      "grad_norm": 3.809696912765503,
      "learning_rate": 0.0002583746368142198,
      "loss": 1.2496,
      "step": 5656
    },
    {
      "epoch": 0.024171701547638376,
      "grad_norm": 1.9070360660552979,
      "learning_rate": 0.0002583319090753717,
      "loss": 0.4263,
      "step": 5657
    },
    {
      "epoch": 0.02417597443106557,
      "grad_norm": 3.7577085494995117,
      "learning_rate": 0.0002582891813365237,
      "loss": 0.9608,
      "step": 5658
    },
    {
      "epoch": 0.024180247314492767,
      "grad_norm": 1.2285264730453491,
      "learning_rate": 0.00025824645359767565,
      "loss": 0.5737,
      "step": 5659
    },
    {
      "epoch": 0.02418452019791996,
      "grad_norm": 3.2627058029174805,
      "learning_rate": 0.00025820372585882756,
      "loss": 1.1047,
      "step": 5660
    },
    {
      "epoch": 0.024188793081347155,
      "grad_norm": 2.1884474754333496,
      "learning_rate": 0.0002581609981199795,
      "loss": 0.8879,
      "step": 5661
    },
    {
      "epoch": 0.02419306596477435,
      "grad_norm": 1.531501054763794,
      "learning_rate": 0.00025811827038113144,
      "loss": 0.6606,
      "step": 5662
    },
    {
      "epoch": 0.024197338848201543,
      "grad_norm": 3.988969564437866,
      "learning_rate": 0.0002580755426422834,
      "loss": 1.6024,
      "step": 5663
    },
    {
      "epoch": 0.024201611731628738,
      "grad_norm": 0.8329660892486572,
      "learning_rate": 0.0002580328149034353,
      "loss": 0.3422,
      "step": 5664
    },
    {
      "epoch": 0.024205884615055934,
      "grad_norm": 3.1671864986419678,
      "learning_rate": 0.0002579900871645873,
      "loss": 1.0046,
      "step": 5665
    },
    {
      "epoch": 0.024210157498483126,
      "grad_norm": 1.1436365842819214,
      "learning_rate": 0.0002579473594257392,
      "loss": 0.5301,
      "step": 5666
    },
    {
      "epoch": 0.02421443038191032,
      "grad_norm": 1.861580491065979,
      "learning_rate": 0.00025790463168689115,
      "loss": 0.3387,
      "step": 5667
    },
    {
      "epoch": 0.024218703265337514,
      "grad_norm": 0.9141102433204651,
      "learning_rate": 0.0002578619039480431,
      "loss": 0.2939,
      "step": 5668
    },
    {
      "epoch": 0.02422297614876471,
      "grad_norm": 1.0731202363967896,
      "learning_rate": 0.00025781917620919503,
      "loss": 0.4632,
      "step": 5669
    },
    {
      "epoch": 0.024227249032191905,
      "grad_norm": 2.1555471420288086,
      "learning_rate": 0.000257776448470347,
      "loss": 0.5163,
      "step": 5670
    },
    {
      "epoch": 0.024231521915619097,
      "grad_norm": 4.040172100067139,
      "learning_rate": 0.0002577337207314989,
      "loss": 1.0224,
      "step": 5671
    },
    {
      "epoch": 0.024235794799046292,
      "grad_norm": 2.297193765640259,
      "learning_rate": 0.0002576909929926508,
      "loss": 1.4477,
      "step": 5672
    },
    {
      "epoch": 0.024240067682473488,
      "grad_norm": 1.836106538772583,
      "learning_rate": 0.0002576482652538027,
      "loss": 0.6252,
      "step": 5673
    },
    {
      "epoch": 0.02424434056590068,
      "grad_norm": 2.4318456649780273,
      "learning_rate": 0.0002576055375149547,
      "loss": 0.779,
      "step": 5674
    },
    {
      "epoch": 0.024248613449327876,
      "grad_norm": 0.8060048818588257,
      "learning_rate": 0.00025756280977610665,
      "loss": 0.3104,
      "step": 5675
    },
    {
      "epoch": 0.02425288633275507,
      "grad_norm": 1.4674220085144043,
      "learning_rate": 0.00025752008203725857,
      "loss": 0.2827,
      "step": 5676
    },
    {
      "epoch": 0.024257159216182263,
      "grad_norm": 1.0588231086730957,
      "learning_rate": 0.00025747735429841053,
      "loss": 0.4374,
      "step": 5677
    },
    {
      "epoch": 0.02426143209960946,
      "grad_norm": 3.7432148456573486,
      "learning_rate": 0.00025743462655956244,
      "loss": 1.7363,
      "step": 5678
    },
    {
      "epoch": 0.02426570498303665,
      "grad_norm": 0.9380170702934265,
      "learning_rate": 0.0002573918988207144,
      "loss": 0.3205,
      "step": 5679
    },
    {
      "epoch": 0.024269977866463847,
      "grad_norm": 0.926790714263916,
      "learning_rate": 0.0002573491710818663,
      "loss": 0.307,
      "step": 5680
    },
    {
      "epoch": 0.024274250749891042,
      "grad_norm": 0.9230501055717468,
      "learning_rate": 0.0002573064433430183,
      "loss": 0.3345,
      "step": 5681
    },
    {
      "epoch": 0.024278523633318234,
      "grad_norm": 1.5108150243759155,
      "learning_rate": 0.00025726371560417025,
      "loss": 0.6717,
      "step": 5682
    },
    {
      "epoch": 0.02428279651674543,
      "grad_norm": 2.7975614070892334,
      "learning_rate": 0.00025722098786532216,
      "loss": 0.8423,
      "step": 5683
    },
    {
      "epoch": 0.024287069400172626,
      "grad_norm": 0.4238596558570862,
      "learning_rate": 0.0002571782601264741,
      "loss": 0.211,
      "step": 5684
    },
    {
      "epoch": 0.024291342283599818,
      "grad_norm": 0.8762937784194946,
      "learning_rate": 0.00025713553238762603,
      "loss": 0.3201,
      "step": 5685
    },
    {
      "epoch": 0.024295615167027013,
      "grad_norm": 2.967186689376831,
      "learning_rate": 0.000257092804648778,
      "loss": 0.9764,
      "step": 5686
    },
    {
      "epoch": 0.02429988805045421,
      "grad_norm": 1.50906240940094,
      "learning_rate": 0.0002570500769099299,
      "loss": 0.3559,
      "step": 5687
    },
    {
      "epoch": 0.0243041609338814,
      "grad_norm": 3.8248908519744873,
      "learning_rate": 0.0002570073491710819,
      "loss": 1.5305,
      "step": 5688
    },
    {
      "epoch": 0.024308433817308597,
      "grad_norm": 0.8265537023544312,
      "learning_rate": 0.00025696462143223384,
      "loss": 0.3069,
      "step": 5689
    },
    {
      "epoch": 0.024312706700735792,
      "grad_norm": 2.0056073665618896,
      "learning_rate": 0.00025692189369338575,
      "loss": 0.8611,
      "step": 5690
    },
    {
      "epoch": 0.024316979584162984,
      "grad_norm": 0.9489129185676575,
      "learning_rate": 0.0002568791659545377,
      "loss": 0.4002,
      "step": 5691
    },
    {
      "epoch": 0.02432125246759018,
      "grad_norm": 0.9734256863594055,
      "learning_rate": 0.0002568364382156896,
      "loss": 0.4533,
      "step": 5692
    },
    {
      "epoch": 0.024325525351017372,
      "grad_norm": 0.9400147199630737,
      "learning_rate": 0.0002567937104768416,
      "loss": 0.3391,
      "step": 5693
    },
    {
      "epoch": 0.024329798234444568,
      "grad_norm": 3.910534143447876,
      "learning_rate": 0.0002567509827379935,
      "loss": 0.8924,
      "step": 5694
    },
    {
      "epoch": 0.024334071117871763,
      "grad_norm": 1.3903461694717407,
      "learning_rate": 0.00025670825499914547,
      "loss": 0.248,
      "step": 5695
    },
    {
      "epoch": 0.024338344001298955,
      "grad_norm": 3.1448276042938232,
      "learning_rate": 0.00025666552726029743,
      "loss": 0.8415,
      "step": 5696
    },
    {
      "epoch": 0.02434261688472615,
      "grad_norm": 4.147643566131592,
      "learning_rate": 0.00025662279952144934,
      "loss": 1.3838,
      "step": 5697
    },
    {
      "epoch": 0.024346889768153347,
      "grad_norm": 1.2204426527023315,
      "learning_rate": 0.0002565800717826013,
      "loss": 0.5549,
      "step": 5698
    },
    {
      "epoch": 0.02435116265158054,
      "grad_norm": 0.5005109310150146,
      "learning_rate": 0.0002565373440437532,
      "loss": 0.2591,
      "step": 5699
    },
    {
      "epoch": 0.024355435535007734,
      "grad_norm": 0.7968816757202148,
      "learning_rate": 0.0002564946163049052,
      "loss": 0.3525,
      "step": 5700
    },
    {
      "epoch": 0.02435970841843493,
      "grad_norm": 1.3303382396697998,
      "learning_rate": 0.0002564518885660571,
      "loss": 0.9901,
      "step": 5701
    },
    {
      "epoch": 0.024363981301862122,
      "grad_norm": 2.8961634635925293,
      "learning_rate": 0.00025640916082720906,
      "loss": 0.8175,
      "step": 5702
    },
    {
      "epoch": 0.024368254185289318,
      "grad_norm": 0.6058226823806763,
      "learning_rate": 0.00025636643308836097,
      "loss": 0.3084,
      "step": 5703
    },
    {
      "epoch": 0.02437252706871651,
      "grad_norm": 1.4755513668060303,
      "learning_rate": 0.00025632370534951293,
      "loss": 0.6337,
      "step": 5704
    },
    {
      "epoch": 0.024376799952143705,
      "grad_norm": 4.16898775100708,
      "learning_rate": 0.00025628097761066484,
      "loss": 1.0334,
      "step": 5705
    },
    {
      "epoch": 0.0243810728355709,
      "grad_norm": 1.611859917640686,
      "learning_rate": 0.00025623824987181676,
      "loss": 0.7063,
      "step": 5706
    },
    {
      "epoch": 0.024385345718998093,
      "grad_norm": 4.743599891662598,
      "learning_rate": 0.0002561955221329687,
      "loss": 1.6221,
      "step": 5707
    },
    {
      "epoch": 0.02438961860242529,
      "grad_norm": 2.1680188179016113,
      "learning_rate": 0.00025615279439412063,
      "loss": 0.8023,
      "step": 5708
    },
    {
      "epoch": 0.024393891485852484,
      "grad_norm": 0.9585638046264648,
      "learning_rate": 0.0002561100666552726,
      "loss": 0.3979,
      "step": 5709
    },
    {
      "epoch": 0.024398164369279676,
      "grad_norm": 3.0703845024108887,
      "learning_rate": 0.0002560673389164245,
      "loss": 0.8518,
      "step": 5710
    },
    {
      "epoch": 0.024402437252706872,
      "grad_norm": 3.6969799995422363,
      "learning_rate": 0.00025602461117757647,
      "loss": 1.4033,
      "step": 5711
    },
    {
      "epoch": 0.024406710136134067,
      "grad_norm": 1.469791293144226,
      "learning_rate": 0.00025598188343872844,
      "loss": 0.2919,
      "step": 5712
    },
    {
      "epoch": 0.02441098301956126,
      "grad_norm": 3.0350122451782227,
      "learning_rate": 0.00025593915569988035,
      "loss": 0.6861,
      "step": 5713
    },
    {
      "epoch": 0.024415255902988455,
      "grad_norm": 2.1196446418762207,
      "learning_rate": 0.0002558964279610323,
      "loss": 0.6662,
      "step": 5714
    },
    {
      "epoch": 0.02441952878641565,
      "grad_norm": 3.594531536102295,
      "learning_rate": 0.0002558537002221842,
      "loss": 1.2786,
      "step": 5715
    },
    {
      "epoch": 0.024423801669842843,
      "grad_norm": 1.2200127840042114,
      "learning_rate": 0.0002558109724833362,
      "loss": 0.4593,
      "step": 5716
    },
    {
      "epoch": 0.02442807455327004,
      "grad_norm": 1.0018848180770874,
      "learning_rate": 0.0002557682447444881,
      "loss": 0.4217,
      "step": 5717
    },
    {
      "epoch": 0.02443234743669723,
      "grad_norm": 1.3791332244873047,
      "learning_rate": 0.00025572551700564006,
      "loss": 0.2321,
      "step": 5718
    },
    {
      "epoch": 0.024436620320124426,
      "grad_norm": 2.112802743911743,
      "learning_rate": 0.00025568278926679203,
      "loss": 1.4033,
      "step": 5719
    },
    {
      "epoch": 0.02444089320355162,
      "grad_norm": 0.5923151969909668,
      "learning_rate": 0.00025564006152794394,
      "loss": 0.253,
      "step": 5720
    },
    {
      "epoch": 0.024445166086978814,
      "grad_norm": 3.791551351547241,
      "learning_rate": 0.0002555973337890959,
      "loss": 1.3372,
      "step": 5721
    },
    {
      "epoch": 0.02444943897040601,
      "grad_norm": 0.6128461360931396,
      "learning_rate": 0.0002555546060502478,
      "loss": 0.2691,
      "step": 5722
    },
    {
      "epoch": 0.024453711853833205,
      "grad_norm": 1.0915050506591797,
      "learning_rate": 0.0002555118783113998,
      "loss": 0.3814,
      "step": 5723
    },
    {
      "epoch": 0.024457984737260397,
      "grad_norm": 0.6087164282798767,
      "learning_rate": 0.0002554691505725517,
      "loss": 0.253,
      "step": 5724
    },
    {
      "epoch": 0.024462257620687593,
      "grad_norm": 1.2455044984817505,
      "learning_rate": 0.00025542642283370366,
      "loss": 0.8918,
      "step": 5725
    },
    {
      "epoch": 0.02446653050411479,
      "grad_norm": 1.6512150764465332,
      "learning_rate": 0.0002553836950948556,
      "loss": 0.6515,
      "step": 5726
    },
    {
      "epoch": 0.02447080338754198,
      "grad_norm": 0.5082964897155762,
      "learning_rate": 0.00025534096735600753,
      "loss": 0.225,
      "step": 5727
    },
    {
      "epoch": 0.024475076270969176,
      "grad_norm": 4.050129413604736,
      "learning_rate": 0.0002552982396171595,
      "loss": 1.0872,
      "step": 5728
    },
    {
      "epoch": 0.024479349154396368,
      "grad_norm": 2.168213367462158,
      "learning_rate": 0.0002552555118783114,
      "loss": 0.7371,
      "step": 5729
    },
    {
      "epoch": 0.024483622037823564,
      "grad_norm": 1.5217581987380981,
      "learning_rate": 0.00025521278413946337,
      "loss": 0.5351,
      "step": 5730
    },
    {
      "epoch": 0.02448789492125076,
      "grad_norm": 3.2830536365509033,
      "learning_rate": 0.0002551700564006153,
      "loss": 1.6258,
      "step": 5731
    },
    {
      "epoch": 0.02449216780467795,
      "grad_norm": 2.213714122772217,
      "learning_rate": 0.00025512732866176725,
      "loss": 0.669,
      "step": 5732
    },
    {
      "epoch": 0.024496440688105147,
      "grad_norm": 3.352766513824463,
      "learning_rate": 0.0002550846009229192,
      "loss": 1.0321,
      "step": 5733
    },
    {
      "epoch": 0.024500713571532343,
      "grad_norm": 3.472071886062622,
      "learning_rate": 0.0002550418731840711,
      "loss": 1.0981,
      "step": 5734
    },
    {
      "epoch": 0.024504986454959535,
      "grad_norm": 1.4907751083374023,
      "learning_rate": 0.0002549991454452231,
      "loss": 0.5535,
      "step": 5735
    },
    {
      "epoch": 0.02450925933838673,
      "grad_norm": 3.301354169845581,
      "learning_rate": 0.000254956417706375,
      "loss": 0.7073,
      "step": 5736
    },
    {
      "epoch": 0.024513532221813926,
      "grad_norm": 1.2500927448272705,
      "learning_rate": 0.0002549136899675269,
      "loss": 0.8486,
      "step": 5737
    },
    {
      "epoch": 0.024517805105241118,
      "grad_norm": 0.9068412780761719,
      "learning_rate": 0.0002548709622286788,
      "loss": 0.2898,
      "step": 5738
    },
    {
      "epoch": 0.024522077988668314,
      "grad_norm": 1.2634923458099365,
      "learning_rate": 0.0002548282344898308,
      "loss": 0.8068,
      "step": 5739
    },
    {
      "epoch": 0.024526350872095506,
      "grad_norm": 2.5335521697998047,
      "learning_rate": 0.0002547855067509827,
      "loss": 0.892,
      "step": 5740
    },
    {
      "epoch": 0.0245306237555227,
      "grad_norm": 0.7689434289932251,
      "learning_rate": 0.00025474277901213466,
      "loss": 0.2785,
      "step": 5741
    },
    {
      "epoch": 0.024534896638949897,
      "grad_norm": 3.48492693901062,
      "learning_rate": 0.0002547000512732866,
      "loss": 1.2654,
      "step": 5742
    },
    {
      "epoch": 0.02453916952237709,
      "grad_norm": 2.1706056594848633,
      "learning_rate": 0.00025465732353443854,
      "loss": 0.5706,
      "step": 5743
    },
    {
      "epoch": 0.024543442405804285,
      "grad_norm": 1.3596855401992798,
      "learning_rate": 0.0002546145957955905,
      "loss": 0.7361,
      "step": 5744
    },
    {
      "epoch": 0.02454771528923148,
      "grad_norm": 2.9559831619262695,
      "learning_rate": 0.0002545718680567424,
      "loss": 0.8892,
      "step": 5745
    },
    {
      "epoch": 0.024551988172658672,
      "grad_norm": 1.5874131917953491,
      "learning_rate": 0.0002545291403178944,
      "loss": 0.7671,
      "step": 5746
    },
    {
      "epoch": 0.024556261056085868,
      "grad_norm": 2.3423070907592773,
      "learning_rate": 0.0002544864125790463,
      "loss": 0.8943,
      "step": 5747
    },
    {
      "epoch": 0.024560533939513064,
      "grad_norm": 1.5612289905548096,
      "learning_rate": 0.00025444368484019825,
      "loss": 0.4261,
      "step": 5748
    },
    {
      "epoch": 0.024564806822940256,
      "grad_norm": 1.9853843450546265,
      "learning_rate": 0.0002544009571013502,
      "loss": 0.4922,
      "step": 5749
    },
    {
      "epoch": 0.02456907970636745,
      "grad_norm": 0.7254699468612671,
      "learning_rate": 0.00025435822936250213,
      "loss": 0.2165,
      "step": 5750
    },
    {
      "epoch": 0.024573352589794647,
      "grad_norm": 1.8251116275787354,
      "learning_rate": 0.0002543155016236541,
      "loss": 0.5379,
      "step": 5751
    },
    {
      "epoch": 0.02457762547322184,
      "grad_norm": 4.637326717376709,
      "learning_rate": 0.000254272773884806,
      "loss": 1.7495,
      "step": 5752
    },
    {
      "epoch": 0.024581898356649035,
      "grad_norm": 4.548403739929199,
      "learning_rate": 0.00025423004614595797,
      "loss": 1.2984,
      "step": 5753
    },
    {
      "epoch": 0.024586171240076227,
      "grad_norm": 2.077363967895508,
      "learning_rate": 0.0002541873184071099,
      "loss": 0.8859,
      "step": 5754
    },
    {
      "epoch": 0.024590444123503422,
      "grad_norm": 1.3196322917938232,
      "learning_rate": 0.00025414459066826184,
      "loss": 0.4267,
      "step": 5755
    },
    {
      "epoch": 0.024594717006930618,
      "grad_norm": 2.454439878463745,
      "learning_rate": 0.0002541018629294138,
      "loss": 0.7981,
      "step": 5756
    },
    {
      "epoch": 0.02459898989035781,
      "grad_norm": 2.952979326248169,
      "learning_rate": 0.0002540591351905657,
      "loss": 0.9934,
      "step": 5757
    },
    {
      "epoch": 0.024603262773785006,
      "grad_norm": 2.98244309425354,
      "learning_rate": 0.0002540164074517177,
      "loss": 0.5586,
      "step": 5758
    },
    {
      "epoch": 0.0246075356572122,
      "grad_norm": 1.584144949913025,
      "learning_rate": 0.0002539736797128696,
      "loss": 0.7213,
      "step": 5759
    },
    {
      "epoch": 0.024611808540639393,
      "grad_norm": 1.2533307075500488,
      "learning_rate": 0.00025393095197402156,
      "loss": 0.5935,
      "step": 5760
    },
    {
      "epoch": 0.02461608142406659,
      "grad_norm": 1.8569432497024536,
      "learning_rate": 0.00025388822423517347,
      "loss": 1.3286,
      "step": 5761
    },
    {
      "epoch": 0.024620354307493784,
      "grad_norm": 1.494287133216858,
      "learning_rate": 0.00025384549649632544,
      "loss": 0.5788,
      "step": 5762
    },
    {
      "epoch": 0.024624627190920977,
      "grad_norm": 1.3925470113754272,
      "learning_rate": 0.0002538027687574774,
      "loss": 0.266,
      "step": 5763
    },
    {
      "epoch": 0.024628900074348172,
      "grad_norm": 0.933839738368988,
      "learning_rate": 0.0002537600410186293,
      "loss": 0.3793,
      "step": 5764
    },
    {
      "epoch": 0.024633172957775364,
      "grad_norm": 0.8472298383712769,
      "learning_rate": 0.0002537173132797813,
      "loss": 0.3383,
      "step": 5765
    },
    {
      "epoch": 0.02463744584120256,
      "grad_norm": 2.219979763031006,
      "learning_rate": 0.0002536745855409332,
      "loss": 0.7102,
      "step": 5766
    },
    {
      "epoch": 0.024641718724629755,
      "grad_norm": 1.7901341915130615,
      "learning_rate": 0.00025363185780208515,
      "loss": 1.2146,
      "step": 5767
    },
    {
      "epoch": 0.024645991608056948,
      "grad_norm": 4.5691728591918945,
      "learning_rate": 0.00025358913006323706,
      "loss": 1.2047,
      "step": 5768
    },
    {
      "epoch": 0.024650264491484143,
      "grad_norm": 0.6164869070053101,
      "learning_rate": 0.00025354640232438903,
      "loss": 0.2639,
      "step": 5769
    },
    {
      "epoch": 0.02465453737491134,
      "grad_norm": 2.1350173950195312,
      "learning_rate": 0.0002535036745855409,
      "loss": 0.707,
      "step": 5770
    },
    {
      "epoch": 0.02465881025833853,
      "grad_norm": 0.3964318037033081,
      "learning_rate": 0.00025346094684669285,
      "loss": 0.1472,
      "step": 5771
    },
    {
      "epoch": 0.024663083141765726,
      "grad_norm": 1.3770006895065308,
      "learning_rate": 0.0002534182191078448,
      "loss": 0.238,
      "step": 5772
    },
    {
      "epoch": 0.024667356025192922,
      "grad_norm": 2.375185966491699,
      "learning_rate": 0.0002533754913689967,
      "loss": 0.8554,
      "step": 5773
    },
    {
      "epoch": 0.024671628908620114,
      "grad_norm": 2.8896501064300537,
      "learning_rate": 0.0002533327636301487,
      "loss": 0.7986,
      "step": 5774
    },
    {
      "epoch": 0.02467590179204731,
      "grad_norm": 3.4052536487579346,
      "learning_rate": 0.0002532900358913006,
      "loss": 1.1078,
      "step": 5775
    },
    {
      "epoch": 0.024680174675474505,
      "grad_norm": 1.5213589668273926,
      "learning_rate": 0.00025324730815245257,
      "loss": 1.1098,
      "step": 5776
    },
    {
      "epoch": 0.024684447558901697,
      "grad_norm": 3.95367169380188,
      "learning_rate": 0.0002532045804136045,
      "loss": 1.0314,
      "step": 5777
    },
    {
      "epoch": 0.024688720442328893,
      "grad_norm": 1.5670862197875977,
      "learning_rate": 0.00025316185267475644,
      "loss": 0.6098,
      "step": 5778
    },
    {
      "epoch": 0.024692993325756085,
      "grad_norm": 2.302129030227661,
      "learning_rate": 0.0002531191249359084,
      "loss": 0.7962,
      "step": 5779
    },
    {
      "epoch": 0.02469726620918328,
      "grad_norm": 0.5395068526268005,
      "learning_rate": 0.0002530763971970603,
      "loss": 0.2099,
      "step": 5780
    },
    {
      "epoch": 0.024701539092610476,
      "grad_norm": 1.9818531274795532,
      "learning_rate": 0.0002530336694582123,
      "loss": 0.4205,
      "step": 5781
    },
    {
      "epoch": 0.02470581197603767,
      "grad_norm": 2.081930160522461,
      "learning_rate": 0.0002529909417193642,
      "loss": 0.5793,
      "step": 5782
    },
    {
      "epoch": 0.024710084859464864,
      "grad_norm": 1.2700375318527222,
      "learning_rate": 0.00025294821398051616,
      "loss": 0.6264,
      "step": 5783
    },
    {
      "epoch": 0.02471435774289206,
      "grad_norm": 1.6705325841903687,
      "learning_rate": 0.00025290548624166807,
      "loss": 0.6264,
      "step": 5784
    },
    {
      "epoch": 0.024718630626319252,
      "grad_norm": 2.208204746246338,
      "learning_rate": 0.00025286275850282003,
      "loss": 0.5585,
      "step": 5785
    },
    {
      "epoch": 0.024722903509746447,
      "grad_norm": 2.737229347229004,
      "learning_rate": 0.000252820030763972,
      "loss": 0.7435,
      "step": 5786
    },
    {
      "epoch": 0.024727176393173643,
      "grad_norm": 2.0324313640594482,
      "learning_rate": 0.0002527773030251239,
      "loss": 0.6582,
      "step": 5787
    },
    {
      "epoch": 0.024731449276600835,
      "grad_norm": 1.7963546514511108,
      "learning_rate": 0.0002527345752862759,
      "loss": 0.5838,
      "step": 5788
    },
    {
      "epoch": 0.02473572216002803,
      "grad_norm": 3.0437164306640625,
      "learning_rate": 0.0002526918475474278,
      "loss": 0.8703,
      "step": 5789
    },
    {
      "epoch": 0.024739995043455223,
      "grad_norm": 4.541996479034424,
      "learning_rate": 0.00025264911980857975,
      "loss": 1.0898,
      "step": 5790
    },
    {
      "epoch": 0.02474426792688242,
      "grad_norm": 1.8304239511489868,
      "learning_rate": 0.00025260639206973166,
      "loss": 0.3371,
      "step": 5791
    },
    {
      "epoch": 0.024748540810309614,
      "grad_norm": 1.8437283039093018,
      "learning_rate": 0.0002525636643308836,
      "loss": 0.4668,
      "step": 5792
    },
    {
      "epoch": 0.024752813693736806,
      "grad_norm": 1.689372181892395,
      "learning_rate": 0.0002525209365920356,
      "loss": 0.5214,
      "step": 5793
    },
    {
      "epoch": 0.024757086577164,
      "grad_norm": 3.0237033367156982,
      "learning_rate": 0.0002524782088531875,
      "loss": 1.107,
      "step": 5794
    },
    {
      "epoch": 0.024761359460591197,
      "grad_norm": 1.3934460878372192,
      "learning_rate": 0.00025243548111433947,
      "loss": 0.6737,
      "step": 5795
    },
    {
      "epoch": 0.02476563234401839,
      "grad_norm": 2.1120853424072266,
      "learning_rate": 0.0002523927533754914,
      "loss": 0.6398,
      "step": 5796
    },
    {
      "epoch": 0.024769905227445585,
      "grad_norm": 1.726539969444275,
      "learning_rate": 0.00025235002563664334,
      "loss": 0.5601,
      "step": 5797
    },
    {
      "epoch": 0.02477417811087278,
      "grad_norm": 1.4810795783996582,
      "learning_rate": 0.00025230729789779525,
      "loss": 0.52,
      "step": 5798
    },
    {
      "epoch": 0.024778450994299973,
      "grad_norm": 5.244105815887451,
      "learning_rate": 0.0002522645701589472,
      "loss": 1.5288,
      "step": 5799
    },
    {
      "epoch": 0.02478272387772717,
      "grad_norm": 2.951582670211792,
      "learning_rate": 0.0002522218424200992,
      "loss": 1.6387,
      "step": 5800
    },
    {
      "epoch": 0.024786996761154364,
      "grad_norm": 4.352843761444092,
      "learning_rate": 0.0002521791146812511,
      "loss": 1.4942,
      "step": 5801
    },
    {
      "epoch": 0.024791269644581556,
      "grad_norm": 3.1265499591827393,
      "learning_rate": 0.00025213638694240306,
      "loss": 0.9405,
      "step": 5802
    },
    {
      "epoch": 0.02479554252800875,
      "grad_norm": 1.726423740386963,
      "learning_rate": 0.0002520936592035549,
      "loss": 0.7314,
      "step": 5803
    },
    {
      "epoch": 0.024799815411435944,
      "grad_norm": 2.414783000946045,
      "learning_rate": 0.0002520509314647069,
      "loss": 0.6405,
      "step": 5804
    },
    {
      "epoch": 0.02480408829486314,
      "grad_norm": 1.1942341327667236,
      "learning_rate": 0.0002520082037258588,
      "loss": 0.5915,
      "step": 5805
    },
    {
      "epoch": 0.024808361178290335,
      "grad_norm": 2.6869776248931885,
      "learning_rate": 0.00025196547598701076,
      "loss": 0.6965,
      "step": 5806
    },
    {
      "epoch": 0.024812634061717527,
      "grad_norm": 1.6529653072357178,
      "learning_rate": 0.00025192274824816267,
      "loss": 0.3971,
      "step": 5807
    },
    {
      "epoch": 0.024816906945144723,
      "grad_norm": 0.604165256023407,
      "learning_rate": 0.00025188002050931463,
      "loss": 0.2397,
      "step": 5808
    },
    {
      "epoch": 0.024821179828571918,
      "grad_norm": 3.4455199241638184,
      "learning_rate": 0.0002518372927704666,
      "loss": 2.4196,
      "step": 5809
    },
    {
      "epoch": 0.02482545271199911,
      "grad_norm": 0.8014400601387024,
      "learning_rate": 0.0002517945650316185,
      "loss": 0.3457,
      "step": 5810
    },
    {
      "epoch": 0.024829725595426306,
      "grad_norm": 1.7602696418762207,
      "learning_rate": 0.00025175183729277047,
      "loss": 0.5553,
      "step": 5811
    },
    {
      "epoch": 0.0248339984788535,
      "grad_norm": 4.4750847816467285,
      "learning_rate": 0.0002517091095539224,
      "loss": 1.1687,
      "step": 5812
    },
    {
      "epoch": 0.024838271362280694,
      "grad_norm": 1.343565583229065,
      "learning_rate": 0.00025166638181507435,
      "loss": 0.4999,
      "step": 5813
    },
    {
      "epoch": 0.02484254424570789,
      "grad_norm": 1.85641610622406,
      "learning_rate": 0.00025162365407622626,
      "loss": 0.5228,
      "step": 5814
    },
    {
      "epoch": 0.02484681712913508,
      "grad_norm": 2.7540090084075928,
      "learning_rate": 0.0002515809263373782,
      "loss": 1.5117,
      "step": 5815
    },
    {
      "epoch": 0.024851090012562277,
      "grad_norm": 2.538860559463501,
      "learning_rate": 0.0002515381985985302,
      "loss": 0.5801,
      "step": 5816
    },
    {
      "epoch": 0.024855362895989472,
      "grad_norm": 3.115105390548706,
      "learning_rate": 0.0002514954708596821,
      "loss": 1.3046,
      "step": 5817
    },
    {
      "epoch": 0.024859635779416665,
      "grad_norm": 0.8010432720184326,
      "learning_rate": 0.00025145274312083406,
      "loss": 0.3215,
      "step": 5818
    },
    {
      "epoch": 0.02486390866284386,
      "grad_norm": 1.581780195236206,
      "learning_rate": 0.000251410015381986,
      "loss": 0.4691,
      "step": 5819
    },
    {
      "epoch": 0.024868181546271056,
      "grad_norm": 0.851111888885498,
      "learning_rate": 0.00025136728764313794,
      "loss": 0.3062,
      "step": 5820
    },
    {
      "epoch": 0.024872454429698248,
      "grad_norm": 0.6278716325759888,
      "learning_rate": 0.00025132455990428985,
      "loss": 0.2547,
      "step": 5821
    },
    {
      "epoch": 0.024876727313125443,
      "grad_norm": 2.0326056480407715,
      "learning_rate": 0.0002512818321654418,
      "loss": 0.8306,
      "step": 5822
    },
    {
      "epoch": 0.02488100019655264,
      "grad_norm": 1.5370776653289795,
      "learning_rate": 0.0002512391044265938,
      "loss": 0.7248,
      "step": 5823
    },
    {
      "epoch": 0.02488527307997983,
      "grad_norm": 1.909440517425537,
      "learning_rate": 0.0002511963766877457,
      "loss": 0.4415,
      "step": 5824
    },
    {
      "epoch": 0.024889545963407027,
      "grad_norm": 3.7454991340637207,
      "learning_rate": 0.00025115364894889766,
      "loss": 1.9415,
      "step": 5825
    },
    {
      "epoch": 0.024893818846834222,
      "grad_norm": 1.6305147409439087,
      "learning_rate": 0.00025111092121004957,
      "loss": 0.3456,
      "step": 5826
    },
    {
      "epoch": 0.024898091730261415,
      "grad_norm": 4.818638324737549,
      "learning_rate": 0.00025106819347120153,
      "loss": 1.3063,
      "step": 5827
    },
    {
      "epoch": 0.02490236461368861,
      "grad_norm": 3.4647960662841797,
      "learning_rate": 0.00025102546573235344,
      "loss": 0.7898,
      "step": 5828
    },
    {
      "epoch": 0.024906637497115802,
      "grad_norm": 2.6713054180145264,
      "learning_rate": 0.0002509827379935054,
      "loss": 1.0407,
      "step": 5829
    },
    {
      "epoch": 0.024910910380542998,
      "grad_norm": 1.6829432249069214,
      "learning_rate": 0.00025094001025465737,
      "loss": 0.8543,
      "step": 5830
    },
    {
      "epoch": 0.024915183263970193,
      "grad_norm": 1.6453638076782227,
      "learning_rate": 0.0002508972825158093,
      "loss": 0.3895,
      "step": 5831
    },
    {
      "epoch": 0.024919456147397386,
      "grad_norm": 1.0163618326187134,
      "learning_rate": 0.00025085455477696125,
      "loss": 0.3906,
      "step": 5832
    },
    {
      "epoch": 0.02492372903082458,
      "grad_norm": 0.9108455777168274,
      "learning_rate": 0.00025081182703811316,
      "loss": 0.5998,
      "step": 5833
    },
    {
      "epoch": 0.024928001914251777,
      "grad_norm": 2.15218186378479,
      "learning_rate": 0.0002507690992992651,
      "loss": 0.5696,
      "step": 5834
    },
    {
      "epoch": 0.02493227479767897,
      "grad_norm": 0.6328805685043335,
      "learning_rate": 0.00025072637156041703,
      "loss": 0.2438,
      "step": 5835
    },
    {
      "epoch": 0.024936547681106164,
      "grad_norm": 0.5187380313873291,
      "learning_rate": 0.00025068364382156895,
      "loss": 0.2546,
      "step": 5836
    },
    {
      "epoch": 0.02494082056453336,
      "grad_norm": 1.6182886362075806,
      "learning_rate": 0.00025064091608272086,
      "loss": 0.3361,
      "step": 5837
    },
    {
      "epoch": 0.024945093447960552,
      "grad_norm": 1.0122768878936768,
      "learning_rate": 0.0002505981883438728,
      "loss": 0.3777,
      "step": 5838
    },
    {
      "epoch": 0.024949366331387748,
      "grad_norm": 5.206020832061768,
      "learning_rate": 0.0002505554606050248,
      "loss": 1.5878,
      "step": 5839
    },
    {
      "epoch": 0.02495363921481494,
      "grad_norm": 1.834710717201233,
      "learning_rate": 0.0002505127328661767,
      "loss": 0.7073,
      "step": 5840
    },
    {
      "epoch": 0.024957912098242135,
      "grad_norm": 0.5327821969985962,
      "learning_rate": 0.00025047000512732866,
      "loss": 0.2674,
      "step": 5841
    },
    {
      "epoch": 0.02496218498166933,
      "grad_norm": 0.7614578008651733,
      "learning_rate": 0.00025042727738848057,
      "loss": 0.306,
      "step": 5842
    },
    {
      "epoch": 0.024966457865096523,
      "grad_norm": 1.5868347883224487,
      "learning_rate": 0.00025038454964963254,
      "loss": 0.8249,
      "step": 5843
    },
    {
      "epoch": 0.02497073074852372,
      "grad_norm": 1.2326220273971558,
      "learning_rate": 0.00025034182191078445,
      "loss": 0.4177,
      "step": 5844
    },
    {
      "epoch": 0.024975003631950914,
      "grad_norm": 1.3624259233474731,
      "learning_rate": 0.0002502990941719364,
      "loss": 0.5088,
      "step": 5845
    },
    {
      "epoch": 0.024979276515378106,
      "grad_norm": 0.49110573530197144,
      "learning_rate": 0.0002502563664330884,
      "loss": 0.2095,
      "step": 5846
    },
    {
      "epoch": 0.024983549398805302,
      "grad_norm": 0.8911148905754089,
      "learning_rate": 0.0002502136386942403,
      "loss": 0.5453,
      "step": 5847
    },
    {
      "epoch": 0.024987822282232498,
      "grad_norm": 0.6534951329231262,
      "learning_rate": 0.00025017091095539225,
      "loss": 0.3241,
      "step": 5848
    },
    {
      "epoch": 0.02499209516565969,
      "grad_norm": 1.8292919397354126,
      "learning_rate": 0.00025012818321654416,
      "loss": 0.6034,
      "step": 5849
    },
    {
      "epoch": 0.024996368049086885,
      "grad_norm": 0.9069421887397766,
      "learning_rate": 0.00025008545547769613,
      "loss": 0.3194,
      "step": 5850
    },
    {
      "epoch": 0.025000640932514077,
      "grad_norm": 1.2905770540237427,
      "learning_rate": 0.00025004272773884804,
      "loss": 0.4746,
      "step": 5851
    },
    {
      "epoch": 0.025004913815941273,
      "grad_norm": 2.9563181400299072,
      "learning_rate": 0.00025,
      "loss": 0.6256,
      "step": 5852
    },
    {
      "epoch": 0.02500918669936847,
      "grad_norm": 1.1816120147705078,
      "learning_rate": 0.00024995727226115197,
      "loss": 0.4172,
      "step": 5853
    },
    {
      "epoch": 0.02501345958279566,
      "grad_norm": 1.4803780317306519,
      "learning_rate": 0.0002499145445223039,
      "loss": 0.4051,
      "step": 5854
    },
    {
      "epoch": 0.025017732466222856,
      "grad_norm": 1.7682429552078247,
      "learning_rate": 0.00024987181678345585,
      "loss": 0.5148,
      "step": 5855
    },
    {
      "epoch": 0.025022005349650052,
      "grad_norm": 1.1497231721878052,
      "learning_rate": 0.00024982908904460776,
      "loss": 0.4701,
      "step": 5856
    },
    {
      "epoch": 0.025026278233077244,
      "grad_norm": 0.8559538125991821,
      "learning_rate": 0.0002497863613057597,
      "loss": 0.3709,
      "step": 5857
    },
    {
      "epoch": 0.02503055111650444,
      "grad_norm": 6.040380477905273,
      "learning_rate": 0.00024974363356691163,
      "loss": 1.0262,
      "step": 5858
    },
    {
      "epoch": 0.025034823999931635,
      "grad_norm": 0.6443814039230347,
      "learning_rate": 0.0002497009058280636,
      "loss": 0.2428,
      "step": 5859
    },
    {
      "epoch": 0.025039096883358827,
      "grad_norm": 1.143947720527649,
      "learning_rate": 0.00024965817808921556,
      "loss": 0.3763,
      "step": 5860
    },
    {
      "epoch": 0.025043369766786023,
      "grad_norm": 1.6830730438232422,
      "learning_rate": 0.00024961545035036747,
      "loss": 0.4071,
      "step": 5861
    },
    {
      "epoch": 0.02504764265021322,
      "grad_norm": 5.789454460144043,
      "learning_rate": 0.0002495727226115194,
      "loss": 1.5646,
      "step": 5862
    },
    {
      "epoch": 0.02505191553364041,
      "grad_norm": 1.2988874912261963,
      "learning_rate": 0.00024952999487267135,
      "loss": 0.5297,
      "step": 5863
    },
    {
      "epoch": 0.025056188417067606,
      "grad_norm": 1.0854673385620117,
      "learning_rate": 0.00024948726713382326,
      "loss": 0.3391,
      "step": 5864
    },
    {
      "epoch": 0.0250604613004948,
      "grad_norm": 0.8414503335952759,
      "learning_rate": 0.0002494445393949752,
      "loss": 0.2766,
      "step": 5865
    },
    {
      "epoch": 0.025064734183921994,
      "grad_norm": 1.0657789707183838,
      "learning_rate": 0.00024940181165612713,
      "loss": 0.3996,
      "step": 5866
    },
    {
      "epoch": 0.02506900706734919,
      "grad_norm": 8.913735389709473,
      "learning_rate": 0.0002493590839172791,
      "loss": 0.5884,
      "step": 5867
    },
    {
      "epoch": 0.02507327995077638,
      "grad_norm": 6.042547225952148,
      "learning_rate": 0.00024931635617843106,
      "loss": 1.6775,
      "step": 5868
    },
    {
      "epoch": 0.025077552834203577,
      "grad_norm": 0.6835028529167175,
      "learning_rate": 0.000249273628439583,
      "loss": 0.2291,
      "step": 5869
    },
    {
      "epoch": 0.025081825717630773,
      "grad_norm": 0.9492341876029968,
      "learning_rate": 0.00024923090070073494,
      "loss": 0.5113,
      "step": 5870
    },
    {
      "epoch": 0.025086098601057965,
      "grad_norm": 1.395983338356018,
      "learning_rate": 0.00024918817296188685,
      "loss": 0.4041,
      "step": 5871
    },
    {
      "epoch": 0.02509037148448516,
      "grad_norm": 0.9652494192123413,
      "learning_rate": 0.0002491454452230388,
      "loss": 0.5062,
      "step": 5872
    },
    {
      "epoch": 0.025094644367912356,
      "grad_norm": 1.5745537281036377,
      "learning_rate": 0.0002491027174841907,
      "loss": 0.783,
      "step": 5873
    },
    {
      "epoch": 0.025098917251339548,
      "grad_norm": 3.620955228805542,
      "learning_rate": 0.0002490599897453427,
      "loss": 1.0242,
      "step": 5874
    },
    {
      "epoch": 0.025103190134766744,
      "grad_norm": 1.311843991279602,
      "learning_rate": 0.00024901726200649466,
      "loss": 0.3636,
      "step": 5875
    },
    {
      "epoch": 0.025107463018193936,
      "grad_norm": 4.414367198944092,
      "learning_rate": 0.00024897453426764657,
      "loss": 1.1803,
      "step": 5876
    },
    {
      "epoch": 0.02511173590162113,
      "grad_norm": 1.6962515115737915,
      "learning_rate": 0.0002489318065287985,
      "loss": 0.5337,
      "step": 5877
    },
    {
      "epoch": 0.025116008785048327,
      "grad_norm": 1.0634961128234863,
      "learning_rate": 0.00024888907878995044,
      "loss": 0.2711,
      "step": 5878
    },
    {
      "epoch": 0.02512028166847552,
      "grad_norm": 3.2404701709747314,
      "learning_rate": 0.00024884635105110235,
      "loss": 1.114,
      "step": 5879
    },
    {
      "epoch": 0.025124554551902715,
      "grad_norm": 0.7257876396179199,
      "learning_rate": 0.0002488036233122543,
      "loss": 0.343,
      "step": 5880
    },
    {
      "epoch": 0.02512882743532991,
      "grad_norm": 1.4017934799194336,
      "learning_rate": 0.00024876089557340623,
      "loss": 1.2058,
      "step": 5881
    },
    {
      "epoch": 0.025133100318757103,
      "grad_norm": 4.993408203125,
      "learning_rate": 0.0002487181678345582,
      "loss": 1.2899,
      "step": 5882
    },
    {
      "epoch": 0.025137373202184298,
      "grad_norm": 4.556079387664795,
      "learning_rate": 0.00024867544009571016,
      "loss": 1.0521,
      "step": 5883
    },
    {
      "epoch": 0.025141646085611494,
      "grad_norm": 1.3090393543243408,
      "learning_rate": 0.00024863271235686207,
      "loss": 0.5495,
      "step": 5884
    },
    {
      "epoch": 0.025145918969038686,
      "grad_norm": 1.4017001390457153,
      "learning_rate": 0.00024858998461801404,
      "loss": 0.3689,
      "step": 5885
    },
    {
      "epoch": 0.02515019185246588,
      "grad_norm": 3.4792683124542236,
      "learning_rate": 0.00024854725687916595,
      "loss": 1.1389,
      "step": 5886
    },
    {
      "epoch": 0.025154464735893077,
      "grad_norm": 1.3806852102279663,
      "learning_rate": 0.0002485045291403179,
      "loss": 1.2106,
      "step": 5887
    },
    {
      "epoch": 0.02515873761932027,
      "grad_norm": 2.5857224464416504,
      "learning_rate": 0.0002484618014014698,
      "loss": 0.5961,
      "step": 5888
    },
    {
      "epoch": 0.025163010502747465,
      "grad_norm": 1.4323656558990479,
      "learning_rate": 0.0002484190736626218,
      "loss": 0.8624,
      "step": 5889
    },
    {
      "epoch": 0.025167283386174657,
      "grad_norm": 2.273104429244995,
      "learning_rate": 0.00024837634592377375,
      "loss": 0.7363,
      "step": 5890
    },
    {
      "epoch": 0.025171556269601852,
      "grad_norm": 0.9153556227684021,
      "learning_rate": 0.00024833361818492566,
      "loss": 0.3163,
      "step": 5891
    },
    {
      "epoch": 0.025175829153029048,
      "grad_norm": 2.211820602416992,
      "learning_rate": 0.00024829089044607763,
      "loss": 0.7257,
      "step": 5892
    },
    {
      "epoch": 0.02518010203645624,
      "grad_norm": 0.7538847327232361,
      "learning_rate": 0.00024824816270722954,
      "loss": 0.2651,
      "step": 5893
    },
    {
      "epoch": 0.025184374919883436,
      "grad_norm": 1.3185697793960571,
      "learning_rate": 0.00024820543496838145,
      "loss": 1.1023,
      "step": 5894
    },
    {
      "epoch": 0.02518864780331063,
      "grad_norm": 1.5142369270324707,
      "learning_rate": 0.0002481627072295334,
      "loss": 0.467,
      "step": 5895
    },
    {
      "epoch": 0.025192920686737823,
      "grad_norm": 1.994253396987915,
      "learning_rate": 0.0002481199794906853,
      "loss": 0.7335,
      "step": 5896
    },
    {
      "epoch": 0.02519719357016502,
      "grad_norm": 0.9924013614654541,
      "learning_rate": 0.0002480772517518373,
      "loss": 0.4999,
      "step": 5897
    },
    {
      "epoch": 0.025201466453592215,
      "grad_norm": 2.653810739517212,
      "learning_rate": 0.00024803452401298925,
      "loss": 1.0035,
      "step": 5898
    },
    {
      "epoch": 0.025205739337019407,
      "grad_norm": 1.5296586751937866,
      "learning_rate": 0.00024799179627414117,
      "loss": 0.3881,
      "step": 5899
    },
    {
      "epoch": 0.025210012220446602,
      "grad_norm": 2.718845844268799,
      "learning_rate": 0.00024794906853529313,
      "loss": 0.8635,
      "step": 5900
    },
    {
      "epoch": 0.025214285103873794,
      "grad_norm": 1.7847375869750977,
      "learning_rate": 0.00024790634079644504,
      "loss": 0.7108,
      "step": 5901
    },
    {
      "epoch": 0.02521855798730099,
      "grad_norm": 2.769705057144165,
      "learning_rate": 0.000247863613057597,
      "loss": 0.819,
      "step": 5902
    },
    {
      "epoch": 0.025222830870728186,
      "grad_norm": 2.0518863201141357,
      "learning_rate": 0.0002478208853187489,
      "loss": 0.8186,
      "step": 5903
    },
    {
      "epoch": 0.025227103754155378,
      "grad_norm": 2.970536470413208,
      "learning_rate": 0.0002477781575799009,
      "loss": 1.5515,
      "step": 5904
    },
    {
      "epoch": 0.025231376637582573,
      "grad_norm": 0.6371331810951233,
      "learning_rate": 0.00024773542984105285,
      "loss": 0.3704,
      "step": 5905
    },
    {
      "epoch": 0.02523564952100977,
      "grad_norm": 0.8281440138816833,
      "learning_rate": 0.00024769270210220476,
      "loss": 0.2838,
      "step": 5906
    },
    {
      "epoch": 0.02523992240443696,
      "grad_norm": 0.49469056725502014,
      "learning_rate": 0.0002476499743633567,
      "loss": 0.1782,
      "step": 5907
    },
    {
      "epoch": 0.025244195287864157,
      "grad_norm": 1.0197639465332031,
      "learning_rate": 0.00024760724662450863,
      "loss": 0.4806,
      "step": 5908
    },
    {
      "epoch": 0.025248468171291352,
      "grad_norm": 3.252622365951538,
      "learning_rate": 0.0002475645188856606,
      "loss": 0.8717,
      "step": 5909
    },
    {
      "epoch": 0.025252741054718544,
      "grad_norm": 2.156651258468628,
      "learning_rate": 0.0002475217911468125,
      "loss": 0.7499,
      "step": 5910
    },
    {
      "epoch": 0.02525701393814574,
      "grad_norm": 1.073256015777588,
      "learning_rate": 0.0002474790634079644,
      "loss": 0.48,
      "step": 5911
    },
    {
      "epoch": 0.025261286821572936,
      "grad_norm": 0.8332013487815857,
      "learning_rate": 0.0002474363356691164,
      "loss": 0.4121,
      "step": 5912
    },
    {
      "epoch": 0.025265559705000128,
      "grad_norm": 2.515233278274536,
      "learning_rate": 0.00024739360793026835,
      "loss": 0.5832,
      "step": 5913
    },
    {
      "epoch": 0.025269832588427323,
      "grad_norm": 1.865498423576355,
      "learning_rate": 0.00024735088019142026,
      "loss": 0.7375,
      "step": 5914
    },
    {
      "epoch": 0.025274105471854515,
      "grad_norm": 0.7519025206565857,
      "learning_rate": 0.0002473081524525722,
      "loss": 0.2294,
      "step": 5915
    },
    {
      "epoch": 0.02527837835528171,
      "grad_norm": 1.5063022375106812,
      "learning_rate": 0.00024726542471372414,
      "loss": 1.0432,
      "step": 5916
    },
    {
      "epoch": 0.025282651238708907,
      "grad_norm": 3.0270819664001465,
      "learning_rate": 0.0002472226969748761,
      "loss": 1.2598,
      "step": 5917
    },
    {
      "epoch": 0.0252869241221361,
      "grad_norm": 1.4672785997390747,
      "learning_rate": 0.000247179969236028,
      "loss": 0.8014,
      "step": 5918
    },
    {
      "epoch": 0.025291197005563294,
      "grad_norm": 4.06309700012207,
      "learning_rate": 0.00024713724149718,
      "loss": 1.5167,
      "step": 5919
    },
    {
      "epoch": 0.02529546988899049,
      "grad_norm": 1.0718427896499634,
      "learning_rate": 0.00024709451375833194,
      "loss": 0.3552,
      "step": 5920
    },
    {
      "epoch": 0.025299742772417682,
      "grad_norm": 2.933887004852295,
      "learning_rate": 0.00024705178601948385,
      "loss": 0.9336,
      "step": 5921
    },
    {
      "epoch": 0.025304015655844878,
      "grad_norm": 2.080387592315674,
      "learning_rate": 0.0002470090582806358,
      "loss": 0.9261,
      "step": 5922
    },
    {
      "epoch": 0.025308288539272073,
      "grad_norm": 1.979249358177185,
      "learning_rate": 0.00024696633054178773,
      "loss": 0.6034,
      "step": 5923
    },
    {
      "epoch": 0.025312561422699265,
      "grad_norm": 1.4270570278167725,
      "learning_rate": 0.0002469236028029397,
      "loss": 0.5727,
      "step": 5924
    },
    {
      "epoch": 0.02531683430612646,
      "grad_norm": 1.2986321449279785,
      "learning_rate": 0.0002468808750640916,
      "loss": 0.5723,
      "step": 5925
    },
    {
      "epoch": 0.025321107189553653,
      "grad_norm": 1.0587694644927979,
      "learning_rate": 0.0002468381473252435,
      "loss": 0.4224,
      "step": 5926
    },
    {
      "epoch": 0.02532538007298085,
      "grad_norm": 1.1788434982299805,
      "learning_rate": 0.0002467954195863955,
      "loss": 0.4056,
      "step": 5927
    },
    {
      "epoch": 0.025329652956408044,
      "grad_norm": 1.327028751373291,
      "learning_rate": 0.00024675269184754744,
      "loss": 0.5257,
      "step": 5928
    },
    {
      "epoch": 0.025333925839835236,
      "grad_norm": 0.910449743270874,
      "learning_rate": 0.00024670996410869935,
      "loss": 0.2944,
      "step": 5929
    },
    {
      "epoch": 0.025338198723262432,
      "grad_norm": 3.0341436862945557,
      "learning_rate": 0.0002466672363698513,
      "loss": 1.6787,
      "step": 5930
    },
    {
      "epoch": 0.025342471606689627,
      "grad_norm": 1.3293784856796265,
      "learning_rate": 0.00024662450863100323,
      "loss": 0.5741,
      "step": 5931
    },
    {
      "epoch": 0.02534674449011682,
      "grad_norm": 0.5835118889808655,
      "learning_rate": 0.0002465817808921552,
      "loss": 0.206,
      "step": 5932
    },
    {
      "epoch": 0.025351017373544015,
      "grad_norm": 1.7579891681671143,
      "learning_rate": 0.0002465390531533071,
      "loss": 0.6351,
      "step": 5933
    },
    {
      "epoch": 0.02535529025697121,
      "grad_norm": 0.8363342881202698,
      "learning_rate": 0.00024649632541445907,
      "loss": 0.2937,
      "step": 5934
    },
    {
      "epoch": 0.025359563140398403,
      "grad_norm": 1.3390752077102661,
      "learning_rate": 0.00024645359767561104,
      "loss": 0.3358,
      "step": 5935
    },
    {
      "epoch": 0.0253638360238256,
      "grad_norm": 2.388117551803589,
      "learning_rate": 0.00024641086993676295,
      "loss": 1.0233,
      "step": 5936
    },
    {
      "epoch": 0.025368108907252794,
      "grad_norm": 1.8305463790893555,
      "learning_rate": 0.0002463681421979149,
      "loss": 0.5823,
      "step": 5937
    },
    {
      "epoch": 0.025372381790679986,
      "grad_norm": 0.8406651616096497,
      "learning_rate": 0.0002463254144590668,
      "loss": 0.4095,
      "step": 5938
    },
    {
      "epoch": 0.025376654674107182,
      "grad_norm": 2.223936080932617,
      "learning_rate": 0.0002462826867202188,
      "loss": 0.8043,
      "step": 5939
    },
    {
      "epoch": 0.025380927557534374,
      "grad_norm": 2.8946099281311035,
      "learning_rate": 0.0002462399589813707,
      "loss": 1.6088,
      "step": 5940
    },
    {
      "epoch": 0.02538520044096157,
      "grad_norm": 1.5714480876922607,
      "learning_rate": 0.00024619723124252266,
      "loss": 0.5929,
      "step": 5941
    },
    {
      "epoch": 0.025389473324388765,
      "grad_norm": 4.118473052978516,
      "learning_rate": 0.00024615450350367463,
      "loss": 2.1657,
      "step": 5942
    },
    {
      "epoch": 0.025393746207815957,
      "grad_norm": 0.7463716864585876,
      "learning_rate": 0.00024611177576482654,
      "loss": 0.3636,
      "step": 5943
    },
    {
      "epoch": 0.025398019091243153,
      "grad_norm": 1.6703925132751465,
      "learning_rate": 0.00024606904802597845,
      "loss": 0.6896,
      "step": 5944
    },
    {
      "epoch": 0.02540229197467035,
      "grad_norm": 3.933272361755371,
      "learning_rate": 0.0002460263202871304,
      "loss": 1.1276,
      "step": 5945
    },
    {
      "epoch": 0.02540656485809754,
      "grad_norm": 1.487868309020996,
      "learning_rate": 0.0002459835925482823,
      "loss": 0.8333,
      "step": 5946
    },
    {
      "epoch": 0.025410837741524736,
      "grad_norm": 2.866870641708374,
      "learning_rate": 0.0002459408648094343,
      "loss": 0.8879,
      "step": 5947
    },
    {
      "epoch": 0.02541511062495193,
      "grad_norm": 2.0185353755950928,
      "learning_rate": 0.0002458981370705862,
      "loss": 0.9057,
      "step": 5948
    },
    {
      "epoch": 0.025419383508379124,
      "grad_norm": 1.5727864503860474,
      "learning_rate": 0.00024585540933173817,
      "loss": 0.3692,
      "step": 5949
    },
    {
      "epoch": 0.02542365639180632,
      "grad_norm": 0.8623220324516296,
      "learning_rate": 0.00024581268159289013,
      "loss": 0.3391,
      "step": 5950
    },
    {
      "epoch": 0.02542792927523351,
      "grad_norm": 2.717031478881836,
      "learning_rate": 0.00024576995385404204,
      "loss": 1.02,
      "step": 5951
    },
    {
      "epoch": 0.025432202158660707,
      "grad_norm": 1.4986357688903809,
      "learning_rate": 0.000245727226115194,
      "loss": 0.5277,
      "step": 5952
    },
    {
      "epoch": 0.025436475042087903,
      "grad_norm": 1.6368273496627808,
      "learning_rate": 0.0002456844983763459,
      "loss": 0.623,
      "step": 5953
    },
    {
      "epoch": 0.025440747925515095,
      "grad_norm": 4.480924606323242,
      "learning_rate": 0.0002456417706374979,
      "loss": 1.259,
      "step": 5954
    },
    {
      "epoch": 0.02544502080894229,
      "grad_norm": 1.2636611461639404,
      "learning_rate": 0.0002455990428986498,
      "loss": 0.4695,
      "step": 5955
    },
    {
      "epoch": 0.025449293692369486,
      "grad_norm": 1.2526689767837524,
      "learning_rate": 0.00024555631515980176,
      "loss": 0.4504,
      "step": 5956
    },
    {
      "epoch": 0.025453566575796678,
      "grad_norm": 0.9181005358695984,
      "learning_rate": 0.0002455135874209537,
      "loss": 0.4658,
      "step": 5957
    },
    {
      "epoch": 0.025457839459223874,
      "grad_norm": 1.3844305276870728,
      "learning_rate": 0.00024547085968210563,
      "loss": 0.7783,
      "step": 5958
    },
    {
      "epoch": 0.02546211234265107,
      "grad_norm": 1.8723618984222412,
      "learning_rate": 0.00024542813194325754,
      "loss": 0.8599,
      "step": 5959
    },
    {
      "epoch": 0.02546638522607826,
      "grad_norm": 0.7644928097724915,
      "learning_rate": 0.0002453854042044095,
      "loss": 0.3704,
      "step": 5960
    },
    {
      "epoch": 0.025470658109505457,
      "grad_norm": 1.6021579504013062,
      "learning_rate": 0.0002453426764655614,
      "loss": 0.4573,
      "step": 5961
    },
    {
      "epoch": 0.02547493099293265,
      "grad_norm": 4.464601039886475,
      "learning_rate": 0.0002452999487267134,
      "loss": 1.6602,
      "step": 5962
    },
    {
      "epoch": 0.025479203876359845,
      "grad_norm": 1.615849494934082,
      "learning_rate": 0.0002452572209878653,
      "loss": 0.4417,
      "step": 5963
    },
    {
      "epoch": 0.02548347675978704,
      "grad_norm": 3.0622072219848633,
      "learning_rate": 0.00024521449324901726,
      "loss": 1.0622,
      "step": 5964
    },
    {
      "epoch": 0.025487749643214232,
      "grad_norm": 1.9192088842391968,
      "learning_rate": 0.0002451717655101692,
      "loss": 0.6412,
      "step": 5965
    },
    {
      "epoch": 0.025492022526641428,
      "grad_norm": 2.9796860218048096,
      "learning_rate": 0.00024512903777132114,
      "loss": 0.7877,
      "step": 5966
    },
    {
      "epoch": 0.025496295410068624,
      "grad_norm": 9.851866722106934,
      "learning_rate": 0.0002450863100324731,
      "loss": 4.5572,
      "step": 5967
    },
    {
      "epoch": 0.025500568293495816,
      "grad_norm": 2.43741774559021,
      "learning_rate": 0.000245043582293625,
      "loss": 0.9644,
      "step": 5968
    },
    {
      "epoch": 0.02550484117692301,
      "grad_norm": 1.7641819715499878,
      "learning_rate": 0.000245000854554777,
      "loss": 0.4753,
      "step": 5969
    },
    {
      "epoch": 0.025509114060350207,
      "grad_norm": 1.091638207435608,
      "learning_rate": 0.0002449581268159289,
      "loss": 0.4921,
      "step": 5970
    },
    {
      "epoch": 0.0255133869437774,
      "grad_norm": 0.9748464822769165,
      "learning_rate": 0.00024491539907708085,
      "loss": 0.3388,
      "step": 5971
    },
    {
      "epoch": 0.025517659827204595,
      "grad_norm": 0.9629720449447632,
      "learning_rate": 0.0002448726713382328,
      "loss": 0.4983,
      "step": 5972
    },
    {
      "epoch": 0.02552193271063179,
      "grad_norm": 2.1610538959503174,
      "learning_rate": 0.00024482994359938473,
      "loss": 0.7333,
      "step": 5973
    },
    {
      "epoch": 0.025526205594058982,
      "grad_norm": 1.3195717334747314,
      "learning_rate": 0.0002447872158605367,
      "loss": 1.0451,
      "step": 5974
    },
    {
      "epoch": 0.025530478477486178,
      "grad_norm": 2.6697793006896973,
      "learning_rate": 0.0002447444881216886,
      "loss": 0.9982,
      "step": 5975
    },
    {
      "epoch": 0.02553475136091337,
      "grad_norm": 1.3424367904663086,
      "learning_rate": 0.0002447017603828405,
      "loss": 1.0532,
      "step": 5976
    },
    {
      "epoch": 0.025539024244340566,
      "grad_norm": 1.7342275381088257,
      "learning_rate": 0.0002446590326439925,
      "loss": 0.4436,
      "step": 5977
    },
    {
      "epoch": 0.02554329712776776,
      "grad_norm": 3.932508707046509,
      "learning_rate": 0.0002446163049051444,
      "loss": 1.3691,
      "step": 5978
    },
    {
      "epoch": 0.025547570011194953,
      "grad_norm": 0.8838486671447754,
      "learning_rate": 0.00024457357716629636,
      "loss": 0.3036,
      "step": 5979
    },
    {
      "epoch": 0.02555184289462215,
      "grad_norm": 1.0198686122894287,
      "learning_rate": 0.0002445308494274483,
      "loss": 0.4513,
      "step": 5980
    },
    {
      "epoch": 0.025556115778049344,
      "grad_norm": 2.866809606552124,
      "learning_rate": 0.00024448812168860023,
      "loss": 0.7994,
      "step": 5981
    },
    {
      "epoch": 0.025560388661476537,
      "grad_norm": 3.2991440296173096,
      "learning_rate": 0.0002444453939497522,
      "loss": 1.401,
      "step": 5982
    },
    {
      "epoch": 0.025564661544903732,
      "grad_norm": 0.8612315058708191,
      "learning_rate": 0.0002444026662109041,
      "loss": 0.3036,
      "step": 5983
    },
    {
      "epoch": 0.025568934428330928,
      "grad_norm": 0.5087279081344604,
      "learning_rate": 0.00024435993847205607,
      "loss": 0.195,
      "step": 5984
    },
    {
      "epoch": 0.02557320731175812,
      "grad_norm": 1.9887127876281738,
      "learning_rate": 0.000244317210733208,
      "loss": 0.7047,
      "step": 5985
    },
    {
      "epoch": 0.025577480195185315,
      "grad_norm": 0.8418389558792114,
      "learning_rate": 0.00024427448299435995,
      "loss": 0.3055,
      "step": 5986
    },
    {
      "epoch": 0.025581753078612508,
      "grad_norm": 1.9561549425125122,
      "learning_rate": 0.0002442317552555119,
      "loss": 0.9156,
      "step": 5987
    },
    {
      "epoch": 0.025586025962039703,
      "grad_norm": 1.4457669258117676,
      "learning_rate": 0.0002441890275166638,
      "loss": 0.7971,
      "step": 5988
    },
    {
      "epoch": 0.0255902988454669,
      "grad_norm": 2.0483131408691406,
      "learning_rate": 0.0002441462997778158,
      "loss": 0.7771,
      "step": 5989
    },
    {
      "epoch": 0.02559457172889409,
      "grad_norm": 1.9065568447113037,
      "learning_rate": 0.00024410357203896773,
      "loss": 0.8952,
      "step": 5990
    },
    {
      "epoch": 0.025598844612321286,
      "grad_norm": 0.9611777067184448,
      "learning_rate": 0.00024406084430011966,
      "loss": 0.4646,
      "step": 5991
    },
    {
      "epoch": 0.025603117495748482,
      "grad_norm": 2.574887275695801,
      "learning_rate": 0.00024401811656127157,
      "loss": 0.8763,
      "step": 5992
    },
    {
      "epoch": 0.025607390379175674,
      "grad_norm": 1.2111262083053589,
      "learning_rate": 0.0002439753888224235,
      "loss": 0.5103,
      "step": 5993
    },
    {
      "epoch": 0.02561166326260287,
      "grad_norm": 1.1551073789596558,
      "learning_rate": 0.00024393266108357545,
      "loss": 0.4166,
      "step": 5994
    },
    {
      "epoch": 0.025615936146030065,
      "grad_norm": 1.9628596305847168,
      "learning_rate": 0.0002438899333447274,
      "loss": 0.5742,
      "step": 5995
    },
    {
      "epoch": 0.025620209029457258,
      "grad_norm": 1.7843797206878662,
      "learning_rate": 0.00024384720560587933,
      "loss": 0.8491,
      "step": 5996
    },
    {
      "epoch": 0.025624481912884453,
      "grad_norm": 0.8041263818740845,
      "learning_rate": 0.00024380447786703126,
      "loss": 0.37,
      "step": 5997
    },
    {
      "epoch": 0.02562875479631165,
      "grad_norm": 2.3437423706054688,
      "learning_rate": 0.00024376175012818323,
      "loss": 0.8015,
      "step": 5998
    },
    {
      "epoch": 0.02563302767973884,
      "grad_norm": 1.6953580379486084,
      "learning_rate": 0.00024371902238933517,
      "loss": 0.8107,
      "step": 5999
    },
    {
      "epoch": 0.025637300563166036,
      "grad_norm": 0.9213038086891174,
      "learning_rate": 0.0002436762946504871,
      "loss": 0.5255,
      "step": 6000
    },
    {
      "epoch": 0.02564157344659323,
      "grad_norm": 0.8837618231773376,
      "learning_rate": 0.00024363356691163904,
      "loss": 0.486,
      "step": 6001
    },
    {
      "epoch": 0.025645846330020424,
      "grad_norm": 0.7155044078826904,
      "learning_rate": 0.00024359083917279098,
      "loss": 0.3446,
      "step": 6002
    },
    {
      "epoch": 0.02565011921344762,
      "grad_norm": 0.3702699542045593,
      "learning_rate": 0.00024354811143394292,
      "loss": 0.1724,
      "step": 6003
    },
    {
      "epoch": 0.025654392096874812,
      "grad_norm": 1.5454113483428955,
      "learning_rate": 0.00024350538369509486,
      "loss": 0.3575,
      "step": 6004
    },
    {
      "epoch": 0.025658664980302007,
      "grad_norm": 1.6217851638793945,
      "learning_rate": 0.00024346265595624682,
      "loss": 0.4135,
      "step": 6005
    },
    {
      "epoch": 0.025662937863729203,
      "grad_norm": 4.1581950187683105,
      "learning_rate": 0.00024341992821739876,
      "loss": 1.0408,
      "step": 6006
    },
    {
      "epoch": 0.025667210747156395,
      "grad_norm": 3.0496273040771484,
      "learning_rate": 0.0002433772004785507,
      "loss": 0.9061,
      "step": 6007
    },
    {
      "epoch": 0.02567148363058359,
      "grad_norm": 1.5969136953353882,
      "learning_rate": 0.00024333447273970263,
      "loss": 0.7717,
      "step": 6008
    },
    {
      "epoch": 0.025675756514010786,
      "grad_norm": 1.582802176475525,
      "learning_rate": 0.00024329174500085454,
      "loss": 0.7494,
      "step": 6009
    },
    {
      "epoch": 0.02568002939743798,
      "grad_norm": 1.5528614521026611,
      "learning_rate": 0.00024324901726200648,
      "loss": 0.7206,
      "step": 6010
    },
    {
      "epoch": 0.025684302280865174,
      "grad_norm": 1.4431873559951782,
      "learning_rate": 0.00024320628952315842,
      "loss": 0.3573,
      "step": 6011
    },
    {
      "epoch": 0.025688575164292366,
      "grad_norm": 1.134572982788086,
      "learning_rate": 0.00024316356178431039,
      "loss": 0.389,
      "step": 6012
    },
    {
      "epoch": 0.02569284804771956,
      "grad_norm": 0.6921495795249939,
      "learning_rate": 0.00024312083404546232,
      "loss": 0.3227,
      "step": 6013
    },
    {
      "epoch": 0.025697120931146757,
      "grad_norm": 1.4538209438323975,
      "learning_rate": 0.00024307810630661426,
      "loss": 0.3344,
      "step": 6014
    },
    {
      "epoch": 0.02570139381457395,
      "grad_norm": 0.6055258512496948,
      "learning_rate": 0.0002430353785677662,
      "loss": 0.2863,
      "step": 6015
    },
    {
      "epoch": 0.025705666698001145,
      "grad_norm": 1.9641677141189575,
      "learning_rate": 0.00024299265082891814,
      "loss": 0.605,
      "step": 6016
    },
    {
      "epoch": 0.02570993958142834,
      "grad_norm": 1.869909405708313,
      "learning_rate": 0.00024294992309007007,
      "loss": 0.8347,
      "step": 6017
    },
    {
      "epoch": 0.025714212464855533,
      "grad_norm": 1.488040566444397,
      "learning_rate": 0.000242907195351222,
      "loss": 0.6545,
      "step": 6018
    },
    {
      "epoch": 0.02571848534828273,
      "grad_norm": 0.38515159487724304,
      "learning_rate": 0.00024286446761237395,
      "loss": 0.1767,
      "step": 6019
    },
    {
      "epoch": 0.025722758231709924,
      "grad_norm": 1.3850687742233276,
      "learning_rate": 0.00024282173987352591,
      "loss": 0.4738,
      "step": 6020
    },
    {
      "epoch": 0.025727031115137116,
      "grad_norm": 1.4498528242111206,
      "learning_rate": 0.00024277901213467785,
      "loss": 0.2626,
      "step": 6021
    },
    {
      "epoch": 0.02573130399856431,
      "grad_norm": 3.374783754348755,
      "learning_rate": 0.0002427362843958298,
      "loss": 1.1659,
      "step": 6022
    },
    {
      "epoch": 0.025735576881991507,
      "grad_norm": 1.3694082498550415,
      "learning_rate": 0.00024269355665698173,
      "loss": 0.3895,
      "step": 6023
    },
    {
      "epoch": 0.0257398497654187,
      "grad_norm": 3.302649736404419,
      "learning_rate": 0.00024265082891813367,
      "loss": 0.7278,
      "step": 6024
    },
    {
      "epoch": 0.025744122648845895,
      "grad_norm": 2.5163540840148926,
      "learning_rate": 0.00024260810117928558,
      "loss": 0.6557,
      "step": 6025
    },
    {
      "epoch": 0.025748395532273087,
      "grad_norm": 0.9517161250114441,
      "learning_rate": 0.00024256537344043751,
      "loss": 0.3034,
      "step": 6026
    },
    {
      "epoch": 0.025752668415700283,
      "grad_norm": 2.919198751449585,
      "learning_rate": 0.00024252264570158948,
      "loss": 0.8579,
      "step": 6027
    },
    {
      "epoch": 0.025756941299127478,
      "grad_norm": 1.285362720489502,
      "learning_rate": 0.00024247991796274142,
      "loss": 0.389,
      "step": 6028
    },
    {
      "epoch": 0.02576121418255467,
      "grad_norm": 3.4167370796203613,
      "learning_rate": 0.00024243719022389336,
      "loss": 1.0679,
      "step": 6029
    },
    {
      "epoch": 0.025765487065981866,
      "grad_norm": 5.007648468017578,
      "learning_rate": 0.0002423944624850453,
      "loss": 1.3714,
      "step": 6030
    },
    {
      "epoch": 0.02576975994940906,
      "grad_norm": 2.952312707901001,
      "learning_rate": 0.00024235173474619723,
      "loss": 0.8073,
      "step": 6031
    },
    {
      "epoch": 0.025774032832836254,
      "grad_norm": 0.9596197605133057,
      "learning_rate": 0.00024230900700734917,
      "loss": 0.2766,
      "step": 6032
    },
    {
      "epoch": 0.02577830571626345,
      "grad_norm": 5.0325212478637695,
      "learning_rate": 0.0002422662792685011,
      "loss": 1.2233,
      "step": 6033
    },
    {
      "epoch": 0.025782578599690645,
      "grad_norm": 1.6449823379516602,
      "learning_rate": 0.00024222355152965304,
      "loss": 0.4485,
      "step": 6034
    },
    {
      "epoch": 0.025786851483117837,
      "grad_norm": 1.024480938911438,
      "learning_rate": 0.000242180823790805,
      "loss": 0.3889,
      "step": 6035
    },
    {
      "epoch": 0.025791124366545033,
      "grad_norm": 2.158853530883789,
      "learning_rate": 0.00024213809605195695,
      "loss": 0.4594,
      "step": 6036
    },
    {
      "epoch": 0.025795397249972225,
      "grad_norm": 2.0195603370666504,
      "learning_rate": 0.00024209536831310889,
      "loss": 0.6125,
      "step": 6037
    },
    {
      "epoch": 0.02579967013339942,
      "grad_norm": 2.5047607421875,
      "learning_rate": 0.00024205264057426082,
      "loss": 0.5705,
      "step": 6038
    },
    {
      "epoch": 0.025803943016826616,
      "grad_norm": 4.441929817199707,
      "learning_rate": 0.00024200991283541276,
      "loss": 1.4645,
      "step": 6039
    },
    {
      "epoch": 0.025808215900253808,
      "grad_norm": 1.006410002708435,
      "learning_rate": 0.0002419671850965647,
      "loss": 0.3702,
      "step": 6040
    },
    {
      "epoch": 0.025812488783681004,
      "grad_norm": 1.220047950744629,
      "learning_rate": 0.00024192445735771664,
      "loss": 0.3152,
      "step": 6041
    },
    {
      "epoch": 0.0258167616671082,
      "grad_norm": 3.376516580581665,
      "learning_rate": 0.00024188172961886857,
      "loss": 1.1879,
      "step": 6042
    },
    {
      "epoch": 0.02582103455053539,
      "grad_norm": 5.49611759185791,
      "learning_rate": 0.0002418390018800205,
      "loss": 1.3428,
      "step": 6043
    },
    {
      "epoch": 0.025825307433962587,
      "grad_norm": 0.7786068916320801,
      "learning_rate": 0.00024179627414117245,
      "loss": 0.2168,
      "step": 6044
    },
    {
      "epoch": 0.025829580317389782,
      "grad_norm": 2.9496047496795654,
      "learning_rate": 0.0002417535464023244,
      "loss": 1.7819,
      "step": 6045
    },
    {
      "epoch": 0.025833853200816975,
      "grad_norm": 1.6026339530944824,
      "learning_rate": 0.00024171081866347633,
      "loss": 0.5292,
      "step": 6046
    },
    {
      "epoch": 0.02583812608424417,
      "grad_norm": 5.303033351898193,
      "learning_rate": 0.00024166809092462826,
      "loss": 1.2574,
      "step": 6047
    },
    {
      "epoch": 0.025842398967671366,
      "grad_norm": 1.151687502861023,
      "learning_rate": 0.0002416253631857802,
      "loss": 0.2935,
      "step": 6048
    },
    {
      "epoch": 0.025846671851098558,
      "grad_norm": 1.281630039215088,
      "learning_rate": 0.00024158263544693214,
      "loss": 0.535,
      "step": 6049
    },
    {
      "epoch": 0.025850944734525753,
      "grad_norm": 1.2965348958969116,
      "learning_rate": 0.0002415399077080841,
      "loss": 0.4514,
      "step": 6050
    },
    {
      "epoch": 0.025855217617952946,
      "grad_norm": 0.8975192308425903,
      "learning_rate": 0.00024149717996923604,
      "loss": 0.3636,
      "step": 6051
    },
    {
      "epoch": 0.02585949050138014,
      "grad_norm": 0.44462820887565613,
      "learning_rate": 0.00024145445223038798,
      "loss": 0.1387,
      "step": 6052
    },
    {
      "epoch": 0.025863763384807337,
      "grad_norm": 1.2109427452087402,
      "learning_rate": 0.00024141172449153992,
      "loss": 0.4324,
      "step": 6053
    },
    {
      "epoch": 0.02586803626823453,
      "grad_norm": 1.7356669902801514,
      "learning_rate": 0.00024136899675269186,
      "loss": 0.9349,
      "step": 6054
    },
    {
      "epoch": 0.025872309151661724,
      "grad_norm": 2.0415902137756348,
      "learning_rate": 0.0002413262690138438,
      "loss": 0.3602,
      "step": 6055
    },
    {
      "epoch": 0.02587658203508892,
      "grad_norm": 3.3376619815826416,
      "learning_rate": 0.00024128354127499573,
      "loss": 0.8383,
      "step": 6056
    },
    {
      "epoch": 0.025880854918516112,
      "grad_norm": 5.208406925201416,
      "learning_rate": 0.0002412408135361477,
      "loss": 1.3386,
      "step": 6057
    },
    {
      "epoch": 0.025885127801943308,
      "grad_norm": 1.1084738969802856,
      "learning_rate": 0.0002411980857972996,
      "loss": 0.4973,
      "step": 6058
    },
    {
      "epoch": 0.025889400685370503,
      "grad_norm": 2.808624029159546,
      "learning_rate": 0.00024115535805845155,
      "loss": 1.7243,
      "step": 6059
    },
    {
      "epoch": 0.025893673568797695,
      "grad_norm": 2.396944522857666,
      "learning_rate": 0.00024111263031960348,
      "loss": 0.4654,
      "step": 6060
    },
    {
      "epoch": 0.02589794645222489,
      "grad_norm": 1.0304338932037354,
      "learning_rate": 0.00024106990258075542,
      "loss": 0.4771,
      "step": 6061
    },
    {
      "epoch": 0.025902219335652083,
      "grad_norm": 1.3283768892288208,
      "learning_rate": 0.00024102717484190736,
      "loss": 0.6026,
      "step": 6062
    },
    {
      "epoch": 0.02590649221907928,
      "grad_norm": 1.3135408163070679,
      "learning_rate": 0.0002409844471030593,
      "loss": 0.5962,
      "step": 6063
    },
    {
      "epoch": 0.025910765102506474,
      "grad_norm": 2.0404584407806396,
      "learning_rate": 0.00024094171936421126,
      "loss": 0.4332,
      "step": 6064
    },
    {
      "epoch": 0.025915037985933666,
      "grad_norm": 1.3229562044143677,
      "learning_rate": 0.0002408989916253632,
      "loss": 0.602,
      "step": 6065
    },
    {
      "epoch": 0.025919310869360862,
      "grad_norm": 2.368285655975342,
      "learning_rate": 0.00024085626388651514,
      "loss": 0.7767,
      "step": 6066
    },
    {
      "epoch": 0.025923583752788058,
      "grad_norm": 3.957916259765625,
      "learning_rate": 0.00024081353614766707,
      "loss": 0.9043,
      "step": 6067
    },
    {
      "epoch": 0.02592785663621525,
      "grad_norm": 1.0147383213043213,
      "learning_rate": 0.000240770808408819,
      "loss": 0.4899,
      "step": 6068
    },
    {
      "epoch": 0.025932129519642445,
      "grad_norm": 1.7384529113769531,
      "learning_rate": 0.00024072808066997095,
      "loss": 0.7192,
      "step": 6069
    },
    {
      "epoch": 0.02593640240306964,
      "grad_norm": 3.5298800468444824,
      "learning_rate": 0.0002406853529311229,
      "loss": 1.5824,
      "step": 6070
    },
    {
      "epoch": 0.025940675286496833,
      "grad_norm": 1.1373602151870728,
      "learning_rate": 0.00024064262519227483,
      "loss": 0.3886,
      "step": 6071
    },
    {
      "epoch": 0.02594494816992403,
      "grad_norm": 1.4377586841583252,
      "learning_rate": 0.0002405998974534268,
      "loss": 0.5073,
      "step": 6072
    },
    {
      "epoch": 0.02594922105335122,
      "grad_norm": 1.2809834480285645,
      "learning_rate": 0.00024055716971457873,
      "loss": 0.54,
      "step": 6073
    },
    {
      "epoch": 0.025953493936778416,
      "grad_norm": 1.6196985244750977,
      "learning_rate": 0.00024051444197573064,
      "loss": 0.5971,
      "step": 6074
    },
    {
      "epoch": 0.025957766820205612,
      "grad_norm": 5.049152374267578,
      "learning_rate": 0.00024047171423688258,
      "loss": 1.094,
      "step": 6075
    },
    {
      "epoch": 0.025962039703632804,
      "grad_norm": 3.012685775756836,
      "learning_rate": 0.00024042898649803452,
      "loss": 1.7135,
      "step": 6076
    },
    {
      "epoch": 0.02596631258706,
      "grad_norm": 3.509110927581787,
      "learning_rate": 0.00024038625875918645,
      "loss": 0.7482,
      "step": 6077
    },
    {
      "epoch": 0.025970585470487195,
      "grad_norm": 1.523359775543213,
      "learning_rate": 0.0002403435310203384,
      "loss": 0.4624,
      "step": 6078
    },
    {
      "epoch": 0.025974858353914387,
      "grad_norm": 2.0149195194244385,
      "learning_rate": 0.00024030080328149036,
      "loss": 0.5746,
      "step": 6079
    },
    {
      "epoch": 0.025979131237341583,
      "grad_norm": 1.091821551322937,
      "learning_rate": 0.0002402580755426423,
      "loss": 0.3383,
      "step": 6080
    },
    {
      "epoch": 0.02598340412076878,
      "grad_norm": 1.763404130935669,
      "learning_rate": 0.00024021534780379423,
      "loss": 1.0265,
      "step": 6081
    },
    {
      "epoch": 0.02598767700419597,
      "grad_norm": 1.3444873094558716,
      "learning_rate": 0.00024017262006494617,
      "loss": 0.5519,
      "step": 6082
    },
    {
      "epoch": 0.025991949887623166,
      "grad_norm": 3.807898759841919,
      "learning_rate": 0.0002401298923260981,
      "loss": 1.3142,
      "step": 6083
    },
    {
      "epoch": 0.025996222771050362,
      "grad_norm": 1.293959617614746,
      "learning_rate": 0.00024008716458725005,
      "loss": 0.4978,
      "step": 6084
    },
    {
      "epoch": 0.026000495654477554,
      "grad_norm": 1.7766923904418945,
      "learning_rate": 0.00024004443684840198,
      "loss": 0.3103,
      "step": 6085
    },
    {
      "epoch": 0.02600476853790475,
      "grad_norm": 3.221306324005127,
      "learning_rate": 0.00024000170910955392,
      "loss": 0.7444,
      "step": 6086
    },
    {
      "epoch": 0.02600904142133194,
      "grad_norm": 0.7258573174476624,
      "learning_rate": 0.00023995898137070589,
      "loss": 0.3229,
      "step": 6087
    },
    {
      "epoch": 0.026013314304759137,
      "grad_norm": 1.1160953044891357,
      "learning_rate": 0.00023991625363185782,
      "loss": 0.4973,
      "step": 6088
    },
    {
      "epoch": 0.026017587188186333,
      "grad_norm": 3.3389945030212402,
      "learning_rate": 0.00023987352589300976,
      "loss": 1.3696,
      "step": 6089
    },
    {
      "epoch": 0.026021860071613525,
      "grad_norm": 4.44182014465332,
      "learning_rate": 0.0002398307981541617,
      "loss": 2.4421,
      "step": 6090
    },
    {
      "epoch": 0.02602613295504072,
      "grad_norm": 1.1245625019073486,
      "learning_rate": 0.0002397880704153136,
      "loss": 0.4995,
      "step": 6091
    },
    {
      "epoch": 0.026030405838467916,
      "grad_norm": 3.2868330478668213,
      "learning_rate": 0.00023974534267646555,
      "loss": 1.2683,
      "step": 6092
    },
    {
      "epoch": 0.026034678721895108,
      "grad_norm": 1.6398742198944092,
      "learning_rate": 0.00023970261493761749,
      "loss": 0.3287,
      "step": 6093
    },
    {
      "epoch": 0.026038951605322304,
      "grad_norm": 2.273577928543091,
      "learning_rate": 0.00023965988719876945,
      "loss": 0.6588,
      "step": 6094
    },
    {
      "epoch": 0.0260432244887495,
      "grad_norm": 1.0548182725906372,
      "learning_rate": 0.0002396171594599214,
      "loss": 0.4848,
      "step": 6095
    },
    {
      "epoch": 0.02604749737217669,
      "grad_norm": 2.9374022483825684,
      "learning_rate": 0.00023957443172107333,
      "loss": 0.7004,
      "step": 6096
    },
    {
      "epoch": 0.026051770255603887,
      "grad_norm": 3.7590785026550293,
      "learning_rate": 0.00023953170398222526,
      "loss": 1.1465,
      "step": 6097
    },
    {
      "epoch": 0.02605604313903108,
      "grad_norm": 0.9568338990211487,
      "learning_rate": 0.0002394889762433772,
      "loss": 0.4599,
      "step": 6098
    },
    {
      "epoch": 0.026060316022458275,
      "grad_norm": 3.54414963722229,
      "learning_rate": 0.00023944624850452914,
      "loss": 1.2292,
      "step": 6099
    },
    {
      "epoch": 0.02606458890588547,
      "grad_norm": 1.071205973625183,
      "learning_rate": 0.00023940352076568108,
      "loss": 0.4041,
      "step": 6100
    },
    {
      "epoch": 0.026068861789312663,
      "grad_norm": 0.5895407795906067,
      "learning_rate": 0.00023936079302683302,
      "loss": 0.2759,
      "step": 6101
    },
    {
      "epoch": 0.026073134672739858,
      "grad_norm": 1.2248096466064453,
      "learning_rate": 0.00023931806528798498,
      "loss": 0.4715,
      "step": 6102
    },
    {
      "epoch": 0.026077407556167054,
      "grad_norm": 1.3149477243423462,
      "learning_rate": 0.00023927533754913692,
      "loss": 0.4512,
      "step": 6103
    },
    {
      "epoch": 0.026081680439594246,
      "grad_norm": 0.8170361518859863,
      "learning_rate": 0.00023923260981028886,
      "loss": 0.2782,
      "step": 6104
    },
    {
      "epoch": 0.02608595332302144,
      "grad_norm": 3.433692455291748,
      "learning_rate": 0.0002391898820714408,
      "loss": 0.8927,
      "step": 6105
    },
    {
      "epoch": 0.026090226206448637,
      "grad_norm": 1.205062747001648,
      "learning_rate": 0.00023914715433259273,
      "loss": 0.5761,
      "step": 6106
    },
    {
      "epoch": 0.02609449908987583,
      "grad_norm": 0.7824959754943848,
      "learning_rate": 0.00023910442659374464,
      "loss": 0.3881,
      "step": 6107
    },
    {
      "epoch": 0.026098771973303025,
      "grad_norm": 0.5323331356048584,
      "learning_rate": 0.00023906169885489658,
      "loss": 0.2248,
      "step": 6108
    },
    {
      "epoch": 0.02610304485673022,
      "grad_norm": 2.988058567047119,
      "learning_rate": 0.00023901897111604855,
      "loss": 0.9243,
      "step": 6109
    },
    {
      "epoch": 0.026107317740157412,
      "grad_norm": 1.2461827993392944,
      "learning_rate": 0.00023897624337720048,
      "loss": 0.4347,
      "step": 6110
    },
    {
      "epoch": 0.026111590623584608,
      "grad_norm": 3.1879189014434814,
      "learning_rate": 0.00023893351563835242,
      "loss": 1.0784,
      "step": 6111
    },
    {
      "epoch": 0.0261158635070118,
      "grad_norm": 4.409423828125,
      "learning_rate": 0.00023889078789950436,
      "loss": 0.9983,
      "step": 6112
    },
    {
      "epoch": 0.026120136390438996,
      "grad_norm": 2.7104082107543945,
      "learning_rate": 0.0002388480601606563,
      "loss": 0.834,
      "step": 6113
    },
    {
      "epoch": 0.02612440927386619,
      "grad_norm": 0.6300988793373108,
      "learning_rate": 0.00023880533242180823,
      "loss": 0.238,
      "step": 6114
    },
    {
      "epoch": 0.026128682157293383,
      "grad_norm": 1.2084128856658936,
      "learning_rate": 0.00023876260468296017,
      "loss": 0.6253,
      "step": 6115
    },
    {
      "epoch": 0.02613295504072058,
      "grad_norm": 2.7100331783294678,
      "learning_rate": 0.00023871987694411214,
      "loss": 1.0054,
      "step": 6116
    },
    {
      "epoch": 0.026137227924147775,
      "grad_norm": 4.169058322906494,
      "learning_rate": 0.00023867714920526408,
      "loss": 1.0382,
      "step": 6117
    },
    {
      "epoch": 0.026141500807574967,
      "grad_norm": 3.1960484981536865,
      "learning_rate": 0.000238634421466416,
      "loss": 0.9864,
      "step": 6118
    },
    {
      "epoch": 0.026145773691002162,
      "grad_norm": 1.1997199058532715,
      "learning_rate": 0.00023859169372756795,
      "loss": 0.582,
      "step": 6119
    },
    {
      "epoch": 0.026150046574429358,
      "grad_norm": 3.0446150302886963,
      "learning_rate": 0.0002385489659887199,
      "loss": 0.9483,
      "step": 6120
    },
    {
      "epoch": 0.02615431945785655,
      "grad_norm": 0.7243643999099731,
      "learning_rate": 0.00023850623824987183,
      "loss": 0.2423,
      "step": 6121
    },
    {
      "epoch": 0.026158592341283746,
      "grad_norm": 4.736178874969482,
      "learning_rate": 0.00023846351051102376,
      "loss": 1.5625,
      "step": 6122
    },
    {
      "epoch": 0.026162865224710938,
      "grad_norm": 3.899188995361328,
      "learning_rate": 0.0002384207827721757,
      "loss": 1.5569,
      "step": 6123
    },
    {
      "epoch": 0.026167138108138133,
      "grad_norm": 2.273252248764038,
      "learning_rate": 0.00023837805503332764,
      "loss": 1.5618,
      "step": 6124
    },
    {
      "epoch": 0.02617141099156533,
      "grad_norm": 1.0909852981567383,
      "learning_rate": 0.00023833532729447958,
      "loss": 0.3996,
      "step": 6125
    },
    {
      "epoch": 0.02617568387499252,
      "grad_norm": 1.6748757362365723,
      "learning_rate": 0.00023829259955563152,
      "loss": 0.6206,
      "step": 6126
    },
    {
      "epoch": 0.026179956758419717,
      "grad_norm": 1.9118543863296509,
      "learning_rate": 0.00023824987181678345,
      "loss": 0.6337,
      "step": 6127
    },
    {
      "epoch": 0.026184229641846912,
      "grad_norm": 0.8734283447265625,
      "learning_rate": 0.0002382071440779354,
      "loss": 0.3641,
      "step": 6128
    },
    {
      "epoch": 0.026188502525274104,
      "grad_norm": 3.810797929763794,
      "learning_rate": 0.00023816441633908733,
      "loss": 1.4101,
      "step": 6129
    },
    {
      "epoch": 0.0261927754087013,
      "grad_norm": 5.135289669036865,
      "learning_rate": 0.00023812168860023927,
      "loss": 1.2595,
      "step": 6130
    },
    {
      "epoch": 0.026197048292128496,
      "grad_norm": 2.570831537246704,
      "learning_rate": 0.00023807896086139123,
      "loss": 0.7549,
      "step": 6131
    },
    {
      "epoch": 0.026201321175555688,
      "grad_norm": 0.8779126405715942,
      "learning_rate": 0.00023803623312254317,
      "loss": 0.2506,
      "step": 6132
    },
    {
      "epoch": 0.026205594058982883,
      "grad_norm": 3.3805654048919678,
      "learning_rate": 0.0002379935053836951,
      "loss": 0.7782,
      "step": 6133
    },
    {
      "epoch": 0.02620986694241008,
      "grad_norm": 2.2483792304992676,
      "learning_rate": 0.00023795077764484705,
      "loss": 1.1193,
      "step": 6134
    },
    {
      "epoch": 0.02621413982583727,
      "grad_norm": 1.1448768377304077,
      "learning_rate": 0.00023790804990599898,
      "loss": 0.3329,
      "step": 6135
    },
    {
      "epoch": 0.026218412709264467,
      "grad_norm": 1.3320589065551758,
      "learning_rate": 0.00023786532216715092,
      "loss": 0.3895,
      "step": 6136
    },
    {
      "epoch": 0.02622268559269166,
      "grad_norm": 1.622542381286621,
      "learning_rate": 0.00023782259442830286,
      "loss": 0.5075,
      "step": 6137
    },
    {
      "epoch": 0.026226958476118854,
      "grad_norm": 1.2163971662521362,
      "learning_rate": 0.0002377798666894548,
      "loss": 0.4688,
      "step": 6138
    },
    {
      "epoch": 0.02623123135954605,
      "grad_norm": 1.9149284362792969,
      "learning_rate": 0.00023773713895060676,
      "loss": 1.4003,
      "step": 6139
    },
    {
      "epoch": 0.026235504242973242,
      "grad_norm": 0.8550453186035156,
      "learning_rate": 0.00023769441121175867,
      "loss": 0.4357,
      "step": 6140
    },
    {
      "epoch": 0.026239777126400438,
      "grad_norm": 1.3161989450454712,
      "learning_rate": 0.0002376516834729106,
      "loss": 0.404,
      "step": 6141
    },
    {
      "epoch": 0.026244050009827633,
      "grad_norm": 1.4196230173110962,
      "learning_rate": 0.00023760895573406255,
      "loss": 0.2763,
      "step": 6142
    },
    {
      "epoch": 0.026248322893254825,
      "grad_norm": 0.8357900977134705,
      "learning_rate": 0.00023756622799521449,
      "loss": 0.4452,
      "step": 6143
    },
    {
      "epoch": 0.02625259577668202,
      "grad_norm": 3.3812243938446045,
      "learning_rate": 0.00023752350025636642,
      "loss": 1.1276,
      "step": 6144
    },
    {
      "epoch": 0.026256868660109216,
      "grad_norm": 0.7824192643165588,
      "learning_rate": 0.00023748077251751836,
      "loss": 0.4142,
      "step": 6145
    },
    {
      "epoch": 0.02626114154353641,
      "grad_norm": 0.7114823460578918,
      "learning_rate": 0.00023743804477867033,
      "loss": 0.2518,
      "step": 6146
    },
    {
      "epoch": 0.026265414426963604,
      "grad_norm": 1.03121018409729,
      "learning_rate": 0.00023739531703982226,
      "loss": 0.4328,
      "step": 6147
    },
    {
      "epoch": 0.026269687310390796,
      "grad_norm": 3.4911866188049316,
      "learning_rate": 0.0002373525893009742,
      "loss": 0.7775,
      "step": 6148
    },
    {
      "epoch": 0.026273960193817992,
      "grad_norm": 1.0697063207626343,
      "learning_rate": 0.00023730986156212614,
      "loss": 0.4516,
      "step": 6149
    },
    {
      "epoch": 0.026278233077245187,
      "grad_norm": 1.2338522672653198,
      "learning_rate": 0.00023726713382327808,
      "loss": 0.5666,
      "step": 6150
    },
    {
      "epoch": 0.02628250596067238,
      "grad_norm": 2.060194492340088,
      "learning_rate": 0.00023722440608443002,
      "loss": 1.1291,
      "step": 6151
    },
    {
      "epoch": 0.026286778844099575,
      "grad_norm": 1.200653076171875,
      "learning_rate": 0.00023718167834558195,
      "loss": 0.5364,
      "step": 6152
    },
    {
      "epoch": 0.02629105172752677,
      "grad_norm": 0.8770164847373962,
      "learning_rate": 0.0002371389506067339,
      "loss": 0.2714,
      "step": 6153
    },
    {
      "epoch": 0.026295324610953963,
      "grad_norm": 4.3986005783081055,
      "learning_rate": 0.00023709622286788586,
      "loss": 0.933,
      "step": 6154
    },
    {
      "epoch": 0.02629959749438116,
      "grad_norm": 3.0339019298553467,
      "learning_rate": 0.0002370534951290378,
      "loss": 1.8481,
      "step": 6155
    },
    {
      "epoch": 0.026303870377808354,
      "grad_norm": 2.0407540798187256,
      "learning_rate": 0.00023701076739018973,
      "loss": 0.86,
      "step": 6156
    },
    {
      "epoch": 0.026308143261235546,
      "grad_norm": 0.5148972272872925,
      "learning_rate": 0.00023696803965134164,
      "loss": 0.209,
      "step": 6157
    },
    {
      "epoch": 0.026312416144662742,
      "grad_norm": 2.020535469055176,
      "learning_rate": 0.00023692531191249358,
      "loss": 1.0981,
      "step": 6158
    },
    {
      "epoch": 0.026316689028089937,
      "grad_norm": 4.571686267852783,
      "learning_rate": 0.00023688258417364552,
      "loss": 1.2136,
      "step": 6159
    },
    {
      "epoch": 0.02632096191151713,
      "grad_norm": 0.8275994062423706,
      "learning_rate": 0.00023683985643479746,
      "loss": 0.2825,
      "step": 6160
    },
    {
      "epoch": 0.026325234794944325,
      "grad_norm": 0.7257040739059448,
      "learning_rate": 0.00023679712869594942,
      "loss": 0.427,
      "step": 6161
    },
    {
      "epoch": 0.026329507678371517,
      "grad_norm": 2.8938958644866943,
      "learning_rate": 0.00023675440095710136,
      "loss": 0.9049,
      "step": 6162
    },
    {
      "epoch": 0.026333780561798713,
      "grad_norm": 3.16984486579895,
      "learning_rate": 0.0002367116732182533,
      "loss": 0.8903,
      "step": 6163
    },
    {
      "epoch": 0.02633805344522591,
      "grad_norm": 0.7212647795677185,
      "learning_rate": 0.00023666894547940524,
      "loss": 0.4269,
      "step": 6164
    },
    {
      "epoch": 0.0263423263286531,
      "grad_norm": 2.821258306503296,
      "learning_rate": 0.00023662621774055717,
      "loss": 0.846,
      "step": 6165
    },
    {
      "epoch": 0.026346599212080296,
      "grad_norm": 0.7366151213645935,
      "learning_rate": 0.0002365834900017091,
      "loss": 0.4354,
      "step": 6166
    },
    {
      "epoch": 0.02635087209550749,
      "grad_norm": 1.6728651523590088,
      "learning_rate": 0.00023654076226286105,
      "loss": 1.3099,
      "step": 6167
    },
    {
      "epoch": 0.026355144978934684,
      "grad_norm": 0.8381168246269226,
      "learning_rate": 0.00023649803452401301,
      "loss": 0.262,
      "step": 6168
    },
    {
      "epoch": 0.02635941786236188,
      "grad_norm": 0.941211462020874,
      "learning_rate": 0.00023645530678516495,
      "loss": 0.4518,
      "step": 6169
    },
    {
      "epoch": 0.026363690745789075,
      "grad_norm": 3.9765193462371826,
      "learning_rate": 0.0002364125790463169,
      "loss": 0.7491,
      "step": 6170
    },
    {
      "epoch": 0.026367963629216267,
      "grad_norm": 4.376814365386963,
      "learning_rate": 0.00023636985130746883,
      "loss": 2.4676,
      "step": 6171
    },
    {
      "epoch": 0.026372236512643463,
      "grad_norm": 1.900071144104004,
      "learning_rate": 0.00023632712356862077,
      "loss": 1.0255,
      "step": 6172
    },
    {
      "epoch": 0.026376509396070655,
      "grad_norm": 0.8061412572860718,
      "learning_rate": 0.00023628439582977268,
      "loss": 0.2719,
      "step": 6173
    },
    {
      "epoch": 0.02638078227949785,
      "grad_norm": 1.6233775615692139,
      "learning_rate": 0.00023624166809092461,
      "loss": 1.3203,
      "step": 6174
    },
    {
      "epoch": 0.026385055162925046,
      "grad_norm": 0.5215272307395935,
      "learning_rate": 0.00023619894035207655,
      "loss": 0.2113,
      "step": 6175
    },
    {
      "epoch": 0.026389328046352238,
      "grad_norm": 1.4567036628723145,
      "learning_rate": 0.00023615621261322852,
      "loss": 0.576,
      "step": 6176
    },
    {
      "epoch": 0.026393600929779434,
      "grad_norm": 0.7273297905921936,
      "learning_rate": 0.00023611348487438045,
      "loss": 0.4074,
      "step": 6177
    },
    {
      "epoch": 0.02639787381320663,
      "grad_norm": 0.7321613430976868,
      "learning_rate": 0.0002360707571355324,
      "loss": 0.3235,
      "step": 6178
    },
    {
      "epoch": 0.02640214669663382,
      "grad_norm": 0.7270848155021667,
      "learning_rate": 0.00023602802939668433,
      "loss": 0.263,
      "step": 6179
    },
    {
      "epoch": 0.026406419580061017,
      "grad_norm": 1.8052674531936646,
      "learning_rate": 0.00023598530165783627,
      "loss": 1.0228,
      "step": 6180
    },
    {
      "epoch": 0.026410692463488213,
      "grad_norm": 0.3538535535335541,
      "learning_rate": 0.0002359425739189882,
      "loss": 0.1049,
      "step": 6181
    },
    {
      "epoch": 0.026414965346915405,
      "grad_norm": 1.618369221687317,
      "learning_rate": 0.00023589984618014014,
      "loss": 0.5721,
      "step": 6182
    },
    {
      "epoch": 0.0264192382303426,
      "grad_norm": 1.2880949974060059,
      "learning_rate": 0.0002358571184412921,
      "loss": 0.703,
      "step": 6183
    },
    {
      "epoch": 0.026423511113769792,
      "grad_norm": 1.4944870471954346,
      "learning_rate": 0.00023581439070244405,
      "loss": 1.203,
      "step": 6184
    },
    {
      "epoch": 0.026427783997196988,
      "grad_norm": 0.8970023393630981,
      "learning_rate": 0.00023577166296359598,
      "loss": 0.316,
      "step": 6185
    },
    {
      "epoch": 0.026432056880624184,
      "grad_norm": 0.4675264060497284,
      "learning_rate": 0.00023572893522474792,
      "loss": 0.16,
      "step": 6186
    },
    {
      "epoch": 0.026436329764051376,
      "grad_norm": 2.057612419128418,
      "learning_rate": 0.00023568620748589986,
      "loss": 0.8495,
      "step": 6187
    },
    {
      "epoch": 0.02644060264747857,
      "grad_norm": 1.746649980545044,
      "learning_rate": 0.0002356434797470518,
      "loss": 0.9357,
      "step": 6188
    },
    {
      "epoch": 0.026444875530905767,
      "grad_norm": 0.9280021786689758,
      "learning_rate": 0.0002356007520082037,
      "loss": 0.3017,
      "step": 6189
    },
    {
      "epoch": 0.02644914841433296,
      "grad_norm": 1.7154597043991089,
      "learning_rate": 0.00023555802426935565,
      "loss": 0.441,
      "step": 6190
    },
    {
      "epoch": 0.026453421297760155,
      "grad_norm": 0.7786876559257507,
      "learning_rate": 0.0002355152965305076,
      "loss": 0.4356,
      "step": 6191
    },
    {
      "epoch": 0.02645769418118735,
      "grad_norm": 1.1207932233810425,
      "learning_rate": 0.00023547256879165955,
      "loss": 0.5717,
      "step": 6192
    },
    {
      "epoch": 0.026461967064614542,
      "grad_norm": 2.2702856063842773,
      "learning_rate": 0.0002354298410528115,
      "loss": 0.7359,
      "step": 6193
    },
    {
      "epoch": 0.026466239948041738,
      "grad_norm": 1.3323725461959839,
      "learning_rate": 0.00023538711331396342,
      "loss": 0.6866,
      "step": 6194
    },
    {
      "epoch": 0.026470512831468933,
      "grad_norm": 1.2609385251998901,
      "learning_rate": 0.00023534438557511536,
      "loss": 0.5847,
      "step": 6195
    },
    {
      "epoch": 0.026474785714896126,
      "grad_norm": 1.1512305736541748,
      "learning_rate": 0.0002353016578362673,
      "loss": 0.5512,
      "step": 6196
    },
    {
      "epoch": 0.02647905859832332,
      "grad_norm": 2.34980845451355,
      "learning_rate": 0.00023525893009741924,
      "loss": 0.8878,
      "step": 6197
    },
    {
      "epoch": 0.026483331481750513,
      "grad_norm": 1.7104840278625488,
      "learning_rate": 0.0002352162023585712,
      "loss": 0.6001,
      "step": 6198
    },
    {
      "epoch": 0.02648760436517771,
      "grad_norm": 1.7799934148788452,
      "learning_rate": 0.00023517347461972314,
      "loss": 0.9163,
      "step": 6199
    },
    {
      "epoch": 0.026491877248604905,
      "grad_norm": 4.0636725425720215,
      "learning_rate": 0.00023513074688087508,
      "loss": 1.3155,
      "step": 6200
    },
    {
      "epoch": 0.026496150132032097,
      "grad_norm": 0.8070932626724243,
      "learning_rate": 0.00023508801914202702,
      "loss": 0.3641,
      "step": 6201
    },
    {
      "epoch": 0.026500423015459292,
      "grad_norm": 1.0586084127426147,
      "learning_rate": 0.00023504529140317895,
      "loss": 0.3528,
      "step": 6202
    },
    {
      "epoch": 0.026504695898886488,
      "grad_norm": 0.5895565152168274,
      "learning_rate": 0.0002350025636643309,
      "loss": 0.23,
      "step": 6203
    },
    {
      "epoch": 0.02650896878231368,
      "grad_norm": 3.0697765350341797,
      "learning_rate": 0.00023495983592548283,
      "loss": 1.1555,
      "step": 6204
    },
    {
      "epoch": 0.026513241665740876,
      "grad_norm": 4.496343612670898,
      "learning_rate": 0.00023491710818663477,
      "loss": 1.3537,
      "step": 6205
    },
    {
      "epoch": 0.02651751454916807,
      "grad_norm": 0.9084194302558899,
      "learning_rate": 0.0002348743804477867,
      "loss": 0.4304,
      "step": 6206
    },
    {
      "epoch": 0.026521787432595263,
      "grad_norm": 4.467215538024902,
      "learning_rate": 0.00023483165270893864,
      "loss": 1.2586,
      "step": 6207
    },
    {
      "epoch": 0.02652606031602246,
      "grad_norm": 1.6922521591186523,
      "learning_rate": 0.00023478892497009058,
      "loss": 0.7269,
      "step": 6208
    },
    {
      "epoch": 0.02653033319944965,
      "grad_norm": 1.6891255378723145,
      "learning_rate": 0.00023474619723124252,
      "loss": 0.7166,
      "step": 6209
    },
    {
      "epoch": 0.026534606082876847,
      "grad_norm": 2.858992338180542,
      "learning_rate": 0.00023470346949239446,
      "loss": 0.8434,
      "step": 6210
    },
    {
      "epoch": 0.026538878966304042,
      "grad_norm": 0.6714669466018677,
      "learning_rate": 0.0002346607417535464,
      "loss": 0.2536,
      "step": 6211
    },
    {
      "epoch": 0.026543151849731234,
      "grad_norm": 0.995840311050415,
      "learning_rate": 0.00023461801401469833,
      "loss": 0.3523,
      "step": 6212
    },
    {
      "epoch": 0.02654742473315843,
      "grad_norm": 1.772266149520874,
      "learning_rate": 0.0002345752862758503,
      "loss": 0.6782,
      "step": 6213
    },
    {
      "epoch": 0.026551697616585625,
      "grad_norm": 2.2332286834716797,
      "learning_rate": 0.00023453255853700224,
      "loss": 0.6831,
      "step": 6214
    },
    {
      "epoch": 0.026555970500012818,
      "grad_norm": 1.6721014976501465,
      "learning_rate": 0.00023448983079815417,
      "loss": 0.8147,
      "step": 6215
    },
    {
      "epoch": 0.026560243383440013,
      "grad_norm": 1.9680410623550415,
      "learning_rate": 0.0002344471030593061,
      "loss": 0.7277,
      "step": 6216
    },
    {
      "epoch": 0.02656451626686721,
      "grad_norm": 1.4322125911712646,
      "learning_rate": 0.00023440437532045805,
      "loss": 0.329,
      "step": 6217
    },
    {
      "epoch": 0.0265687891502944,
      "grad_norm": 1.4220999479293823,
      "learning_rate": 0.00023436164758161,
      "loss": 0.324,
      "step": 6218
    },
    {
      "epoch": 0.026573062033721596,
      "grad_norm": 2.074526071548462,
      "learning_rate": 0.00023431891984276192,
      "loss": 0.5917,
      "step": 6219
    },
    {
      "epoch": 0.026577334917148792,
      "grad_norm": 1.3336092233657837,
      "learning_rate": 0.0002342761921039139,
      "loss": 0.4753,
      "step": 6220
    },
    {
      "epoch": 0.026581607800575984,
      "grad_norm": 1.7204618453979492,
      "learning_rate": 0.00023423346436506583,
      "loss": 0.5389,
      "step": 6221
    },
    {
      "epoch": 0.02658588068400318,
      "grad_norm": 0.661971390247345,
      "learning_rate": 0.00023419073662621774,
      "loss": 0.2538,
      "step": 6222
    },
    {
      "epoch": 0.026590153567430372,
      "grad_norm": 3.973579168319702,
      "learning_rate": 0.00023414800888736968,
      "loss": 1.3642,
      "step": 6223
    },
    {
      "epoch": 0.026594426450857567,
      "grad_norm": 2.084723711013794,
      "learning_rate": 0.00023410528114852161,
      "loss": 0.6582,
      "step": 6224
    },
    {
      "epoch": 0.026598699334284763,
      "grad_norm": 3.8075919151306152,
      "learning_rate": 0.00023406255340967355,
      "loss": 1.2601,
      "step": 6225
    },
    {
      "epoch": 0.026602972217711955,
      "grad_norm": 2.1446313858032227,
      "learning_rate": 0.0002340198256708255,
      "loss": 0.5786,
      "step": 6226
    },
    {
      "epoch": 0.02660724510113915,
      "grad_norm": 1.2020474672317505,
      "learning_rate": 0.00023397709793197743,
      "loss": 0.4188,
      "step": 6227
    },
    {
      "epoch": 0.026611517984566346,
      "grad_norm": 2.0565500259399414,
      "learning_rate": 0.0002339343701931294,
      "loss": 0.8584,
      "step": 6228
    },
    {
      "epoch": 0.02661579086799354,
      "grad_norm": 2.9533324241638184,
      "learning_rate": 0.00023389164245428133,
      "loss": 0.9623,
      "step": 6229
    },
    {
      "epoch": 0.026620063751420734,
      "grad_norm": 3.796217203140259,
      "learning_rate": 0.00023384891471543327,
      "loss": 1.1816,
      "step": 6230
    },
    {
      "epoch": 0.02662433663484793,
      "grad_norm": 0.6798263788223267,
      "learning_rate": 0.0002338061869765852,
      "loss": 0.2112,
      "step": 6231
    },
    {
      "epoch": 0.026628609518275122,
      "grad_norm": 0.983910322189331,
      "learning_rate": 0.00023376345923773714,
      "loss": 0.4159,
      "step": 6232
    },
    {
      "epoch": 0.026632882401702317,
      "grad_norm": 1.8959064483642578,
      "learning_rate": 0.00023372073149888908,
      "loss": 0.6021,
      "step": 6233
    },
    {
      "epoch": 0.02663715528512951,
      "grad_norm": 1.5285075902938843,
      "learning_rate": 0.00023367800376004102,
      "loss": 0.7925,
      "step": 6234
    },
    {
      "epoch": 0.026641428168556705,
      "grad_norm": 0.9636269807815552,
      "learning_rate": 0.00023363527602119298,
      "loss": 0.29,
      "step": 6235
    },
    {
      "epoch": 0.0266457010519839,
      "grad_norm": 0.8935514092445374,
      "learning_rate": 0.00023359254828234492,
      "loss": 0.2645,
      "step": 6236
    },
    {
      "epoch": 0.026649973935411093,
      "grad_norm": 3.4151601791381836,
      "learning_rate": 0.00023354982054349686,
      "loss": 1.7555,
      "step": 6237
    },
    {
      "epoch": 0.02665424681883829,
      "grad_norm": 1.4818156957626343,
      "learning_rate": 0.0002335070928046488,
      "loss": 0.7619,
      "step": 6238
    },
    {
      "epoch": 0.026658519702265484,
      "grad_norm": 2.94545316696167,
      "learning_rate": 0.0002334643650658007,
      "loss": 0.923,
      "step": 6239
    },
    {
      "epoch": 0.026662792585692676,
      "grad_norm": 2.1336939334869385,
      "learning_rate": 0.00023342163732695265,
      "loss": 0.8708,
      "step": 6240
    },
    {
      "epoch": 0.02666706546911987,
      "grad_norm": 1.819502592086792,
      "learning_rate": 0.00023337890958810458,
      "loss": 0.6615,
      "step": 6241
    },
    {
      "epoch": 0.026671338352547067,
      "grad_norm": 0.8940148949623108,
      "learning_rate": 0.00023333618184925652,
      "loss": 0.3648,
      "step": 6242
    },
    {
      "epoch": 0.02667561123597426,
      "grad_norm": 3.4028987884521484,
      "learning_rate": 0.0002332934541104085,
      "loss": 1.7453,
      "step": 6243
    },
    {
      "epoch": 0.026679884119401455,
      "grad_norm": 1.0785400867462158,
      "learning_rate": 0.00023325072637156043,
      "loss": 0.2876,
      "step": 6244
    },
    {
      "epoch": 0.02668415700282865,
      "grad_norm": 1.4842660427093506,
      "learning_rate": 0.00023320799863271236,
      "loss": 0.4839,
      "step": 6245
    },
    {
      "epoch": 0.026688429886255843,
      "grad_norm": 1.2890784740447998,
      "learning_rate": 0.0002331652708938643,
      "loss": 0.7305,
      "step": 6246
    },
    {
      "epoch": 0.026692702769683038,
      "grad_norm": 0.9497275352478027,
      "learning_rate": 0.00023312254315501624,
      "loss": 0.294,
      "step": 6247
    },
    {
      "epoch": 0.02669697565311023,
      "grad_norm": 0.7946103811264038,
      "learning_rate": 0.00023307981541616818,
      "loss": 0.2593,
      "step": 6248
    },
    {
      "epoch": 0.026701248536537426,
      "grad_norm": 3.29742693901062,
      "learning_rate": 0.00023303708767732011,
      "loss": 1.6719,
      "step": 6249
    },
    {
      "epoch": 0.02670552141996462,
      "grad_norm": 0.5389488339424133,
      "learning_rate": 0.00023299435993847208,
      "loss": 0.1594,
      "step": 6250
    },
    {
      "epoch": 0.026709794303391814,
      "grad_norm": 0.8405171036720276,
      "learning_rate": 0.00023295163219962402,
      "loss": 0.2784,
      "step": 6251
    },
    {
      "epoch": 0.02671406718681901,
      "grad_norm": 3.6533026695251465,
      "learning_rate": 0.00023290890446077596,
      "loss": 1.0078,
      "step": 6252
    },
    {
      "epoch": 0.026718340070246205,
      "grad_norm": 1.4688262939453125,
      "learning_rate": 0.0002328661767219279,
      "loss": 0.601,
      "step": 6253
    },
    {
      "epoch": 0.026722612953673397,
      "grad_norm": 1.7829145193099976,
      "learning_rate": 0.00023282344898307983,
      "loss": 0.4907,
      "step": 6254
    },
    {
      "epoch": 0.026726885837100593,
      "grad_norm": 1.6166244745254517,
      "learning_rate": 0.00023278072124423174,
      "loss": 0.9703,
      "step": 6255
    },
    {
      "epoch": 0.026731158720527788,
      "grad_norm": 4.421103477478027,
      "learning_rate": 0.00023273799350538368,
      "loss": 1.1372,
      "step": 6256
    },
    {
      "epoch": 0.02673543160395498,
      "grad_norm": 0.8362006545066833,
      "learning_rate": 0.00023269526576653562,
      "loss": 0.4198,
      "step": 6257
    },
    {
      "epoch": 0.026739704487382176,
      "grad_norm": 0.773084819316864,
      "learning_rate": 0.00023265253802768758,
      "loss": 0.2391,
      "step": 6258
    },
    {
      "epoch": 0.026743977370809368,
      "grad_norm": 1.1078312397003174,
      "learning_rate": 0.00023260981028883952,
      "loss": 0.4185,
      "step": 6259
    },
    {
      "epoch": 0.026748250254236564,
      "grad_norm": 1.0034563541412354,
      "learning_rate": 0.00023256708254999146,
      "loss": 0.3265,
      "step": 6260
    },
    {
      "epoch": 0.02675252313766376,
      "grad_norm": 1.8632006645202637,
      "learning_rate": 0.0002325243548111434,
      "loss": 0.6683,
      "step": 6261
    },
    {
      "epoch": 0.02675679602109095,
      "grad_norm": 1.2179346084594727,
      "learning_rate": 0.00023248162707229533,
      "loss": 0.2715,
      "step": 6262
    },
    {
      "epoch": 0.026761068904518147,
      "grad_norm": 0.8394402265548706,
      "learning_rate": 0.00023243889933344727,
      "loss": 0.2376,
      "step": 6263
    },
    {
      "epoch": 0.026765341787945342,
      "grad_norm": 2.766354560852051,
      "learning_rate": 0.0002323961715945992,
      "loss": 0.7918,
      "step": 6264
    },
    {
      "epoch": 0.026769614671372535,
      "grad_norm": 1.190225601196289,
      "learning_rate": 0.00023235344385575117,
      "loss": 0.2497,
      "step": 6265
    },
    {
      "epoch": 0.02677388755479973,
      "grad_norm": 0.8660884499549866,
      "learning_rate": 0.0002323107161169031,
      "loss": 0.429,
      "step": 6266
    },
    {
      "epoch": 0.026778160438226926,
      "grad_norm": 1.4212238788604736,
      "learning_rate": 0.00023226798837805505,
      "loss": 0.6327,
      "step": 6267
    },
    {
      "epoch": 0.026782433321654118,
      "grad_norm": 3.2191367149353027,
      "learning_rate": 0.000232225260639207,
      "loss": 1.6512,
      "step": 6268
    },
    {
      "epoch": 0.026786706205081313,
      "grad_norm": 1.0781596899032593,
      "learning_rate": 0.00023218253290035893,
      "loss": 0.6512,
      "step": 6269
    },
    {
      "epoch": 0.02679097908850851,
      "grad_norm": 2.6682379245758057,
      "learning_rate": 0.00023213980516151086,
      "loss": 0.722,
      "step": 6270
    },
    {
      "epoch": 0.0267952519719357,
      "grad_norm": 1.6363695859909058,
      "learning_rate": 0.0002320970774226628,
      "loss": 0.4959,
      "step": 6271
    },
    {
      "epoch": 0.026799524855362897,
      "grad_norm": 1.1545957326889038,
      "learning_rate": 0.0002320543496838147,
      "loss": 0.4145,
      "step": 6272
    },
    {
      "epoch": 0.02680379773879009,
      "grad_norm": 2.273977041244507,
      "learning_rate": 0.00023201162194496668,
      "loss": 0.5985,
      "step": 6273
    },
    {
      "epoch": 0.026808070622217284,
      "grad_norm": 0.8020696640014648,
      "learning_rate": 0.00023196889420611861,
      "loss": 0.3232,
      "step": 6274
    },
    {
      "epoch": 0.02681234350564448,
      "grad_norm": 4.0801849365234375,
      "learning_rate": 0.00023192616646727055,
      "loss": 1.7576,
      "step": 6275
    },
    {
      "epoch": 0.026816616389071672,
      "grad_norm": 0.6678780317306519,
      "learning_rate": 0.0002318834387284225,
      "loss": 0.2383,
      "step": 6276
    },
    {
      "epoch": 0.026820889272498868,
      "grad_norm": 4.407714366912842,
      "learning_rate": 0.00023184071098957443,
      "loss": 1.4545,
      "step": 6277
    },
    {
      "epoch": 0.026825162155926063,
      "grad_norm": 3.272167205810547,
      "learning_rate": 0.00023179798325072637,
      "loss": 1.3269,
      "step": 6278
    },
    {
      "epoch": 0.026829435039353255,
      "grad_norm": 3.509234666824341,
      "learning_rate": 0.0002317552555118783,
      "loss": 0.9371,
      "step": 6279
    },
    {
      "epoch": 0.02683370792278045,
      "grad_norm": 2.90576434135437,
      "learning_rate": 0.00023171252777303027,
      "loss": 1.0875,
      "step": 6280
    },
    {
      "epoch": 0.026837980806207647,
      "grad_norm": 3.384810447692871,
      "learning_rate": 0.0002316698000341822,
      "loss": 0.8432,
      "step": 6281
    },
    {
      "epoch": 0.02684225368963484,
      "grad_norm": 0.7274770736694336,
      "learning_rate": 0.00023162707229533414,
      "loss": 0.2509,
      "step": 6282
    },
    {
      "epoch": 0.026846526573062034,
      "grad_norm": 1.634764552116394,
      "learning_rate": 0.00023158434455648608,
      "loss": 0.6255,
      "step": 6283
    },
    {
      "epoch": 0.026850799456489226,
      "grad_norm": 1.5039957761764526,
      "learning_rate": 0.00023154161681763802,
      "loss": 1.2347,
      "step": 6284
    },
    {
      "epoch": 0.026855072339916422,
      "grad_norm": 1.5030182600021362,
      "learning_rate": 0.00023149888907878996,
      "loss": 1.225,
      "step": 6285
    },
    {
      "epoch": 0.026859345223343618,
      "grad_norm": 1.0723668336868286,
      "learning_rate": 0.0002314561613399419,
      "loss": 0.3046,
      "step": 6286
    },
    {
      "epoch": 0.02686361810677081,
      "grad_norm": 5.184942722320557,
      "learning_rate": 0.00023141343360109386,
      "loss": 3.0758,
      "step": 6287
    },
    {
      "epoch": 0.026867890990198005,
      "grad_norm": 1.9394524097442627,
      "learning_rate": 0.00023137070586224577,
      "loss": 0.7745,
      "step": 6288
    },
    {
      "epoch": 0.0268721638736252,
      "grad_norm": 0.44681882858276367,
      "learning_rate": 0.0002313279781233977,
      "loss": 0.1488,
      "step": 6289
    },
    {
      "epoch": 0.026876436757052393,
      "grad_norm": 2.433006763458252,
      "learning_rate": 0.00023128525038454965,
      "loss": 0.6388,
      "step": 6290
    },
    {
      "epoch": 0.02688070964047959,
      "grad_norm": 1.0503413677215576,
      "learning_rate": 0.00023124252264570159,
      "loss": 0.6511,
      "step": 6291
    },
    {
      "epoch": 0.026884982523906784,
      "grad_norm": 0.6803025603294373,
      "learning_rate": 0.00023119979490685352,
      "loss": 0.2378,
      "step": 6292
    },
    {
      "epoch": 0.026889255407333976,
      "grad_norm": 1.7356281280517578,
      "learning_rate": 0.00023115706716800546,
      "loss": 0.7012,
      "step": 6293
    },
    {
      "epoch": 0.026893528290761172,
      "grad_norm": 3.7882192134857178,
      "learning_rate": 0.0002311143394291574,
      "loss": 1.6433,
      "step": 6294
    },
    {
      "epoch": 0.026897801174188364,
      "grad_norm": 3.6544599533081055,
      "learning_rate": 0.00023107161169030936,
      "loss": 1.3958,
      "step": 6295
    },
    {
      "epoch": 0.02690207405761556,
      "grad_norm": 2.4576022624969482,
      "learning_rate": 0.0002310288839514613,
      "loss": 0.7888,
      "step": 6296
    },
    {
      "epoch": 0.026906346941042755,
      "grad_norm": 3.7278733253479004,
      "learning_rate": 0.00023098615621261324,
      "loss": 1.4682,
      "step": 6297
    },
    {
      "epoch": 0.026910619824469947,
      "grad_norm": 2.816537618637085,
      "learning_rate": 0.00023094342847376518,
      "loss": 0.899,
      "step": 6298
    },
    {
      "epoch": 0.026914892707897143,
      "grad_norm": 0.4730430543422699,
      "learning_rate": 0.00023090070073491711,
      "loss": 0.1489,
      "step": 6299
    },
    {
      "epoch": 0.02691916559132434,
      "grad_norm": 1.3677253723144531,
      "learning_rate": 0.00023085797299606905,
      "loss": 1.1032,
      "step": 6300
    },
    {
      "epoch": 0.02692343847475153,
      "grad_norm": 1.5115336179733276,
      "learning_rate": 0.000230815245257221,
      "loss": 0.5711,
      "step": 6301
    },
    {
      "epoch": 0.026927711358178726,
      "grad_norm": 1.5589555501937866,
      "learning_rate": 0.00023077251751837296,
      "loss": 0.5863,
      "step": 6302
    },
    {
      "epoch": 0.026931984241605922,
      "grad_norm": 3.735593795776367,
      "learning_rate": 0.0002307297897795249,
      "loss": 1.2293,
      "step": 6303
    },
    {
      "epoch": 0.026936257125033114,
      "grad_norm": 2.967665195465088,
      "learning_rate": 0.0002306870620406768,
      "loss": 0.8456,
      "step": 6304
    },
    {
      "epoch": 0.02694053000846031,
      "grad_norm": 1.5421432256698608,
      "learning_rate": 0.00023064433430182874,
      "loss": 1.0094,
      "step": 6305
    },
    {
      "epoch": 0.026944802891887505,
      "grad_norm": 1.5034763813018799,
      "learning_rate": 0.00023060160656298068,
      "loss": 0.9568,
      "step": 6306
    },
    {
      "epoch": 0.026949075775314697,
      "grad_norm": 1.575619101524353,
      "learning_rate": 0.00023055887882413262,
      "loss": 0.6405,
      "step": 6307
    },
    {
      "epoch": 0.026953348658741893,
      "grad_norm": 3.628211498260498,
      "learning_rate": 0.00023051615108528456,
      "loss": 1.3744,
      "step": 6308
    },
    {
      "epoch": 0.026957621542169085,
      "grad_norm": 2.3712799549102783,
      "learning_rate": 0.0002304734233464365,
      "loss": 0.6533,
      "step": 6309
    },
    {
      "epoch": 0.02696189442559628,
      "grad_norm": 3.78092098236084,
      "learning_rate": 0.00023043069560758846,
      "loss": 1.1038,
      "step": 6310
    },
    {
      "epoch": 0.026966167309023476,
      "grad_norm": 0.9988676905632019,
      "learning_rate": 0.0002303879678687404,
      "loss": 0.5366,
      "step": 6311
    },
    {
      "epoch": 0.02697044019245067,
      "grad_norm": 1.4747438430786133,
      "learning_rate": 0.00023034524012989233,
      "loss": 0.5741,
      "step": 6312
    },
    {
      "epoch": 0.026974713075877864,
      "grad_norm": 3.481194257736206,
      "learning_rate": 0.00023030251239104427,
      "loss": 1.1379,
      "step": 6313
    },
    {
      "epoch": 0.02697898595930506,
      "grad_norm": 0.9407263398170471,
      "learning_rate": 0.0002302597846521962,
      "loss": 0.5235,
      "step": 6314
    },
    {
      "epoch": 0.02698325884273225,
      "grad_norm": 4.379743576049805,
      "learning_rate": 0.00023021705691334815,
      "loss": 2.0346,
      "step": 6315
    },
    {
      "epoch": 0.026987531726159447,
      "grad_norm": 1.033913016319275,
      "learning_rate": 0.00023017432917450009,
      "loss": 0.4804,
      "step": 6316
    },
    {
      "epoch": 0.026991804609586643,
      "grad_norm": 0.6949605345726013,
      "learning_rate": 0.00023013160143565205,
      "loss": 0.2177,
      "step": 6317
    },
    {
      "epoch": 0.026996077493013835,
      "grad_norm": 1.3673064708709717,
      "learning_rate": 0.000230088873696804,
      "loss": 0.261,
      "step": 6318
    },
    {
      "epoch": 0.02700035037644103,
      "grad_norm": 0.6861377358436584,
      "learning_rate": 0.00023004614595795593,
      "loss": 0.2181,
      "step": 6319
    },
    {
      "epoch": 0.027004623259868223,
      "grad_norm": 1.3727664947509766,
      "learning_rate": 0.00023000341821910786,
      "loss": 1.0586,
      "step": 6320
    },
    {
      "epoch": 0.027008896143295418,
      "grad_norm": 0.7239595651626587,
      "learning_rate": 0.00022996069048025977,
      "loss": 0.2722,
      "step": 6321
    },
    {
      "epoch": 0.027013169026722614,
      "grad_norm": 3.8355064392089844,
      "learning_rate": 0.0002299179627414117,
      "loss": 0.9016,
      "step": 6322
    },
    {
      "epoch": 0.027017441910149806,
      "grad_norm": 0.9274864792823792,
      "learning_rate": 0.00022987523500256365,
      "loss": 0.2724,
      "step": 6323
    },
    {
      "epoch": 0.027021714793577,
      "grad_norm": 1.7139068841934204,
      "learning_rate": 0.0002298325072637156,
      "loss": 0.6591,
      "step": 6324
    },
    {
      "epoch": 0.027025987677004197,
      "grad_norm": 2.7838134765625,
      "learning_rate": 0.00022978977952486755,
      "loss": 0.9223,
      "step": 6325
    },
    {
      "epoch": 0.02703026056043139,
      "grad_norm": 1.062146782875061,
      "learning_rate": 0.0002297470517860195,
      "loss": 0.4168,
      "step": 6326
    },
    {
      "epoch": 0.027034533443858585,
      "grad_norm": 3.284090757369995,
      "learning_rate": 0.00022970432404717143,
      "loss": 1.1339,
      "step": 6327
    },
    {
      "epoch": 0.02703880632728578,
      "grad_norm": 3.9614927768707275,
      "learning_rate": 0.00022966159630832337,
      "loss": 1.0574,
      "step": 6328
    },
    {
      "epoch": 0.027043079210712972,
      "grad_norm": 0.3732130825519562,
      "learning_rate": 0.0002296188685694753,
      "loss": 0.1047,
      "step": 6329
    },
    {
      "epoch": 0.027047352094140168,
      "grad_norm": 1.36371648311615,
      "learning_rate": 0.00022957614083062724,
      "loss": 1.0154,
      "step": 6330
    },
    {
      "epoch": 0.027051624977567364,
      "grad_norm": 0.8769404292106628,
      "learning_rate": 0.00022953341309177918,
      "loss": 0.3852,
      "step": 6331
    },
    {
      "epoch": 0.027055897860994556,
      "grad_norm": 0.9647132754325867,
      "learning_rate": 0.00022949068535293115,
      "loss": 0.3642,
      "step": 6332
    },
    {
      "epoch": 0.02706017074442175,
      "grad_norm": 1.6662365198135376,
      "learning_rate": 0.00022944795761408308,
      "loss": 0.4894,
      "step": 6333
    },
    {
      "epoch": 0.027064443627848944,
      "grad_norm": 3.5112996101379395,
      "learning_rate": 0.00022940522987523502,
      "loss": 0.9403,
      "step": 6334
    },
    {
      "epoch": 0.02706871651127614,
      "grad_norm": 0.712151050567627,
      "learning_rate": 0.00022936250213638696,
      "loss": 0.2545,
      "step": 6335
    },
    {
      "epoch": 0.027072989394703335,
      "grad_norm": 1.3870140314102173,
      "learning_rate": 0.0002293197743975389,
      "loss": 1.0064,
      "step": 6336
    },
    {
      "epoch": 0.027077262278130527,
      "grad_norm": 4.185520648956299,
      "learning_rate": 0.0002292770466586908,
      "loss": 1.1688,
      "step": 6337
    },
    {
      "epoch": 0.027081535161557722,
      "grad_norm": 0.956142783164978,
      "learning_rate": 0.00022923431891984275,
      "loss": 0.3055,
      "step": 6338
    },
    {
      "epoch": 0.027085808044984918,
      "grad_norm": 1.6529732942581177,
      "learning_rate": 0.00022919159118099468,
      "loss": 0.5702,
      "step": 6339
    },
    {
      "epoch": 0.02709008092841211,
      "grad_norm": 3.220942258834839,
      "learning_rate": 0.00022914886344214665,
      "loss": 1.2028,
      "step": 6340
    },
    {
      "epoch": 0.027094353811839306,
      "grad_norm": 0.760707437992096,
      "learning_rate": 0.00022910613570329859,
      "loss": 0.3244,
      "step": 6341
    },
    {
      "epoch": 0.0270986266952665,
      "grad_norm": 0.7204275727272034,
      "learning_rate": 0.00022906340796445052,
      "loss": 0.3245,
      "step": 6342
    },
    {
      "epoch": 0.027102899578693693,
      "grad_norm": 1.1042615175247192,
      "learning_rate": 0.00022902068022560246,
      "loss": 0.5471,
      "step": 6343
    },
    {
      "epoch": 0.02710717246212089,
      "grad_norm": 4.377224445343018,
      "learning_rate": 0.0002289779524867544,
      "loss": 1.3039,
      "step": 6344
    },
    {
      "epoch": 0.02711144534554808,
      "grad_norm": 0.36837080121040344,
      "learning_rate": 0.00022893522474790634,
      "loss": 0.1122,
      "step": 6345
    },
    {
      "epoch": 0.027115718228975277,
      "grad_norm": 0.4388122260570526,
      "learning_rate": 0.00022889249700905827,
      "loss": 0.14,
      "step": 6346
    },
    {
      "epoch": 0.027119991112402472,
      "grad_norm": 1.0469576120376587,
      "learning_rate": 0.00022884976927021024,
      "loss": 0.5285,
      "step": 6347
    },
    {
      "epoch": 0.027124263995829664,
      "grad_norm": 3.7779643535614014,
      "learning_rate": 0.00022880704153136218,
      "loss": 2.2255,
      "step": 6348
    },
    {
      "epoch": 0.02712853687925686,
      "grad_norm": 0.7460213899612427,
      "learning_rate": 0.00022876431379251412,
      "loss": 0.3629,
      "step": 6349
    },
    {
      "epoch": 0.027132809762684056,
      "grad_norm": 2.7514073848724365,
      "learning_rate": 0.00022872158605366605,
      "loss": 1.4373,
      "step": 6350
    },
    {
      "epoch": 0.027137082646111248,
      "grad_norm": 2.5289905071258545,
      "learning_rate": 0.000228678858314818,
      "loss": 0.8971,
      "step": 6351
    },
    {
      "epoch": 0.027141355529538443,
      "grad_norm": 3.673910140991211,
      "learning_rate": 0.00022863613057596993,
      "loss": 0.9212,
      "step": 6352
    },
    {
      "epoch": 0.02714562841296564,
      "grad_norm": 0.5086626410484314,
      "learning_rate": 0.00022859340283712187,
      "loss": 0.1881,
      "step": 6353
    },
    {
      "epoch": 0.02714990129639283,
      "grad_norm": 2.6194632053375244,
      "learning_rate": 0.0002285506750982738,
      "loss": 0.9006,
      "step": 6354
    },
    {
      "epoch": 0.027154174179820027,
      "grad_norm": 1.07497239112854,
      "learning_rate": 0.00022850794735942574,
      "loss": 0.3112,
      "step": 6355
    },
    {
      "epoch": 0.027158447063247222,
      "grad_norm": 1.037360429763794,
      "learning_rate": 0.00022846521962057768,
      "loss": 0.5178,
      "step": 6356
    },
    {
      "epoch": 0.027162719946674414,
      "grad_norm": 0.4402615427970886,
      "learning_rate": 0.00022842249188172962,
      "loss": 0.1529,
      "step": 6357
    },
    {
      "epoch": 0.02716699283010161,
      "grad_norm": 2.362576484680176,
      "learning_rate": 0.00022837976414288156,
      "loss": 0.6425,
      "step": 6358
    },
    {
      "epoch": 0.027171265713528802,
      "grad_norm": 1.7080800533294678,
      "learning_rate": 0.0002283370364040335,
      "loss": 0.6955,
      "step": 6359
    },
    {
      "epoch": 0.027175538596955998,
      "grad_norm": 0.99103182554245,
      "learning_rate": 0.00022829430866518543,
      "loss": 0.4851,
      "step": 6360
    },
    {
      "epoch": 0.027179811480383193,
      "grad_norm": 1.32703697681427,
      "learning_rate": 0.00022825158092633737,
      "loss": 0.8223,
      "step": 6361
    },
    {
      "epoch": 0.027184084363810385,
      "grad_norm": 3.580148220062256,
      "learning_rate": 0.00022820885318748933,
      "loss": 1.1637,
      "step": 6362
    },
    {
      "epoch": 0.02718835724723758,
      "grad_norm": 2.090514659881592,
      "learning_rate": 0.00022816612544864127,
      "loss": 0.5631,
      "step": 6363
    },
    {
      "epoch": 0.027192630130664776,
      "grad_norm": 2.659933090209961,
      "learning_rate": 0.0002281233977097932,
      "loss": 0.8984,
      "step": 6364
    },
    {
      "epoch": 0.02719690301409197,
      "grad_norm": 0.13179194927215576,
      "learning_rate": 0.00022808066997094515,
      "loss": 0.0257,
      "step": 6365
    },
    {
      "epoch": 0.027201175897519164,
      "grad_norm": 2.9311351776123047,
      "learning_rate": 0.00022803794223209709,
      "loss": 0.7133,
      "step": 6366
    },
    {
      "epoch": 0.02720544878094636,
      "grad_norm": 1.2552976608276367,
      "learning_rate": 0.00022799521449324902,
      "loss": 0.7957,
      "step": 6367
    },
    {
      "epoch": 0.027209721664373552,
      "grad_norm": 1.5708787441253662,
      "learning_rate": 0.00022795248675440096,
      "loss": 0.753,
      "step": 6368
    },
    {
      "epoch": 0.027213994547800748,
      "grad_norm": 2.535383939743042,
      "learning_rate": 0.00022790975901555293,
      "loss": 0.804,
      "step": 6369
    },
    {
      "epoch": 0.02721826743122794,
      "grad_norm": 2.6703662872314453,
      "learning_rate": 0.00022786703127670484,
      "loss": 0.7527,
      "step": 6370
    },
    {
      "epoch": 0.027222540314655135,
      "grad_norm": 2.4753355979919434,
      "learning_rate": 0.00022782430353785678,
      "loss": 0.7052,
      "step": 6371
    },
    {
      "epoch": 0.02722681319808233,
      "grad_norm": 2.95782732963562,
      "learning_rate": 0.0002277815757990087,
      "loss": 0.6374,
      "step": 6372
    },
    {
      "epoch": 0.027231086081509523,
      "grad_norm": 2.0386571884155273,
      "learning_rate": 0.00022773884806016065,
      "loss": 0.4714,
      "step": 6373
    },
    {
      "epoch": 0.02723535896493672,
      "grad_norm": 1.1758966445922852,
      "learning_rate": 0.0002276961203213126,
      "loss": 0.7407,
      "step": 6374
    },
    {
      "epoch": 0.027239631848363914,
      "grad_norm": 4.699629306793213,
      "learning_rate": 0.00022765339258246453,
      "loss": 1.1814,
      "step": 6375
    },
    {
      "epoch": 0.027243904731791106,
      "grad_norm": 1.2947604656219482,
      "learning_rate": 0.00022761066484361646,
      "loss": 1.0448,
      "step": 6376
    },
    {
      "epoch": 0.027248177615218302,
      "grad_norm": 0.7289628386497498,
      "learning_rate": 0.00022756793710476843,
      "loss": 0.1752,
      "step": 6377
    },
    {
      "epoch": 0.027252450498645497,
      "grad_norm": 1.5563626289367676,
      "learning_rate": 0.00022752520936592037,
      "loss": 1.0678,
      "step": 6378
    },
    {
      "epoch": 0.02725672338207269,
      "grad_norm": 3.3184781074523926,
      "learning_rate": 0.0002274824816270723,
      "loss": 1.321,
      "step": 6379
    },
    {
      "epoch": 0.027260996265499885,
      "grad_norm": 1.4372886419296265,
      "learning_rate": 0.00022743975388822424,
      "loss": 0.6645,
      "step": 6380
    },
    {
      "epoch": 0.02726526914892708,
      "grad_norm": 1.2366005182266235,
      "learning_rate": 0.00022739702614937618,
      "loss": 0.4962,
      "step": 6381
    },
    {
      "epoch": 0.027269542032354273,
      "grad_norm": 0.4869314432144165,
      "learning_rate": 0.00022735429841052812,
      "loss": 0.1137,
      "step": 6382
    },
    {
      "epoch": 0.02727381491578147,
      "grad_norm": 3.54374623298645,
      "learning_rate": 0.00022731157067168006,
      "loss": 1.3081,
      "step": 6383
    },
    {
      "epoch": 0.02727808779920866,
      "grad_norm": 1.1300681829452515,
      "learning_rate": 0.00022726884293283202,
      "loss": 0.7007,
      "step": 6384
    },
    {
      "epoch": 0.027282360682635856,
      "grad_norm": 2.8992552757263184,
      "learning_rate": 0.00022722611519398396,
      "loss": 1.2385,
      "step": 6385
    },
    {
      "epoch": 0.02728663356606305,
      "grad_norm": 0.5232786536216736,
      "learning_rate": 0.0002271833874551359,
      "loss": 0.1443,
      "step": 6386
    },
    {
      "epoch": 0.027290906449490244,
      "grad_norm": 0.7195369601249695,
      "learning_rate": 0.0002271406597162878,
      "loss": 0.4666,
      "step": 6387
    },
    {
      "epoch": 0.02729517933291744,
      "grad_norm": 1.1104801893234253,
      "learning_rate": 0.00022709793197743975,
      "loss": 0.4056,
      "step": 6388
    },
    {
      "epoch": 0.027299452216344635,
      "grad_norm": 3.2820816040039062,
      "learning_rate": 0.00022705520423859168,
      "loss": 0.8676,
      "step": 6389
    },
    {
      "epoch": 0.027303725099771827,
      "grad_norm": 2.1297833919525146,
      "learning_rate": 0.00022701247649974362,
      "loss": 0.8632,
      "step": 6390
    },
    {
      "epoch": 0.027307997983199023,
      "grad_norm": 2.950584650039673,
      "learning_rate": 0.00022696974876089556,
      "loss": 0.8497,
      "step": 6391
    },
    {
      "epoch": 0.02731227086662622,
      "grad_norm": 1.2319011688232422,
      "learning_rate": 0.00022692702102204752,
      "loss": 0.4018,
      "step": 6392
    },
    {
      "epoch": 0.02731654375005341,
      "grad_norm": 2.2093374729156494,
      "learning_rate": 0.00022688429328319946,
      "loss": 0.8691,
      "step": 6393
    },
    {
      "epoch": 0.027320816633480606,
      "grad_norm": 1.0717238187789917,
      "learning_rate": 0.0002268415655443514,
      "loss": 0.3853,
      "step": 6394
    },
    {
      "epoch": 0.027325089516907798,
      "grad_norm": 2.899470090866089,
      "learning_rate": 0.00022679883780550334,
      "loss": 0.7833,
      "step": 6395
    },
    {
      "epoch": 0.027329362400334994,
      "grad_norm": 0.8581339716911316,
      "learning_rate": 0.00022675611006665528,
      "loss": 0.2776,
      "step": 6396
    },
    {
      "epoch": 0.02733363528376219,
      "grad_norm": 0.761550784111023,
      "learning_rate": 0.0002267133823278072,
      "loss": 0.5002,
      "step": 6397
    },
    {
      "epoch": 0.02733790816718938,
      "grad_norm": 4.064758777618408,
      "learning_rate": 0.00022667065458895915,
      "loss": 1.1332,
      "step": 6398
    },
    {
      "epoch": 0.027342181050616577,
      "grad_norm": 3.43096661567688,
      "learning_rate": 0.00022662792685011112,
      "loss": 0.6139,
      "step": 6399
    },
    {
      "epoch": 0.027346453934043773,
      "grad_norm": 1.1421635150909424,
      "learning_rate": 0.00022658519911126305,
      "loss": 0.6604,
      "step": 6400
    },
    {
      "epoch": 0.027350726817470965,
      "grad_norm": 2.5413601398468018,
      "learning_rate": 0.000226542471372415,
      "loss": 0.7412,
      "step": 6401
    },
    {
      "epoch": 0.02735499970089816,
      "grad_norm": 1.3977943658828735,
      "learning_rate": 0.00022649974363356693,
      "loss": 1.0488,
      "step": 6402
    },
    {
      "epoch": 0.027359272584325356,
      "grad_norm": 0.5633097887039185,
      "learning_rate": 0.00022645701589471884,
      "loss": 0.1922,
      "step": 6403
    },
    {
      "epoch": 0.027363545467752548,
      "grad_norm": 1.3507691621780396,
      "learning_rate": 0.00022641428815587078,
      "loss": 0.5942,
      "step": 6404
    },
    {
      "epoch": 0.027367818351179744,
      "grad_norm": 0.5151451230049133,
      "learning_rate": 0.00022637156041702272,
      "loss": 0.1783,
      "step": 6405
    },
    {
      "epoch": 0.027372091234606936,
      "grad_norm": 0.7659170627593994,
      "learning_rate": 0.00022632883267817468,
      "loss": 0.4593,
      "step": 6406
    },
    {
      "epoch": 0.02737636411803413,
      "grad_norm": 3.2389674186706543,
      "learning_rate": 0.00022628610493932662,
      "loss": 1.076,
      "step": 6407
    },
    {
      "epoch": 0.027380637001461327,
      "grad_norm": 1.1415950059890747,
      "learning_rate": 0.00022624337720047856,
      "loss": 0.4326,
      "step": 6408
    },
    {
      "epoch": 0.02738490988488852,
      "grad_norm": 0.8582851886749268,
      "learning_rate": 0.0002262006494616305,
      "loss": 0.29,
      "step": 6409
    },
    {
      "epoch": 0.027389182768315715,
      "grad_norm": 1.607019066810608,
      "learning_rate": 0.00022615792172278243,
      "loss": 1.0176,
      "step": 6410
    },
    {
      "epoch": 0.02739345565174291,
      "grad_norm": 2.957932472229004,
      "learning_rate": 0.00022611519398393437,
      "loss": 0.8634,
      "step": 6411
    },
    {
      "epoch": 0.027397728535170102,
      "grad_norm": 1.148952603340149,
      "learning_rate": 0.0002260724662450863,
      "loss": 0.6426,
      "step": 6412
    },
    {
      "epoch": 0.027402001418597298,
      "grad_norm": 0.6449921131134033,
      "learning_rate": 0.00022602973850623825,
      "loss": 0.2059,
      "step": 6413
    },
    {
      "epoch": 0.027406274302024494,
      "grad_norm": 2.6687424182891846,
      "learning_rate": 0.0002259870107673902,
      "loss": 0.753,
      "step": 6414
    },
    {
      "epoch": 0.027410547185451686,
      "grad_norm": 2.721829414367676,
      "learning_rate": 0.00022594428302854215,
      "loss": 0.6404,
      "step": 6415
    },
    {
      "epoch": 0.02741482006887888,
      "grad_norm": 4.901286602020264,
      "learning_rate": 0.00022590155528969409,
      "loss": 1.2447,
      "step": 6416
    },
    {
      "epoch": 0.027419092952306077,
      "grad_norm": 2.772772789001465,
      "learning_rate": 0.00022585882755084602,
      "loss": 0.8445,
      "step": 6417
    },
    {
      "epoch": 0.02742336583573327,
      "grad_norm": 1.7724920511245728,
      "learning_rate": 0.00022581609981199796,
      "loss": 0.4071,
      "step": 6418
    },
    {
      "epoch": 0.027427638719160465,
      "grad_norm": 0.7347180247306824,
      "learning_rate": 0.0002257733720731499,
      "loss": 0.1609,
      "step": 6419
    },
    {
      "epoch": 0.027431911602587657,
      "grad_norm": 0.8146919012069702,
      "learning_rate": 0.0002257306443343018,
      "loss": 0.472,
      "step": 6420
    },
    {
      "epoch": 0.027436184486014852,
      "grad_norm": 1.182791829109192,
      "learning_rate": 0.00022568791659545378,
      "loss": 0.4041,
      "step": 6421
    },
    {
      "epoch": 0.027440457369442048,
      "grad_norm": 3.684769630432129,
      "learning_rate": 0.0002256451888566057,
      "loss": 1.1297,
      "step": 6422
    },
    {
      "epoch": 0.02744473025286924,
      "grad_norm": 0.8021291494369507,
      "learning_rate": 0.00022560246111775765,
      "loss": 0.4427,
      "step": 6423
    },
    {
      "epoch": 0.027449003136296436,
      "grad_norm": 1.2335606813430786,
      "learning_rate": 0.0002255597333789096,
      "loss": 0.4513,
      "step": 6424
    },
    {
      "epoch": 0.02745327601972363,
      "grad_norm": 1.0969407558441162,
      "learning_rate": 0.00022551700564006153,
      "loss": 0.3634,
      "step": 6425
    },
    {
      "epoch": 0.027457548903150823,
      "grad_norm": 2.0061254501342773,
      "learning_rate": 0.00022547427790121346,
      "loss": 0.6062,
      "step": 6426
    },
    {
      "epoch": 0.02746182178657802,
      "grad_norm": 0.8231009244918823,
      "learning_rate": 0.0002254315501623654,
      "loss": 0.323,
      "step": 6427
    },
    {
      "epoch": 0.027466094670005214,
      "grad_norm": 2.1399621963500977,
      "learning_rate": 0.00022538882242351734,
      "loss": 0.7135,
      "step": 6428
    },
    {
      "epoch": 0.027470367553432407,
      "grad_norm": 0.8080106973648071,
      "learning_rate": 0.0002253460946846693,
      "loss": 0.4043,
      "step": 6429
    },
    {
      "epoch": 0.027474640436859602,
      "grad_norm": 2.438718795776367,
      "learning_rate": 0.00022530336694582124,
      "loss": 0.6102,
      "step": 6430
    },
    {
      "epoch": 0.027478913320286794,
      "grad_norm": 1.6494605541229248,
      "learning_rate": 0.00022526063920697318,
      "loss": 0.6448,
      "step": 6431
    },
    {
      "epoch": 0.02748318620371399,
      "grad_norm": 4.302483081817627,
      "learning_rate": 0.00022521791146812512,
      "loss": 1.2884,
      "step": 6432
    },
    {
      "epoch": 0.027487459087141185,
      "grad_norm": 2.959843873977661,
      "learning_rate": 0.00022517518372927706,
      "loss": 0.796,
      "step": 6433
    },
    {
      "epoch": 0.027491731970568378,
      "grad_norm": 2.9271562099456787,
      "learning_rate": 0.000225132455990429,
      "loss": 0.8319,
      "step": 6434
    },
    {
      "epoch": 0.027496004853995573,
      "grad_norm": 3.6257688999176025,
      "learning_rate": 0.00022508972825158093,
      "loss": 1.1203,
      "step": 6435
    },
    {
      "epoch": 0.02750027773742277,
      "grad_norm": 2.4139578342437744,
      "learning_rate": 0.00022504700051273287,
      "loss": 0.5655,
      "step": 6436
    },
    {
      "epoch": 0.02750455062084996,
      "grad_norm": 2.173374891281128,
      "learning_rate": 0.0002250042727738848,
      "loss": 0.4843,
      "step": 6437
    },
    {
      "epoch": 0.027508823504277156,
      "grad_norm": 1.1964497566223145,
      "learning_rate": 0.00022496154503503675,
      "loss": 0.6964,
      "step": 6438
    },
    {
      "epoch": 0.027513096387704352,
      "grad_norm": 1.8822996616363525,
      "learning_rate": 0.00022491881729618868,
      "loss": 1.1355,
      "step": 6439
    },
    {
      "epoch": 0.027517369271131544,
      "grad_norm": 1.603614091873169,
      "learning_rate": 0.00022487608955734062,
      "loss": 0.5736,
      "step": 6440
    },
    {
      "epoch": 0.02752164215455874,
      "grad_norm": 1.226737380027771,
      "learning_rate": 0.00022483336181849256,
      "loss": 0.6979,
      "step": 6441
    },
    {
      "epoch": 0.027525915037985935,
      "grad_norm": 1.4610291719436646,
      "learning_rate": 0.0002247906340796445,
      "loss": 0.4699,
      "step": 6442
    },
    {
      "epoch": 0.027530187921413127,
      "grad_norm": 0.840097963809967,
      "learning_rate": 0.00022474790634079644,
      "loss": 0.2862,
      "step": 6443
    },
    {
      "epoch": 0.027534460804840323,
      "grad_norm": 1.7258756160736084,
      "learning_rate": 0.0002247051786019484,
      "loss": 0.4548,
      "step": 6444
    },
    {
      "epoch": 0.027538733688267515,
      "grad_norm": 1.0848053693771362,
      "learning_rate": 0.00022466245086310034,
      "loss": 0.2784,
      "step": 6445
    },
    {
      "epoch": 0.02754300657169471,
      "grad_norm": 1.0551691055297852,
      "learning_rate": 0.00022461972312425228,
      "loss": 0.2169,
      "step": 6446
    },
    {
      "epoch": 0.027547279455121906,
      "grad_norm": 1.5523282289505005,
      "learning_rate": 0.00022457699538540421,
      "loss": 0.4363,
      "step": 6447
    },
    {
      "epoch": 0.0275515523385491,
      "grad_norm": 3.3615927696228027,
      "learning_rate": 0.00022453426764655615,
      "loss": 1.0811,
      "step": 6448
    },
    {
      "epoch": 0.027555825221976294,
      "grad_norm": 0.9215115904808044,
      "learning_rate": 0.0002244915399077081,
      "loss": 0.4315,
      "step": 6449
    },
    {
      "epoch": 0.02756009810540349,
      "grad_norm": 0.7096413969993591,
      "learning_rate": 0.00022444881216886003,
      "loss": 0.1285,
      "step": 6450
    },
    {
      "epoch": 0.027564370988830682,
      "grad_norm": 1.488895058631897,
      "learning_rate": 0.000224406084430012,
      "loss": 0.2764,
      "step": 6451
    },
    {
      "epoch": 0.027568643872257877,
      "grad_norm": 1.4429103136062622,
      "learning_rate": 0.0002243633566911639,
      "loss": 0.4132,
      "step": 6452
    },
    {
      "epoch": 0.027572916755685073,
      "grad_norm": 2.981435775756836,
      "learning_rate": 0.00022432062895231584,
      "loss": 0.6397,
      "step": 6453
    },
    {
      "epoch": 0.027577189639112265,
      "grad_norm": 0.8986434936523438,
      "learning_rate": 0.00022427790121346778,
      "loss": 0.3699,
      "step": 6454
    },
    {
      "epoch": 0.02758146252253946,
      "grad_norm": 3.501396894454956,
      "learning_rate": 0.00022423517347461972,
      "loss": 2.0357,
      "step": 6455
    },
    {
      "epoch": 0.027585735405966653,
      "grad_norm": 3.2424724102020264,
      "learning_rate": 0.00022419244573577165,
      "loss": 0.9198,
      "step": 6456
    },
    {
      "epoch": 0.02759000828939385,
      "grad_norm": 3.4095239639282227,
      "learning_rate": 0.0002241497179969236,
      "loss": 1.2227,
      "step": 6457
    },
    {
      "epoch": 0.027594281172821044,
      "grad_norm": 2.1447367668151855,
      "learning_rate": 0.00022410699025807556,
      "loss": 0.3516,
      "step": 6458
    },
    {
      "epoch": 0.027598554056248236,
      "grad_norm": 1.2636237144470215,
      "learning_rate": 0.0002240642625192275,
      "loss": 0.3466,
      "step": 6459
    },
    {
      "epoch": 0.02760282693967543,
      "grad_norm": 1.304746150970459,
      "learning_rate": 0.00022402153478037943,
      "loss": 0.6126,
      "step": 6460
    },
    {
      "epoch": 0.027607099823102627,
      "grad_norm": 1.2079089879989624,
      "learning_rate": 0.00022397880704153137,
      "loss": 0.3682,
      "step": 6461
    },
    {
      "epoch": 0.02761137270652982,
      "grad_norm": 0.8537999987602234,
      "learning_rate": 0.0002239360793026833,
      "loss": 0.2365,
      "step": 6462
    },
    {
      "epoch": 0.027615645589957015,
      "grad_norm": 0.910216748714447,
      "learning_rate": 0.00022389335156383525,
      "loss": 0.3627,
      "step": 6463
    },
    {
      "epoch": 0.02761991847338421,
      "grad_norm": 3.7329697608947754,
      "learning_rate": 0.00022385062382498718,
      "loss": 1.1468,
      "step": 6464
    },
    {
      "epoch": 0.027624191356811403,
      "grad_norm": 3.3867547512054443,
      "learning_rate": 0.00022380789608613912,
      "loss": 0.9698,
      "step": 6465
    },
    {
      "epoch": 0.027628464240238598,
      "grad_norm": 2.47821307182312,
      "learning_rate": 0.0002237651683472911,
      "loss": 0.6127,
      "step": 6466
    },
    {
      "epoch": 0.027632737123665794,
      "grad_norm": 1.3580843210220337,
      "learning_rate": 0.00022372244060844302,
      "loss": 0.6727,
      "step": 6467
    },
    {
      "epoch": 0.027637010007092986,
      "grad_norm": 4.928380012512207,
      "learning_rate": 0.00022367971286959496,
      "loss": 2.7327,
      "step": 6468
    },
    {
      "epoch": 0.02764128289052018,
      "grad_norm": 4.241981506347656,
      "learning_rate": 0.00022363698513074687,
      "loss": 1.0295,
      "step": 6469
    },
    {
      "epoch": 0.027645555773947374,
      "grad_norm": 0.5059248208999634,
      "learning_rate": 0.0002235942573918988,
      "loss": 0.1791,
      "step": 6470
    },
    {
      "epoch": 0.02764982865737457,
      "grad_norm": 4.222667217254639,
      "learning_rate": 0.00022355152965305075,
      "loss": 0.9839,
      "step": 6471
    },
    {
      "epoch": 0.027654101540801765,
      "grad_norm": 0.8831092119216919,
      "learning_rate": 0.0002235088019142027,
      "loss": 0.3518,
      "step": 6472
    },
    {
      "epoch": 0.027658374424228957,
      "grad_norm": 3.014982223510742,
      "learning_rate": 0.00022346607417535465,
      "loss": 0.6851,
      "step": 6473
    },
    {
      "epoch": 0.027662647307656153,
      "grad_norm": 0.691349983215332,
      "learning_rate": 0.0002234233464365066,
      "loss": 0.1701,
      "step": 6474
    },
    {
      "epoch": 0.027666920191083348,
      "grad_norm": 3.5843725204467773,
      "learning_rate": 0.00022338061869765853,
      "loss": 1.9649,
      "step": 6475
    },
    {
      "epoch": 0.02767119307451054,
      "grad_norm": 0.6849709749221802,
      "learning_rate": 0.00022333789095881047,
      "loss": 0.1825,
      "step": 6476
    },
    {
      "epoch": 0.027675465957937736,
      "grad_norm": 23.92562484741211,
      "learning_rate": 0.0002232951632199624,
      "loss": 3.9904,
      "step": 6477
    },
    {
      "epoch": 0.02767973884136493,
      "grad_norm": 4.621212482452393,
      "learning_rate": 0.00022325243548111434,
      "loss": 1.112,
      "step": 6478
    },
    {
      "epoch": 0.027684011724792124,
      "grad_norm": 2.7297844886779785,
      "learning_rate": 0.00022320970774226628,
      "loss": 0.5973,
      "step": 6479
    },
    {
      "epoch": 0.02768828460821932,
      "grad_norm": 2.578939199447632,
      "learning_rate": 0.00022316698000341822,
      "loss": 1.3932,
      "step": 6480
    },
    {
      "epoch": 0.02769255749164651,
      "grad_norm": 2.7247378826141357,
      "learning_rate": 0.00022312425226457018,
      "loss": 0.7108,
      "step": 6481
    },
    {
      "epoch": 0.027696830375073707,
      "grad_norm": 2.7993881702423096,
      "learning_rate": 0.00022308152452572212,
      "loss": 0.8391,
      "step": 6482
    },
    {
      "epoch": 0.027701103258500902,
      "grad_norm": 2.248023748397827,
      "learning_rate": 0.00022303879678687406,
      "loss": 1.1671,
      "step": 6483
    },
    {
      "epoch": 0.027705376141928095,
      "grad_norm": 35.71198272705078,
      "learning_rate": 0.000222996069048026,
      "loss": 2.3056,
      "step": 6484
    },
    {
      "epoch": 0.02770964902535529,
      "grad_norm": 4.86949348449707,
      "learning_rate": 0.0002229533413091779,
      "loss": 2.5143,
      "step": 6485
    },
    {
      "epoch": 0.027713921908782486,
      "grad_norm": 3.324901580810547,
      "learning_rate": 0.00022291061357032984,
      "loss": 1.7998,
      "step": 6486
    },
    {
      "epoch": 0.027718194792209678,
      "grad_norm": 0.6056678891181946,
      "learning_rate": 0.00022286788583148178,
      "loss": 0.2253,
      "step": 6487
    },
    {
      "epoch": 0.027722467675636873,
      "grad_norm": 3.7073264122009277,
      "learning_rate": 0.00022282515809263375,
      "loss": 1.0743,
      "step": 6488
    },
    {
      "epoch": 0.02772674055906407,
      "grad_norm": 1.4682732820510864,
      "learning_rate": 0.00022278243035378568,
      "loss": 0.3525,
      "step": 6489
    },
    {
      "epoch": 0.02773101344249126,
      "grad_norm": 2.284930944442749,
      "learning_rate": 0.00022273970261493762,
      "loss": 0.852,
      "step": 6490
    },
    {
      "epoch": 0.027735286325918457,
      "grad_norm": 1.0420717000961304,
      "learning_rate": 0.00022269697487608956,
      "loss": 0.3827,
      "step": 6491
    },
    {
      "epoch": 0.027739559209345652,
      "grad_norm": 1.693853735923767,
      "learning_rate": 0.0002226542471372415,
      "loss": 0.417,
      "step": 6492
    },
    {
      "epoch": 0.027743832092772844,
      "grad_norm": 1.6845335960388184,
      "learning_rate": 0.00022261151939839344,
      "loss": 0.4597,
      "step": 6493
    },
    {
      "epoch": 0.02774810497620004,
      "grad_norm": 2.1638128757476807,
      "learning_rate": 0.00022256879165954537,
      "loss": 0.6214,
      "step": 6494
    },
    {
      "epoch": 0.027752377859627232,
      "grad_norm": 2.8480489253997803,
      "learning_rate": 0.0002225260639206973,
      "loss": 0.8531,
      "step": 6495
    },
    {
      "epoch": 0.027756650743054428,
      "grad_norm": 0.8775101900100708,
      "learning_rate": 0.00022248333618184928,
      "loss": 0.4208,
      "step": 6496
    },
    {
      "epoch": 0.027760923626481623,
      "grad_norm": 0.9870281219482422,
      "learning_rate": 0.00022244060844300121,
      "loss": 0.3202,
      "step": 6497
    },
    {
      "epoch": 0.027765196509908815,
      "grad_norm": 1.5413496494293213,
      "learning_rate": 0.00022239788070415315,
      "loss": 0.4209,
      "step": 6498
    },
    {
      "epoch": 0.02776946939333601,
      "grad_norm": 4.181499004364014,
      "learning_rate": 0.0002223551529653051,
      "loss": 1.2057,
      "step": 6499
    },
    {
      "epoch": 0.027773742276763207,
      "grad_norm": 3.1582260131835938,
      "learning_rate": 0.00022231242522645703,
      "loss": 1.0707,
      "step": 6500
    },
    {
      "epoch": 0.0277780151601904,
      "grad_norm": 4.071981906890869,
      "learning_rate": 0.00022226969748760897,
      "loss": 0.9159,
      "step": 6501
    },
    {
      "epoch": 0.027782288043617594,
      "grad_norm": 1.6230887174606323,
      "learning_rate": 0.00022222696974876088,
      "loss": 1.0472,
      "step": 6502
    },
    {
      "epoch": 0.02778656092704479,
      "grad_norm": 1.1530917882919312,
      "learning_rate": 0.00022218424200991284,
      "loss": 0.3499,
      "step": 6503
    },
    {
      "epoch": 0.027790833810471982,
      "grad_norm": 1.1536709070205688,
      "learning_rate": 0.00022214151427106478,
      "loss": 0.3537,
      "step": 6504
    },
    {
      "epoch": 0.027795106693899178,
      "grad_norm": 3.118382215499878,
      "learning_rate": 0.00022209878653221672,
      "loss": 0.94,
      "step": 6505
    },
    {
      "epoch": 0.02779937957732637,
      "grad_norm": 3.7342185974121094,
      "learning_rate": 0.00022205605879336865,
      "loss": 1.3676,
      "step": 6506
    },
    {
      "epoch": 0.027803652460753565,
      "grad_norm": 0.9104762077331543,
      "learning_rate": 0.0002220133310545206,
      "loss": 0.3241,
      "step": 6507
    },
    {
      "epoch": 0.02780792534418076,
      "grad_norm": 2.0647037029266357,
      "learning_rate": 0.00022197060331567253,
      "loss": 0.8123,
      "step": 6508
    },
    {
      "epoch": 0.027812198227607953,
      "grad_norm": 1.236696720123291,
      "learning_rate": 0.00022192787557682447,
      "loss": 0.365,
      "step": 6509
    },
    {
      "epoch": 0.02781647111103515,
      "grad_norm": 1.1486369371414185,
      "learning_rate": 0.00022188514783797643,
      "loss": 0.4314,
      "step": 6510
    },
    {
      "epoch": 0.027820743994462344,
      "grad_norm": 1.668320894241333,
      "learning_rate": 0.00022184242009912837,
      "loss": 0.6363,
      "step": 6511
    },
    {
      "epoch": 0.027825016877889536,
      "grad_norm": 0.8089962601661682,
      "learning_rate": 0.0002217996923602803,
      "loss": 0.3998,
      "step": 6512
    },
    {
      "epoch": 0.027829289761316732,
      "grad_norm": 1.0481261014938354,
      "learning_rate": 0.00022175696462143225,
      "loss": 0.3827,
      "step": 6513
    },
    {
      "epoch": 0.027833562644743928,
      "grad_norm": 4.4841132164001465,
      "learning_rate": 0.00022171423688258418,
      "loss": 1.533,
      "step": 6514
    },
    {
      "epoch": 0.02783783552817112,
      "grad_norm": 0.9941089153289795,
      "learning_rate": 0.00022167150914373612,
      "loss": 0.3201,
      "step": 6515
    },
    {
      "epoch": 0.027842108411598315,
      "grad_norm": 0.42270785570144653,
      "learning_rate": 0.00022162878140488806,
      "loss": 0.0924,
      "step": 6516
    },
    {
      "epoch": 0.027846381295025507,
      "grad_norm": 1.656327247619629,
      "learning_rate": 0.00022158605366604,
      "loss": 0.4632,
      "step": 6517
    },
    {
      "epoch": 0.027850654178452703,
      "grad_norm": 2.0107364654541016,
      "learning_rate": 0.00022154332592719194,
      "loss": 0.7197,
      "step": 6518
    },
    {
      "epoch": 0.0278549270618799,
      "grad_norm": 1.9645731449127197,
      "learning_rate": 0.00022150059818834387,
      "loss": 0.7017,
      "step": 6519
    },
    {
      "epoch": 0.02785919994530709,
      "grad_norm": 4.4369049072265625,
      "learning_rate": 0.0002214578704494958,
      "loss": 1.3135,
      "step": 6520
    },
    {
      "epoch": 0.027863472828734286,
      "grad_norm": 1.6417783498764038,
      "learning_rate": 0.00022141514271064775,
      "loss": 0.4158,
      "step": 6521
    },
    {
      "epoch": 0.027867745712161482,
      "grad_norm": 1.0355851650238037,
      "learning_rate": 0.0002213724149717997,
      "loss": 0.3336,
      "step": 6522
    },
    {
      "epoch": 0.027872018595588674,
      "grad_norm": 3.101649045944214,
      "learning_rate": 0.00022132968723295163,
      "loss": 1.0591,
      "step": 6523
    },
    {
      "epoch": 0.02787629147901587,
      "grad_norm": 3.564300060272217,
      "learning_rate": 0.00022128695949410356,
      "loss": 1.0457,
      "step": 6524
    },
    {
      "epoch": 0.027880564362443065,
      "grad_norm": 0.34941214323043823,
      "learning_rate": 0.00022124423175525553,
      "loss": 0.0696,
      "step": 6525
    },
    {
      "epoch": 0.027884837245870257,
      "grad_norm": 1.013027310371399,
      "learning_rate": 0.00022120150401640747,
      "loss": 0.7063,
      "step": 6526
    },
    {
      "epoch": 0.027889110129297453,
      "grad_norm": 1.5645252466201782,
      "learning_rate": 0.0002211587762775594,
      "loss": 1.0546,
      "step": 6527
    },
    {
      "epoch": 0.02789338301272465,
      "grad_norm": 3.0539791584014893,
      "learning_rate": 0.00022111604853871134,
      "loss": 0.758,
      "step": 6528
    },
    {
      "epoch": 0.02789765589615184,
      "grad_norm": 3.088552236557007,
      "learning_rate": 0.00022107332079986328,
      "loss": 0.7793,
      "step": 6529
    },
    {
      "epoch": 0.027901928779579036,
      "grad_norm": 2.279035806655884,
      "learning_rate": 0.00022103059306101522,
      "loss": 0.8944,
      "step": 6530
    },
    {
      "epoch": 0.02790620166300623,
      "grad_norm": 1.1233839988708496,
      "learning_rate": 0.00022098786532216716,
      "loss": 0.339,
      "step": 6531
    },
    {
      "epoch": 0.027910474546433424,
      "grad_norm": 0.9987502694129944,
      "learning_rate": 0.0002209451375833191,
      "loss": 0.6761,
      "step": 6532
    },
    {
      "epoch": 0.02791474742986062,
      "grad_norm": 1.3195749521255493,
      "learning_rate": 0.00022090240984447106,
      "loss": 0.3518,
      "step": 6533
    },
    {
      "epoch": 0.02791902031328781,
      "grad_norm": 2.1514060497283936,
      "learning_rate": 0.000220859682105623,
      "loss": 0.497,
      "step": 6534
    },
    {
      "epoch": 0.027923293196715007,
      "grad_norm": 1.0212335586547852,
      "learning_rate": 0.0002208169543667749,
      "loss": 0.2896,
      "step": 6535
    },
    {
      "epoch": 0.027927566080142203,
      "grad_norm": 4.500999450683594,
      "learning_rate": 0.00022077422662792684,
      "loss": 1.2093,
      "step": 6536
    },
    {
      "epoch": 0.027931838963569395,
      "grad_norm": 3.052778959274292,
      "learning_rate": 0.00022073149888907878,
      "loss": 0.6299,
      "step": 6537
    },
    {
      "epoch": 0.02793611184699659,
      "grad_norm": 1.1661335229873657,
      "learning_rate": 0.00022068877115023072,
      "loss": 0.2934,
      "step": 6538
    },
    {
      "epoch": 0.027940384730423786,
      "grad_norm": 1.3866444826126099,
      "learning_rate": 0.00022064604341138266,
      "loss": 0.3162,
      "step": 6539
    },
    {
      "epoch": 0.027944657613850978,
      "grad_norm": 1.5986988544464111,
      "learning_rate": 0.00022060331567253462,
      "loss": 0.9992,
      "step": 6540
    },
    {
      "epoch": 0.027948930497278174,
      "grad_norm": 0.9992147088050842,
      "learning_rate": 0.00022056058793368656,
      "loss": 0.668,
      "step": 6541
    },
    {
      "epoch": 0.027953203380705366,
      "grad_norm": 1.1384209394454956,
      "learning_rate": 0.0002205178601948385,
      "loss": 0.3346,
      "step": 6542
    },
    {
      "epoch": 0.02795747626413256,
      "grad_norm": 2.8933217525482178,
      "learning_rate": 0.00022047513245599044,
      "loss": 0.7597,
      "step": 6543
    },
    {
      "epoch": 0.027961749147559757,
      "grad_norm": 1.0104645490646362,
      "learning_rate": 0.00022043240471714237,
      "loss": 0.6418,
      "step": 6544
    },
    {
      "epoch": 0.02796602203098695,
      "grad_norm": 0.20739197731018066,
      "learning_rate": 0.0002203896769782943,
      "loss": 0.0328,
      "step": 6545
    },
    {
      "epoch": 0.027970294914414145,
      "grad_norm": 4.5011467933654785,
      "learning_rate": 0.00022034694923944625,
      "loss": 1.2158,
      "step": 6546
    },
    {
      "epoch": 0.02797456779784134,
      "grad_norm": 1.274548053741455,
      "learning_rate": 0.0002203042215005982,
      "loss": 0.4589,
      "step": 6547
    },
    {
      "epoch": 0.027978840681268533,
      "grad_norm": 0.9124815464019775,
      "learning_rate": 0.00022026149376175015,
      "loss": 0.2559,
      "step": 6548
    },
    {
      "epoch": 0.027983113564695728,
      "grad_norm": 1.669063925743103,
      "learning_rate": 0.0002202187660229021,
      "loss": 0.5197,
      "step": 6549
    },
    {
      "epoch": 0.027987386448122924,
      "grad_norm": 2.7638697624206543,
      "learning_rate": 0.00022017603828405403,
      "loss": 1.6262,
      "step": 6550
    },
    {
      "epoch": 0.027991659331550116,
      "grad_norm": 2.1758437156677246,
      "learning_rate": 0.00022013331054520594,
      "loss": 0.7455,
      "step": 6551
    },
    {
      "epoch": 0.02799593221497731,
      "grad_norm": 2.0310325622558594,
      "learning_rate": 0.00022009058280635788,
      "loss": 0.5139,
      "step": 6552
    },
    {
      "epoch": 0.028000205098404507,
      "grad_norm": 1.322052240371704,
      "learning_rate": 0.00022004785506750981,
      "loss": 0.545,
      "step": 6553
    },
    {
      "epoch": 0.0280044779818317,
      "grad_norm": 1.313173532485962,
      "learning_rate": 0.00022000512732866175,
      "loss": 0.4588,
      "step": 6554
    },
    {
      "epoch": 0.028008750865258895,
      "grad_norm": 0.9500210285186768,
      "learning_rate": 0.00021996239958981372,
      "loss": 0.5017,
      "step": 6555
    },
    {
      "epoch": 0.028013023748686087,
      "grad_norm": 3.1513774394989014,
      "learning_rate": 0.00021991967185096566,
      "loss": 0.8452,
      "step": 6556
    },
    {
      "epoch": 0.028017296632113282,
      "grad_norm": 3.0465188026428223,
      "learning_rate": 0.0002198769441121176,
      "loss": 0.7946,
      "step": 6557
    },
    {
      "epoch": 0.028021569515540478,
      "grad_norm": 1.0490150451660156,
      "learning_rate": 0.00021983421637326953,
      "loss": 0.2761,
      "step": 6558
    },
    {
      "epoch": 0.02802584239896767,
      "grad_norm": 3.4874842166900635,
      "learning_rate": 0.00021979148863442147,
      "loss": 0.9288,
      "step": 6559
    },
    {
      "epoch": 0.028030115282394866,
      "grad_norm": 1.393837571144104,
      "learning_rate": 0.0002197487608955734,
      "loss": 0.3926,
      "step": 6560
    },
    {
      "epoch": 0.02803438816582206,
      "grad_norm": 1.0450810194015503,
      "learning_rate": 0.00021970603315672534,
      "loss": 0.3635,
      "step": 6561
    },
    {
      "epoch": 0.028038661049249253,
      "grad_norm": 2.064830780029297,
      "learning_rate": 0.0002196633054178773,
      "loss": 0.5831,
      "step": 6562
    },
    {
      "epoch": 0.02804293393267645,
      "grad_norm": 2.8323922157287598,
      "learning_rate": 0.00021962057767902925,
      "loss": 0.8358,
      "step": 6563
    },
    {
      "epoch": 0.028047206816103645,
      "grad_norm": 0.8666843175888062,
      "learning_rate": 0.00021957784994018119,
      "loss": 0.2288,
      "step": 6564
    },
    {
      "epoch": 0.028051479699530837,
      "grad_norm": 1.0244802236557007,
      "learning_rate": 0.00021953512220133312,
      "loss": 0.6006,
      "step": 6565
    },
    {
      "epoch": 0.028055752582958032,
      "grad_norm": 1.4557931423187256,
      "learning_rate": 0.00021949239446248506,
      "loss": 0.3262,
      "step": 6566
    },
    {
      "epoch": 0.028060025466385224,
      "grad_norm": 1.5382564067840576,
      "learning_rate": 0.00021944966672363697,
      "loss": 0.4379,
      "step": 6567
    },
    {
      "epoch": 0.02806429834981242,
      "grad_norm": 1.9532043933868408,
      "learning_rate": 0.0002194069389847889,
      "loss": 0.6401,
      "step": 6568
    },
    {
      "epoch": 0.028068571233239616,
      "grad_norm": 28.42440414428711,
      "learning_rate": 0.00021936421124594085,
      "loss": 2.7765,
      "step": 6569
    },
    {
      "epoch": 0.028072844116666808,
      "grad_norm": 1.806528925895691,
      "learning_rate": 0.0002193214835070928,
      "loss": 0.5696,
      "step": 6570
    },
    {
      "epoch": 0.028077117000094003,
      "grad_norm": 0.8718221783638,
      "learning_rate": 0.00021927875576824475,
      "loss": 0.304,
      "step": 6571
    },
    {
      "epoch": 0.0280813898835212,
      "grad_norm": 1.523905873298645,
      "learning_rate": 0.0002192360280293967,
      "loss": 0.3925,
      "step": 6572
    },
    {
      "epoch": 0.02808566276694839,
      "grad_norm": 0.8683369755744934,
      "learning_rate": 0.00021919330029054863,
      "loss": 0.4949,
      "step": 6573
    },
    {
      "epoch": 0.028089935650375587,
      "grad_norm": 3.6260457038879395,
      "learning_rate": 0.00021915057255170056,
      "loss": 0.8846,
      "step": 6574
    },
    {
      "epoch": 0.028094208533802782,
      "grad_norm": 1.2732348442077637,
      "learning_rate": 0.0002191078448128525,
      "loss": 0.4857,
      "step": 6575
    },
    {
      "epoch": 0.028098481417229974,
      "grad_norm": 3.1002559661865234,
      "learning_rate": 0.00021906511707400444,
      "loss": 1.0331,
      "step": 6576
    },
    {
      "epoch": 0.02810275430065717,
      "grad_norm": 0.8269487619400024,
      "learning_rate": 0.0002190223893351564,
      "loss": 0.4852,
      "step": 6577
    },
    {
      "epoch": 0.028107027184084366,
      "grad_norm": 1.6286957263946533,
      "learning_rate": 0.00021897966159630834,
      "loss": 1.2352,
      "step": 6578
    },
    {
      "epoch": 0.028111300067511558,
      "grad_norm": 1.3405991792678833,
      "learning_rate": 0.00021893693385746028,
      "loss": 0.3024,
      "step": 6579
    },
    {
      "epoch": 0.028115572950938753,
      "grad_norm": 1.9468308687210083,
      "learning_rate": 0.00021889420611861222,
      "loss": 0.6096,
      "step": 6580
    },
    {
      "epoch": 0.028119845834365945,
      "grad_norm": 2.0578622817993164,
      "learning_rate": 0.00021885147837976416,
      "loss": 0.7729,
      "step": 6581
    },
    {
      "epoch": 0.02812411871779314,
      "grad_norm": 4.043220043182373,
      "learning_rate": 0.0002188087506409161,
      "loss": 1.2757,
      "step": 6582
    },
    {
      "epoch": 0.028128391601220337,
      "grad_norm": 1.295021653175354,
      "learning_rate": 0.00021876602290206803,
      "loss": 0.4344,
      "step": 6583
    },
    {
      "epoch": 0.02813266448464753,
      "grad_norm": 0.9755528569221497,
      "learning_rate": 0.00021872329516321994,
      "loss": 0.2764,
      "step": 6584
    },
    {
      "epoch": 0.028136937368074724,
      "grad_norm": 1.5619672536849976,
      "learning_rate": 0.0002186805674243719,
      "loss": 1.0151,
      "step": 6585
    },
    {
      "epoch": 0.02814121025150192,
      "grad_norm": 2.482912063598633,
      "learning_rate": 0.00021863783968552384,
      "loss": 0.9571,
      "step": 6586
    },
    {
      "epoch": 0.028145483134929112,
      "grad_norm": 3.852168560028076,
      "learning_rate": 0.00021859511194667578,
      "loss": 2.5783,
      "step": 6587
    },
    {
      "epoch": 0.028149756018356308,
      "grad_norm": 1.3160234689712524,
      "learning_rate": 0.00021855238420782772,
      "loss": 0.4436,
      "step": 6588
    },
    {
      "epoch": 0.028154028901783503,
      "grad_norm": 0.8280379176139832,
      "learning_rate": 0.00021850965646897966,
      "loss": 0.2641,
      "step": 6589
    },
    {
      "epoch": 0.028158301785210695,
      "grad_norm": 0.9648182392120361,
      "learning_rate": 0.0002184669287301316,
      "loss": 0.5916,
      "step": 6590
    },
    {
      "epoch": 0.02816257466863789,
      "grad_norm": 0.9337142109870911,
      "learning_rate": 0.00021842420099128353,
      "loss": 0.5548,
      "step": 6591
    },
    {
      "epoch": 0.028166847552065083,
      "grad_norm": 1.9053188562393188,
      "learning_rate": 0.0002183814732524355,
      "loss": 0.5214,
      "step": 6592
    },
    {
      "epoch": 0.02817112043549228,
      "grad_norm": 0.6142510175704956,
      "learning_rate": 0.00021833874551358744,
      "loss": 0.2766,
      "step": 6593
    },
    {
      "epoch": 0.028175393318919474,
      "grad_norm": 1.543798565864563,
      "learning_rate": 0.00021829601777473937,
      "loss": 1.0173,
      "step": 6594
    },
    {
      "epoch": 0.028179666202346666,
      "grad_norm": 2.027440071105957,
      "learning_rate": 0.0002182532900358913,
      "loss": 0.6421,
      "step": 6595
    },
    {
      "epoch": 0.028183939085773862,
      "grad_norm": 0.7803807258605957,
      "learning_rate": 0.00021821056229704325,
      "loss": 0.4929,
      "step": 6596
    },
    {
      "epoch": 0.028188211969201057,
      "grad_norm": 1.5205925703048706,
      "learning_rate": 0.0002181678345581952,
      "loss": 0.4165,
      "step": 6597
    },
    {
      "epoch": 0.02819248485262825,
      "grad_norm": 1.5785096883773804,
      "learning_rate": 0.00021812510681934713,
      "loss": 1.1802,
      "step": 6598
    },
    {
      "epoch": 0.028196757736055445,
      "grad_norm": 0.9681513905525208,
      "learning_rate": 0.00021808237908049906,
      "loss": 0.389,
      "step": 6599
    },
    {
      "epoch": 0.02820103061948264,
      "grad_norm": 4.071450710296631,
      "learning_rate": 0.000218039651341651,
      "loss": 1.073,
      "step": 6600
    },
    {
      "epoch": 0.028205303502909833,
      "grad_norm": 2.8596601486206055,
      "learning_rate": 0.00021799692360280294,
      "loss": 0.8655,
      "step": 6601
    },
    {
      "epoch": 0.02820957638633703,
      "grad_norm": 2.9684572219848633,
      "learning_rate": 0.00021795419586395488,
      "loss": 1.2638,
      "step": 6602
    },
    {
      "epoch": 0.028213849269764224,
      "grad_norm": 0.9768805503845215,
      "learning_rate": 0.00021791146812510682,
      "loss": 0.5556,
      "step": 6603
    },
    {
      "epoch": 0.028218122153191416,
      "grad_norm": 0.9863189458847046,
      "learning_rate": 0.00021786874038625875,
      "loss": 0.5554,
      "step": 6604
    },
    {
      "epoch": 0.028222395036618612,
      "grad_norm": 1.0095000267028809,
      "learning_rate": 0.0002178260126474107,
      "loss": 0.5646,
      "step": 6605
    },
    {
      "epoch": 0.028226667920045804,
      "grad_norm": 0.8191778063774109,
      "learning_rate": 0.00021778328490856263,
      "loss": 0.4928,
      "step": 6606
    },
    {
      "epoch": 0.028230940803473,
      "grad_norm": 1.2372609376907349,
      "learning_rate": 0.0002177405571697146,
      "loss": 0.3814,
      "step": 6607
    },
    {
      "epoch": 0.028235213686900195,
      "grad_norm": 3.196289539337158,
      "learning_rate": 0.00021769782943086653,
      "loss": 1.0405,
      "step": 6608
    },
    {
      "epoch": 0.028239486570327387,
      "grad_norm": 4.491398334503174,
      "learning_rate": 0.00021765510169201847,
      "loss": 1.2234,
      "step": 6609
    },
    {
      "epoch": 0.028243759453754583,
      "grad_norm": 0.957097053527832,
      "learning_rate": 0.0002176123739531704,
      "loss": 0.2891,
      "step": 6610
    },
    {
      "epoch": 0.02824803233718178,
      "grad_norm": 0.6053217649459839,
      "learning_rate": 0.00021756964621432235,
      "loss": 0.2224,
      "step": 6611
    },
    {
      "epoch": 0.02825230522060897,
      "grad_norm": 4.104554176330566,
      "learning_rate": 0.00021752691847547428,
      "loss": 0.9046,
      "step": 6612
    },
    {
      "epoch": 0.028256578104036166,
      "grad_norm": 1.7203476428985596,
      "learning_rate": 0.00021748419073662622,
      "loss": 0.5359,
      "step": 6613
    },
    {
      "epoch": 0.02826085098746336,
      "grad_norm": 3.514787197113037,
      "learning_rate": 0.00021744146299777819,
      "loss": 0.7782,
      "step": 6614
    },
    {
      "epoch": 0.028265123870890554,
      "grad_norm": 3.4629032611846924,
      "learning_rate": 0.00021739873525893012,
      "loss": 1.409,
      "step": 6615
    },
    {
      "epoch": 0.02826939675431775,
      "grad_norm": 3.1094837188720703,
      "learning_rate": 0.00021735600752008206,
      "loss": 1.1267,
      "step": 6616
    },
    {
      "epoch": 0.02827366963774494,
      "grad_norm": 1.3605951070785522,
      "learning_rate": 0.00021731327978123397,
      "loss": 0.2577,
      "step": 6617
    },
    {
      "epoch": 0.028277942521172137,
      "grad_norm": 1.3117841482162476,
      "learning_rate": 0.0002172705520423859,
      "loss": 0.3521,
      "step": 6618
    },
    {
      "epoch": 0.028282215404599333,
      "grad_norm": 2.3217806816101074,
      "learning_rate": 0.00021722782430353785,
      "loss": 0.7292,
      "step": 6619
    },
    {
      "epoch": 0.028286488288026525,
      "grad_norm": 1.639186143875122,
      "learning_rate": 0.00021718509656468979,
      "loss": 0.4437,
      "step": 6620
    },
    {
      "epoch": 0.02829076117145372,
      "grad_norm": 1.0351965427398682,
      "learning_rate": 0.00021714236882584172,
      "loss": 0.5501,
      "step": 6621
    },
    {
      "epoch": 0.028295034054880916,
      "grad_norm": 5.682267189025879,
      "learning_rate": 0.0002170996410869937,
      "loss": 1.348,
      "step": 6622
    },
    {
      "epoch": 0.028299306938308108,
      "grad_norm": 1.0150163173675537,
      "learning_rate": 0.00021705691334814563,
      "loss": 0.3185,
      "step": 6623
    },
    {
      "epoch": 0.028303579821735304,
      "grad_norm": 0.8805031776428223,
      "learning_rate": 0.00021701418560929756,
      "loss": 0.2779,
      "step": 6624
    },
    {
      "epoch": 0.0283078527051625,
      "grad_norm": 4.739731311798096,
      "learning_rate": 0.0002169714578704495,
      "loss": 1.0594,
      "step": 6625
    },
    {
      "epoch": 0.02831212558858969,
      "grad_norm": 0.8965810537338257,
      "learning_rate": 0.00021692873013160144,
      "loss": 0.2778,
      "step": 6626
    },
    {
      "epoch": 0.028316398472016887,
      "grad_norm": 1.7575982809066772,
      "learning_rate": 0.00021688600239275338,
      "loss": 1.0948,
      "step": 6627
    },
    {
      "epoch": 0.02832067135544408,
      "grad_norm": 1.0730669498443604,
      "learning_rate": 0.00021684327465390532,
      "loss": 0.484,
      "step": 6628
    },
    {
      "epoch": 0.028324944238871275,
      "grad_norm": 3.1545112133026123,
      "learning_rate": 0.00021680054691505728,
      "loss": 1.8567,
      "step": 6629
    },
    {
      "epoch": 0.02832921712229847,
      "grad_norm": 1.6940197944641113,
      "learning_rate": 0.00021675781917620922,
      "loss": 0.9972,
      "step": 6630
    },
    {
      "epoch": 0.028333490005725662,
      "grad_norm": 3.0768375396728516,
      "learning_rate": 0.00021671509143736116,
      "loss": 0.927,
      "step": 6631
    },
    {
      "epoch": 0.028337762889152858,
      "grad_norm": 3.967418670654297,
      "learning_rate": 0.0002166723636985131,
      "loss": 1.1757,
      "step": 6632
    },
    {
      "epoch": 0.028342035772580054,
      "grad_norm": 1.7774943113327026,
      "learning_rate": 0.000216629635959665,
      "loss": 0.4734,
      "step": 6633
    },
    {
      "epoch": 0.028346308656007246,
      "grad_norm": 2.184195041656494,
      "learning_rate": 0.00021658690822081694,
      "loss": 0.8459,
      "step": 6634
    },
    {
      "epoch": 0.02835058153943444,
      "grad_norm": 1.0775095224380493,
      "learning_rate": 0.00021654418048196888,
      "loss": 0.4922,
      "step": 6635
    },
    {
      "epoch": 0.028354854422861637,
      "grad_norm": 2.80275559425354,
      "learning_rate": 0.00021650145274312082,
      "loss": 0.7663,
      "step": 6636
    },
    {
      "epoch": 0.02835912730628883,
      "grad_norm": 4.5634870529174805,
      "learning_rate": 0.00021645872500427278,
      "loss": 0.8305,
      "step": 6637
    },
    {
      "epoch": 0.028363400189716025,
      "grad_norm": 0.9719557762145996,
      "learning_rate": 0.00021641599726542472,
      "loss": 0.4921,
      "step": 6638
    },
    {
      "epoch": 0.02836767307314322,
      "grad_norm": 4.401827335357666,
      "learning_rate": 0.00021637326952657666,
      "loss": 0.7334,
      "step": 6639
    },
    {
      "epoch": 0.028371945956570412,
      "grad_norm": 1.1114474534988403,
      "learning_rate": 0.0002163305417877286,
      "loss": 0.4939,
      "step": 6640
    },
    {
      "epoch": 0.028376218839997608,
      "grad_norm": 4.273969650268555,
      "learning_rate": 0.00021628781404888053,
      "loss": 0.6492,
      "step": 6641
    },
    {
      "epoch": 0.0283804917234248,
      "grad_norm": 1.0466647148132324,
      "learning_rate": 0.00021624508631003247,
      "loss": 0.5442,
      "step": 6642
    },
    {
      "epoch": 0.028384764606851996,
      "grad_norm": 0.7617221474647522,
      "learning_rate": 0.0002162023585711844,
      "loss": 0.2082,
      "step": 6643
    },
    {
      "epoch": 0.02838903749027919,
      "grad_norm": 3.0959601402282715,
      "learning_rate": 0.00021615963083233638,
      "loss": 1.1071,
      "step": 6644
    },
    {
      "epoch": 0.028393310373706383,
      "grad_norm": 1.9154706001281738,
      "learning_rate": 0.0002161169030934883,
      "loss": 0.4951,
      "step": 6645
    },
    {
      "epoch": 0.02839758325713358,
      "grad_norm": 2.601565361022949,
      "learning_rate": 0.00021607417535464025,
      "loss": 0.8972,
      "step": 6646
    },
    {
      "epoch": 0.028401856140560774,
      "grad_norm": 4.809577465057373,
      "learning_rate": 0.0002160314476157922,
      "loss": 1.2753,
      "step": 6647
    },
    {
      "epoch": 0.028406129023987967,
      "grad_norm": 1.0952574014663696,
      "learning_rate": 0.00021598871987694413,
      "loss": 0.4786,
      "step": 6648
    },
    {
      "epoch": 0.028410401907415162,
      "grad_norm": 0.9801904559135437,
      "learning_rate": 0.00021594599213809606,
      "loss": 0.293,
      "step": 6649
    },
    {
      "epoch": 0.028414674790842358,
      "grad_norm": 2.766848564147949,
      "learning_rate": 0.00021590326439924798,
      "loss": 0.8134,
      "step": 6650
    },
    {
      "epoch": 0.02841894767426955,
      "grad_norm": 1.697292685508728,
      "learning_rate": 0.0002158605366603999,
      "loss": 0.4601,
      "step": 6651
    },
    {
      "epoch": 0.028423220557696745,
      "grad_norm": 4.799001216888428,
      "learning_rate": 0.00021581780892155188,
      "loss": 1.1253,
      "step": 6652
    },
    {
      "epoch": 0.028427493441123938,
      "grad_norm": 4.739413738250732,
      "learning_rate": 0.00021577508118270382,
      "loss": 2.6043,
      "step": 6653
    },
    {
      "epoch": 0.028431766324551133,
      "grad_norm": 1.9561426639556885,
      "learning_rate": 0.00021573235344385575,
      "loss": 0.7263,
      "step": 6654
    },
    {
      "epoch": 0.02843603920797833,
      "grad_norm": 3.7064785957336426,
      "learning_rate": 0.0002156896257050077,
      "loss": 1.2907,
      "step": 6655
    },
    {
      "epoch": 0.02844031209140552,
      "grad_norm": 4.740166187286377,
      "learning_rate": 0.00021564689796615963,
      "loss": 0.9561,
      "step": 6656
    },
    {
      "epoch": 0.028444584974832716,
      "grad_norm": 2.0215888023376465,
      "learning_rate": 0.00021560417022731157,
      "loss": 0.6093,
      "step": 6657
    },
    {
      "epoch": 0.028448857858259912,
      "grad_norm": 1.6848613023757935,
      "learning_rate": 0.0002155614424884635,
      "loss": 0.5288,
      "step": 6658
    },
    {
      "epoch": 0.028453130741687104,
      "grad_norm": 0.9417855739593506,
      "learning_rate": 0.00021551871474961547,
      "loss": 0.2504,
      "step": 6659
    },
    {
      "epoch": 0.0284574036251143,
      "grad_norm": 2.79254412651062,
      "learning_rate": 0.0002154759870107674,
      "loss": 0.6217,
      "step": 6660
    },
    {
      "epoch": 0.028461676508541495,
      "grad_norm": 2.120178461074829,
      "learning_rate": 0.00021543325927191935,
      "loss": 0.5325,
      "step": 6661
    },
    {
      "epoch": 0.028465949391968687,
      "grad_norm": 1.7243729829788208,
      "learning_rate": 0.00021539053153307128,
      "loss": 1.0413,
      "step": 6662
    },
    {
      "epoch": 0.028470222275395883,
      "grad_norm": 1.025552749633789,
      "learning_rate": 0.00021534780379422322,
      "loss": 0.5306,
      "step": 6663
    },
    {
      "epoch": 0.02847449515882308,
      "grad_norm": 2.100036382675171,
      "learning_rate": 0.00021530507605537516,
      "loss": 1.3538,
      "step": 6664
    },
    {
      "epoch": 0.02847876804225027,
      "grad_norm": 2.225266933441162,
      "learning_rate": 0.0002152623483165271,
      "loss": 0.668,
      "step": 6665
    },
    {
      "epoch": 0.028483040925677466,
      "grad_norm": 1.5663024187088013,
      "learning_rate": 0.000215219620577679,
      "loss": 0.5288,
      "step": 6666
    },
    {
      "epoch": 0.02848731380910466,
      "grad_norm": 3.2134900093078613,
      "learning_rate": 0.00021517689283883097,
      "loss": 0.9823,
      "step": 6667
    },
    {
      "epoch": 0.028491586692531854,
      "grad_norm": 0.9053745269775391,
      "learning_rate": 0.0002151341650999829,
      "loss": 0.4795,
      "step": 6668
    },
    {
      "epoch": 0.02849585957595905,
      "grad_norm": 1.9907724857330322,
      "learning_rate": 0.00021509143736113485,
      "loss": 1.3539,
      "step": 6669
    },
    {
      "epoch": 0.028500132459386242,
      "grad_norm": 0.9373041987419128,
      "learning_rate": 0.00021504870962228679,
      "loss": 0.2528,
      "step": 6670
    },
    {
      "epoch": 0.028504405342813437,
      "grad_norm": 0.8673322200775146,
      "learning_rate": 0.00021500598188343872,
      "loss": 0.4797,
      "step": 6671
    },
    {
      "epoch": 0.028508678226240633,
      "grad_norm": 1.1034278869628906,
      "learning_rate": 0.00021496325414459066,
      "loss": 0.315,
      "step": 6672
    },
    {
      "epoch": 0.028512951109667825,
      "grad_norm": 2.8171372413635254,
      "learning_rate": 0.0002149205264057426,
      "loss": 0.549,
      "step": 6673
    },
    {
      "epoch": 0.02851722399309502,
      "grad_norm": 0.9551379680633545,
      "learning_rate": 0.00021487779866689456,
      "loss": 0.485,
      "step": 6674
    },
    {
      "epoch": 0.028521496876522216,
      "grad_norm": 1.0013988018035889,
      "learning_rate": 0.0002148350709280465,
      "loss": 0.4294,
      "step": 6675
    },
    {
      "epoch": 0.02852576975994941,
      "grad_norm": 1.5482019186019897,
      "learning_rate": 0.00021479234318919844,
      "loss": 0.9999,
      "step": 6676
    },
    {
      "epoch": 0.028530042643376604,
      "grad_norm": 5.054090976715088,
      "learning_rate": 0.00021474961545035038,
      "loss": 1.0965,
      "step": 6677
    },
    {
      "epoch": 0.028534315526803796,
      "grad_norm": 0.7638745307922363,
      "learning_rate": 0.00021470688771150232,
      "loss": 0.2382,
      "step": 6678
    },
    {
      "epoch": 0.02853858841023099,
      "grad_norm": 1.1583623886108398,
      "learning_rate": 0.00021466415997265425,
      "loss": 0.4511,
      "step": 6679
    },
    {
      "epoch": 0.028542861293658187,
      "grad_norm": 2.3585588932037354,
      "learning_rate": 0.0002146214322338062,
      "loss": 0.7396,
      "step": 6680
    },
    {
      "epoch": 0.02854713417708538,
      "grad_norm": 0.7385451793670654,
      "learning_rate": 0.00021457870449495816,
      "loss": 0.2269,
      "step": 6681
    },
    {
      "epoch": 0.028551407060512575,
      "grad_norm": 1.2455180883407593,
      "learning_rate": 0.00021453597675611007,
      "loss": 0.5081,
      "step": 6682
    },
    {
      "epoch": 0.02855567994393977,
      "grad_norm": 0.9389076232910156,
      "learning_rate": 0.000214493249017262,
      "loss": 0.3633,
      "step": 6683
    },
    {
      "epoch": 0.028559952827366963,
      "grad_norm": 0.8070324063301086,
      "learning_rate": 0.00021445052127841394,
      "loss": 0.3442,
      "step": 6684
    },
    {
      "epoch": 0.02856422571079416,
      "grad_norm": 2.5286998748779297,
      "learning_rate": 0.00021440779353956588,
      "loss": 0.7844,
      "step": 6685
    },
    {
      "epoch": 0.028568498594221354,
      "grad_norm": 0.815031886100769,
      "learning_rate": 0.00021436506580071782,
      "loss": 0.276,
      "step": 6686
    },
    {
      "epoch": 0.028572771477648546,
      "grad_norm": 3.384892702102661,
      "learning_rate": 0.00021432233806186976,
      "loss": 0.8155,
      "step": 6687
    },
    {
      "epoch": 0.02857704436107574,
      "grad_norm": 2.1358869075775146,
      "learning_rate": 0.0002142796103230217,
      "loss": 0.6474,
      "step": 6688
    },
    {
      "epoch": 0.028581317244502937,
      "grad_norm": 3.2038841247558594,
      "learning_rate": 0.00021423688258417366,
      "loss": 1.0021,
      "step": 6689
    },
    {
      "epoch": 0.02858559012793013,
      "grad_norm": 0.7478234767913818,
      "learning_rate": 0.0002141941548453256,
      "loss": 0.2779,
      "step": 6690
    },
    {
      "epoch": 0.028589863011357325,
      "grad_norm": 0.8518131375312805,
      "learning_rate": 0.00021415142710647754,
      "loss": 0.5029,
      "step": 6691
    },
    {
      "epoch": 0.028594135894784517,
      "grad_norm": 3.3062238693237305,
      "learning_rate": 0.00021410869936762947,
      "loss": 0.8019,
      "step": 6692
    },
    {
      "epoch": 0.028598408778211713,
      "grad_norm": 1.7678439617156982,
      "learning_rate": 0.0002140659716287814,
      "loss": 0.6732,
      "step": 6693
    },
    {
      "epoch": 0.028602681661638908,
      "grad_norm": 1.2000247240066528,
      "learning_rate": 0.00021402324388993335,
      "loss": 0.4691,
      "step": 6694
    },
    {
      "epoch": 0.0286069545450661,
      "grad_norm": 3.8094637393951416,
      "learning_rate": 0.00021398051615108529,
      "loss": 1.1484,
      "step": 6695
    },
    {
      "epoch": 0.028611227428493296,
      "grad_norm": 2.939438819885254,
      "learning_rate": 0.00021393778841223725,
      "loss": 0.7923,
      "step": 6696
    },
    {
      "epoch": 0.02861550031192049,
      "grad_norm": 2.0397021770477295,
      "learning_rate": 0.0002138950606733892,
      "loss": 0.6256,
      "step": 6697
    },
    {
      "epoch": 0.028619773195347684,
      "grad_norm": 2.9861538410186768,
      "learning_rate": 0.00021385233293454113,
      "loss": 0.6975,
      "step": 6698
    },
    {
      "epoch": 0.02862404607877488,
      "grad_norm": 4.413573265075684,
      "learning_rate": 0.00021380960519569304,
      "loss": 1.3106,
      "step": 6699
    },
    {
      "epoch": 0.028628318962202075,
      "grad_norm": 2.249887704849243,
      "learning_rate": 0.00021376687745684498,
      "loss": 0.668,
      "step": 6700
    },
    {
      "epoch": 0.028632591845629267,
      "grad_norm": 1.497741460800171,
      "learning_rate": 0.0002137241497179969,
      "loss": 1.0027,
      "step": 6701
    },
    {
      "epoch": 0.028636864729056462,
      "grad_norm": 1.3420312404632568,
      "learning_rate": 0.00021368142197914885,
      "loss": 0.434,
      "step": 6702
    },
    {
      "epoch": 0.028641137612483655,
      "grad_norm": 0.8326394557952881,
      "learning_rate": 0.0002136386942403008,
      "loss": 0.4403,
      "step": 6703
    },
    {
      "epoch": 0.02864541049591085,
      "grad_norm": 0.8179321885108948,
      "learning_rate": 0.00021359596650145275,
      "loss": 0.4253,
      "step": 6704
    },
    {
      "epoch": 0.028649683379338046,
      "grad_norm": 1.2492154836654663,
      "learning_rate": 0.0002135532387626047,
      "loss": 0.3889,
      "step": 6705
    },
    {
      "epoch": 0.028653956262765238,
      "grad_norm": 3.066694498062134,
      "learning_rate": 0.00021351051102375663,
      "loss": 0.8815,
      "step": 6706
    },
    {
      "epoch": 0.028658229146192434,
      "grad_norm": 1.5300142765045166,
      "learning_rate": 0.00021346778328490857,
      "loss": 1.0111,
      "step": 6707
    },
    {
      "epoch": 0.02866250202961963,
      "grad_norm": 3.03328013420105,
      "learning_rate": 0.0002134250555460605,
      "loss": 0.7544,
      "step": 6708
    },
    {
      "epoch": 0.02866677491304682,
      "grad_norm": 3.1519370079040527,
      "learning_rate": 0.00021338232780721244,
      "loss": 1.7698,
      "step": 6709
    },
    {
      "epoch": 0.028671047796474017,
      "grad_norm": 3.042257785797119,
      "learning_rate": 0.00021333960006836438,
      "loss": 0.7427,
      "step": 6710
    },
    {
      "epoch": 0.028675320679901212,
      "grad_norm": 1.0010474920272827,
      "learning_rate": 0.00021329687232951635,
      "loss": 0.6449,
      "step": 6711
    },
    {
      "epoch": 0.028679593563328405,
      "grad_norm": 0.7691868543624878,
      "learning_rate": 0.00021325414459066828,
      "loss": 0.2778,
      "step": 6712
    },
    {
      "epoch": 0.0286838664467556,
      "grad_norm": 2.7709240913391113,
      "learning_rate": 0.00021321141685182022,
      "loss": 0.9195,
      "step": 6713
    },
    {
      "epoch": 0.028688139330182792,
      "grad_norm": 1.094517707824707,
      "learning_rate": 0.00021316868911297216,
      "loss": 0.381,
      "step": 6714
    },
    {
      "epoch": 0.028692412213609988,
      "grad_norm": 1.6443349123001099,
      "learning_rate": 0.00021312596137412407,
      "loss": 0.5995,
      "step": 6715
    },
    {
      "epoch": 0.028696685097037183,
      "grad_norm": 1.6721190214157104,
      "learning_rate": 0.000213083233635276,
      "loss": 0.4258,
      "step": 6716
    },
    {
      "epoch": 0.028700957980464376,
      "grad_norm": 2.6640565395355225,
      "learning_rate": 0.00021304050589642795,
      "loss": 0.9403,
      "step": 6717
    },
    {
      "epoch": 0.02870523086389157,
      "grad_norm": 1.8590611219406128,
      "learning_rate": 0.00021299777815757988,
      "loss": 0.5255,
      "step": 6718
    },
    {
      "epoch": 0.028709503747318767,
      "grad_norm": 1.1703823804855347,
      "learning_rate": 0.00021295505041873185,
      "loss": 0.418,
      "step": 6719
    },
    {
      "epoch": 0.02871377663074596,
      "grad_norm": 2.7135112285614014,
      "learning_rate": 0.0002129123226798838,
      "loss": 1.0181,
      "step": 6720
    },
    {
      "epoch": 0.028718049514173154,
      "grad_norm": 1.141677737236023,
      "learning_rate": 0.00021286959494103572,
      "loss": 0.4028,
      "step": 6721
    },
    {
      "epoch": 0.02872232239760035,
      "grad_norm": 1.6763057708740234,
      "learning_rate": 0.00021282686720218766,
      "loss": 0.4894,
      "step": 6722
    },
    {
      "epoch": 0.028726595281027542,
      "grad_norm": 1.809525728225708,
      "learning_rate": 0.0002127841394633396,
      "loss": 0.7807,
      "step": 6723
    },
    {
      "epoch": 0.028730868164454738,
      "grad_norm": 1.6597683429718018,
      "learning_rate": 0.00021274141172449154,
      "loss": 0.4756,
      "step": 6724
    },
    {
      "epoch": 0.028735141047881933,
      "grad_norm": 4.607907295227051,
      "learning_rate": 0.00021269868398564348,
      "loss": 1.2346,
      "step": 6725
    },
    {
      "epoch": 0.028739413931309125,
      "grad_norm": 0.9205686450004578,
      "learning_rate": 0.00021265595624679544,
      "loss": 0.5739,
      "step": 6726
    },
    {
      "epoch": 0.02874368681473632,
      "grad_norm": 0.9907315969467163,
      "learning_rate": 0.00021261322850794738,
      "loss": 0.3813,
      "step": 6727
    },
    {
      "epoch": 0.028747959698163513,
      "grad_norm": 0.9106393456459045,
      "learning_rate": 0.00021257050076909932,
      "loss": 0.5601,
      "step": 6728
    },
    {
      "epoch": 0.02875223258159071,
      "grad_norm": 0.9092304706573486,
      "learning_rate": 0.00021252777303025125,
      "loss": 0.5599,
      "step": 6729
    },
    {
      "epoch": 0.028756505465017904,
      "grad_norm": 2.997720241546631,
      "learning_rate": 0.0002124850452914032,
      "loss": 1.026,
      "step": 6730
    },
    {
      "epoch": 0.028760778348445096,
      "grad_norm": 0.9416459202766418,
      "learning_rate": 0.00021244231755255513,
      "loss": 0.3815,
      "step": 6731
    },
    {
      "epoch": 0.028765051231872292,
      "grad_norm": 2.6337947845458984,
      "learning_rate": 0.00021239958981370704,
      "loss": 0.9167,
      "step": 6732
    },
    {
      "epoch": 0.028769324115299488,
      "grad_norm": 2.5219175815582275,
      "learning_rate": 0.00021235686207485898,
      "loss": 0.5913,
      "step": 6733
    },
    {
      "epoch": 0.02877359699872668,
      "grad_norm": 0.7996031641960144,
      "learning_rate": 0.00021231413433601094,
      "loss": 0.3188,
      "step": 6734
    },
    {
      "epoch": 0.028777869882153875,
      "grad_norm": 0.7546573281288147,
      "learning_rate": 0.00021227140659716288,
      "loss": 0.3264,
      "step": 6735
    },
    {
      "epoch": 0.02878214276558107,
      "grad_norm": 1.512526512145996,
      "learning_rate": 0.00021222867885831482,
      "loss": 0.5355,
      "step": 6736
    },
    {
      "epoch": 0.028786415649008263,
      "grad_norm": 1.530824065208435,
      "learning_rate": 0.00021218595111946676,
      "loss": 0.5507,
      "step": 6737
    },
    {
      "epoch": 0.02879068853243546,
      "grad_norm": 0.9012845754623413,
      "learning_rate": 0.0002121432233806187,
      "loss": 0.3188,
      "step": 6738
    },
    {
      "epoch": 0.02879496141586265,
      "grad_norm": 0.9175384044647217,
      "learning_rate": 0.00021210049564177063,
      "loss": 0.3329,
      "step": 6739
    },
    {
      "epoch": 0.028799234299289846,
      "grad_norm": 0.8011786341667175,
      "learning_rate": 0.00021205776790292257,
      "loss": 0.3442,
      "step": 6740
    },
    {
      "epoch": 0.028803507182717042,
      "grad_norm": 1.934809923171997,
      "learning_rate": 0.00021201504016407454,
      "loss": 1.3604,
      "step": 6741
    },
    {
      "epoch": 0.028807780066144234,
      "grad_norm": 1.030484676361084,
      "learning_rate": 0.00021197231242522647,
      "loss": 0.3519,
      "step": 6742
    },
    {
      "epoch": 0.02881205294957143,
      "grad_norm": 1.4106028079986572,
      "learning_rate": 0.0002119295846863784,
      "loss": 0.3535,
      "step": 6743
    },
    {
      "epoch": 0.028816325832998625,
      "grad_norm": 3.5405049324035645,
      "learning_rate": 0.00021188685694753035,
      "loss": 1.2785,
      "step": 6744
    },
    {
      "epoch": 0.028820598716425817,
      "grad_norm": 3.0126893520355225,
      "learning_rate": 0.0002118441292086823,
      "loss": 0.5254,
      "step": 6745
    },
    {
      "epoch": 0.028824871599853013,
      "grad_norm": 1.0212763547897339,
      "learning_rate": 0.00021180140146983422,
      "loss": 0.3992,
      "step": 6746
    },
    {
      "epoch": 0.02882914448328021,
      "grad_norm": 1.3551853895187378,
      "learning_rate": 0.00021175867373098616,
      "loss": 0.4921,
      "step": 6747
    },
    {
      "epoch": 0.0288334173667074,
      "grad_norm": 4.544274806976318,
      "learning_rate": 0.0002117159459921381,
      "loss": 2.5061,
      "step": 6748
    },
    {
      "epoch": 0.028837690250134596,
      "grad_norm": 3.44610595703125,
      "learning_rate": 0.00021167321825329004,
      "loss": 0.7842,
      "step": 6749
    },
    {
      "epoch": 0.028841963133561792,
      "grad_norm": 0.5149135589599609,
      "learning_rate": 0.00021163049051444198,
      "loss": 0.1954,
      "step": 6750
    },
    {
      "epoch": 0.028846236016988984,
      "grad_norm": 1.6543405055999756,
      "learning_rate": 0.00021158776277559391,
      "loss": 0.7402,
      "step": 6751
    },
    {
      "epoch": 0.02885050890041618,
      "grad_norm": 1.7612667083740234,
      "learning_rate": 0.00021154503503674585,
      "loss": 0.685,
      "step": 6752
    },
    {
      "epoch": 0.02885478178384337,
      "grad_norm": 3.8955135345458984,
      "learning_rate": 0.0002115023072978978,
      "loss": 1.2187,
      "step": 6753
    },
    {
      "epoch": 0.028859054667270567,
      "grad_norm": 0.4967748820781708,
      "learning_rate": 0.00021145957955904973,
      "loss": 0.1822,
      "step": 6754
    },
    {
      "epoch": 0.028863327550697763,
      "grad_norm": 1.9202038049697876,
      "learning_rate": 0.00021141685182020167,
      "loss": 0.533,
      "step": 6755
    },
    {
      "epoch": 0.028867600434124955,
      "grad_norm": 3.4463679790496826,
      "learning_rate": 0.00021137412408135363,
      "loss": 1.0906,
      "step": 6756
    },
    {
      "epoch": 0.02887187331755215,
      "grad_norm": 1.2210822105407715,
      "learning_rate": 0.00021133139634250557,
      "loss": 0.3889,
      "step": 6757
    },
    {
      "epoch": 0.028876146200979346,
      "grad_norm": 0.8255118131637573,
      "learning_rate": 0.0002112886686036575,
      "loss": 0.264,
      "step": 6758
    },
    {
      "epoch": 0.028880419084406538,
      "grad_norm": 2.91024112701416,
      "learning_rate": 0.00021124594086480944,
      "loss": 0.869,
      "step": 6759
    },
    {
      "epoch": 0.028884691967833734,
      "grad_norm": 0.9559406042098999,
      "learning_rate": 0.00021120321312596138,
      "loss": 0.5011,
      "step": 6760
    },
    {
      "epoch": 0.02888896485126093,
      "grad_norm": 1.5510245561599731,
      "learning_rate": 0.00021116048538711332,
      "loss": 0.6015,
      "step": 6761
    },
    {
      "epoch": 0.02889323773468812,
      "grad_norm": 0.8293738961219788,
      "learning_rate": 0.00021111775764826526,
      "loss": 0.3444,
      "step": 6762
    },
    {
      "epoch": 0.028897510618115317,
      "grad_norm": 1.435131311416626,
      "learning_rate": 0.00021107502990941722,
      "loss": 0.9558,
      "step": 6763
    },
    {
      "epoch": 0.02890178350154251,
      "grad_norm": 1.9065937995910645,
      "learning_rate": 0.00021103230217056916,
      "loss": 0.4355,
      "step": 6764
    },
    {
      "epoch": 0.028906056384969705,
      "grad_norm": 1.6001993417739868,
      "learning_rate": 0.00021098957443172107,
      "loss": 0.5575,
      "step": 6765
    },
    {
      "epoch": 0.0289103292683969,
      "grad_norm": 1.8267607688903809,
      "learning_rate": 0.000210946846692873,
      "loss": 0.7331,
      "step": 6766
    },
    {
      "epoch": 0.028914602151824093,
      "grad_norm": 3.450096845626831,
      "learning_rate": 0.00021090411895402495,
      "loss": 1.5114,
      "step": 6767
    },
    {
      "epoch": 0.028918875035251288,
      "grad_norm": 2.632680892944336,
      "learning_rate": 0.00021086139121517688,
      "loss": 0.8758,
      "step": 6768
    },
    {
      "epoch": 0.028923147918678484,
      "grad_norm": 0.5803912281990051,
      "learning_rate": 0.00021081866347632882,
      "loss": 0.187,
      "step": 6769
    },
    {
      "epoch": 0.028927420802105676,
      "grad_norm": 3.0670793056488037,
      "learning_rate": 0.00021077593573748076,
      "loss": 0.8853,
      "step": 6770
    },
    {
      "epoch": 0.02893169368553287,
      "grad_norm": 1.370283842086792,
      "learning_rate": 0.00021073320799863272,
      "loss": 0.4483,
      "step": 6771
    },
    {
      "epoch": 0.028935966568960067,
      "grad_norm": 1.7299532890319824,
      "learning_rate": 0.00021069048025978466,
      "loss": 0.3975,
      "step": 6772
    },
    {
      "epoch": 0.02894023945238726,
      "grad_norm": 1.441986322402954,
      "learning_rate": 0.0002106477525209366,
      "loss": 0.9558,
      "step": 6773
    },
    {
      "epoch": 0.028944512335814455,
      "grad_norm": 1.2941560745239258,
      "learning_rate": 0.00021060502478208854,
      "loss": 0.4044,
      "step": 6774
    },
    {
      "epoch": 0.02894878521924165,
      "grad_norm": 0.9970303773880005,
      "learning_rate": 0.00021056229704324048,
      "loss": 0.3036,
      "step": 6775
    },
    {
      "epoch": 0.028953058102668842,
      "grad_norm": 4.328343868255615,
      "learning_rate": 0.00021051956930439241,
      "loss": 1.3206,
      "step": 6776
    },
    {
      "epoch": 0.028957330986096038,
      "grad_norm": 0.8955198526382446,
      "learning_rate": 0.00021047684156554435,
      "loss": 0.5281,
      "step": 6777
    },
    {
      "epoch": 0.02896160386952323,
      "grad_norm": 1.7300976514816284,
      "learning_rate": 0.00021043411382669632,
      "loss": 0.3673,
      "step": 6778
    },
    {
      "epoch": 0.028965876752950426,
      "grad_norm": 1.1906577348709106,
      "learning_rate": 0.00021039138608784825,
      "loss": 0.4702,
      "step": 6779
    },
    {
      "epoch": 0.02897014963637762,
      "grad_norm": 1.755600094795227,
      "learning_rate": 0.0002103486583490002,
      "loss": 0.6518,
      "step": 6780
    },
    {
      "epoch": 0.028974422519804813,
      "grad_norm": 0.4926758408546448,
      "learning_rate": 0.0002103059306101521,
      "loss": 0.1828,
      "step": 6781
    },
    {
      "epoch": 0.02897869540323201,
      "grad_norm": 1.1088027954101562,
      "learning_rate": 0.00021026320287130404,
      "loss": 0.4889,
      "step": 6782
    },
    {
      "epoch": 0.028982968286659205,
      "grad_norm": 0.9419219493865967,
      "learning_rate": 0.00021022047513245598,
      "loss": 0.4717,
      "step": 6783
    },
    {
      "epoch": 0.028987241170086397,
      "grad_norm": 0.9753703474998474,
      "learning_rate": 0.00021017774739360792,
      "loss": 0.352,
      "step": 6784
    },
    {
      "epoch": 0.028991514053513592,
      "grad_norm": 1.1207479238510132,
      "learning_rate": 0.00021013501965475985,
      "loss": 0.4301,
      "step": 6785
    },
    {
      "epoch": 0.028995786936940788,
      "grad_norm": 4.57499885559082,
      "learning_rate": 0.00021009229191591182,
      "loss": 1.2722,
      "step": 6786
    },
    {
      "epoch": 0.02900005982036798,
      "grad_norm": 4.035481929779053,
      "learning_rate": 0.00021004956417706376,
      "loss": 1.0092,
      "step": 6787
    },
    {
      "epoch": 0.029004332703795176,
      "grad_norm": 1.5306416749954224,
      "learning_rate": 0.0002100068364382157,
      "loss": 0.6645,
      "step": 6788
    },
    {
      "epoch": 0.029008605587222368,
      "grad_norm": 0.7140620946884155,
      "learning_rate": 0.00020996410869936763,
      "loss": 0.3049,
      "step": 6789
    },
    {
      "epoch": 0.029012878470649563,
      "grad_norm": 3.0142574310302734,
      "learning_rate": 0.00020992138096051957,
      "loss": 0.9409,
      "step": 6790
    },
    {
      "epoch": 0.02901715135407676,
      "grad_norm": 0.9730172753334045,
      "learning_rate": 0.0002098786532216715,
      "loss": 0.4149,
      "step": 6791
    },
    {
      "epoch": 0.02902142423750395,
      "grad_norm": 0.8537961840629578,
      "learning_rate": 0.00020983592548282345,
      "loss": 0.294,
      "step": 6792
    },
    {
      "epoch": 0.029025697120931147,
      "grad_norm": 0.9329156279563904,
      "learning_rate": 0.0002097931977439754,
      "loss": 0.419,
      "step": 6793
    },
    {
      "epoch": 0.029029970004358342,
      "grad_norm": 1.7409101724624634,
      "learning_rate": 0.00020975047000512735,
      "loss": 0.5443,
      "step": 6794
    },
    {
      "epoch": 0.029034242887785534,
      "grad_norm": 0.7601951360702515,
      "learning_rate": 0.0002097077422662793,
      "loss": 0.3272,
      "step": 6795
    },
    {
      "epoch": 0.02903851577121273,
      "grad_norm": 2.6437830924987793,
      "learning_rate": 0.00020966501452743123,
      "loss": 0.6306,
      "step": 6796
    },
    {
      "epoch": 0.029042788654639926,
      "grad_norm": 1.2119743824005127,
      "learning_rate": 0.00020962228678858316,
      "loss": 0.4928,
      "step": 6797
    },
    {
      "epoch": 0.029047061538067118,
      "grad_norm": 2.484920024871826,
      "learning_rate": 0.00020957955904973507,
      "loss": 0.7034,
      "step": 6798
    },
    {
      "epoch": 0.029051334421494313,
      "grad_norm": 4.263877868652344,
      "learning_rate": 0.000209536831310887,
      "loss": 1.1408,
      "step": 6799
    },
    {
      "epoch": 0.02905560730492151,
      "grad_norm": 0.9508766531944275,
      "learning_rate": 0.00020949410357203898,
      "loss": 0.4502,
      "step": 6800
    },
    {
      "epoch": 0.0290598801883487,
      "grad_norm": 0.8195330500602722,
      "learning_rate": 0.00020945137583319091,
      "loss": 0.3351,
      "step": 6801
    },
    {
      "epoch": 0.029064153071775897,
      "grad_norm": 0.9453494548797607,
      "learning_rate": 0.00020940864809434285,
      "loss": 0.3187,
      "step": 6802
    },
    {
      "epoch": 0.02906842595520309,
      "grad_norm": 1.222605586051941,
      "learning_rate": 0.0002093659203554948,
      "loss": 0.4752,
      "step": 6803
    },
    {
      "epoch": 0.029072698838630284,
      "grad_norm": 0.9997275471687317,
      "learning_rate": 0.00020932319261664673,
      "loss": 0.4599,
      "step": 6804
    },
    {
      "epoch": 0.02907697172205748,
      "grad_norm": 0.6824619770050049,
      "learning_rate": 0.00020928046487779867,
      "loss": 0.2715,
      "step": 6805
    },
    {
      "epoch": 0.029081244605484672,
      "grad_norm": 0.8904062509536743,
      "learning_rate": 0.0002092377371389506,
      "loss": 0.504,
      "step": 6806
    },
    {
      "epoch": 0.029085517488911868,
      "grad_norm": 3.9356632232666016,
      "learning_rate": 0.00020919500940010254,
      "loss": 1.3426,
      "step": 6807
    },
    {
      "epoch": 0.029089790372339063,
      "grad_norm": 2.197148323059082,
      "learning_rate": 0.0002091522816612545,
      "loss": 0.5267,
      "step": 6808
    },
    {
      "epoch": 0.029094063255766255,
      "grad_norm": 4.088473320007324,
      "learning_rate": 0.00020910955392240644,
      "loss": 1.1148,
      "step": 6809
    },
    {
      "epoch": 0.02909833613919345,
      "grad_norm": 1.28455650806427,
      "learning_rate": 0.00020906682618355838,
      "loss": 0.3886,
      "step": 6810
    },
    {
      "epoch": 0.029102609022620646,
      "grad_norm": 0.6008555293083191,
      "learning_rate": 0.00020902409844471032,
      "loss": 0.2236,
      "step": 6811
    },
    {
      "epoch": 0.02910688190604784,
      "grad_norm": 1.911281943321228,
      "learning_rate": 0.00020898137070586226,
      "loss": 0.4842,
      "step": 6812
    },
    {
      "epoch": 0.029111154789475034,
      "grad_norm": 1.4175647497177124,
      "learning_rate": 0.0002089386429670142,
      "loss": 0.536,
      "step": 6813
    },
    {
      "epoch": 0.029115427672902226,
      "grad_norm": 1.4294672012329102,
      "learning_rate": 0.0002088959152281661,
      "loss": 0.5031,
      "step": 6814
    },
    {
      "epoch": 0.029119700556329422,
      "grad_norm": 1.5733038187026978,
      "learning_rate": 0.00020885318748931807,
      "loss": 0.6004,
      "step": 6815
    },
    {
      "epoch": 0.029123973439756617,
      "grad_norm": 2.852931261062622,
      "learning_rate": 0.00020881045975047,
      "loss": 0.7509,
      "step": 6816
    },
    {
      "epoch": 0.02912824632318381,
      "grad_norm": 0.9875213503837585,
      "learning_rate": 0.00020876773201162195,
      "loss": 0.4154,
      "step": 6817
    },
    {
      "epoch": 0.029132519206611005,
      "grad_norm": 0.6818537712097168,
      "learning_rate": 0.00020872500427277388,
      "loss": 0.3043,
      "step": 6818
    },
    {
      "epoch": 0.0291367920900382,
      "grad_norm": 2.0721397399902344,
      "learning_rate": 0.00020868227653392582,
      "loss": 1.3138,
      "step": 6819
    },
    {
      "epoch": 0.029141064973465393,
      "grad_norm": 1.4608060121536255,
      "learning_rate": 0.00020863954879507776,
      "loss": 0.9389,
      "step": 6820
    },
    {
      "epoch": 0.02914533785689259,
      "grad_norm": 0.6353972554206848,
      "learning_rate": 0.0002085968210562297,
      "loss": 0.229,
      "step": 6821
    },
    {
      "epoch": 0.029149610740319784,
      "grad_norm": 4.371092319488525,
      "learning_rate": 0.00020855409331738164,
      "loss": 1.1493,
      "step": 6822
    },
    {
      "epoch": 0.029153883623746976,
      "grad_norm": 3.3197593688964844,
      "learning_rate": 0.0002085113655785336,
      "loss": 1.083,
      "step": 6823
    },
    {
      "epoch": 0.029158156507174172,
      "grad_norm": 2.02111554145813,
      "learning_rate": 0.00020846863783968554,
      "loss": 1.3138,
      "step": 6824
    },
    {
      "epoch": 0.029162429390601364,
      "grad_norm": 2.848015069961548,
      "learning_rate": 0.00020842591010083748,
      "loss": 0.9711,
      "step": 6825
    },
    {
      "epoch": 0.02916670227402856,
      "grad_norm": 1.017574667930603,
      "learning_rate": 0.00020838318236198941,
      "loss": 0.5639,
      "step": 6826
    },
    {
      "epoch": 0.029170975157455755,
      "grad_norm": 1.101259469985962,
      "learning_rate": 0.00020834045462314135,
      "loss": 0.4516,
      "step": 6827
    },
    {
      "epoch": 0.029175248040882947,
      "grad_norm": 0.9547580480575562,
      "learning_rate": 0.0002082977268842933,
      "loss": 0.3783,
      "step": 6828
    },
    {
      "epoch": 0.029179520924310143,
      "grad_norm": 0.6844862699508667,
      "learning_rate": 0.00020825499914544523,
      "loss": 0.2641,
      "step": 6829
    },
    {
      "epoch": 0.02918379380773734,
      "grad_norm": 1.0034990310668945,
      "learning_rate": 0.00020821227140659717,
      "loss": 0.3975,
      "step": 6830
    },
    {
      "epoch": 0.02918806669116453,
      "grad_norm": 0.5950153470039368,
      "learning_rate": 0.0002081695436677491,
      "loss": 0.2054,
      "step": 6831
    },
    {
      "epoch": 0.029192339574591726,
      "grad_norm": 1.0300490856170654,
      "learning_rate": 0.00020812681592890104,
      "loss": 0.6331,
      "step": 6832
    },
    {
      "epoch": 0.02919661245801892,
      "grad_norm": 1.01925528049469,
      "learning_rate": 0.00020808408819005298,
      "loss": 0.6331,
      "step": 6833
    },
    {
      "epoch": 0.029200885341446114,
      "grad_norm": 0.5877968668937683,
      "learning_rate": 0.00020804136045120492,
      "loss": 0.2424,
      "step": 6834
    },
    {
      "epoch": 0.02920515822487331,
      "grad_norm": 2.890772581100464,
      "learning_rate": 0.00020799863271235686,
      "loss": 0.9069,
      "step": 6835
    },
    {
      "epoch": 0.029209431108300505,
      "grad_norm": 2.9000749588012695,
      "learning_rate": 0.0002079559049735088,
      "loss": 0.8924,
      "step": 6836
    },
    {
      "epoch": 0.029213703991727697,
      "grad_norm": 0.8867054581642151,
      "learning_rate": 0.00020791317723466073,
      "loss": 0.5466,
      "step": 6837
    },
    {
      "epoch": 0.029217976875154893,
      "grad_norm": 0.8904255032539368,
      "learning_rate": 0.0002078704494958127,
      "loss": 0.5621,
      "step": 6838
    },
    {
      "epoch": 0.029222249758582085,
      "grad_norm": 3.955029010772705,
      "learning_rate": 0.00020782772175696463,
      "loss": 0.9218,
      "step": 6839
    },
    {
      "epoch": 0.02922652264200928,
      "grad_norm": 3.8695359230041504,
      "learning_rate": 0.00020778499401811657,
      "loss": 1.1808,
      "step": 6840
    },
    {
      "epoch": 0.029230795525436476,
      "grad_norm": 1.4732508659362793,
      "learning_rate": 0.0002077422662792685,
      "loss": 0.5028,
      "step": 6841
    },
    {
      "epoch": 0.029235068408863668,
      "grad_norm": 1.1056042909622192,
      "learning_rate": 0.00020769953854042045,
      "loss": 0.3638,
      "step": 6842
    },
    {
      "epoch": 0.029239341292290864,
      "grad_norm": 2.7620606422424316,
      "learning_rate": 0.00020765681080157239,
      "loss": 0.7567,
      "step": 6843
    },
    {
      "epoch": 0.02924361417571806,
      "grad_norm": 4.076987266540527,
      "learning_rate": 0.00020761408306272432,
      "loss": 0.9326,
      "step": 6844
    },
    {
      "epoch": 0.02924788705914525,
      "grad_norm": 2.6943986415863037,
      "learning_rate": 0.0002075713553238763,
      "loss": 0.6898,
      "step": 6845
    },
    {
      "epoch": 0.029252159942572447,
      "grad_norm": 0.4675576686859131,
      "learning_rate": 0.00020752862758502823,
      "loss": 0.2099,
      "step": 6846
    },
    {
      "epoch": 0.029256432825999643,
      "grad_norm": 2.6993627548217773,
      "learning_rate": 0.00020748589984618014,
      "loss": 0.6511,
      "step": 6847
    },
    {
      "epoch": 0.029260705709426835,
      "grad_norm": 0.6491357088088989,
      "learning_rate": 0.00020744317210733207,
      "loss": 0.2271,
      "step": 6848
    },
    {
      "epoch": 0.02926497859285403,
      "grad_norm": 0.9360054135322571,
      "learning_rate": 0.000207400444368484,
      "loss": 0.3392,
      "step": 6849
    },
    {
      "epoch": 0.029269251476281222,
      "grad_norm": 3.3526828289031982,
      "learning_rate": 0.00020735771662963595,
      "loss": 1.0893,
      "step": 6850
    },
    {
      "epoch": 0.029273524359708418,
      "grad_norm": 1.0190329551696777,
      "learning_rate": 0.0002073149888907879,
      "loss": 0.3524,
      "step": 6851
    },
    {
      "epoch": 0.029277797243135614,
      "grad_norm": 1.6420955657958984,
      "learning_rate": 0.00020727226115193985,
      "loss": 0.4122,
      "step": 6852
    },
    {
      "epoch": 0.029282070126562806,
      "grad_norm": 1.5859358310699463,
      "learning_rate": 0.0002072295334130918,
      "loss": 1.0894,
      "step": 6853
    },
    {
      "epoch": 0.02928634300999,
      "grad_norm": 0.9754301309585571,
      "learning_rate": 0.00020718680567424373,
      "loss": 0.4592,
      "step": 6854
    },
    {
      "epoch": 0.029290615893417197,
      "grad_norm": 3.6922318935394287,
      "learning_rate": 0.00020714407793539567,
      "loss": 1.9763,
      "step": 6855
    },
    {
      "epoch": 0.02929488877684439,
      "grad_norm": 4.245894432067871,
      "learning_rate": 0.0002071013501965476,
      "loss": 1.1167,
      "step": 6856
    },
    {
      "epoch": 0.029299161660271585,
      "grad_norm": 2.1423559188842773,
      "learning_rate": 0.00020705862245769954,
      "loss": 1.4021,
      "step": 6857
    },
    {
      "epoch": 0.02930343454369878,
      "grad_norm": 1.1225279569625854,
      "learning_rate": 0.00020701589471885148,
      "loss": 0.4716,
      "step": 6858
    },
    {
      "epoch": 0.029307707427125972,
      "grad_norm": 4.572977542877197,
      "learning_rate": 0.00020697316698000342,
      "loss": 1.6488,
      "step": 6859
    },
    {
      "epoch": 0.029311980310553168,
      "grad_norm": 4.565420627593994,
      "learning_rate": 0.00020693043924115538,
      "loss": 1.5869,
      "step": 6860
    },
    {
      "epoch": 0.029316253193980363,
      "grad_norm": 4.4891839027404785,
      "learning_rate": 0.00020688771150230732,
      "loss": 1.5483,
      "step": 6861
    },
    {
      "epoch": 0.029320526077407556,
      "grad_norm": 4.393252849578857,
      "learning_rate": 0.00020684498376345926,
      "loss": 1.5059,
      "step": 6862
    },
    {
      "epoch": 0.02932479896083475,
      "grad_norm": 4.470669269561768,
      "learning_rate": 0.00020680225602461117,
      "loss": 1.6589,
      "step": 6863
    },
    {
      "epoch": 0.029329071844261943,
      "grad_norm": 4.164721488952637,
      "learning_rate": 0.0002067595282857631,
      "loss": 0.9946,
      "step": 6864
    },
    {
      "epoch": 0.02933334472768914,
      "grad_norm": 1.6989392042160034,
      "learning_rate": 0.00020671680054691504,
      "loss": 0.5554,
      "step": 6865
    },
    {
      "epoch": 0.029337617611116334,
      "grad_norm": 2.690358877182007,
      "learning_rate": 0.00020667407280806698,
      "loss": 0.8334,
      "step": 6866
    },
    {
      "epoch": 0.029341890494543527,
      "grad_norm": 0.8287748694419861,
      "learning_rate": 0.00020663134506921895,
      "loss": 0.216,
      "step": 6867
    },
    {
      "epoch": 0.029346163377970722,
      "grad_norm": 2.6244571208953857,
      "learning_rate": 0.00020658861733037089,
      "loss": 0.8256,
      "step": 6868
    },
    {
      "epoch": 0.029350436261397918,
      "grad_norm": 0.7757602334022522,
      "learning_rate": 0.00020654588959152282,
      "loss": 0.441,
      "step": 6869
    },
    {
      "epoch": 0.02935470914482511,
      "grad_norm": 1.5142691135406494,
      "learning_rate": 0.00020650316185267476,
      "loss": 0.3734,
      "step": 6870
    },
    {
      "epoch": 0.029358982028252305,
      "grad_norm": 1.9316202402114868,
      "learning_rate": 0.0002064604341138267,
      "loss": 0.6669,
      "step": 6871
    },
    {
      "epoch": 0.0293632549116795,
      "grad_norm": 0.7639932036399841,
      "learning_rate": 0.00020641770637497864,
      "loss": 0.4411,
      "step": 6872
    },
    {
      "epoch": 0.029367527795106693,
      "grad_norm": 0.8987748026847839,
      "learning_rate": 0.00020637497863613057,
      "loss": 0.5833,
      "step": 6873
    },
    {
      "epoch": 0.02937180067853389,
      "grad_norm": 3.0367674827575684,
      "learning_rate": 0.0002063322508972825,
      "loss": 0.9731,
      "step": 6874
    },
    {
      "epoch": 0.02937607356196108,
      "grad_norm": 0.9775216579437256,
      "learning_rate": 0.00020628952315843448,
      "loss": 0.3999,
      "step": 6875
    },
    {
      "epoch": 0.029380346445388276,
      "grad_norm": 3.2289090156555176,
      "learning_rate": 0.00020624679541958642,
      "loss": 1.4034,
      "step": 6876
    },
    {
      "epoch": 0.029384619328815472,
      "grad_norm": 2.741203546524048,
      "learning_rate": 0.00020620406768073835,
      "loss": 0.8699,
      "step": 6877
    },
    {
      "epoch": 0.029388892212242664,
      "grad_norm": 3.2508907318115234,
      "learning_rate": 0.0002061613399418903,
      "loss": 0.9075,
      "step": 6878
    },
    {
      "epoch": 0.02939316509566986,
      "grad_norm": 1.1601371765136719,
      "learning_rate": 0.00020611861220304223,
      "loss": 0.4349,
      "step": 6879
    },
    {
      "epoch": 0.029397437979097055,
      "grad_norm": 1.47206449508667,
      "learning_rate": 0.00020607588446419414,
      "loss": 0.5014,
      "step": 6880
    },
    {
      "epoch": 0.029401710862524248,
      "grad_norm": 0.7339754700660706,
      "learning_rate": 0.00020603315672534608,
      "loss": 0.3639,
      "step": 6881
    },
    {
      "epoch": 0.029405983745951443,
      "grad_norm": 0.8523664474487305,
      "learning_rate": 0.00020599042898649804,
      "loss": 0.282,
      "step": 6882
    },
    {
      "epoch": 0.02941025662937864,
      "grad_norm": 1.049190878868103,
      "learning_rate": 0.00020594770124764998,
      "loss": 0.3893,
      "step": 6883
    },
    {
      "epoch": 0.02941452951280583,
      "grad_norm": 0.45762568712234497,
      "learning_rate": 0.00020590497350880192,
      "loss": 0.1599,
      "step": 6884
    },
    {
      "epoch": 0.029418802396233026,
      "grad_norm": 1.7343298196792603,
      "learning_rate": 0.00020586224576995386,
      "loss": 0.7458,
      "step": 6885
    },
    {
      "epoch": 0.029423075279660222,
      "grad_norm": 3.183549165725708,
      "learning_rate": 0.0002058195180311058,
      "loss": 0.7602,
      "step": 6886
    },
    {
      "epoch": 0.029427348163087414,
      "grad_norm": 1.0812442302703857,
      "learning_rate": 0.00020577679029225773,
      "loss": 0.4698,
      "step": 6887
    },
    {
      "epoch": 0.02943162104651461,
      "grad_norm": 1.130829930305481,
      "learning_rate": 0.00020573406255340967,
      "loss": 0.4515,
      "step": 6888
    },
    {
      "epoch": 0.029435893929941802,
      "grad_norm": 0.8713144659996033,
      "learning_rate": 0.0002056913348145616,
      "loss": 0.5502,
      "step": 6889
    },
    {
      "epoch": 0.029440166813368997,
      "grad_norm": 3.123744487762451,
      "learning_rate": 0.00020564860707571357,
      "loss": 0.8868,
      "step": 6890
    },
    {
      "epoch": 0.029444439696796193,
      "grad_norm": 0.8510161638259888,
      "learning_rate": 0.0002056058793368655,
      "loss": 0.3701,
      "step": 6891
    },
    {
      "epoch": 0.029448712580223385,
      "grad_norm": 2.2637619972229004,
      "learning_rate": 0.00020556315159801745,
      "loss": 0.6014,
      "step": 6892
    },
    {
      "epoch": 0.02945298546365058,
      "grad_norm": 3.2141449451446533,
      "learning_rate": 0.00020552042385916939,
      "loss": 1.7396,
      "step": 6893
    },
    {
      "epoch": 0.029457258347077776,
      "grad_norm": 1.6891758441925049,
      "learning_rate": 0.00020547769612032132,
      "loss": 0.6901,
      "step": 6894
    },
    {
      "epoch": 0.02946153123050497,
      "grad_norm": 1.3245956897735596,
      "learning_rate": 0.00020543496838147326,
      "loss": 1.0007,
      "step": 6895
    },
    {
      "epoch": 0.029465804113932164,
      "grad_norm": 0.8886213898658752,
      "learning_rate": 0.00020539224064262517,
      "loss": 0.553,
      "step": 6896
    },
    {
      "epoch": 0.02947007699735936,
      "grad_norm": 0.9408119320869446,
      "learning_rate": 0.00020534951290377714,
      "loss": 0.3483,
      "step": 6897
    },
    {
      "epoch": 0.02947434988078655,
      "grad_norm": 0.7867489457130432,
      "learning_rate": 0.00020530678516492907,
      "loss": 0.4329,
      "step": 6898
    },
    {
      "epoch": 0.029478622764213747,
      "grad_norm": 3.181027889251709,
      "learning_rate": 0.000205264057426081,
      "loss": 0.9463,
      "step": 6899
    },
    {
      "epoch": 0.02948289564764094,
      "grad_norm": 1.7360328435897827,
      "learning_rate": 0.00020522132968723295,
      "loss": 1.3024,
      "step": 6900
    },
    {
      "epoch": 0.029487168531068135,
      "grad_norm": 0.9883173704147339,
      "learning_rate": 0.0002051786019483849,
      "loss": 0.3817,
      "step": 6901
    },
    {
      "epoch": 0.02949144141449533,
      "grad_norm": 1.612527847290039,
      "learning_rate": 0.00020513587420953683,
      "loss": 0.3727,
      "step": 6902
    },
    {
      "epoch": 0.029495714297922523,
      "grad_norm": 3.546809434890747,
      "learning_rate": 0.00020509314647068876,
      "loss": 1.2708,
      "step": 6903
    },
    {
      "epoch": 0.02949998718134972,
      "grad_norm": 2.2291669845581055,
      "learning_rate": 0.00020505041873184073,
      "loss": 0.7132,
      "step": 6904
    },
    {
      "epoch": 0.029504260064776914,
      "grad_norm": 2.6309049129486084,
      "learning_rate": 0.00020500769099299267,
      "loss": 0.6999,
      "step": 6905
    },
    {
      "epoch": 0.029508532948204106,
      "grad_norm": 0.814703106880188,
      "learning_rate": 0.0002049649632541446,
      "loss": 0.4516,
      "step": 6906
    },
    {
      "epoch": 0.0295128058316313,
      "grad_norm": 0.3201931416988373,
      "learning_rate": 0.00020492223551529654,
      "loss": 0.0892,
      "step": 6907
    },
    {
      "epoch": 0.029517078715058497,
      "grad_norm": 2.5850560665130615,
      "learning_rate": 0.00020487950777644848,
      "loss": 0.6546,
      "step": 6908
    },
    {
      "epoch": 0.02952135159848569,
      "grad_norm": 0.7953089475631714,
      "learning_rate": 0.00020483678003760042,
      "loss": 0.4433,
      "step": 6909
    },
    {
      "epoch": 0.029525624481912885,
      "grad_norm": 0.6364697217941284,
      "learning_rate": 0.00020479405229875236,
      "loss": 0.2165,
      "step": 6910
    },
    {
      "epoch": 0.02952989736534008,
      "grad_norm": 4.578354358673096,
      "learning_rate": 0.0002047513245599043,
      "loss": 1.2856,
      "step": 6911
    },
    {
      "epoch": 0.029534170248767273,
      "grad_norm": 3.7686429023742676,
      "learning_rate": 0.00020470859682105626,
      "loss": 1.2954,
      "step": 6912
    },
    {
      "epoch": 0.029538443132194468,
      "grad_norm": 2.2815093994140625,
      "learning_rate": 0.00020466586908220817,
      "loss": 1.0213,
      "step": 6913
    },
    {
      "epoch": 0.02954271601562166,
      "grad_norm": 1.6125130653381348,
      "learning_rate": 0.0002046231413433601,
      "loss": 0.518,
      "step": 6914
    },
    {
      "epoch": 0.029546988899048856,
      "grad_norm": 0.9169929623603821,
      "learning_rate": 0.00020458041360451205,
      "loss": 0.3812,
      "step": 6915
    },
    {
      "epoch": 0.02955126178247605,
      "grad_norm": 1.9937134981155396,
      "learning_rate": 0.00020453768586566398,
      "loss": 0.7002,
      "step": 6916
    },
    {
      "epoch": 0.029555534665903244,
      "grad_norm": 3.0453526973724365,
      "learning_rate": 0.00020449495812681592,
      "loss": 0.8558,
      "step": 6917
    },
    {
      "epoch": 0.02955980754933044,
      "grad_norm": 0.9639694094657898,
      "learning_rate": 0.00020445223038796786,
      "loss": 0.4033,
      "step": 6918
    },
    {
      "epoch": 0.029564080432757635,
      "grad_norm": 0.8653599619865417,
      "learning_rate": 0.00020440950264911982,
      "loss": 0.5492,
      "step": 6919
    },
    {
      "epoch": 0.029568353316184827,
      "grad_norm": 1.9269877672195435,
      "learning_rate": 0.00020436677491027176,
      "loss": 0.6374,
      "step": 6920
    },
    {
      "epoch": 0.029572626199612023,
      "grad_norm": 4.580826759338379,
      "learning_rate": 0.0002043240471714237,
      "loss": 1.1433,
      "step": 6921
    },
    {
      "epoch": 0.029576899083039218,
      "grad_norm": 3.0069735050201416,
      "learning_rate": 0.00020428131943257564,
      "loss": 0.7762,
      "step": 6922
    },
    {
      "epoch": 0.02958117196646641,
      "grad_norm": 1.909604549407959,
      "learning_rate": 0.00020423859169372758,
      "loss": 0.6041,
      "step": 6923
    },
    {
      "epoch": 0.029585444849893606,
      "grad_norm": 0.8231461644172668,
      "learning_rate": 0.0002041958639548795,
      "loss": 0.2893,
      "step": 6924
    },
    {
      "epoch": 0.029589717733320798,
      "grad_norm": 3.348243236541748,
      "learning_rate": 0.00020415313621603145,
      "loss": 0.993,
      "step": 6925
    },
    {
      "epoch": 0.029593990616747994,
      "grad_norm": 0.8730226159095764,
      "learning_rate": 0.0002041104084771834,
      "loss": 0.535,
      "step": 6926
    },
    {
      "epoch": 0.02959826350017519,
      "grad_norm": 4.048458576202393,
      "learning_rate": 0.00020406768073833535,
      "loss": 0.9061,
      "step": 6927
    },
    {
      "epoch": 0.02960253638360238,
      "grad_norm": 0.767217218875885,
      "learning_rate": 0.0002040249529994873,
      "loss": 0.4218,
      "step": 6928
    },
    {
      "epoch": 0.029606809267029577,
      "grad_norm": 1.442076325416565,
      "learning_rate": 0.0002039822252606392,
      "loss": 1.0468,
      "step": 6929
    },
    {
      "epoch": 0.029611082150456772,
      "grad_norm": 0.7063400149345398,
      "learning_rate": 0.00020393949752179114,
      "loss": 0.2712,
      "step": 6930
    },
    {
      "epoch": 0.029615355033883965,
      "grad_norm": 0.7056816816329956,
      "learning_rate": 0.00020389676978294308,
      "loss": 0.3264,
      "step": 6931
    },
    {
      "epoch": 0.02961962791731116,
      "grad_norm": 1.860053539276123,
      "learning_rate": 0.00020385404204409502,
      "loss": 1.2963,
      "step": 6932
    },
    {
      "epoch": 0.029623900800738356,
      "grad_norm": 0.8722347021102905,
      "learning_rate": 0.00020381131430524695,
      "loss": 0.3326,
      "step": 6933
    },
    {
      "epoch": 0.029628173684165548,
      "grad_norm": 1.8217034339904785,
      "learning_rate": 0.00020376858656639892,
      "loss": 0.4403,
      "step": 6934
    },
    {
      "epoch": 0.029632446567592743,
      "grad_norm": 0.6970439553260803,
      "learning_rate": 0.00020372585882755086,
      "loss": 0.2781,
      "step": 6935
    },
    {
      "epoch": 0.029636719451019936,
      "grad_norm": 4.3379693031311035,
      "learning_rate": 0.0002036831310887028,
      "loss": 0.9629,
      "step": 6936
    },
    {
      "epoch": 0.02964099233444713,
      "grad_norm": 0.6438173651695251,
      "learning_rate": 0.00020364040334985473,
      "loss": 0.2687,
      "step": 6937
    },
    {
      "epoch": 0.029645265217874327,
      "grad_norm": 4.805554389953613,
      "learning_rate": 0.00020359767561100667,
      "loss": 1.0164,
      "step": 6938
    },
    {
      "epoch": 0.02964953810130152,
      "grad_norm": 1.2936967611312866,
      "learning_rate": 0.0002035549478721586,
      "loss": 0.519,
      "step": 6939
    },
    {
      "epoch": 0.029653810984728714,
      "grad_norm": 1.052480936050415,
      "learning_rate": 0.00020351222013331055,
      "loss": 0.3263,
      "step": 6940
    },
    {
      "epoch": 0.02965808386815591,
      "grad_norm": 0.8696834444999695,
      "learning_rate": 0.00020346949239446248,
      "loss": 0.4808,
      "step": 6941
    },
    {
      "epoch": 0.029662356751583102,
      "grad_norm": 1.1789734363555908,
      "learning_rate": 0.00020342676465561445,
      "loss": 0.4749,
      "step": 6942
    },
    {
      "epoch": 0.029666629635010298,
      "grad_norm": 2.380361795425415,
      "learning_rate": 0.00020338403691676639,
      "loss": 0.7919,
      "step": 6943
    },
    {
      "epoch": 0.029670902518437493,
      "grad_norm": 1.9398441314697266,
      "learning_rate": 0.00020334130917791832,
      "loss": 0.6979,
      "step": 6944
    },
    {
      "epoch": 0.029675175401864685,
      "grad_norm": 0.7686476707458496,
      "learning_rate": 0.00020329858143907023,
      "loss": 0.2893,
      "step": 6945
    },
    {
      "epoch": 0.02967944828529188,
      "grad_norm": 2.866753101348877,
      "learning_rate": 0.00020325585370022217,
      "loss": 0.7461,
      "step": 6946
    },
    {
      "epoch": 0.029683721168719077,
      "grad_norm": 2.2460765838623047,
      "learning_rate": 0.0002032131259613741,
      "loss": 0.5651,
      "step": 6947
    },
    {
      "epoch": 0.02968799405214627,
      "grad_norm": 4.3384857177734375,
      "learning_rate": 0.00020317039822252605,
      "loss": 1.4401,
      "step": 6948
    },
    {
      "epoch": 0.029692266935573464,
      "grad_norm": 0.9927940368652344,
      "learning_rate": 0.000203127670483678,
      "loss": 0.4142,
      "step": 6949
    },
    {
      "epoch": 0.029696539819000656,
      "grad_norm": 0.8981658220291138,
      "learning_rate": 0.00020308494274482995,
      "loss": 0.4895,
      "step": 6950
    },
    {
      "epoch": 0.029700812702427852,
      "grad_norm": 1.668299913406372,
      "learning_rate": 0.0002030422150059819,
      "loss": 0.5162,
      "step": 6951
    },
    {
      "epoch": 0.029705085585855048,
      "grad_norm": 2.3437294960021973,
      "learning_rate": 0.00020299948726713383,
      "loss": 0.9853,
      "step": 6952
    },
    {
      "epoch": 0.02970935846928224,
      "grad_norm": 3.852571487426758,
      "learning_rate": 0.00020295675952828576,
      "loss": 1.3969,
      "step": 6953
    },
    {
      "epoch": 0.029713631352709435,
      "grad_norm": 2.9440770149230957,
      "learning_rate": 0.0002029140317894377,
      "loss": 1.0507,
      "step": 6954
    },
    {
      "epoch": 0.02971790423613663,
      "grad_norm": 5.4626898765563965,
      "learning_rate": 0.00020287130405058964,
      "loss": 1.2142,
      "step": 6955
    },
    {
      "epoch": 0.029722177119563823,
      "grad_norm": 0.9330459237098694,
      "learning_rate": 0.00020282857631174158,
      "loss": 0.4894,
      "step": 6956
    },
    {
      "epoch": 0.02972645000299102,
      "grad_norm": 1.4616936445236206,
      "learning_rate": 0.00020278584857289354,
      "loss": 1.046,
      "step": 6957
    },
    {
      "epoch": 0.029730722886418214,
      "grad_norm": 1.6843602657318115,
      "learning_rate": 0.00020274312083404548,
      "loss": 0.347,
      "step": 6958
    },
    {
      "epoch": 0.029734995769845406,
      "grad_norm": 0.7879637479782104,
      "learning_rate": 0.00020270039309519742,
      "loss": 0.3187,
      "step": 6959
    },
    {
      "epoch": 0.029739268653272602,
      "grad_norm": 4.187029838562012,
      "learning_rate": 0.00020265766535634936,
      "loss": 0.7998,
      "step": 6960
    },
    {
      "epoch": 0.029743541536699794,
      "grad_norm": 4.845961570739746,
      "learning_rate": 0.0002026149376175013,
      "loss": 0.9767,
      "step": 6961
    },
    {
      "epoch": 0.02974781442012699,
      "grad_norm": 1.4704869985580444,
      "learning_rate": 0.0002025722098786532,
      "loss": 1.0367,
      "step": 6962
    },
    {
      "epoch": 0.029752087303554185,
      "grad_norm": 1.9551373720169067,
      "learning_rate": 0.00020252948213980514,
      "loss": 0.4617,
      "step": 6963
    },
    {
      "epoch": 0.029756360186981377,
      "grad_norm": 1.4467438459396362,
      "learning_rate": 0.0002024867544009571,
      "loss": 0.9886,
      "step": 6964
    },
    {
      "epoch": 0.029760633070408573,
      "grad_norm": 3.3352339267730713,
      "learning_rate": 0.00020244402666210905,
      "loss": 1.092,
      "step": 6965
    },
    {
      "epoch": 0.02976490595383577,
      "grad_norm": 2.7242331504821777,
      "learning_rate": 0.00020240129892326098,
      "loss": 0.7212,
      "step": 6966
    },
    {
      "epoch": 0.02976917883726296,
      "grad_norm": 1.4632900953292847,
      "learning_rate": 0.00020235857118441292,
      "loss": 0.9916,
      "step": 6967
    },
    {
      "epoch": 0.029773451720690156,
      "grad_norm": 3.872288703918457,
      "learning_rate": 0.00020231584344556486,
      "loss": 0.6558,
      "step": 6968
    },
    {
      "epoch": 0.029777724604117352,
      "grad_norm": 2.333455801010132,
      "learning_rate": 0.0002022731157067168,
      "loss": 0.6649,
      "step": 6969
    },
    {
      "epoch": 0.029781997487544544,
      "grad_norm": 0.7137975692749023,
      "learning_rate": 0.00020223038796786873,
      "loss": 0.3446,
      "step": 6970
    },
    {
      "epoch": 0.02978627037097174,
      "grad_norm": 1.0400850772857666,
      "learning_rate": 0.0002021876602290207,
      "loss": 0.5634,
      "step": 6971
    },
    {
      "epoch": 0.029790543254398935,
      "grad_norm": 1.7033008337020874,
      "learning_rate": 0.00020214493249017264,
      "loss": 1.2651,
      "step": 6972
    },
    {
      "epoch": 0.029794816137826127,
      "grad_norm": 0.9934771656990051,
      "learning_rate": 0.00020210220475132458,
      "loss": 0.4739,
      "step": 6973
    },
    {
      "epoch": 0.029799089021253323,
      "grad_norm": 1.515045404434204,
      "learning_rate": 0.0002020594770124765,
      "loss": 0.7321,
      "step": 6974
    },
    {
      "epoch": 0.029803361904680515,
      "grad_norm": 0.7347941398620605,
      "learning_rate": 0.00020201674927362845,
      "loss": 0.3189,
      "step": 6975
    },
    {
      "epoch": 0.02980763478810771,
      "grad_norm": 1.695902705192566,
      "learning_rate": 0.0002019740215347804,
      "loss": 1.2663,
      "step": 6976
    },
    {
      "epoch": 0.029811907671534906,
      "grad_norm": 0.8895835876464844,
      "learning_rate": 0.00020193129379593233,
      "loss": 0.3639,
      "step": 6977
    },
    {
      "epoch": 0.029816180554962098,
      "grad_norm": 0.5612019300460815,
      "learning_rate": 0.00020188856605708424,
      "loss": 0.208,
      "step": 6978
    },
    {
      "epoch": 0.029820453438389294,
      "grad_norm": 3.2513062953948975,
      "learning_rate": 0.0002018458383182362,
      "loss": 1.0238,
      "step": 6979
    },
    {
      "epoch": 0.02982472632181649,
      "grad_norm": 5.517038345336914,
      "learning_rate": 0.00020180311057938814,
      "loss": 1.4889,
      "step": 6980
    },
    {
      "epoch": 0.02982899920524368,
      "grad_norm": 3.925771713256836,
      "learning_rate": 0.00020176038284054008,
      "loss": 1.0591,
      "step": 6981
    },
    {
      "epoch": 0.029833272088670877,
      "grad_norm": 2.5899240970611572,
      "learning_rate": 0.00020171765510169202,
      "loss": 0.8414,
      "step": 6982
    },
    {
      "epoch": 0.029837544972098073,
      "grad_norm": 2.5696592330932617,
      "learning_rate": 0.00020167492736284395,
      "loss": 0.66,
      "step": 6983
    },
    {
      "epoch": 0.029841817855525265,
      "grad_norm": 3.5062639713287354,
      "learning_rate": 0.0002016321996239959,
      "loss": 1.0502,
      "step": 6984
    },
    {
      "epoch": 0.02984609073895246,
      "grad_norm": 0.963230550289154,
      "learning_rate": 0.00020158947188514783,
      "loss": 0.4724,
      "step": 6985
    },
    {
      "epoch": 0.029850363622379653,
      "grad_norm": 3.1930713653564453,
      "learning_rate": 0.0002015467441462998,
      "loss": 1.1323,
      "step": 6986
    },
    {
      "epoch": 0.029854636505806848,
      "grad_norm": 0.9274480938911438,
      "learning_rate": 0.00020150401640745173,
      "loss": 0.3518,
      "step": 6987
    },
    {
      "epoch": 0.029858909389234044,
      "grad_norm": 0.9110950827598572,
      "learning_rate": 0.00020146128866860367,
      "loss": 0.3662,
      "step": 6988
    },
    {
      "epoch": 0.029863182272661236,
      "grad_norm": 2.5288403034210205,
      "learning_rate": 0.0002014185609297556,
      "loss": 0.3973,
      "step": 6989
    },
    {
      "epoch": 0.02986745515608843,
      "grad_norm": 0.7582411766052246,
      "learning_rate": 0.00020137583319090755,
      "loss": 0.3037,
      "step": 6990
    },
    {
      "epoch": 0.029871728039515627,
      "grad_norm": 1.3886572122573853,
      "learning_rate": 0.00020133310545205948,
      "loss": 0.3763,
      "step": 6991
    },
    {
      "epoch": 0.02987600092294282,
      "grad_norm": 2.1988749504089355,
      "learning_rate": 0.00020129037771321142,
      "loss": 0.8059,
      "step": 6992
    },
    {
      "epoch": 0.029880273806370015,
      "grad_norm": 2.501786947250366,
      "learning_rate": 0.00020124764997436336,
      "loss": 0.5193,
      "step": 6993
    },
    {
      "epoch": 0.02988454668979721,
      "grad_norm": 1.07177734375,
      "learning_rate": 0.00020120492223551532,
      "loss": 0.3891,
      "step": 6994
    },
    {
      "epoch": 0.029888819573224402,
      "grad_norm": 1.6709620952606201,
      "learning_rate": 0.00020116219449666724,
      "loss": 0.5222,
      "step": 6995
    },
    {
      "epoch": 0.029893092456651598,
      "grad_norm": 4.341943264007568,
      "learning_rate": 0.00020111946675781917,
      "loss": 1.1708,
      "step": 6996
    },
    {
      "epoch": 0.029897365340078794,
      "grad_norm": 5.680032730102539,
      "learning_rate": 0.0002010767390189711,
      "loss": 1.4326,
      "step": 6997
    },
    {
      "epoch": 0.029901638223505986,
      "grad_norm": 1.8096741437911987,
      "learning_rate": 0.00020103401128012305,
      "loss": 0.7364,
      "step": 6998
    },
    {
      "epoch": 0.02990591110693318,
      "grad_norm": 5.641058444976807,
      "learning_rate": 0.000200991283541275,
      "loss": 1.3905,
      "step": 6999
    },
    {
      "epoch": 0.029910183990360373,
      "grad_norm": 1.7811253070831299,
      "learning_rate": 0.00020094855580242692,
      "loss": 0.6196,
      "step": 7000
    },
    {
      "epoch": 0.02991445687378757,
      "grad_norm": 1.5424245595932007,
      "learning_rate": 0.0002009058280635789,
      "loss": 0.8112,
      "step": 7001
    },
    {
      "epoch": 0.029918729757214765,
      "grad_norm": 1.5878454446792603,
      "learning_rate": 0.00020086310032473083,
      "loss": 0.8627,
      "step": 7002
    },
    {
      "epoch": 0.029923002640641957,
      "grad_norm": 2.361189365386963,
      "learning_rate": 0.00020082037258588277,
      "loss": 0.7628,
      "step": 7003
    },
    {
      "epoch": 0.029927275524069152,
      "grad_norm": 2.733896017074585,
      "learning_rate": 0.0002007776448470347,
      "loss": 0.4166,
      "step": 7004
    },
    {
      "epoch": 0.029931548407496348,
      "grad_norm": 1.104000449180603,
      "learning_rate": 0.00020073491710818664,
      "loss": 0.4711,
      "step": 7005
    },
    {
      "epoch": 0.02993582129092354,
      "grad_norm": 0.7753691673278809,
      "learning_rate": 0.00020069218936933858,
      "loss": 0.2418,
      "step": 7006
    },
    {
      "epoch": 0.029940094174350736,
      "grad_norm": 5.291820049285889,
      "learning_rate": 0.00020064946163049052,
      "loss": 1.3015,
      "step": 7007
    },
    {
      "epoch": 0.02994436705777793,
      "grad_norm": 3.5890963077545166,
      "learning_rate": 0.00020060673389164245,
      "loss": 1.0709,
      "step": 7008
    },
    {
      "epoch": 0.029948639941205123,
      "grad_norm": 1.0919855833053589,
      "learning_rate": 0.00020056400615279442,
      "loss": 0.5137,
      "step": 7009
    },
    {
      "epoch": 0.02995291282463232,
      "grad_norm": 1.9087400436401367,
      "learning_rate": 0.00020052127841394636,
      "loss": 0.6976,
      "step": 7010
    },
    {
      "epoch": 0.02995718570805951,
      "grad_norm": 1.3540382385253906,
      "learning_rate": 0.00020047855067509827,
      "loss": 0.6203,
      "step": 7011
    },
    {
      "epoch": 0.029961458591486707,
      "grad_norm": 1.6814314126968384,
      "learning_rate": 0.0002004358229362502,
      "loss": 0.7223,
      "step": 7012
    },
    {
      "epoch": 0.029965731474913902,
      "grad_norm": 2.2771637439727783,
      "learning_rate": 0.00020039309519740214,
      "loss": 0.7152,
      "step": 7013
    },
    {
      "epoch": 0.029970004358341094,
      "grad_norm": 2.7724380493164062,
      "learning_rate": 0.00020035036745855408,
      "loss": 0.7727,
      "step": 7014
    },
    {
      "epoch": 0.02997427724176829,
      "grad_norm": 4.9346184730529785,
      "learning_rate": 0.00020030763971970602,
      "loss": 1.0556,
      "step": 7015
    },
    {
      "epoch": 0.029978550125195486,
      "grad_norm": 0.9758750796318054,
      "learning_rate": 0.00020026491198085798,
      "loss": 0.3044,
      "step": 7016
    },
    {
      "epoch": 0.029982823008622678,
      "grad_norm": 3.5795257091522217,
      "learning_rate": 0.00020022218424200992,
      "loss": 0.9339,
      "step": 7017
    },
    {
      "epoch": 0.029987095892049873,
      "grad_norm": 2.1337499618530273,
      "learning_rate": 0.00020017945650316186,
      "loss": 0.6964,
      "step": 7018
    },
    {
      "epoch": 0.02999136877547707,
      "grad_norm": 0.7701748013496399,
      "learning_rate": 0.0002001367287643138,
      "loss": 0.3041,
      "step": 7019
    },
    {
      "epoch": 0.02999564165890426,
      "grad_norm": 3.483067750930786,
      "learning_rate": 0.00020009400102546574,
      "loss": 1.006,
      "step": 7020
    },
    {
      "epoch": 0.029999914542331457,
      "grad_norm": 2.2740321159362793,
      "learning_rate": 0.00020005127328661767,
      "loss": 0.4688,
      "step": 7021
    },
    {
      "epoch": 0.030004187425758652,
      "grad_norm": 4.275036811828613,
      "learning_rate": 0.0002000085455477696,
      "loss": 1.0112,
      "step": 7022
    },
    {
      "epoch": 0.030008460309185844,
      "grad_norm": 0.7134227156639099,
      "learning_rate": 0.00019996581780892158,
      "loss": 0.2709,
      "step": 7023
    },
    {
      "epoch": 0.03001273319261304,
      "grad_norm": 1.832449197769165,
      "learning_rate": 0.00019992309007007351,
      "loss": 0.6395,
      "step": 7024
    },
    {
      "epoch": 0.030017006076040232,
      "grad_norm": 1.7248597145080566,
      "learning_rate": 0.00019988036233122545,
      "loss": 0.8095,
      "step": 7025
    },
    {
      "epoch": 0.030021278959467428,
      "grad_norm": 3.1724281311035156,
      "learning_rate": 0.0001998376345923774,
      "loss": 1.872,
      "step": 7026
    },
    {
      "epoch": 0.030025551842894623,
      "grad_norm": 1.55293869972229,
      "learning_rate": 0.00019979490685352933,
      "loss": 0.6845,
      "step": 7027
    },
    {
      "epoch": 0.030029824726321815,
      "grad_norm": 1.48257315158844,
      "learning_rate": 0.00019975217911468124,
      "loss": 0.6297,
      "step": 7028
    },
    {
      "epoch": 0.03003409760974901,
      "grad_norm": 4.839303970336914,
      "learning_rate": 0.00019970945137583318,
      "loss": 1.079,
      "step": 7029
    },
    {
      "epoch": 0.030038370493176206,
      "grad_norm": 1.4087718725204468,
      "learning_rate": 0.00019966672363698511,
      "loss": 0.6297,
      "step": 7030
    },
    {
      "epoch": 0.0300426433766034,
      "grad_norm": 1.7388200759887695,
      "learning_rate": 0.00019962399589813708,
      "loss": 0.4285,
      "step": 7031
    },
    {
      "epoch": 0.030046916260030594,
      "grad_norm": 1.7229487895965576,
      "learning_rate": 0.00019958126815928902,
      "loss": 0.5625,
      "step": 7032
    },
    {
      "epoch": 0.03005118914345779,
      "grad_norm": 1.2739893198013306,
      "learning_rate": 0.00019953854042044095,
      "loss": 0.6298,
      "step": 7033
    },
    {
      "epoch": 0.030055462026884982,
      "grad_norm": 2.1258270740509033,
      "learning_rate": 0.0001994958126815929,
      "loss": 0.6494,
      "step": 7034
    },
    {
      "epoch": 0.030059734910312177,
      "grad_norm": 3.1080236434936523,
      "learning_rate": 0.00019945308494274483,
      "loss": 0.7384,
      "step": 7035
    },
    {
      "epoch": 0.03006400779373937,
      "grad_norm": 1.0421326160430908,
      "learning_rate": 0.00019941035720389677,
      "loss": 0.5517,
      "step": 7036
    },
    {
      "epoch": 0.030068280677166565,
      "grad_norm": 1.063726544380188,
      "learning_rate": 0.0001993676294650487,
      "loss": 0.4696,
      "step": 7037
    },
    {
      "epoch": 0.03007255356059376,
      "grad_norm": 2.022021532058716,
      "learning_rate": 0.00019932490172620067,
      "loss": 0.5935,
      "step": 7038
    },
    {
      "epoch": 0.030076826444020953,
      "grad_norm": 1.0174320936203003,
      "learning_rate": 0.0001992821739873526,
      "loss": 0.3636,
      "step": 7039
    },
    {
      "epoch": 0.03008109932744815,
      "grad_norm": 1.8839645385742188,
      "learning_rate": 0.00019923944624850455,
      "loss": 0.6319,
      "step": 7040
    },
    {
      "epoch": 0.030085372210875344,
      "grad_norm": 4.065624237060547,
      "learning_rate": 0.00019919671850965648,
      "loss": 2.8262,
      "step": 7041
    },
    {
      "epoch": 0.030089645094302536,
      "grad_norm": 0.7736479043960571,
      "learning_rate": 0.00019915399077080842,
      "loss": 0.3521,
      "step": 7042
    },
    {
      "epoch": 0.030093917977729732,
      "grad_norm": 3.0365149974823,
      "learning_rate": 0.00019911126303196036,
      "loss": 0.9808,
      "step": 7043
    },
    {
      "epoch": 0.030098190861156927,
      "grad_norm": 2.374708890914917,
      "learning_rate": 0.00019906853529311227,
      "loss": 0.9142,
      "step": 7044
    },
    {
      "epoch": 0.03010246374458412,
      "grad_norm": 1.4027119874954224,
      "learning_rate": 0.0001990258075542642,
      "loss": 0.8479,
      "step": 7045
    },
    {
      "epoch": 0.030106736628011315,
      "grad_norm": 0.8458361625671387,
      "learning_rate": 0.00019898307981541617,
      "loss": 0.3646,
      "step": 7046
    },
    {
      "epoch": 0.030111009511438507,
      "grad_norm": 0.9709360003471375,
      "learning_rate": 0.0001989403520765681,
      "loss": 0.4332,
      "step": 7047
    },
    {
      "epoch": 0.030115282394865703,
      "grad_norm": 2.0120513439178467,
      "learning_rate": 0.00019889762433772005,
      "loss": 0.5736,
      "step": 7048
    },
    {
      "epoch": 0.0301195552782929,
      "grad_norm": 2.4034969806671143,
      "learning_rate": 0.000198854896598872,
      "loss": 0.5894,
      "step": 7049
    },
    {
      "epoch": 0.03012382816172009,
      "grad_norm": 2.1887195110321045,
      "learning_rate": 0.00019881216886002392,
      "loss": 0.6085,
      "step": 7050
    },
    {
      "epoch": 0.030128101045147286,
      "grad_norm": 1.9762489795684814,
      "learning_rate": 0.00019876944112117586,
      "loss": 0.5513,
      "step": 7051
    },
    {
      "epoch": 0.03013237392857448,
      "grad_norm": 1.414337158203125,
      "learning_rate": 0.0001987267133823278,
      "loss": 0.8945,
      "step": 7052
    },
    {
      "epoch": 0.030136646812001674,
      "grad_norm": 2.5571916103363037,
      "learning_rate": 0.00019868398564347977,
      "loss": 0.7033,
      "step": 7053
    },
    {
      "epoch": 0.03014091969542887,
      "grad_norm": 0.8684143424034119,
      "learning_rate": 0.0001986412579046317,
      "loss": 0.3667,
      "step": 7054
    },
    {
      "epoch": 0.030145192578856065,
      "grad_norm": 1.4039376974105835,
      "learning_rate": 0.00019859853016578364,
      "loss": 0.849,
      "step": 7055
    },
    {
      "epoch": 0.030149465462283257,
      "grad_norm": 3.805222749710083,
      "learning_rate": 0.00019855580242693558,
      "loss": 1.1963,
      "step": 7056
    },
    {
      "epoch": 0.030153738345710453,
      "grad_norm": 2.814103841781616,
      "learning_rate": 0.00019851307468808752,
      "loss": 0.9877,
      "step": 7057
    },
    {
      "epoch": 0.03015801122913765,
      "grad_norm": 0.8700258135795593,
      "learning_rate": 0.00019847034694923945,
      "loss": 0.3668,
      "step": 7058
    },
    {
      "epoch": 0.03016228411256484,
      "grad_norm": 0.6549680829048157,
      "learning_rate": 0.0001984276192103914,
      "loss": 0.294,
      "step": 7059
    },
    {
      "epoch": 0.030166556995992036,
      "grad_norm": 3.6360278129577637,
      "learning_rate": 0.00019838489147154333,
      "loss": 1.2026,
      "step": 7060
    },
    {
      "epoch": 0.030170829879419228,
      "grad_norm": 0.8105714321136475,
      "learning_rate": 0.00019834216373269527,
      "loss": 0.3339,
      "step": 7061
    },
    {
      "epoch": 0.030175102762846424,
      "grad_norm": 1.5614020824432373,
      "learning_rate": 0.0001982994359938472,
      "loss": 0.6405,
      "step": 7062
    },
    {
      "epoch": 0.03017937564627362,
      "grad_norm": 3.810931921005249,
      "learning_rate": 0.00019825670825499914,
      "loss": 1.0595,
      "step": 7063
    },
    {
      "epoch": 0.03018364852970081,
      "grad_norm": 0.5426172614097595,
      "learning_rate": 0.00019821398051615108,
      "loss": 0.2108,
      "step": 7064
    },
    {
      "epoch": 0.030187921413128007,
      "grad_norm": 0.8019000291824341,
      "learning_rate": 0.00019817125277730302,
      "loss": 0.3647,
      "step": 7065
    },
    {
      "epoch": 0.030192194296555203,
      "grad_norm": 0.8338350653648376,
      "learning_rate": 0.00019812852503845496,
      "loss": 0.3645,
      "step": 7066
    },
    {
      "epoch": 0.030196467179982395,
      "grad_norm": 1.0346852540969849,
      "learning_rate": 0.0001980857972996069,
      "loss": 0.5734,
      "step": 7067
    },
    {
      "epoch": 0.03020074006340959,
      "grad_norm": 0.8599351048469543,
      "learning_rate": 0.00019804306956075886,
      "loss": 0.3488,
      "step": 7068
    },
    {
      "epoch": 0.030205012946836786,
      "grad_norm": 3.044865846633911,
      "learning_rate": 0.0001980003418219108,
      "loss": 1.6983,
      "step": 7069
    },
    {
      "epoch": 0.030209285830263978,
      "grad_norm": 0.9163071513175964,
      "learning_rate": 0.00019795761408306274,
      "loss": 0.5875,
      "step": 7070
    },
    {
      "epoch": 0.030213558713691174,
      "grad_norm": 0.41977569460868835,
      "learning_rate": 0.00019791488634421467,
      "loss": 0.1274,
      "step": 7071
    },
    {
      "epoch": 0.030217831597118366,
      "grad_norm": 1.4453206062316895,
      "learning_rate": 0.0001978721586053666,
      "loss": 0.7607,
      "step": 7072
    },
    {
      "epoch": 0.03022210448054556,
      "grad_norm": 1.446671962738037,
      "learning_rate": 0.00019782943086651855,
      "loss": 0.7608,
      "step": 7073
    },
    {
      "epoch": 0.030226377363972757,
      "grad_norm": 0.7697532773017883,
      "learning_rate": 0.0001977867031276705,
      "loss": 0.2938,
      "step": 7074
    },
    {
      "epoch": 0.03023065024739995,
      "grad_norm": 3.911968946456909,
      "learning_rate": 0.00019774397538882245,
      "loss": 2.6127,
      "step": 7075
    },
    {
      "epoch": 0.030234923130827145,
      "grad_norm": 0.9070085883140564,
      "learning_rate": 0.0001977012476499744,
      "loss": 0.5876,
      "step": 7076
    },
    {
      "epoch": 0.03023919601425434,
      "grad_norm": 2.5691983699798584,
      "learning_rate": 0.0001976585199111263,
      "loss": 0.9367,
      "step": 7077
    },
    {
      "epoch": 0.030243468897681532,
      "grad_norm": 0.8839930295944214,
      "learning_rate": 0.00019761579217227824,
      "loss": 0.5733,
      "step": 7078
    },
    {
      "epoch": 0.030247741781108728,
      "grad_norm": 0.45758840441703796,
      "learning_rate": 0.00019757306443343018,
      "loss": 0.1515,
      "step": 7079
    },
    {
      "epoch": 0.030252014664535923,
      "grad_norm": 1.5902349948883057,
      "learning_rate": 0.00019753033669458211,
      "loss": 0.3998,
      "step": 7080
    },
    {
      "epoch": 0.030256287547963116,
      "grad_norm": 1.5746489763259888,
      "learning_rate": 0.00019748760895573405,
      "loss": 0.4758,
      "step": 7081
    },
    {
      "epoch": 0.03026056043139031,
      "grad_norm": 0.9550029635429382,
      "learning_rate": 0.000197444881216886,
      "loss": 0.433,
      "step": 7082
    },
    {
      "epoch": 0.030264833314817507,
      "grad_norm": 0.9780275225639343,
      "learning_rate": 0.00019740215347803796,
      "loss": 0.452,
      "step": 7083
    },
    {
      "epoch": 0.0302691061982447,
      "grad_norm": 3.8775269985198975,
      "learning_rate": 0.0001973594257391899,
      "loss": 1.3602,
      "step": 7084
    },
    {
      "epoch": 0.030273379081671895,
      "grad_norm": 2.587460994720459,
      "learning_rate": 0.00019731669800034183,
      "loss": 1.0124,
      "step": 7085
    },
    {
      "epoch": 0.030277651965099087,
      "grad_norm": 2.0848000049591064,
      "learning_rate": 0.00019727397026149377,
      "loss": 0.8527,
      "step": 7086
    },
    {
      "epoch": 0.030281924848526282,
      "grad_norm": 0.7168655395507812,
      "learning_rate": 0.0001972312425226457,
      "loss": 0.3928,
      "step": 7087
    },
    {
      "epoch": 0.030286197731953478,
      "grad_norm": 1.5045735836029053,
      "learning_rate": 0.00019718851478379764,
      "loss": 0.5346,
      "step": 7088
    },
    {
      "epoch": 0.03029047061538067,
      "grad_norm": 1.7120705842971802,
      "learning_rate": 0.00019714578704494958,
      "loss": 0.5705,
      "step": 7089
    },
    {
      "epoch": 0.030294743498807866,
      "grad_norm": 0.7047712206840515,
      "learning_rate": 0.00019710305930610155,
      "loss": 0.3523,
      "step": 7090
    },
    {
      "epoch": 0.03029901638223506,
      "grad_norm": 1.4852770566940308,
      "learning_rate": 0.00019706033156725348,
      "loss": 0.5224,
      "step": 7091
    },
    {
      "epoch": 0.030303289265662253,
      "grad_norm": 0.8082090616226196,
      "learning_rate": 0.00019701760382840542,
      "loss": 0.5101,
      "step": 7092
    },
    {
      "epoch": 0.03030756214908945,
      "grad_norm": 0.6626925468444824,
      "learning_rate": 0.00019697487608955733,
      "loss": 0.2644,
      "step": 7093
    },
    {
      "epoch": 0.030311835032516644,
      "grad_norm": 1.9084973335266113,
      "learning_rate": 0.00019693214835070927,
      "loss": 0.4928,
      "step": 7094
    },
    {
      "epoch": 0.030316107915943837,
      "grad_norm": 0.7940077781677246,
      "learning_rate": 0.0001968894206118612,
      "loss": 0.4761,
      "step": 7095
    },
    {
      "epoch": 0.030320380799371032,
      "grad_norm": 3.7032909393310547,
      "learning_rate": 0.00019684669287301315,
      "loss": 1.0791,
      "step": 7096
    },
    {
      "epoch": 0.030324653682798224,
      "grad_norm": 1.1612287759780884,
      "learning_rate": 0.00019680396513416508,
      "loss": 0.6315,
      "step": 7097
    },
    {
      "epoch": 0.03032892656622542,
      "grad_norm": 4.692802429199219,
      "learning_rate": 0.00019676123739531705,
      "loss": 1.259,
      "step": 7098
    },
    {
      "epoch": 0.030333199449652615,
      "grad_norm": 0.7952397465705872,
      "learning_rate": 0.000196718509656469,
      "loss": 0.389,
      "step": 7099
    },
    {
      "epoch": 0.030337472333079808,
      "grad_norm": 1.5492501258850098,
      "learning_rate": 0.00019667578191762093,
      "loss": 0.328,
      "step": 7100
    },
    {
      "epoch": 0.030341745216507003,
      "grad_norm": 4.140754699707031,
      "learning_rate": 0.00019663305417877286,
      "loss": 1.0314,
      "step": 7101
    },
    {
      "epoch": 0.0303460180999342,
      "grad_norm": 2.328775405883789,
      "learning_rate": 0.0001965903264399248,
      "loss": 0.8209,
      "step": 7102
    },
    {
      "epoch": 0.03035029098336139,
      "grad_norm": 3.8687868118286133,
      "learning_rate": 0.00019654759870107674,
      "loss": 1.4495,
      "step": 7103
    },
    {
      "epoch": 0.030354563866788586,
      "grad_norm": 2.087256669998169,
      "learning_rate": 0.00019650487096222868,
      "loss": 0.711,
      "step": 7104
    },
    {
      "epoch": 0.030358836750215782,
      "grad_norm": 1.3321932554244995,
      "learning_rate": 0.00019646214322338064,
      "loss": 0.4013,
      "step": 7105
    },
    {
      "epoch": 0.030363109633642974,
      "grad_norm": 1.5036169290542603,
      "learning_rate": 0.00019641941548453258,
      "loss": 0.6516,
      "step": 7106
    },
    {
      "epoch": 0.03036738251707017,
      "grad_norm": 0.8843379616737366,
      "learning_rate": 0.00019637668774568452,
      "loss": 0.4327,
      "step": 7107
    },
    {
      "epoch": 0.030371655400497365,
      "grad_norm": 1.751312494277954,
      "learning_rate": 0.00019633396000683646,
      "loss": 0.5562,
      "step": 7108
    },
    {
      "epoch": 0.030375928283924557,
      "grad_norm": 2.680182695388794,
      "learning_rate": 0.0001962912322679884,
      "loss": 0.7714,
      "step": 7109
    },
    {
      "epoch": 0.030380201167351753,
      "grad_norm": 3.735520839691162,
      "learning_rate": 0.0001962485045291403,
      "loss": 0.8697,
      "step": 7110
    },
    {
      "epoch": 0.030384474050778945,
      "grad_norm": 2.6919798851013184,
      "learning_rate": 0.00019620577679029224,
      "loss": 0.7801,
      "step": 7111
    },
    {
      "epoch": 0.03038874693420614,
      "grad_norm": 1.8168474435806274,
      "learning_rate": 0.00019616304905144418,
      "loss": 1.2965,
      "step": 7112
    },
    {
      "epoch": 0.030393019817633336,
      "grad_norm": 1.9193779230117798,
      "learning_rate": 0.00019612032131259614,
      "loss": 0.5834,
      "step": 7113
    },
    {
      "epoch": 0.03039729270106053,
      "grad_norm": 3.1250908374786377,
      "learning_rate": 0.00019607759357374808,
      "loss": 0.8971,
      "step": 7114
    },
    {
      "epoch": 0.030401565584487724,
      "grad_norm": 3.2651782035827637,
      "learning_rate": 0.00019603486583490002,
      "loss": 1.8215,
      "step": 7115
    },
    {
      "epoch": 0.03040583846791492,
      "grad_norm": 0.5268078446388245,
      "learning_rate": 0.00019599213809605196,
      "loss": 0.2169,
      "step": 7116
    },
    {
      "epoch": 0.030410111351342112,
      "grad_norm": 0.8368648290634155,
      "learning_rate": 0.0001959494103572039,
      "loss": 0.3265,
      "step": 7117
    },
    {
      "epoch": 0.030414384234769307,
      "grad_norm": 1.1304144859313965,
      "learning_rate": 0.00019590668261835583,
      "loss": 0.6122,
      "step": 7118
    },
    {
      "epoch": 0.030418657118196503,
      "grad_norm": 1.1439404487609863,
      "learning_rate": 0.00019586395487950777,
      "loss": 0.6421,
      "step": 7119
    },
    {
      "epoch": 0.030422930001623695,
      "grad_norm": 2.852308988571167,
      "learning_rate": 0.00019582122714065974,
      "loss": 1.0095,
      "step": 7120
    },
    {
      "epoch": 0.03042720288505089,
      "grad_norm": 0.8539336919784546,
      "learning_rate": 0.00019577849940181167,
      "loss": 0.4752,
      "step": 7121
    },
    {
      "epoch": 0.030431475768478083,
      "grad_norm": 2.8942313194274902,
      "learning_rate": 0.0001957357716629636,
      "loss": 0.8539,
      "step": 7122
    },
    {
      "epoch": 0.03043574865190528,
      "grad_norm": 0.8567010164260864,
      "learning_rate": 0.00019569304392411555,
      "loss": 0.4922,
      "step": 7123
    },
    {
      "epoch": 0.030440021535332474,
      "grad_norm": 0.6073071360588074,
      "learning_rate": 0.0001956503161852675,
      "loss": 0.3431,
      "step": 7124
    },
    {
      "epoch": 0.030444294418759666,
      "grad_norm": 4.177702903747559,
      "learning_rate": 0.00019560758844641943,
      "loss": 1.4787,
      "step": 7125
    },
    {
      "epoch": 0.03044856730218686,
      "grad_norm": 0.8539088368415833,
      "learning_rate": 0.00019556486070757134,
      "loss": 0.3974,
      "step": 7126
    },
    {
      "epoch": 0.030452840185614057,
      "grad_norm": 2.549116373062134,
      "learning_rate": 0.00019552213296872327,
      "loss": 0.581,
      "step": 7127
    },
    {
      "epoch": 0.03045711306904125,
      "grad_norm": 0.8452838659286499,
      "learning_rate": 0.00019547940522987524,
      "loss": 0.3974,
      "step": 7128
    },
    {
      "epoch": 0.030461385952468445,
      "grad_norm": 2.6429412364959717,
      "learning_rate": 0.00019543667749102718,
      "loss": 0.8635,
      "step": 7129
    },
    {
      "epoch": 0.03046565883589564,
      "grad_norm": 1.728834867477417,
      "learning_rate": 0.00019539394975217911,
      "loss": 1.2871,
      "step": 7130
    },
    {
      "epoch": 0.030469931719322833,
      "grad_norm": 1.5080926418304443,
      "learning_rate": 0.00019535122201333105,
      "loss": 0.4639,
      "step": 7131
    },
    {
      "epoch": 0.030474204602750028,
      "grad_norm": 3.9775643348693848,
      "learning_rate": 0.000195308494274483,
      "loss": 1.296,
      "step": 7132
    },
    {
      "epoch": 0.030478477486177224,
      "grad_norm": 2.310577869415283,
      "learning_rate": 0.00019526576653563493,
      "loss": 0.7202,
      "step": 7133
    },
    {
      "epoch": 0.030482750369604416,
      "grad_norm": 3.077094554901123,
      "learning_rate": 0.00019522303879678687,
      "loss": 0.9023,
      "step": 7134
    },
    {
      "epoch": 0.03048702325303161,
      "grad_norm": 0.8406394720077515,
      "learning_rate": 0.00019518031105793883,
      "loss": 0.3665,
      "step": 7135
    },
    {
      "epoch": 0.030491296136458804,
      "grad_norm": 2.2878339290618896,
      "learning_rate": 0.00019513758331909077,
      "loss": 0.69,
      "step": 7136
    },
    {
      "epoch": 0.030495569019886,
      "grad_norm": 4.048938274383545,
      "learning_rate": 0.0001950948555802427,
      "loss": 1.1084,
      "step": 7137
    },
    {
      "epoch": 0.030499841903313195,
      "grad_norm": 2.243039846420288,
      "learning_rate": 0.00019505212784139464,
      "loss": 0.644,
      "step": 7138
    },
    {
      "epoch": 0.030504114786740387,
      "grad_norm": 1.067059874534607,
      "learning_rate": 0.00019500940010254658,
      "loss": 0.8802,
      "step": 7139
    },
    {
      "epoch": 0.030508387670167583,
      "grad_norm": 0.8411823511123657,
      "learning_rate": 0.00019496667236369852,
      "loss": 0.4638,
      "step": 7140
    },
    {
      "epoch": 0.030512660553594778,
      "grad_norm": 2.6564323902130127,
      "learning_rate": 0.00019492394462485046,
      "loss": 0.8427,
      "step": 7141
    },
    {
      "epoch": 0.03051693343702197,
      "grad_norm": 0.814511239528656,
      "learning_rate": 0.00019488121688600242,
      "loss": 0.3669,
      "step": 7142
    },
    {
      "epoch": 0.030521206320449166,
      "grad_norm": 3.6370437145233154,
      "learning_rate": 0.00019483848914715433,
      "loss": 1.2963,
      "step": 7143
    },
    {
      "epoch": 0.03052547920387636,
      "grad_norm": 0.6894741058349609,
      "learning_rate": 0.00019479576140830627,
      "loss": 0.3352,
      "step": 7144
    },
    {
      "epoch": 0.030529752087303554,
      "grad_norm": 1.091947317123413,
      "learning_rate": 0.0001947530336694582,
      "loss": 0.8552,
      "step": 7145
    },
    {
      "epoch": 0.03053402497073075,
      "grad_norm": 2.960292339324951,
      "learning_rate": 0.00019471030593061015,
      "loss": 1.2214,
      "step": 7146
    },
    {
      "epoch": 0.03053829785415794,
      "grad_norm": 3.1697285175323486,
      "learning_rate": 0.00019466757819176209,
      "loss": 1.7302,
      "step": 7147
    },
    {
      "epoch": 0.030542570737585137,
      "grad_norm": 4.695755481719971,
      "learning_rate": 0.00019462485045291402,
      "loss": 1.0878,
      "step": 7148
    },
    {
      "epoch": 0.030546843621012332,
      "grad_norm": 1.6235578060150146,
      "learning_rate": 0.00019458212271406596,
      "loss": 1.2441,
      "step": 7149
    },
    {
      "epoch": 0.030551116504439525,
      "grad_norm": 0.9086291790008545,
      "learning_rate": 0.00019453939497521793,
      "loss": 0.4348,
      "step": 7150
    },
    {
      "epoch": 0.03055538938786672,
      "grad_norm": 1.7080599069595337,
      "learning_rate": 0.00019449666723636986,
      "loss": 0.5197,
      "step": 7151
    },
    {
      "epoch": 0.030559662271293916,
      "grad_norm": 2.6716811656951904,
      "learning_rate": 0.0001944539394975218,
      "loss": 0.8842,
      "step": 7152
    },
    {
      "epoch": 0.030563935154721108,
      "grad_norm": 1.0498127937316895,
      "learning_rate": 0.00019441121175867374,
      "loss": 0.4484,
      "step": 7153
    },
    {
      "epoch": 0.030568208038148303,
      "grad_norm": 1.0638337135314941,
      "learning_rate": 0.00019436848401982568,
      "loss": 0.6529,
      "step": 7154
    },
    {
      "epoch": 0.0305724809215755,
      "grad_norm": 2.7033441066741943,
      "learning_rate": 0.00019432575628097762,
      "loss": 0.7993,
      "step": 7155
    },
    {
      "epoch": 0.03057675380500269,
      "grad_norm": 0.7980729341506958,
      "learning_rate": 0.00019428302854212955,
      "loss": 0.398,
      "step": 7156
    },
    {
      "epoch": 0.030581026688429887,
      "grad_norm": 1.0283764600753784,
      "learning_rate": 0.00019424030080328152,
      "loss": 0.629,
      "step": 7157
    },
    {
      "epoch": 0.03058529957185708,
      "grad_norm": 0.7455877065658569,
      "learning_rate": 0.00019419757306443346,
      "loss": 0.3488,
      "step": 7158
    },
    {
      "epoch": 0.030589572455284274,
      "grad_norm": 3.9290218353271484,
      "learning_rate": 0.00019415484532558537,
      "loss": 1.035,
      "step": 7159
    },
    {
      "epoch": 0.03059384533871147,
      "grad_norm": 4.037751197814941,
      "learning_rate": 0.0001941121175867373,
      "loss": 1.0151,
      "step": 7160
    },
    {
      "epoch": 0.030598118222138662,
      "grad_norm": 1.6566567420959473,
      "learning_rate": 0.00019406938984788924,
      "loss": 0.4746,
      "step": 7161
    },
    {
      "epoch": 0.030602391105565858,
      "grad_norm": 1.8262226581573486,
      "learning_rate": 0.00019402666210904118,
      "loss": 0.5306,
      "step": 7162
    },
    {
      "epoch": 0.030606663988993053,
      "grad_norm": 1.3195749521255493,
      "learning_rate": 0.00019398393437019312,
      "loss": 0.3816,
      "step": 7163
    },
    {
      "epoch": 0.030610936872420245,
      "grad_norm": 0.728125810623169,
      "learning_rate": 0.00019394120663134506,
      "loss": 0.3704,
      "step": 7164
    },
    {
      "epoch": 0.03061520975584744,
      "grad_norm": 2.9767487049102783,
      "learning_rate": 0.00019389847889249702,
      "loss": 1.6281,
      "step": 7165
    },
    {
      "epoch": 0.030619482639274637,
      "grad_norm": 0.7473494410514832,
      "learning_rate": 0.00019385575115364896,
      "loss": 0.5227,
      "step": 7166
    },
    {
      "epoch": 0.03062375552270183,
      "grad_norm": 3.7351491451263428,
      "learning_rate": 0.0001938130234148009,
      "loss": 0.9707,
      "step": 7167
    },
    {
      "epoch": 0.030628028406129024,
      "grad_norm": 2.8616178035736084,
      "learning_rate": 0.00019377029567595283,
      "loss": 0.764,
      "step": 7168
    },
    {
      "epoch": 0.03063230128955622,
      "grad_norm": 0.5669580101966858,
      "learning_rate": 0.00019372756793710477,
      "loss": 0.2718,
      "step": 7169
    },
    {
      "epoch": 0.030636574172983412,
      "grad_norm": 1.4318877458572388,
      "learning_rate": 0.0001936848401982567,
      "loss": 1.1785,
      "step": 7170
    },
    {
      "epoch": 0.030640847056410608,
      "grad_norm": 3.9434866905212402,
      "learning_rate": 0.00019364211245940865,
      "loss": 0.9442,
      "step": 7171
    },
    {
      "epoch": 0.0306451199398378,
      "grad_norm": 0.9878047704696655,
      "learning_rate": 0.0001935993847205606,
      "loss": 0.6315,
      "step": 7172
    },
    {
      "epoch": 0.030649392823264995,
      "grad_norm": 1.127561092376709,
      "learning_rate": 0.00019355665698171255,
      "loss": 0.819,
      "step": 7173
    },
    {
      "epoch": 0.03065366570669219,
      "grad_norm": 2.116316318511963,
      "learning_rate": 0.0001935139292428645,
      "loss": 0.63,
      "step": 7174
    },
    {
      "epoch": 0.030657938590119383,
      "grad_norm": 0.9288281798362732,
      "learning_rate": 0.00019347120150401643,
      "loss": 0.4149,
      "step": 7175
    },
    {
      "epoch": 0.03066221147354658,
      "grad_norm": 0.5543435215950012,
      "learning_rate": 0.00019342847376516834,
      "loss": 0.2424,
      "step": 7176
    },
    {
      "epoch": 0.030666484356973774,
      "grad_norm": 0.9682973623275757,
      "learning_rate": 0.00019338574602632027,
      "loss": 0.4712,
      "step": 7177
    },
    {
      "epoch": 0.030670757240400966,
      "grad_norm": 0.5718201994895935,
      "learning_rate": 0.0001933430182874722,
      "loss": 0.2717,
      "step": 7178
    },
    {
      "epoch": 0.030675030123828162,
      "grad_norm": 1.0471423864364624,
      "learning_rate": 0.00019330029054862415,
      "loss": 0.4594,
      "step": 7179
    },
    {
      "epoch": 0.030679303007255358,
      "grad_norm": 3.5589847564697266,
      "learning_rate": 0.00019325756280977612,
      "loss": 1.0798,
      "step": 7180
    },
    {
      "epoch": 0.03068357589068255,
      "grad_norm": 4.039648532867432,
      "learning_rate": 0.00019321483507092805,
      "loss": 1.1118,
      "step": 7181
    },
    {
      "epoch": 0.030687848774109745,
      "grad_norm": 1.178633213043213,
      "learning_rate": 0.00019317210733208,
      "loss": 0.8225,
      "step": 7182
    },
    {
      "epoch": 0.030692121657536937,
      "grad_norm": 0.9639265537261963,
      "learning_rate": 0.00019312937959323193,
      "loss": 0.5933,
      "step": 7183
    },
    {
      "epoch": 0.030696394540964133,
      "grad_norm": 0.4898871183395386,
      "learning_rate": 0.00019308665185438387,
      "loss": 0.2109,
      "step": 7184
    },
    {
      "epoch": 0.03070066742439133,
      "grad_norm": 4.046260356903076,
      "learning_rate": 0.0001930439241155358,
      "loss": 0.9992,
      "step": 7185
    },
    {
      "epoch": 0.03070494030781852,
      "grad_norm": 1.0310641527175903,
      "learning_rate": 0.00019300119637668774,
      "loss": 0.4437,
      "step": 7186
    },
    {
      "epoch": 0.030709213191245716,
      "grad_norm": 2.3545939922332764,
      "learning_rate": 0.0001929584686378397,
      "loss": 0.5442,
      "step": 7187
    },
    {
      "epoch": 0.030713486074672912,
      "grad_norm": 0.5529667735099792,
      "learning_rate": 0.00019291574089899165,
      "loss": 0.2423,
      "step": 7188
    },
    {
      "epoch": 0.030717758958100104,
      "grad_norm": 0.6068684458732605,
      "learning_rate": 0.00019287301316014358,
      "loss": 0.2508,
      "step": 7189
    },
    {
      "epoch": 0.0307220318415273,
      "grad_norm": 0.6556079387664795,
      "learning_rate": 0.00019283028542129552,
      "loss": 0.3446,
      "step": 7190
    },
    {
      "epoch": 0.030726304724954495,
      "grad_norm": 4.80870246887207,
      "learning_rate": 0.00019278755768244746,
      "loss": 1.3758,
      "step": 7191
    },
    {
      "epoch": 0.030730577608381687,
      "grad_norm": 0.6464992761611938,
      "learning_rate": 0.00019274482994359937,
      "loss": 0.3265,
      "step": 7192
    },
    {
      "epoch": 0.030734850491808883,
      "grad_norm": 3.5324714183807373,
      "learning_rate": 0.0001927021022047513,
      "loss": 0.8548,
      "step": 7193
    },
    {
      "epoch": 0.03073912337523608,
      "grad_norm": 0.9909905195236206,
      "learning_rate": 0.00019265937446590325,
      "loss": 0.5611,
      "step": 7194
    },
    {
      "epoch": 0.03074339625866327,
      "grad_norm": 3.899827480316162,
      "learning_rate": 0.0001926166467270552,
      "loss": 0.7289,
      "step": 7195
    },
    {
      "epoch": 0.030747669142090466,
      "grad_norm": 2.694046974182129,
      "learning_rate": 0.00019257391898820715,
      "loss": 0.8925,
      "step": 7196
    },
    {
      "epoch": 0.03075194202551766,
      "grad_norm": 1.1120262145996094,
      "learning_rate": 0.00019253119124935909,
      "loss": 0.4878,
      "step": 7197
    },
    {
      "epoch": 0.030756214908944854,
      "grad_norm": 1.1477100849151611,
      "learning_rate": 0.00019248846351051102,
      "loss": 0.444,
      "step": 7198
    },
    {
      "epoch": 0.03076048779237205,
      "grad_norm": 2.174295425415039,
      "learning_rate": 0.00019244573577166296,
      "loss": 0.751,
      "step": 7199
    },
    {
      "epoch": 0.03076476067579924,
      "grad_norm": 0.5445476174354553,
      "learning_rate": 0.0001924030080328149,
      "loss": 0.2164,
      "step": 7200
    },
    {
      "epoch": 0.030769033559226437,
      "grad_norm": 2.9930260181427,
      "learning_rate": 0.00019236028029396684,
      "loss": 0.7709,
      "step": 7201
    },
    {
      "epoch": 0.030773306442653633,
      "grad_norm": 3.962662935256958,
      "learning_rate": 0.0001923175525551188,
      "loss": 1.1358,
      "step": 7202
    },
    {
      "epoch": 0.030777579326080825,
      "grad_norm": 2.9496617317199707,
      "learning_rate": 0.00019227482481627074,
      "loss": 0.7369,
      "step": 7203
    },
    {
      "epoch": 0.03078185220950802,
      "grad_norm": 0.984066367149353,
      "learning_rate": 0.00019223209707742268,
      "loss": 0.4514,
      "step": 7204
    },
    {
      "epoch": 0.030786125092935216,
      "grad_norm": 2.292402744293213,
      "learning_rate": 0.00019218936933857462,
      "loss": 0.8379,
      "step": 7205
    },
    {
      "epoch": 0.030790397976362408,
      "grad_norm": 2.9832730293273926,
      "learning_rate": 0.00019214664159972655,
      "loss": 0.7899,
      "step": 7206
    },
    {
      "epoch": 0.030794670859789604,
      "grad_norm": 1.4425665140151978,
      "learning_rate": 0.0001921039138608785,
      "loss": 0.6094,
      "step": 7207
    },
    {
      "epoch": 0.030798943743216796,
      "grad_norm": 0.994257926940918,
      "learning_rate": 0.0001920611861220304,
      "loss": 0.5009,
      "step": 7208
    },
    {
      "epoch": 0.03080321662664399,
      "grad_norm": 4.516117095947266,
      "learning_rate": 0.00019201845838318237,
      "loss": 0.9946,
      "step": 7209
    },
    {
      "epoch": 0.030807489510071187,
      "grad_norm": 1.503612995147705,
      "learning_rate": 0.0001919757306443343,
      "loss": 1.1825,
      "step": 7210
    },
    {
      "epoch": 0.03081176239349838,
      "grad_norm": 4.609023094177246,
      "learning_rate": 0.00019193300290548624,
      "loss": 1.1982,
      "step": 7211
    },
    {
      "epoch": 0.030816035276925575,
      "grad_norm": 2.9277937412261963,
      "learning_rate": 0.00019189027516663818,
      "loss": 0.722,
      "step": 7212
    },
    {
      "epoch": 0.03082030816035277,
      "grad_norm": 0.8013920187950134,
      "learning_rate": 0.00019184754742779012,
      "loss": 0.3888,
      "step": 7213
    },
    {
      "epoch": 0.030824581043779962,
      "grad_norm": 0.9432604908943176,
      "learning_rate": 0.00019180481968894206,
      "loss": 0.4514,
      "step": 7214
    },
    {
      "epoch": 0.030828853927207158,
      "grad_norm": 0.919882595539093,
      "learning_rate": 0.000191762091950094,
      "loss": 0.4322,
      "step": 7215
    },
    {
      "epoch": 0.030833126810634354,
      "grad_norm": 1.2179678678512573,
      "learning_rate": 0.00019171936421124593,
      "loss": 0.7698,
      "step": 7216
    },
    {
      "epoch": 0.030837399694061546,
      "grad_norm": 1.8352930545806885,
      "learning_rate": 0.0001916766364723979,
      "loss": 0.7364,
      "step": 7217
    },
    {
      "epoch": 0.03084167257748874,
      "grad_norm": 2.6999361515045166,
      "learning_rate": 0.00019163390873354983,
      "loss": 0.9,
      "step": 7218
    },
    {
      "epoch": 0.030845945460915937,
      "grad_norm": 1.9753793478012085,
      "learning_rate": 0.00019159118099470177,
      "loss": 0.5326,
      "step": 7219
    },
    {
      "epoch": 0.03085021834434313,
      "grad_norm": 1.7226520776748657,
      "learning_rate": 0.0001915484532558537,
      "loss": 0.4739,
      "step": 7220
    },
    {
      "epoch": 0.030854491227770325,
      "grad_norm": 2.8359501361846924,
      "learning_rate": 0.00019150572551700565,
      "loss": 0.856,
      "step": 7221
    },
    {
      "epoch": 0.030858764111197517,
      "grad_norm": 0.4513300061225891,
      "learning_rate": 0.00019146299777815759,
      "loss": 0.1271,
      "step": 7222
    },
    {
      "epoch": 0.030863036994624712,
      "grad_norm": 1.0116853713989258,
      "learning_rate": 0.00019142027003930952,
      "loss": 0.4185,
      "step": 7223
    },
    {
      "epoch": 0.030867309878051908,
      "grad_norm": 1.1836506128311157,
      "learning_rate": 0.0001913775423004615,
      "loss": 0.6539,
      "step": 7224
    },
    {
      "epoch": 0.0308715827614791,
      "grad_norm": 0.8325778841972351,
      "learning_rate": 0.0001913348145616134,
      "loss": 0.333,
      "step": 7225
    },
    {
      "epoch": 0.030875855644906296,
      "grad_norm": 1.8820809125900269,
      "learning_rate": 0.00019129208682276534,
      "loss": 0.4689,
      "step": 7226
    },
    {
      "epoch": 0.03088012852833349,
      "grad_norm": 1.5167760848999023,
      "learning_rate": 0.00019124935908391728,
      "loss": 0.5222,
      "step": 7227
    },
    {
      "epoch": 0.030884401411760683,
      "grad_norm": 0.6722149848937988,
      "learning_rate": 0.0001912066313450692,
      "loss": 0.2933,
      "step": 7228
    },
    {
      "epoch": 0.03088867429518788,
      "grad_norm": 1.49777352809906,
      "learning_rate": 0.00019116390360622115,
      "loss": 1.1448,
      "step": 7229
    },
    {
      "epoch": 0.030892947178615075,
      "grad_norm": 0.7848467230796814,
      "learning_rate": 0.0001911211758673731,
      "loss": 0.37,
      "step": 7230
    },
    {
      "epoch": 0.030897220062042267,
      "grad_norm": 3.9704318046569824,
      "learning_rate": 0.00019107844812852503,
      "loss": 1.0306,
      "step": 7231
    },
    {
      "epoch": 0.030901492945469462,
      "grad_norm": 0.5406790375709534,
      "learning_rate": 0.000191035720389677,
      "loss": 0.1425,
      "step": 7232
    },
    {
      "epoch": 0.030905765828896654,
      "grad_norm": 1.1270902156829834,
      "learning_rate": 0.00019099299265082893,
      "loss": 0.6444,
      "step": 7233
    },
    {
      "epoch": 0.03091003871232385,
      "grad_norm": 1.104775071144104,
      "learning_rate": 0.00019095026491198087,
      "loss": 0.6287,
      "step": 7234
    },
    {
      "epoch": 0.030914311595751046,
      "grad_norm": 1.0285128355026245,
      "learning_rate": 0.0001909075371731328,
      "loss": 0.5799,
      "step": 7235
    },
    {
      "epoch": 0.030918584479178238,
      "grad_norm": 1.041526436805725,
      "learning_rate": 0.00019086480943428474,
      "loss": 0.5639,
      "step": 7236
    },
    {
      "epoch": 0.030922857362605433,
      "grad_norm": 4.678715705871582,
      "learning_rate": 0.00019082208169543668,
      "loss": 1.3583,
      "step": 7237
    },
    {
      "epoch": 0.03092713024603263,
      "grad_norm": 2.0654520988464355,
      "learning_rate": 0.00019077935395658862,
      "loss": 0.6987,
      "step": 7238
    },
    {
      "epoch": 0.03093140312945982,
      "grad_norm": 1.13213312625885,
      "learning_rate": 0.00019073662621774058,
      "loss": 0.5102,
      "step": 7239
    },
    {
      "epoch": 0.030935676012887017,
      "grad_norm": 3.0338754653930664,
      "learning_rate": 0.00019069389847889252,
      "loss": 1.6889,
      "step": 7240
    },
    {
      "epoch": 0.030939948896314212,
      "grad_norm": 0.7066248059272766,
      "learning_rate": 0.00019065117074004443,
      "loss": 0.3264,
      "step": 7241
    },
    {
      "epoch": 0.030944221779741404,
      "grad_norm": 2.207915782928467,
      "learning_rate": 0.00019060844300119637,
      "loss": 0.6724,
      "step": 7242
    },
    {
      "epoch": 0.0309484946631686,
      "grad_norm": 2.6527814865112305,
      "learning_rate": 0.0001905657152623483,
      "loss": 0.8398,
      "step": 7243
    },
    {
      "epoch": 0.030952767546595795,
      "grad_norm": 0.9771631360054016,
      "learning_rate": 0.00019052298752350025,
      "loss": 0.5436,
      "step": 7244
    },
    {
      "epoch": 0.030957040430022988,
      "grad_norm": 3.004995107650757,
      "learning_rate": 0.00019048025978465218,
      "loss": 1.6297,
      "step": 7245
    },
    {
      "epoch": 0.030961313313450183,
      "grad_norm": 4.409664154052734,
      "learning_rate": 0.00019043753204580412,
      "loss": 0.8459,
      "step": 7246
    },
    {
      "epoch": 0.030965586196877375,
      "grad_norm": 2.306946039199829,
      "learning_rate": 0.00019039480430695609,
      "loss": 0.5747,
      "step": 7247
    },
    {
      "epoch": 0.03096985908030457,
      "grad_norm": 1.0893449783325195,
      "learning_rate": 0.00019035207656810802,
      "loss": 0.8179,
      "step": 7248
    },
    {
      "epoch": 0.030974131963731766,
      "grad_norm": 1.3897267580032349,
      "learning_rate": 0.00019030934882925996,
      "loss": 0.3907,
      "step": 7249
    },
    {
      "epoch": 0.03097840484715896,
      "grad_norm": 0.656998872756958,
      "learning_rate": 0.0001902666210904119,
      "loss": 0.2782,
      "step": 7250
    },
    {
      "epoch": 0.030982677730586154,
      "grad_norm": 1.0279815196990967,
      "learning_rate": 0.00019022389335156384,
      "loss": 0.4182,
      "step": 7251
    },
    {
      "epoch": 0.03098695061401335,
      "grad_norm": 0.92792809009552,
      "learning_rate": 0.00019018116561271578,
      "loss": 0.3264,
      "step": 7252
    },
    {
      "epoch": 0.030991223497440542,
      "grad_norm": 0.7512296438217163,
      "learning_rate": 0.0001901384378738677,
      "loss": 0.3187,
      "step": 7253
    },
    {
      "epoch": 0.030995496380867738,
      "grad_norm": 0.7276873588562012,
      "learning_rate": 0.00019009571013501968,
      "loss": 0.2508,
      "step": 7254
    },
    {
      "epoch": 0.030999769264294933,
      "grad_norm": 0.9887693524360657,
      "learning_rate": 0.00019005298239617162,
      "loss": 0.4184,
      "step": 7255
    },
    {
      "epoch": 0.031004042147722125,
      "grad_norm": 0.5241648554801941,
      "learning_rate": 0.00019001025465732355,
      "loss": 0.2379,
      "step": 7256
    },
    {
      "epoch": 0.03100831503114932,
      "grad_norm": 2.401109218597412,
      "learning_rate": 0.0001899675269184755,
      "loss": 0.7074,
      "step": 7257
    },
    {
      "epoch": 0.031012587914576513,
      "grad_norm": 2.89497447013855,
      "learning_rate": 0.0001899247991796274,
      "loss": 0.6344,
      "step": 7258
    },
    {
      "epoch": 0.03101686079800371,
      "grad_norm": 0.815996527671814,
      "learning_rate": 0.00018988207144077934,
      "loss": 0.3645,
      "step": 7259
    },
    {
      "epoch": 0.031021133681430904,
      "grad_norm": 3.754755973815918,
      "learning_rate": 0.00018983934370193128,
      "loss": 2.53,
      "step": 7260
    },
    {
      "epoch": 0.031025406564858096,
      "grad_norm": 0.7752013206481934,
      "learning_rate": 0.00018979661596308324,
      "loss": 0.5533,
      "step": 7261
    },
    {
      "epoch": 0.031029679448285292,
      "grad_norm": 1.7307548522949219,
      "learning_rate": 0.00018975388822423518,
      "loss": 0.6745,
      "step": 7262
    },
    {
      "epoch": 0.031033952331712487,
      "grad_norm": 0.7375409603118896,
      "learning_rate": 0.00018971116048538712,
      "loss": 0.2824,
      "step": 7263
    },
    {
      "epoch": 0.03103822521513968,
      "grad_norm": 3.431516170501709,
      "learning_rate": 0.00018966843274653906,
      "loss": 1.1762,
      "step": 7264
    },
    {
      "epoch": 0.031042498098566875,
      "grad_norm": 0.9298346042633057,
      "learning_rate": 0.000189625705007691,
      "loss": 0.5727,
      "step": 7265
    },
    {
      "epoch": 0.03104677098199407,
      "grad_norm": 1.0621079206466675,
      "learning_rate": 0.00018958297726884293,
      "loss": 0.8236,
      "step": 7266
    },
    {
      "epoch": 0.031051043865421263,
      "grad_norm": 2.8041844367980957,
      "learning_rate": 0.00018954024952999487,
      "loss": 1.5741,
      "step": 7267
    },
    {
      "epoch": 0.03105531674884846,
      "grad_norm": 3.140552520751953,
      "learning_rate": 0.0001894975217911468,
      "loss": 1.0432,
      "step": 7268
    },
    {
      "epoch": 0.03105958963227565,
      "grad_norm": 0.6999018788337708,
      "learning_rate": 0.00018945479405229877,
      "loss": 0.3194,
      "step": 7269
    },
    {
      "epoch": 0.031063862515702846,
      "grad_norm": 0.7474995851516724,
      "learning_rate": 0.0001894120663134507,
      "loss": 0.3196,
      "step": 7270
    },
    {
      "epoch": 0.03106813539913004,
      "grad_norm": 1.4312872886657715,
      "learning_rate": 0.00018936933857460265,
      "loss": 0.4772,
      "step": 7271
    },
    {
      "epoch": 0.031072408282557234,
      "grad_norm": 2.355553150177002,
      "learning_rate": 0.0001893266108357546,
      "loss": 0.5995,
      "step": 7272
    },
    {
      "epoch": 0.03107668116598443,
      "grad_norm": 2.7785072326660156,
      "learning_rate": 0.00018928388309690652,
      "loss": 0.7564,
      "step": 7273
    },
    {
      "epoch": 0.031080954049411625,
      "grad_norm": 0.6672065854072571,
      "learning_rate": 0.00018924115535805844,
      "loss": 0.233,
      "step": 7274
    },
    {
      "epoch": 0.031085226932838817,
      "grad_norm": 0.9323419332504272,
      "learning_rate": 0.00018919842761921037,
      "loss": 0.5789,
      "step": 7275
    },
    {
      "epoch": 0.031089499816266013,
      "grad_norm": 0.9108437299728394,
      "learning_rate": 0.00018915569988036234,
      "loss": 0.4336,
      "step": 7276
    },
    {
      "epoch": 0.03109377269969321,
      "grad_norm": 1.0616698265075684,
      "learning_rate": 0.00018911297214151428,
      "loss": 0.8279,
      "step": 7277
    },
    {
      "epoch": 0.0310980455831204,
      "grad_norm": 4.172522068023682,
      "learning_rate": 0.00018907024440266621,
      "loss": 1.0228,
      "step": 7278
    },
    {
      "epoch": 0.031102318466547596,
      "grad_norm": 0.7304279208183289,
      "learning_rate": 0.00018902751666381815,
      "loss": 0.3356,
      "step": 7279
    },
    {
      "epoch": 0.03110659134997479,
      "grad_norm": 1.0637911558151245,
      "learning_rate": 0.0001889847889249701,
      "loss": 0.8019,
      "step": 7280
    },
    {
      "epoch": 0.031110864233401984,
      "grad_norm": 1.28702712059021,
      "learning_rate": 0.00018894206118612203,
      "loss": 1.1403,
      "step": 7281
    },
    {
      "epoch": 0.03111513711682918,
      "grad_norm": 0.9421910047531128,
      "learning_rate": 0.00018889933344727397,
      "loss": 0.342,
      "step": 7282
    },
    {
      "epoch": 0.03111941000025637,
      "grad_norm": 0.915136456489563,
      "learning_rate": 0.0001888566057084259,
      "loss": 0.5771,
      "step": 7283
    },
    {
      "epoch": 0.031123682883683567,
      "grad_norm": 3.2584805488586426,
      "learning_rate": 0.00018881387796957787,
      "loss": 1.2554,
      "step": 7284
    },
    {
      "epoch": 0.031127955767110763,
      "grad_norm": 0.7839069962501526,
      "learning_rate": 0.0001887711502307298,
      "loss": 0.3653,
      "step": 7285
    },
    {
      "epoch": 0.031132228650537955,
      "grad_norm": 2.270556926727295,
      "learning_rate": 0.00018872842249188174,
      "loss": 0.5153,
      "step": 7286
    },
    {
      "epoch": 0.03113650153396515,
      "grad_norm": 2.5209314823150635,
      "learning_rate": 0.00018868569475303368,
      "loss": 0.7109,
      "step": 7287
    },
    {
      "epoch": 0.031140774417392346,
      "grad_norm": 1.0834815502166748,
      "learning_rate": 0.00018864296701418562,
      "loss": 0.8065,
      "step": 7288
    },
    {
      "epoch": 0.031145047300819538,
      "grad_norm": 1.673912763595581,
      "learning_rate": 0.00018860023927533756,
      "loss": 0.5195,
      "step": 7289
    },
    {
      "epoch": 0.031149320184246734,
      "grad_norm": 2.718402624130249,
      "learning_rate": 0.0001885575115364895,
      "loss": 1.1698,
      "step": 7290
    },
    {
      "epoch": 0.03115359306767393,
      "grad_norm": 0.7718645334243774,
      "learning_rate": 0.00018851478379764143,
      "loss": 0.449,
      "step": 7291
    },
    {
      "epoch": 0.03115786595110112,
      "grad_norm": 1.7402743101119995,
      "learning_rate": 0.00018847205605879337,
      "loss": 0.5509,
      "step": 7292
    },
    {
      "epoch": 0.031162138834528317,
      "grad_norm": 0.7829252481460571,
      "learning_rate": 0.0001884293283199453,
      "loss": 0.3485,
      "step": 7293
    },
    {
      "epoch": 0.03116641171795551,
      "grad_norm": 2.832305431365967,
      "learning_rate": 0.00018838660058109725,
      "loss": 1.5415,
      "step": 7294
    },
    {
      "epoch": 0.031170684601382705,
      "grad_norm": 0.8733055591583252,
      "learning_rate": 0.00018834387284224918,
      "loss": 0.6729,
      "step": 7295
    },
    {
      "epoch": 0.0311749574848099,
      "grad_norm": 2.7689385414123535,
      "learning_rate": 0.00018830114510340112,
      "loss": 0.573,
      "step": 7296
    },
    {
      "epoch": 0.031179230368237092,
      "grad_norm": 0.7681515216827393,
      "learning_rate": 0.00018825841736455306,
      "loss": 0.3483,
      "step": 7297
    },
    {
      "epoch": 0.031183503251664288,
      "grad_norm": 0.7444702386856079,
      "learning_rate": 0.000188215689625705,
      "loss": 0.3333,
      "step": 7298
    },
    {
      "epoch": 0.031187776135091484,
      "grad_norm": 1.3497803211212158,
      "learning_rate": 0.00018817296188685696,
      "loss": 0.4162,
      "step": 7299
    },
    {
      "epoch": 0.031192049018518676,
      "grad_norm": 4.257161617279053,
      "learning_rate": 0.0001881302341480089,
      "loss": 2.3009,
      "step": 7300
    },
    {
      "epoch": 0.03119632190194587,
      "grad_norm": 4.955909729003906,
      "learning_rate": 0.00018808750640916084,
      "loss": 3.1135,
      "step": 7301
    },
    {
      "epoch": 0.031200594785373067,
      "grad_norm": 0.7468649744987488,
      "learning_rate": 0.00018804477867031278,
      "loss": 0.3191,
      "step": 7302
    },
    {
      "epoch": 0.03120486766880026,
      "grad_norm": 0.8147889971733093,
      "learning_rate": 0.00018800205093146471,
      "loss": 0.4187,
      "step": 7303
    },
    {
      "epoch": 0.031209140552227455,
      "grad_norm": 0.7254717946052551,
      "learning_rate": 0.00018795932319261665,
      "loss": 0.2933,
      "step": 7304
    },
    {
      "epoch": 0.03121341343565465,
      "grad_norm": 1.3093832731246948,
      "learning_rate": 0.0001879165954537686,
      "loss": 0.5747,
      "step": 7305
    },
    {
      "epoch": 0.031217686319081842,
      "grad_norm": 1.110152244567871,
      "learning_rate": 0.00018787386771492055,
      "loss": 0.5192,
      "step": 7306
    },
    {
      "epoch": 0.031221959202509038,
      "grad_norm": 2.758178234100342,
      "learning_rate": 0.00018783113997607247,
      "loss": 1.4325,
      "step": 7307
    },
    {
      "epoch": 0.03122623208593623,
      "grad_norm": 0.8148463368415833,
      "learning_rate": 0.0001877884122372244,
      "loss": 0.6397,
      "step": 7308
    },
    {
      "epoch": 0.031230504969363426,
      "grad_norm": 0.9145693182945251,
      "learning_rate": 0.00018774568449837634,
      "loss": 0.5968,
      "step": 7309
    },
    {
      "epoch": 0.03123477785279062,
      "grad_norm": 0.8531560301780701,
      "learning_rate": 0.00018770295675952828,
      "loss": 0.4149,
      "step": 7310
    },
    {
      "epoch": 0.031239050736217813,
      "grad_norm": 3.2527434825897217,
      "learning_rate": 0.00018766022902068022,
      "loss": 1.203,
      "step": 7311
    },
    {
      "epoch": 0.03124332361964501,
      "grad_norm": 1.6621451377868652,
      "learning_rate": 0.00018761750128183215,
      "loss": 0.716,
      "step": 7312
    },
    {
      "epoch": 0.031247596503072204,
      "grad_norm": 2.9741265773773193,
      "learning_rate": 0.00018757477354298412,
      "loss": 0.86,
      "step": 7313
    },
    {
      "epoch": 0.0312518693864994,
      "grad_norm": 1.4017035961151123,
      "learning_rate": 0.00018753204580413606,
      "loss": 0.3573,
      "step": 7314
    },
    {
      "epoch": 0.03125614226992659,
      "grad_norm": 0.7523388862609863,
      "learning_rate": 0.000187489318065288,
      "loss": 0.3817,
      "step": 7315
    },
    {
      "epoch": 0.03126041515335379,
      "grad_norm": 0.6959409713745117,
      "learning_rate": 0.00018744659032643993,
      "loss": 0.3521,
      "step": 7316
    },
    {
      "epoch": 0.03126468803678098,
      "grad_norm": 0.8053902983665466,
      "learning_rate": 0.00018740386258759187,
      "loss": 0.3664,
      "step": 7317
    },
    {
      "epoch": 0.03126896092020817,
      "grad_norm": 4.27688455581665,
      "learning_rate": 0.0001873611348487438,
      "loss": 1.3284,
      "step": 7318
    },
    {
      "epoch": 0.03127323380363537,
      "grad_norm": 2.5849337577819824,
      "learning_rate": 0.00018731840710989575,
      "loss": 0.8136,
      "step": 7319
    },
    {
      "epoch": 0.03127750668706256,
      "grad_norm": 2.577352523803711,
      "learning_rate": 0.00018727567937104768,
      "loss": 1.0229,
      "step": 7320
    },
    {
      "epoch": 0.031281779570489755,
      "grad_norm": 0.9866889715194702,
      "learning_rate": 0.00018723295163219965,
      "loss": 0.3928,
      "step": 7321
    },
    {
      "epoch": 0.031286052453916954,
      "grad_norm": 2.637000322341919,
      "learning_rate": 0.0001871902238933516,
      "loss": 0.8598,
      "step": 7322
    },
    {
      "epoch": 0.031290325337344146,
      "grad_norm": 3.4121272563934326,
      "learning_rate": 0.0001871474961545035,
      "loss": 1.1305,
      "step": 7323
    },
    {
      "epoch": 0.03129459822077134,
      "grad_norm": 0.817533552646637,
      "learning_rate": 0.00018710476841565544,
      "loss": 0.3667,
      "step": 7324
    },
    {
      "epoch": 0.03129887110419854,
      "grad_norm": 0.6350669264793396,
      "learning_rate": 0.00018706204067680737,
      "loss": 0.2644,
      "step": 7325
    },
    {
      "epoch": 0.03130314398762573,
      "grad_norm": 2.169147491455078,
      "learning_rate": 0.0001870193129379593,
      "loss": 0.6277,
      "step": 7326
    },
    {
      "epoch": 0.03130741687105292,
      "grad_norm": 4.172654628753662,
      "learning_rate": 0.00018697658519911125,
      "loss": 2.042,
      "step": 7327
    },
    {
      "epoch": 0.03131168975448012,
      "grad_norm": 1.6658459901809692,
      "learning_rate": 0.00018693385746026321,
      "loss": 0.5633,
      "step": 7328
    },
    {
      "epoch": 0.03131596263790731,
      "grad_norm": 0.7345778942108154,
      "learning_rate": 0.00018689112972141515,
      "loss": 0.409,
      "step": 7329
    },
    {
      "epoch": 0.031320235521334505,
      "grad_norm": 3.153301954269409,
      "learning_rate": 0.0001868484019825671,
      "loss": 1.1484,
      "step": 7330
    },
    {
      "epoch": 0.031324508404761704,
      "grad_norm": 2.0722153186798096,
      "learning_rate": 0.00018680567424371903,
      "loss": 0.857,
      "step": 7331
    },
    {
      "epoch": 0.031328781288188896,
      "grad_norm": 0.8377480506896973,
      "learning_rate": 0.00018676294650487097,
      "loss": 0.4147,
      "step": 7332
    },
    {
      "epoch": 0.03133305417161609,
      "grad_norm": 2.524667978286743,
      "learning_rate": 0.0001867202187660229,
      "loss": 0.7484,
      "step": 7333
    },
    {
      "epoch": 0.03133732705504329,
      "grad_norm": 2.623408794403076,
      "learning_rate": 0.00018667749102717484,
      "loss": 1.3691,
      "step": 7334
    },
    {
      "epoch": 0.03134159993847048,
      "grad_norm": 2.0418500900268555,
      "learning_rate": 0.00018663476328832678,
      "loss": 0.8212,
      "step": 7335
    },
    {
      "epoch": 0.03134587282189767,
      "grad_norm": 3.8712363243103027,
      "learning_rate": 0.00018659203554947874,
      "loss": 1.2461,
      "step": 7336
    },
    {
      "epoch": 0.031350145705324864,
      "grad_norm": 1.6240692138671875,
      "learning_rate": 0.00018654930781063068,
      "loss": 0.4692,
      "step": 7337
    },
    {
      "epoch": 0.03135441858875206,
      "grad_norm": 0.3120647072792053,
      "learning_rate": 0.00018650658007178262,
      "loss": 0.086,
      "step": 7338
    },
    {
      "epoch": 0.031358691472179255,
      "grad_norm": 1.5568184852600098,
      "learning_rate": 0.00018646385233293456,
      "loss": 0.5353,
      "step": 7339
    },
    {
      "epoch": 0.03136296435560645,
      "grad_norm": 2.0578770637512207,
      "learning_rate": 0.00018642112459408647,
      "loss": 0.685,
      "step": 7340
    },
    {
      "epoch": 0.031367237239033646,
      "grad_norm": 3.208822727203369,
      "learning_rate": 0.0001863783968552384,
      "loss": 1.0239,
      "step": 7341
    },
    {
      "epoch": 0.03137151012246084,
      "grad_norm": 0.8402189016342163,
      "learning_rate": 0.00018633566911639034,
      "loss": 0.627,
      "step": 7342
    },
    {
      "epoch": 0.03137578300588803,
      "grad_norm": 3.100572347640991,
      "learning_rate": 0.0001862929413775423,
      "loss": 1.0214,
      "step": 7343
    },
    {
      "epoch": 0.03138005588931523,
      "grad_norm": 1.527830958366394,
      "learning_rate": 0.00018625021363869425,
      "loss": 0.505,
      "step": 7344
    },
    {
      "epoch": 0.03138432877274242,
      "grad_norm": 1.645056128501892,
      "learning_rate": 0.00018620748589984618,
      "loss": 0.5452,
      "step": 7345
    },
    {
      "epoch": 0.031388601656169614,
      "grad_norm": 1.5616837739944458,
      "learning_rate": 0.00018616475816099812,
      "loss": 0.4843,
      "step": 7346
    },
    {
      "epoch": 0.03139287453959681,
      "grad_norm": 0.8393746018409729,
      "learning_rate": 0.00018612203042215006,
      "loss": 0.627,
      "step": 7347
    },
    {
      "epoch": 0.031397147423024005,
      "grad_norm": 2.1299562454223633,
      "learning_rate": 0.000186079302683302,
      "loss": 0.5511,
      "step": 7348
    },
    {
      "epoch": 0.0314014203064512,
      "grad_norm": 1.900486946105957,
      "learning_rate": 0.00018603657494445394,
      "loss": 0.5944,
      "step": 7349
    },
    {
      "epoch": 0.031405693189878396,
      "grad_norm": 4.531674385070801,
      "learning_rate": 0.00018599384720560587,
      "loss": 1.3175,
      "step": 7350
    },
    {
      "epoch": 0.03140996607330559,
      "grad_norm": 0.9468349814414978,
      "learning_rate": 0.00018595111946675784,
      "loss": 0.4031,
      "step": 7351
    },
    {
      "epoch": 0.03141423895673278,
      "grad_norm": 1.6124801635742188,
      "learning_rate": 0.00018590839172790978,
      "loss": 0.589,
      "step": 7352
    },
    {
      "epoch": 0.03141851184015998,
      "grad_norm": 1.0918363332748413,
      "learning_rate": 0.00018586566398906171,
      "loss": 0.7831,
      "step": 7353
    },
    {
      "epoch": 0.03142278472358717,
      "grad_norm": 0.7601977586746216,
      "learning_rate": 0.00018582293625021365,
      "loss": 0.3189,
      "step": 7354
    },
    {
      "epoch": 0.031427057607014364,
      "grad_norm": 0.8063416481018066,
      "learning_rate": 0.0001857802085113656,
      "loss": 0.5755,
      "step": 7355
    },
    {
      "epoch": 0.03143133049044156,
      "grad_norm": 1.2975183725357056,
      "learning_rate": 0.0001857374807725175,
      "loss": 0.319,
      "step": 7356
    },
    {
      "epoch": 0.031435603373868755,
      "grad_norm": 4.598361968994141,
      "learning_rate": 0.00018569475303366944,
      "loss": 1.2288,
      "step": 7357
    },
    {
      "epoch": 0.03143987625729595,
      "grad_norm": 1.4397993087768555,
      "learning_rate": 0.0001856520252948214,
      "loss": 0.4609,
      "step": 7358
    },
    {
      "epoch": 0.031444149140723146,
      "grad_norm": 0.8065111041069031,
      "learning_rate": 0.00018560929755597334,
      "loss": 0.5461,
      "step": 7359
    },
    {
      "epoch": 0.03144842202415034,
      "grad_norm": 0.7987105250358582,
      "learning_rate": 0.00018556656981712528,
      "loss": 0.3516,
      "step": 7360
    },
    {
      "epoch": 0.03145269490757753,
      "grad_norm": 0.6298127770423889,
      "learning_rate": 0.00018552384207827722,
      "loss": 0.2383,
      "step": 7361
    },
    {
      "epoch": 0.03145696779100472,
      "grad_norm": 0.7894080281257629,
      "learning_rate": 0.00018548111433942916,
      "loss": 0.3033,
      "step": 7362
    },
    {
      "epoch": 0.03146124067443192,
      "grad_norm": 1.0552071332931519,
      "learning_rate": 0.0001854383866005811,
      "loss": 0.4179,
      "step": 7363
    },
    {
      "epoch": 0.031465513557859114,
      "grad_norm": 0.8964090347290039,
      "learning_rate": 0.00018539565886173303,
      "loss": 0.5432,
      "step": 7364
    },
    {
      "epoch": 0.031469786441286306,
      "grad_norm": 0.6741600632667542,
      "learning_rate": 0.000185352931122885,
      "loss": 0.278,
      "step": 7365
    },
    {
      "epoch": 0.031474059324713505,
      "grad_norm": 2.0421574115753174,
      "learning_rate": 0.00018531020338403693,
      "loss": 0.6419,
      "step": 7366
    },
    {
      "epoch": 0.0314783322081407,
      "grad_norm": 0.9488042593002319,
      "learning_rate": 0.00018526747564518887,
      "loss": 0.5038,
      "step": 7367
    },
    {
      "epoch": 0.03148260509156789,
      "grad_norm": 2.7436790466308594,
      "learning_rate": 0.0001852247479063408,
      "loss": 0.9867,
      "step": 7368
    },
    {
      "epoch": 0.03148687797499509,
      "grad_norm": 1.5122630596160889,
      "learning_rate": 0.00018518202016749275,
      "loss": 1.2189,
      "step": 7369
    },
    {
      "epoch": 0.03149115085842228,
      "grad_norm": 1.0130231380462646,
      "learning_rate": 0.00018513929242864468,
      "loss": 0.4884,
      "step": 7370
    },
    {
      "epoch": 0.03149542374184947,
      "grad_norm": 0.6290337443351746,
      "learning_rate": 0.00018509656468979662,
      "loss": 0.2857,
      "step": 7371
    },
    {
      "epoch": 0.03149969662527667,
      "grad_norm": 1.199178695678711,
      "learning_rate": 0.00018505383695094856,
      "loss": 0.6796,
      "step": 7372
    },
    {
      "epoch": 0.031503969508703863,
      "grad_norm": 1.1363884210586548,
      "learning_rate": 0.0001850111092121005,
      "loss": 0.4884,
      "step": 7373
    },
    {
      "epoch": 0.031508242392131056,
      "grad_norm": 1.3484481573104858,
      "learning_rate": 0.00018496838147325244,
      "loss": 0.4874,
      "step": 7374
    },
    {
      "epoch": 0.031512515275558255,
      "grad_norm": 1.1026535034179688,
      "learning_rate": 0.00018492565373440437,
      "loss": 0.4142,
      "step": 7375
    },
    {
      "epoch": 0.03151678815898545,
      "grad_norm": 1.8002817630767822,
      "learning_rate": 0.0001848829259955563,
      "loss": 1.2882,
      "step": 7376
    },
    {
      "epoch": 0.03152106104241264,
      "grad_norm": 1.3168662786483765,
      "learning_rate": 0.00018484019825670825,
      "loss": 0.2791,
      "step": 7377
    },
    {
      "epoch": 0.03152533392583984,
      "grad_norm": 2.842465877532959,
      "learning_rate": 0.0001847974705178602,
      "loss": 0.7342,
      "step": 7378
    },
    {
      "epoch": 0.03152960680926703,
      "grad_norm": 1.366467833518982,
      "learning_rate": 0.00018475474277901213,
      "loss": 0.6983,
      "step": 7379
    },
    {
      "epoch": 0.03153387969269422,
      "grad_norm": 1.854175090789795,
      "learning_rate": 0.0001847120150401641,
      "loss": 0.5202,
      "step": 7380
    },
    {
      "epoch": 0.03153815257612142,
      "grad_norm": 0.2801079750061035,
      "learning_rate": 0.00018466928730131603,
      "loss": 0.0679,
      "step": 7381
    },
    {
      "epoch": 0.03154242545954861,
      "grad_norm": 0.551156759262085,
      "learning_rate": 0.00018462655956246797,
      "loss": 0.1661,
      "step": 7382
    },
    {
      "epoch": 0.031546698342975805,
      "grad_norm": 3.386484146118164,
      "learning_rate": 0.0001845838318236199,
      "loss": 0.9223,
      "step": 7383
    },
    {
      "epoch": 0.031550971226403005,
      "grad_norm": 1.7204519510269165,
      "learning_rate": 0.00018454110408477184,
      "loss": 0.4468,
      "step": 7384
    },
    {
      "epoch": 0.0315552441098302,
      "grad_norm": 0.6053766012191772,
      "learning_rate": 0.00018449837634592378,
      "loss": 0.2231,
      "step": 7385
    },
    {
      "epoch": 0.03155951699325739,
      "grad_norm": 1.3592216968536377,
      "learning_rate": 0.00018445564860707572,
      "loss": 0.4745,
      "step": 7386
    },
    {
      "epoch": 0.03156378987668458,
      "grad_norm": 1.824616551399231,
      "learning_rate": 0.00018441292086822766,
      "loss": 0.4593,
      "step": 7387
    },
    {
      "epoch": 0.03156806276011178,
      "grad_norm": 2.816415786743164,
      "learning_rate": 0.00018437019312937962,
      "loss": 0.8729,
      "step": 7388
    },
    {
      "epoch": 0.03157233564353897,
      "grad_norm": 1.2556902170181274,
      "learning_rate": 0.00018432746539053153,
      "loss": 0.3638,
      "step": 7389
    },
    {
      "epoch": 0.031576608526966164,
      "grad_norm": 1.4100390672683716,
      "learning_rate": 0.00018428473765168347,
      "loss": 0.4476,
      "step": 7390
    },
    {
      "epoch": 0.03158088141039336,
      "grad_norm": 4.8623809814453125,
      "learning_rate": 0.0001842420099128354,
      "loss": 0.8749,
      "step": 7391
    },
    {
      "epoch": 0.031585154293820555,
      "grad_norm": 1.6992568969726562,
      "learning_rate": 0.00018419928217398734,
      "loss": 0.4265,
      "step": 7392
    },
    {
      "epoch": 0.03158942717724775,
      "grad_norm": 4.090126037597656,
      "learning_rate": 0.00018415655443513928,
      "loss": 1.5119,
      "step": 7393
    },
    {
      "epoch": 0.03159370006067495,
      "grad_norm": 0.683094322681427,
      "learning_rate": 0.00018411382669629122,
      "loss": 0.3465,
      "step": 7394
    },
    {
      "epoch": 0.03159797294410214,
      "grad_norm": 1.3629029989242554,
      "learning_rate": 0.00018407109895744319,
      "loss": 0.4599,
      "step": 7395
    },
    {
      "epoch": 0.03160224582752933,
      "grad_norm": 1.5945708751678467,
      "learning_rate": 0.00018402837121859512,
      "loss": 0.9545,
      "step": 7396
    },
    {
      "epoch": 0.03160651871095653,
      "grad_norm": 2.138885498046875,
      "learning_rate": 0.00018398564347974706,
      "loss": 1.3465,
      "step": 7397
    },
    {
      "epoch": 0.03161079159438372,
      "grad_norm": 2.8288257122039795,
      "learning_rate": 0.000183942915740899,
      "loss": 0.7594,
      "step": 7398
    },
    {
      "epoch": 0.031615064477810914,
      "grad_norm": 1.4765090942382812,
      "learning_rate": 0.00018390018800205094,
      "loss": 0.9231,
      "step": 7399
    },
    {
      "epoch": 0.03161933736123811,
      "grad_norm": 0.4448311924934387,
      "learning_rate": 0.00018385746026320287,
      "loss": 0.1478,
      "step": 7400
    },
    {
      "epoch": 0.031623610244665305,
      "grad_norm": 1.1051031351089478,
      "learning_rate": 0.0001838147325243548,
      "loss": 0.3885,
      "step": 7401
    },
    {
      "epoch": 0.0316278831280925,
      "grad_norm": 1.8053181171417236,
      "learning_rate": 0.00018377200478550675,
      "loss": 0.4422,
      "step": 7402
    },
    {
      "epoch": 0.031632156011519696,
      "grad_norm": 1.1879075765609741,
      "learning_rate": 0.00018372927704665871,
      "loss": 0.3632,
      "step": 7403
    },
    {
      "epoch": 0.03163642889494689,
      "grad_norm": 4.88840913772583,
      "learning_rate": 0.00018368654930781065,
      "loss": 1.1711,
      "step": 7404
    },
    {
      "epoch": 0.03164070177837408,
      "grad_norm": 3.835712432861328,
      "learning_rate": 0.0001836438215689626,
      "loss": 2.5039,
      "step": 7405
    },
    {
      "epoch": 0.03164497466180128,
      "grad_norm": 2.1138622760772705,
      "learning_rate": 0.0001836010938301145,
      "loss": 0.7506,
      "step": 7406
    },
    {
      "epoch": 0.03164924754522847,
      "grad_norm": 1.2637792825698853,
      "learning_rate": 0.00018355836609126644,
      "loss": 0.4291,
      "step": 7407
    },
    {
      "epoch": 0.031653520428655664,
      "grad_norm": 3.911904811859131,
      "learning_rate": 0.00018351563835241838,
      "loss": 1.1533,
      "step": 7408
    },
    {
      "epoch": 0.03165779331208286,
      "grad_norm": 2.8575491905212402,
      "learning_rate": 0.00018347291061357031,
      "loss": 0.9234,
      "step": 7409
    },
    {
      "epoch": 0.031662066195510055,
      "grad_norm": 1.6449038982391357,
      "learning_rate": 0.00018343018287472228,
      "loss": 0.4657,
      "step": 7410
    },
    {
      "epoch": 0.03166633907893725,
      "grad_norm": 2.3052289485931396,
      "learning_rate": 0.00018338745513587422,
      "loss": 0.8309,
      "step": 7411
    },
    {
      "epoch": 0.03167061196236444,
      "grad_norm": 3.237448215484619,
      "learning_rate": 0.00018334472739702616,
      "loss": 0.77,
      "step": 7412
    },
    {
      "epoch": 0.03167488484579164,
      "grad_norm": 3.145686626434326,
      "learning_rate": 0.0001833019996581781,
      "loss": 0.8719,
      "step": 7413
    },
    {
      "epoch": 0.03167915772921883,
      "grad_norm": 0.8233677744865417,
      "learning_rate": 0.00018325927191933003,
      "loss": 0.2634,
      "step": 7414
    },
    {
      "epoch": 0.03168343061264602,
      "grad_norm": 1.1086784601211548,
      "learning_rate": 0.00018321654418048197,
      "loss": 0.451,
      "step": 7415
    },
    {
      "epoch": 0.03168770349607322,
      "grad_norm": 1.2010316848754883,
      "learning_rate": 0.0001831738164416339,
      "loss": 0.4003,
      "step": 7416
    },
    {
      "epoch": 0.031691976379500414,
      "grad_norm": 1.1892471313476562,
      "learning_rate": 0.00018313108870278587,
      "loss": 0.6949,
      "step": 7417
    },
    {
      "epoch": 0.031696249262927606,
      "grad_norm": 3.0928499698638916,
      "learning_rate": 0.0001830883609639378,
      "loss": 0.795,
      "step": 7418
    },
    {
      "epoch": 0.031700522146354805,
      "grad_norm": 2.2382631301879883,
      "learning_rate": 0.00018304563322508975,
      "loss": 0.6774,
      "step": 7419
    },
    {
      "epoch": 0.031704795029782,
      "grad_norm": 1.6651201248168945,
      "learning_rate": 0.00018300290548624169,
      "loss": 0.3821,
      "step": 7420
    },
    {
      "epoch": 0.03170906791320919,
      "grad_norm": 3.8955178260803223,
      "learning_rate": 0.00018296017774739362,
      "loss": 1.1403,
      "step": 7421
    },
    {
      "epoch": 0.03171334079663639,
      "grad_norm": 1.6543997526168823,
      "learning_rate": 0.00018291745000854553,
      "loss": 0.3677,
      "step": 7422
    },
    {
      "epoch": 0.03171761368006358,
      "grad_norm": 3.725163698196411,
      "learning_rate": 0.00018287472226969747,
      "loss": 1.078,
      "step": 7423
    },
    {
      "epoch": 0.03172188656349077,
      "grad_norm": 2.4612345695495605,
      "learning_rate": 0.0001828319945308494,
      "loss": 0.6392,
      "step": 7424
    },
    {
      "epoch": 0.03172615944691797,
      "grad_norm": 0.8308967351913452,
      "learning_rate": 0.00018278926679200137,
      "loss": 0.276,
      "step": 7425
    },
    {
      "epoch": 0.031730432330345164,
      "grad_norm": 1.603975772857666,
      "learning_rate": 0.0001827465390531533,
      "loss": 0.7563,
      "step": 7426
    },
    {
      "epoch": 0.031734705213772356,
      "grad_norm": 3.70831036567688,
      "learning_rate": 0.00018270381131430525,
      "loss": 1.0149,
      "step": 7427
    },
    {
      "epoch": 0.031738978097199555,
      "grad_norm": 1.5747184753417969,
      "learning_rate": 0.0001826610835754572,
      "loss": 0.7562,
      "step": 7428
    },
    {
      "epoch": 0.03174325098062675,
      "grad_norm": 0.8314217329025269,
      "learning_rate": 0.00018261835583660913,
      "loss": 0.2893,
      "step": 7429
    },
    {
      "epoch": 0.03174752386405394,
      "grad_norm": 0.8536683917045593,
      "learning_rate": 0.00018257562809776106,
      "loss": 0.2523,
      "step": 7430
    },
    {
      "epoch": 0.03175179674748114,
      "grad_norm": 1.202437400817871,
      "learning_rate": 0.000182532900358913,
      "loss": 0.3814,
      "step": 7431
    },
    {
      "epoch": 0.03175606963090833,
      "grad_norm": 3.7486281394958496,
      "learning_rate": 0.00018249017262006497,
      "loss": 1.2845,
      "step": 7432
    },
    {
      "epoch": 0.03176034251433552,
      "grad_norm": 1.2696280479431152,
      "learning_rate": 0.0001824474448812169,
      "loss": 0.415,
      "step": 7433
    },
    {
      "epoch": 0.03176461539776272,
      "grad_norm": 1.3255715370178223,
      "learning_rate": 0.00018240471714236884,
      "loss": 0.5101,
      "step": 7434
    },
    {
      "epoch": 0.031768888281189914,
      "grad_norm": 0.7672498822212219,
      "learning_rate": 0.00018236198940352078,
      "loss": 0.2506,
      "step": 7435
    },
    {
      "epoch": 0.031773161164617106,
      "grad_norm": 1.5262877941131592,
      "learning_rate": 0.00018231926166467272,
      "loss": 0.426,
      "step": 7436
    },
    {
      "epoch": 0.0317774340480443,
      "grad_norm": 5.374307632446289,
      "learning_rate": 0.00018227653392582466,
      "loss": 2.9142,
      "step": 7437
    },
    {
      "epoch": 0.0317817069314715,
      "grad_norm": 1.5116908550262451,
      "learning_rate": 0.0001822338061869766,
      "loss": 0.4082,
      "step": 7438
    },
    {
      "epoch": 0.03178597981489869,
      "grad_norm": 3.260580539703369,
      "learning_rate": 0.0001821910784481285,
      "loss": 1.7085,
      "step": 7439
    },
    {
      "epoch": 0.03179025269832588,
      "grad_norm": 2.24070405960083,
      "learning_rate": 0.00018214835070928047,
      "loss": 0.7692,
      "step": 7440
    },
    {
      "epoch": 0.03179452558175308,
      "grad_norm": 1.019203543663025,
      "learning_rate": 0.0001821056229704324,
      "loss": 0.3383,
      "step": 7441
    },
    {
      "epoch": 0.03179879846518027,
      "grad_norm": 3.0235447883605957,
      "learning_rate": 0.00018206289523158434,
      "loss": 0.836,
      "step": 7442
    },
    {
      "epoch": 0.031803071348607465,
      "grad_norm": 0.8846574425697327,
      "learning_rate": 0.00018202016749273628,
      "loss": 0.2615,
      "step": 7443
    },
    {
      "epoch": 0.031807344232034664,
      "grad_norm": 0.8018284440040588,
      "learning_rate": 0.00018197743975388822,
      "loss": 0.2522,
      "step": 7444
    },
    {
      "epoch": 0.031811617115461856,
      "grad_norm": 3.701084852218628,
      "learning_rate": 0.00018193471201504016,
      "loss": 1.1696,
      "step": 7445
    },
    {
      "epoch": 0.03181588999888905,
      "grad_norm": 3.0968642234802246,
      "learning_rate": 0.0001818919842761921,
      "loss": 0.953,
      "step": 7446
    },
    {
      "epoch": 0.03182016288231625,
      "grad_norm": 1.0539793968200684,
      "learning_rate": 0.00018184925653734406,
      "loss": 0.3478,
      "step": 7447
    },
    {
      "epoch": 0.03182443576574344,
      "grad_norm": 0.8399128317832947,
      "learning_rate": 0.000181806528798496,
      "loss": 0.2526,
      "step": 7448
    },
    {
      "epoch": 0.03182870864917063,
      "grad_norm": 1.3318331241607666,
      "learning_rate": 0.00018176380105964794,
      "loss": 0.325,
      "step": 7449
    },
    {
      "epoch": 0.03183298153259783,
      "grad_norm": 4.77252721786499,
      "learning_rate": 0.00018172107332079987,
      "loss": 1.4031,
      "step": 7450
    },
    {
      "epoch": 0.03183725441602502,
      "grad_norm": 0.7226448655128479,
      "learning_rate": 0.0001816783455819518,
      "loss": 0.2323,
      "step": 7451
    },
    {
      "epoch": 0.031841527299452214,
      "grad_norm": 0.7689729928970337,
      "learning_rate": 0.00018163561784310375,
      "loss": 0.2635,
      "step": 7452
    },
    {
      "epoch": 0.031845800182879413,
      "grad_norm": 0.9000850319862366,
      "learning_rate": 0.0001815928901042557,
      "loss": 0.2949,
      "step": 7453
    },
    {
      "epoch": 0.031850073066306606,
      "grad_norm": 0.888870894908905,
      "learning_rate": 0.00018155016236540763,
      "loss": 0.2861,
      "step": 7454
    },
    {
      "epoch": 0.0318543459497338,
      "grad_norm": 2.062404155731201,
      "learning_rate": 0.00018150743462655956,
      "loss": 0.5933,
      "step": 7455
    },
    {
      "epoch": 0.031858618833161,
      "grad_norm": 1.242945909500122,
      "learning_rate": 0.0001814647068877115,
      "loss": 0.6447,
      "step": 7456
    },
    {
      "epoch": 0.03186289171658819,
      "grad_norm": 2.122244358062744,
      "learning_rate": 0.00018142197914886344,
      "loss": 0.6757,
      "step": 7457
    },
    {
      "epoch": 0.03186716460001538,
      "grad_norm": 1.7575918436050415,
      "learning_rate": 0.00018137925141001538,
      "loss": 1.2512,
      "step": 7458
    },
    {
      "epoch": 0.03187143748344258,
      "grad_norm": 0.6836839914321899,
      "learning_rate": 0.00018133652367116732,
      "loss": 0.1848,
      "step": 7459
    },
    {
      "epoch": 0.03187571036686977,
      "grad_norm": 3.720974922180176,
      "learning_rate": 0.00018129379593231925,
      "loss": 1.0512,
      "step": 7460
    },
    {
      "epoch": 0.031879983250296964,
      "grad_norm": 1.4203606843948364,
      "learning_rate": 0.0001812510681934712,
      "loss": 0.7554,
      "step": 7461
    },
    {
      "epoch": 0.031884256133724156,
      "grad_norm": 2.969599962234497,
      "learning_rate": 0.00018120834045462316,
      "loss": 1.5558,
      "step": 7462
    },
    {
      "epoch": 0.031888529017151356,
      "grad_norm": 1.2950047254562378,
      "learning_rate": 0.0001811656127157751,
      "loss": 0.6963,
      "step": 7463
    },
    {
      "epoch": 0.03189280190057855,
      "grad_norm": 1.6581223011016846,
      "learning_rate": 0.00018112288497692703,
      "loss": 0.4628,
      "step": 7464
    },
    {
      "epoch": 0.03189707478400574,
      "grad_norm": 1.06201171875,
      "learning_rate": 0.00018108015723807897,
      "loss": 0.4971,
      "step": 7465
    },
    {
      "epoch": 0.03190134766743294,
      "grad_norm": 0.7890142798423767,
      "learning_rate": 0.0001810374294992309,
      "loss": 0.2374,
      "step": 7466
    },
    {
      "epoch": 0.03190562055086013,
      "grad_norm": 1.6323214769363403,
      "learning_rate": 0.00018099470176038285,
      "loss": 0.4482,
      "step": 7467
    },
    {
      "epoch": 0.03190989343428732,
      "grad_norm": 1.2267045974731445,
      "learning_rate": 0.00018095197402153478,
      "loss": 0.4362,
      "step": 7468
    },
    {
      "epoch": 0.03191416631771452,
      "grad_norm": 0.9449611306190491,
      "learning_rate": 0.00018090924628268675,
      "loss": 0.8876,
      "step": 7469
    },
    {
      "epoch": 0.031918439201141714,
      "grad_norm": 1.0287659168243408,
      "learning_rate": 0.00018086651854383869,
      "loss": 0.5206,
      "step": 7470
    },
    {
      "epoch": 0.031922712084568906,
      "grad_norm": 0.6514937281608582,
      "learning_rate": 0.0001808237908049906,
      "loss": 0.1982,
      "step": 7471
    },
    {
      "epoch": 0.031926984967996105,
      "grad_norm": 0.9103482961654663,
      "learning_rate": 0.00018078106306614253,
      "loss": 0.8631,
      "step": 7472
    },
    {
      "epoch": 0.0319312578514233,
      "grad_norm": 2.8599138259887695,
      "learning_rate": 0.00018073833532729447,
      "loss": 0.6681,
      "step": 7473
    },
    {
      "epoch": 0.03193553073485049,
      "grad_norm": 1.1905224323272705,
      "learning_rate": 0.0001806956075884464,
      "loss": 0.4484,
      "step": 7474
    },
    {
      "epoch": 0.03193980361827769,
      "grad_norm": 1.1588164567947388,
      "learning_rate": 0.00018065287984959835,
      "loss": 0.5357,
      "step": 7475
    },
    {
      "epoch": 0.03194407650170488,
      "grad_norm": 1.181107521057129,
      "learning_rate": 0.00018061015211075029,
      "loss": 0.6984,
      "step": 7476
    },
    {
      "epoch": 0.03194834938513207,
      "grad_norm": 1.464403748512268,
      "learning_rate": 0.00018056742437190225,
      "loss": 1.1984,
      "step": 7477
    },
    {
      "epoch": 0.03195262226855927,
      "grad_norm": 0.9336699843406677,
      "learning_rate": 0.0001805246966330542,
      "loss": 0.3149,
      "step": 7478
    },
    {
      "epoch": 0.031956895151986464,
      "grad_norm": 0.8732745051383972,
      "learning_rate": 0.00018048196889420613,
      "loss": 0.2576,
      "step": 7479
    },
    {
      "epoch": 0.031961168035413656,
      "grad_norm": 2.8105170726776123,
      "learning_rate": 0.00018043924115535806,
      "loss": 0.5863,
      "step": 7480
    },
    {
      "epoch": 0.031965440918840855,
      "grad_norm": 0.6082919836044312,
      "learning_rate": 0.00018039651341651,
      "loss": 0.2161,
      "step": 7481
    },
    {
      "epoch": 0.03196971380226805,
      "grad_norm": 3.1354947090148926,
      "learning_rate": 0.00018035378567766194,
      "loss": 1.0635,
      "step": 7482
    },
    {
      "epoch": 0.03197398668569524,
      "grad_norm": 4.010043144226074,
      "learning_rate": 0.00018031105793881388,
      "loss": 1.4037,
      "step": 7483
    },
    {
      "epoch": 0.03197825956912244,
      "grad_norm": 0.7861441969871521,
      "learning_rate": 0.00018026833019996584,
      "loss": 0.2614,
      "step": 7484
    },
    {
      "epoch": 0.03198253245254963,
      "grad_norm": 2.0813896656036377,
      "learning_rate": 0.00018022560246111778,
      "loss": 0.7992,
      "step": 7485
    },
    {
      "epoch": 0.03198680533597682,
      "grad_norm": 2.855675458908081,
      "learning_rate": 0.00018018287472226972,
      "loss": 0.9719,
      "step": 7486
    },
    {
      "epoch": 0.031991078219404015,
      "grad_norm": 2.046311855316162,
      "learning_rate": 0.00018014014698342166,
      "loss": 0.5633,
      "step": 7487
    },
    {
      "epoch": 0.031995351102831214,
      "grad_norm": 3.9486422538757324,
      "learning_rate": 0.00018009741924457357,
      "loss": 1.572,
      "step": 7488
    },
    {
      "epoch": 0.031999623986258406,
      "grad_norm": 0.7790383696556091,
      "learning_rate": 0.0001800546915057255,
      "loss": 0.2527,
      "step": 7489
    },
    {
      "epoch": 0.0320038968696856,
      "grad_norm": 1.368756890296936,
      "learning_rate": 0.00018001196376687744,
      "loss": 1.128,
      "step": 7490
    },
    {
      "epoch": 0.0320081697531128,
      "grad_norm": 2.041217565536499,
      "learning_rate": 0.00017996923602802938,
      "loss": 0.7483,
      "step": 7491
    },
    {
      "epoch": 0.03201244263653999,
      "grad_norm": 2.7180416584014893,
      "learning_rate": 0.00017992650828918135,
      "loss": 0.8097,
      "step": 7492
    },
    {
      "epoch": 0.03201671551996718,
      "grad_norm": 2.8707265853881836,
      "learning_rate": 0.00017988378055033328,
      "loss": 1.1586,
      "step": 7493
    },
    {
      "epoch": 0.03202098840339438,
      "grad_norm": 1.5292609930038452,
      "learning_rate": 0.00017984105281148522,
      "loss": 0.3697,
      "step": 7494
    },
    {
      "epoch": 0.03202526128682157,
      "grad_norm": 0.8037368655204773,
      "learning_rate": 0.00017979832507263716,
      "loss": 0.2706,
      "step": 7495
    },
    {
      "epoch": 0.032029534170248765,
      "grad_norm": 1.1653939485549927,
      "learning_rate": 0.0001797555973337891,
      "loss": 0.4881,
      "step": 7496
    },
    {
      "epoch": 0.032033807053675964,
      "grad_norm": 0.7599102854728699,
      "learning_rate": 0.00017971286959494103,
      "loss": 0.2448,
      "step": 7497
    },
    {
      "epoch": 0.032038079937103156,
      "grad_norm": 0.6856732964515686,
      "learning_rate": 0.00017967014185609297,
      "loss": 0.2235,
      "step": 7498
    },
    {
      "epoch": 0.03204235282053035,
      "grad_norm": 1.168657898902893,
      "learning_rate": 0.00017962741411724494,
      "loss": 0.503,
      "step": 7499
    },
    {
      "epoch": 0.03204662570395755,
      "grad_norm": 0.903090238571167,
      "learning_rate": 0.00017958468637839688,
      "loss": 0.8123,
      "step": 7500
    },
    {
      "epoch": 0.03205089858738474,
      "grad_norm": 0.7222786545753479,
      "learning_rate": 0.0001795419586395488,
      "loss": 0.2447,
      "step": 7501
    },
    {
      "epoch": 0.03205517147081193,
      "grad_norm": 1.8202370405197144,
      "learning_rate": 0.00017949923090070075,
      "loss": 0.7553,
      "step": 7502
    },
    {
      "epoch": 0.03205944435423913,
      "grad_norm": 3.1079213619232178,
      "learning_rate": 0.0001794565031618527,
      "loss": 0.8366,
      "step": 7503
    },
    {
      "epoch": 0.03206371723766632,
      "grad_norm": 0.7163575887680054,
      "learning_rate": 0.0001794137754230046,
      "loss": 0.2245,
      "step": 7504
    },
    {
      "epoch": 0.032067990121093515,
      "grad_norm": 1.3789911270141602,
      "learning_rate": 0.00017937104768415654,
      "loss": 1.1277,
      "step": 7505
    },
    {
      "epoch": 0.032072263004520714,
      "grad_norm": 1.297313928604126,
      "learning_rate": 0.00017932831994530848,
      "loss": 0.7366,
      "step": 7506
    },
    {
      "epoch": 0.032076535887947906,
      "grad_norm": 0.2689507305622101,
      "learning_rate": 0.00017928559220646044,
      "loss": 0.0657,
      "step": 7507
    },
    {
      "epoch": 0.0320808087713751,
      "grad_norm": 1.901338815689087,
      "learning_rate": 0.00017924286446761238,
      "loss": 0.5192,
      "step": 7508
    },
    {
      "epoch": 0.0320850816548023,
      "grad_norm": 0.7450479865074158,
      "learning_rate": 0.00017920013672876432,
      "loss": 0.2308,
      "step": 7509
    },
    {
      "epoch": 0.03208935453822949,
      "grad_norm": 3.302762746810913,
      "learning_rate": 0.00017915740898991625,
      "loss": 1.0785,
      "step": 7510
    },
    {
      "epoch": 0.03209362742165668,
      "grad_norm": 2.6767666339874268,
      "learning_rate": 0.0001791146812510682,
      "loss": 0.5346,
      "step": 7511
    },
    {
      "epoch": 0.032097900305083873,
      "grad_norm": 1.5670075416564941,
      "learning_rate": 0.00017907195351222013,
      "loss": 0.4125,
      "step": 7512
    },
    {
      "epoch": 0.03210217318851107,
      "grad_norm": 2.7017455101013184,
      "learning_rate": 0.00017902922577337207,
      "loss": 0.8632,
      "step": 7513
    },
    {
      "epoch": 0.032106446071938265,
      "grad_norm": 0.9757823348045349,
      "learning_rate": 0.00017898649803452403,
      "loss": 0.5149,
      "step": 7514
    },
    {
      "epoch": 0.03211071895536546,
      "grad_norm": 1.1220545768737793,
      "learning_rate": 0.00017894377029567597,
      "loss": 0.6854,
      "step": 7515
    },
    {
      "epoch": 0.032114991838792656,
      "grad_norm": 1.0771207809448242,
      "learning_rate": 0.0001789010425568279,
      "loss": 0.5449,
      "step": 7516
    },
    {
      "epoch": 0.03211926472221985,
      "grad_norm": 3.8538401126861572,
      "learning_rate": 0.00017885831481797985,
      "loss": 1.3009,
      "step": 7517
    },
    {
      "epoch": 0.03212353760564704,
      "grad_norm": 4.153408050537109,
      "learning_rate": 0.00017881558707913178,
      "loss": 2.196,
      "step": 7518
    },
    {
      "epoch": 0.03212781048907424,
      "grad_norm": 3.7731540203094482,
      "learning_rate": 0.00017877285934028372,
      "loss": 1.1976,
      "step": 7519
    },
    {
      "epoch": 0.03213208337250143,
      "grad_norm": 1.3911505937576294,
      "learning_rate": 0.00017873013160143566,
      "loss": 1.0675,
      "step": 7520
    },
    {
      "epoch": 0.03213635625592862,
      "grad_norm": 2.7498090267181396,
      "learning_rate": 0.00017868740386258757,
      "loss": 0.809,
      "step": 7521
    },
    {
      "epoch": 0.03214062913935582,
      "grad_norm": 1.063173770904541,
      "learning_rate": 0.00017864467612373953,
      "loss": 0.6637,
      "step": 7522
    },
    {
      "epoch": 0.032144902022783015,
      "grad_norm": 1.0647640228271484,
      "learning_rate": 0.00017860194838489147,
      "loss": 0.5503,
      "step": 7523
    },
    {
      "epoch": 0.03214917490621021,
      "grad_norm": 0.9345749020576477,
      "learning_rate": 0.0001785592206460434,
      "loss": 0.8404,
      "step": 7524
    },
    {
      "epoch": 0.032153447789637406,
      "grad_norm": 1.2112319469451904,
      "learning_rate": 0.00017851649290719535,
      "loss": 0.5505,
      "step": 7525
    },
    {
      "epoch": 0.0321577206730646,
      "grad_norm": 1.20988929271698,
      "learning_rate": 0.00017847376516834729,
      "loss": 0.5504,
      "step": 7526
    },
    {
      "epoch": 0.03216199355649179,
      "grad_norm": 4.1981658935546875,
      "learning_rate": 0.00017843103742949922,
      "loss": 1.2757,
      "step": 7527
    },
    {
      "epoch": 0.03216626643991899,
      "grad_norm": 1.4649810791015625,
      "learning_rate": 0.00017838830969065116,
      "loss": 0.4517,
      "step": 7528
    },
    {
      "epoch": 0.03217053932334618,
      "grad_norm": 0.9363862872123718,
      "learning_rate": 0.00017834558195180313,
      "loss": 0.8174,
      "step": 7529
    },
    {
      "epoch": 0.03217481220677337,
      "grad_norm": 1.0908582210540771,
      "learning_rate": 0.00017830285421295506,
      "loss": 0.6923,
      "step": 7530
    },
    {
      "epoch": 0.03217908509020057,
      "grad_norm": 0.8865196704864502,
      "learning_rate": 0.000178260126474107,
      "loss": 0.3042,
      "step": 7531
    },
    {
      "epoch": 0.032183357973627764,
      "grad_norm": 1.4597564935684204,
      "learning_rate": 0.00017821739873525894,
      "loss": 0.4312,
      "step": 7532
    },
    {
      "epoch": 0.03218763085705496,
      "grad_norm": 1.1184691190719604,
      "learning_rate": 0.00017817467099641088,
      "loss": 0.5289,
      "step": 7533
    },
    {
      "epoch": 0.03219190374048215,
      "grad_norm": 1.0404599905014038,
      "learning_rate": 0.00017813194325756282,
      "loss": 0.6251,
      "step": 7534
    },
    {
      "epoch": 0.03219617662390935,
      "grad_norm": 1.9438800811767578,
      "learning_rate": 0.00017808921551871475,
      "loss": 0.6213,
      "step": 7535
    },
    {
      "epoch": 0.03220044950733654,
      "grad_norm": 0.30386653542518616,
      "learning_rate": 0.00017804648777986672,
      "loss": 0.0857,
      "step": 7536
    },
    {
      "epoch": 0.03220472239076373,
      "grad_norm": 0.9825403690338135,
      "learning_rate": 0.00017800376004101863,
      "loss": 0.7804,
      "step": 7537
    },
    {
      "epoch": 0.03220899527419093,
      "grad_norm": 0.9390528798103333,
      "learning_rate": 0.00017796103230217057,
      "loss": 0.3047,
      "step": 7538
    },
    {
      "epoch": 0.03221326815761812,
      "grad_norm": 1.2412136793136597,
      "learning_rate": 0.0001779183045633225,
      "loss": 0.5727,
      "step": 7539
    },
    {
      "epoch": 0.032217541041045315,
      "grad_norm": 2.302790641784668,
      "learning_rate": 0.00017787557682447444,
      "loss": 0.6277,
      "step": 7540
    },
    {
      "epoch": 0.032221813924472514,
      "grad_norm": 1.0589011907577515,
      "learning_rate": 0.00017783284908562638,
      "loss": 0.7817,
      "step": 7541
    },
    {
      "epoch": 0.032226086807899706,
      "grad_norm": 2.803870439529419,
      "learning_rate": 0.00017779012134677832,
      "loss": 0.7754,
      "step": 7542
    },
    {
      "epoch": 0.0322303596913269,
      "grad_norm": 1.0094976425170898,
      "learning_rate": 0.00017774739360793026,
      "loss": 0.4919,
      "step": 7543
    },
    {
      "epoch": 0.0322346325747541,
      "grad_norm": 1.3153228759765625,
      "learning_rate": 0.00017770466586908222,
      "loss": 0.5676,
      "step": 7544
    },
    {
      "epoch": 0.03223890545818129,
      "grad_norm": 1.0002306699752808,
      "learning_rate": 0.00017766193813023416,
      "loss": 0.4322,
      "step": 7545
    },
    {
      "epoch": 0.03224317834160848,
      "grad_norm": 3.026660203933716,
      "learning_rate": 0.0001776192103913861,
      "loss": 0.9413,
      "step": 7546
    },
    {
      "epoch": 0.03224745122503568,
      "grad_norm": 1.8832321166992188,
      "learning_rate": 0.00017757648265253804,
      "loss": 0.6908,
      "step": 7547
    },
    {
      "epoch": 0.03225172410846287,
      "grad_norm": 1.126494288444519,
      "learning_rate": 0.00017753375491368997,
      "loss": 0.6851,
      "step": 7548
    },
    {
      "epoch": 0.032255996991890065,
      "grad_norm": 1.852752923965454,
      "learning_rate": 0.0001774910271748419,
      "loss": 0.5305,
      "step": 7549
    },
    {
      "epoch": 0.032260269875317264,
      "grad_norm": 0.7193363904953003,
      "learning_rate": 0.00017744829943599385,
      "loss": 0.2894,
      "step": 7550
    },
    {
      "epoch": 0.032264542758744456,
      "grad_norm": 1.3367869853973389,
      "learning_rate": 0.00017740557169714581,
      "loss": 0.332,
      "step": 7551
    },
    {
      "epoch": 0.03226881564217165,
      "grad_norm": 0.8060067296028137,
      "learning_rate": 0.00017736284395829775,
      "loss": 0.3056,
      "step": 7552
    },
    {
      "epoch": 0.03227308852559885,
      "grad_norm": 3.361339807510376,
      "learning_rate": 0.0001773201162194497,
      "loss": 1.6003,
      "step": 7553
    },
    {
      "epoch": 0.03227736140902604,
      "grad_norm": 1.8106945753097534,
      "learning_rate": 0.0001772773884806016,
      "loss": 0.6351,
      "step": 7554
    },
    {
      "epoch": 0.03228163429245323,
      "grad_norm": 1.0620079040527344,
      "learning_rate": 0.00017723466074175354,
      "loss": 0.4145,
      "step": 7555
    },
    {
      "epoch": 0.03228590717588043,
      "grad_norm": 2.8510382175445557,
      "learning_rate": 0.00017719193300290548,
      "loss": 0.8307,
      "step": 7556
    },
    {
      "epoch": 0.03229018005930762,
      "grad_norm": 1.1769112348556519,
      "learning_rate": 0.00017714920526405741,
      "loss": 0.6809,
      "step": 7557
    },
    {
      "epoch": 0.032294452942734815,
      "grad_norm": 1.2836354970932007,
      "learning_rate": 0.00017710647752520935,
      "loss": 0.7255,
      "step": 7558
    },
    {
      "epoch": 0.03229872582616201,
      "grad_norm": 2.7856013774871826,
      "learning_rate": 0.00017706374978636132,
      "loss": 0.788,
      "step": 7559
    },
    {
      "epoch": 0.032302998709589206,
      "grad_norm": 1.6027977466583252,
      "learning_rate": 0.00017702102204751325,
      "loss": 1.1535,
      "step": 7560
    },
    {
      "epoch": 0.0323072715930164,
      "grad_norm": 1.5967203378677368,
      "learning_rate": 0.0001769782943086652,
      "loss": 1.1182,
      "step": 7561
    },
    {
      "epoch": 0.03231154447644359,
      "grad_norm": 3.678011417388916,
      "learning_rate": 0.00017693556656981713,
      "loss": 1.2345,
      "step": 7562
    },
    {
      "epoch": 0.03231581735987079,
      "grad_norm": 1.3132379055023193,
      "learning_rate": 0.00017689283883096907,
      "loss": 0.335,
      "step": 7563
    },
    {
      "epoch": 0.03232009024329798,
      "grad_norm": 0.6949974298477173,
      "learning_rate": 0.000176850111092121,
      "loss": 0.3082,
      "step": 7564
    },
    {
      "epoch": 0.032324363126725174,
      "grad_norm": 1.1872754096984863,
      "learning_rate": 0.00017680738335327294,
      "loss": 0.6396,
      "step": 7565
    },
    {
      "epoch": 0.03232863601015237,
      "grad_norm": 1.6345787048339844,
      "learning_rate": 0.0001767646556144249,
      "loss": 0.6396,
      "step": 7566
    },
    {
      "epoch": 0.032332908893579565,
      "grad_norm": 1.5420418977737427,
      "learning_rate": 0.00017672192787557685,
      "loss": 0.4605,
      "step": 7567
    },
    {
      "epoch": 0.03233718177700676,
      "grad_norm": 3.3628551959991455,
      "learning_rate": 0.00017667920013672878,
      "loss": 1.1898,
      "step": 7568
    },
    {
      "epoch": 0.032341454660433956,
      "grad_norm": 3.8967442512512207,
      "learning_rate": 0.00017663647239788072,
      "loss": 1.0654,
      "step": 7569
    },
    {
      "epoch": 0.03234572754386115,
      "grad_norm": 1.5578278303146362,
      "learning_rate": 0.00017659374465903263,
      "loss": 0.6202,
      "step": 7570
    },
    {
      "epoch": 0.03235000042728834,
      "grad_norm": 1.6568773984909058,
      "learning_rate": 0.00017655101692018457,
      "loss": 0.6501,
      "step": 7571
    },
    {
      "epoch": 0.03235427331071554,
      "grad_norm": 4.70412540435791,
      "learning_rate": 0.0001765082891813365,
      "loss": 1.5695,
      "step": 7572
    },
    {
      "epoch": 0.03235854619414273,
      "grad_norm": 1.8395065069198608,
      "learning_rate": 0.00017646556144248845,
      "loss": 0.5208,
      "step": 7573
    },
    {
      "epoch": 0.032362819077569924,
      "grad_norm": 2.3509464263916016,
      "learning_rate": 0.0001764228337036404,
      "loss": 0.6536,
      "step": 7574
    },
    {
      "epoch": 0.03236709196099712,
      "grad_norm": 1.565553903579712,
      "learning_rate": 0.00017638010596479235,
      "loss": 0.5502,
      "step": 7575
    },
    {
      "epoch": 0.032371364844424315,
      "grad_norm": 1.3121144771575928,
      "learning_rate": 0.0001763373782259443,
      "loss": 0.4749,
      "step": 7576
    },
    {
      "epoch": 0.03237563772785151,
      "grad_norm": 1.3099111318588257,
      "learning_rate": 0.00017629465048709622,
      "loss": 0.5542,
      "step": 7577
    },
    {
      "epoch": 0.032379910611278706,
      "grad_norm": 1.2493090629577637,
      "learning_rate": 0.00017625192274824816,
      "loss": 0.4165,
      "step": 7578
    },
    {
      "epoch": 0.0323841834947059,
      "grad_norm": 0.5149208307266235,
      "learning_rate": 0.0001762091950094001,
      "loss": 0.2109,
      "step": 7579
    },
    {
      "epoch": 0.03238845637813309,
      "grad_norm": 0.5579809546470642,
      "learning_rate": 0.00017616646727055204,
      "loss": 0.1949,
      "step": 7580
    },
    {
      "epoch": 0.03239272926156029,
      "grad_norm": 2.12622332572937,
      "learning_rate": 0.000176123739531704,
      "loss": 0.6386,
      "step": 7581
    },
    {
      "epoch": 0.03239700214498748,
      "grad_norm": 1.0458505153656006,
      "learning_rate": 0.00017608101179285594,
      "loss": 0.3893,
      "step": 7582
    },
    {
      "epoch": 0.032401275028414674,
      "grad_norm": 1.190408706665039,
      "learning_rate": 0.00017603828405400788,
      "loss": 0.6877,
      "step": 7583
    },
    {
      "epoch": 0.032405547911841866,
      "grad_norm": 1.2953261137008667,
      "learning_rate": 0.00017599555631515982,
      "loss": 0.7224,
      "step": 7584
    },
    {
      "epoch": 0.032409820795269065,
      "grad_norm": 1.301003098487854,
      "learning_rate": 0.00017595282857631175,
      "loss": 0.7224,
      "step": 7585
    },
    {
      "epoch": 0.03241409367869626,
      "grad_norm": 2.4509458541870117,
      "learning_rate": 0.00017591010083746367,
      "loss": 0.7588,
      "step": 7586
    },
    {
      "epoch": 0.03241836656212345,
      "grad_norm": 0.8066877126693726,
      "learning_rate": 0.0001758673730986156,
      "loss": 0.3192,
      "step": 7587
    },
    {
      "epoch": 0.03242263944555065,
      "grad_norm": 1.5687264204025269,
      "learning_rate": 0.00017582464535976754,
      "loss": 0.6298,
      "step": 7588
    },
    {
      "epoch": 0.03242691232897784,
      "grad_norm": 1.589921474456787,
      "learning_rate": 0.0001757819176209195,
      "loss": 0.6396,
      "step": 7589
    },
    {
      "epoch": 0.03243118521240503,
      "grad_norm": 0.9245666265487671,
      "learning_rate": 0.00017573918988207144,
      "loss": 0.3663,
      "step": 7590
    },
    {
      "epoch": 0.03243545809583223,
      "grad_norm": 1.5585628747940063,
      "learning_rate": 0.00017569646214322338,
      "loss": 0.3505,
      "step": 7591
    },
    {
      "epoch": 0.032439730979259424,
      "grad_norm": 1.3435837030410767,
      "learning_rate": 0.00017565373440437532,
      "loss": 0.3387,
      "step": 7592
    },
    {
      "epoch": 0.032444003862686616,
      "grad_norm": 0.9239012598991394,
      "learning_rate": 0.00017561100666552726,
      "loss": 0.3392,
      "step": 7593
    },
    {
      "epoch": 0.032448276746113815,
      "grad_norm": 0.9613307118415833,
      "learning_rate": 0.0001755682789266792,
      "loss": 0.4329,
      "step": 7594
    },
    {
      "epoch": 0.03245254962954101,
      "grad_norm": 0.6683975458145142,
      "learning_rate": 0.00017552555118783113,
      "loss": 0.2885,
      "step": 7595
    },
    {
      "epoch": 0.0324568225129682,
      "grad_norm": 1.4556771516799927,
      "learning_rate": 0.0001754828234489831,
      "loss": 0.55,
      "step": 7596
    },
    {
      "epoch": 0.0324610953963954,
      "grad_norm": 0.7676732540130615,
      "learning_rate": 0.00017544009571013504,
      "loss": 0.2897,
      "step": 7597
    },
    {
      "epoch": 0.03246536827982259,
      "grad_norm": 3.7837018966674805,
      "learning_rate": 0.00017539736797128697,
      "loss": 1.141,
      "step": 7598
    },
    {
      "epoch": 0.03246964116324978,
      "grad_norm": 1.218001365661621,
      "learning_rate": 0.0001753546402324389,
      "loss": 0.6775,
      "step": 7599
    },
    {
      "epoch": 0.03247391404667698,
      "grad_norm": 3.9068167209625244,
      "learning_rate": 0.00017531191249359085,
      "loss": 1.2879,
      "step": 7600
    },
    {
      "epoch": 0.03247818693010417,
      "grad_norm": 3.902777910232544,
      "learning_rate": 0.0001752691847547428,
      "loss": 1.269,
      "step": 7601
    },
    {
      "epoch": 0.032482459813531366,
      "grad_norm": 3.7362842559814453,
      "learning_rate": 0.00017522645701589472,
      "loss": 1.0266,
      "step": 7602
    },
    {
      "epoch": 0.032486732696958565,
      "grad_norm": 1.203927755355835,
      "learning_rate": 0.00017518372927704666,
      "loss": 0.6662,
      "step": 7603
    },
    {
      "epoch": 0.03249100558038576,
      "grad_norm": 1.1283758878707886,
      "learning_rate": 0.0001751410015381986,
      "loss": 0.5879,
      "step": 7604
    },
    {
      "epoch": 0.03249527846381295,
      "grad_norm": 1.4671822786331177,
      "learning_rate": 0.00017509827379935054,
      "loss": 0.5879,
      "step": 7605
    },
    {
      "epoch": 0.03249955134724015,
      "grad_norm": 0.9594087600708008,
      "learning_rate": 0.00017505554606050248,
      "loss": 0.2951,
      "step": 7606
    },
    {
      "epoch": 0.03250382423066734,
      "grad_norm": 1.6485434770584106,
      "learning_rate": 0.00017501281832165441,
      "loss": 1.148,
      "step": 7607
    },
    {
      "epoch": 0.03250809711409453,
      "grad_norm": 0.3904207944869995,
      "learning_rate": 0.00017497009058280635,
      "loss": 0.0963,
      "step": 7608
    },
    {
      "epoch": 0.032512369997521724,
      "grad_norm": 2.8724663257598877,
      "learning_rate": 0.0001749273628439583,
      "loss": 0.9929,
      "step": 7609
    },
    {
      "epoch": 0.03251664288094892,
      "grad_norm": 2.726099967956543,
      "learning_rate": 0.00017488463510511023,
      "loss": 0.6302,
      "step": 7610
    },
    {
      "epoch": 0.032520915764376115,
      "grad_norm": 2.821498155593872,
      "learning_rate": 0.0001748419073662622,
      "loss": 0.7296,
      "step": 7611
    },
    {
      "epoch": 0.03252518864780331,
      "grad_norm": 2.8083336353302,
      "learning_rate": 0.00017479917962741413,
      "loss": 0.7153,
      "step": 7612
    },
    {
      "epoch": 0.03252946153123051,
      "grad_norm": 2.285661220550537,
      "learning_rate": 0.00017475645188856607,
      "loss": 0.7308,
      "step": 7613
    },
    {
      "epoch": 0.0325337344146577,
      "grad_norm": 1.9488310813903809,
      "learning_rate": 0.000174713724149718,
      "loss": 0.5754,
      "step": 7614
    },
    {
      "epoch": 0.03253800729808489,
      "grad_norm": 1.3159297704696655,
      "learning_rate": 0.00017467099641086994,
      "loss": 0.5113,
      "step": 7615
    },
    {
      "epoch": 0.03254228018151209,
      "grad_norm": 3.2330517768859863,
      "learning_rate": 0.00017462826867202188,
      "loss": 0.887,
      "step": 7616
    },
    {
      "epoch": 0.03254655306493928,
      "grad_norm": 2.2464277744293213,
      "learning_rate": 0.00017458554093317382,
      "loss": 0.5494,
      "step": 7617
    },
    {
      "epoch": 0.032550825948366474,
      "grad_norm": 3.4965903759002686,
      "learning_rate": 0.00017454281319432578,
      "loss": 0.7714,
      "step": 7618
    },
    {
      "epoch": 0.03255509883179367,
      "grad_norm": 1.5953187942504883,
      "learning_rate": 0.0001745000854554777,
      "loss": 0.3296,
      "step": 7619
    },
    {
      "epoch": 0.032559371715220865,
      "grad_norm": 1.173227071762085,
      "learning_rate": 0.00017445735771662963,
      "loss": 0.6302,
      "step": 7620
    },
    {
      "epoch": 0.03256364459864806,
      "grad_norm": 0.8548464179039001,
      "learning_rate": 0.00017441462997778157,
      "loss": 0.2823,
      "step": 7621
    },
    {
      "epoch": 0.032567917482075256,
      "grad_norm": 1.119170904159546,
      "learning_rate": 0.0001743719022389335,
      "loss": 0.5864,
      "step": 7622
    },
    {
      "epoch": 0.03257219036550245,
      "grad_norm": 3.0142602920532227,
      "learning_rate": 0.00017432917450008545,
      "loss": 0.7457,
      "step": 7623
    },
    {
      "epoch": 0.03257646324892964,
      "grad_norm": 4.1892900466918945,
      "learning_rate": 0.00017428644676123738,
      "loss": 1.5824,
      "step": 7624
    },
    {
      "epoch": 0.03258073613235684,
      "grad_norm": 1.307539939880371,
      "learning_rate": 0.00017424371902238932,
      "loss": 0.5368,
      "step": 7625
    },
    {
      "epoch": 0.03258500901578403,
      "grad_norm": 0.7483826279640198,
      "learning_rate": 0.0001742009912835413,
      "loss": 0.1956,
      "step": 7626
    },
    {
      "epoch": 0.032589281899211224,
      "grad_norm": 2.9506349563598633,
      "learning_rate": 0.00017415826354469323,
      "loss": 0.9098,
      "step": 7627
    },
    {
      "epoch": 0.03259355478263842,
      "grad_norm": 0.8889668583869934,
      "learning_rate": 0.00017411553580584516,
      "loss": 0.409,
      "step": 7628
    },
    {
      "epoch": 0.032597827666065615,
      "grad_norm": 0.8650642037391663,
      "learning_rate": 0.0001740728080669971,
      "loss": 0.2716,
      "step": 7629
    },
    {
      "epoch": 0.03260210054949281,
      "grad_norm": 4.739242076873779,
      "learning_rate": 0.00017403008032814904,
      "loss": 1.2415,
      "step": 7630
    },
    {
      "epoch": 0.032606373432920006,
      "grad_norm": 0.8221860527992249,
      "learning_rate": 0.00017398735258930098,
      "loss": 0.404,
      "step": 7631
    },
    {
      "epoch": 0.0326106463163472,
      "grad_norm": 3.1287899017333984,
      "learning_rate": 0.00017394462485045291,
      "loss": 1.2445,
      "step": 7632
    },
    {
      "epoch": 0.03261491919977439,
      "grad_norm": 1.3396854400634766,
      "learning_rate": 0.00017390189711160488,
      "loss": 0.5418,
      "step": 7633
    },
    {
      "epoch": 0.03261919208320158,
      "grad_norm": 5.466536045074463,
      "learning_rate": 0.00017385916937275682,
      "loss": 3.4167,
      "step": 7634
    },
    {
      "epoch": 0.03262346496662878,
      "grad_norm": 1.3330076932907104,
      "learning_rate": 0.00017381644163390876,
      "loss": 0.5421,
      "step": 7635
    },
    {
      "epoch": 0.032627737850055974,
      "grad_norm": 0.6543543338775635,
      "learning_rate": 0.00017377371389506067,
      "loss": 0.2696,
      "step": 7636
    },
    {
      "epoch": 0.032632010733483166,
      "grad_norm": 0.7974420785903931,
      "learning_rate": 0.0001737309861562126,
      "loss": 0.3832,
      "step": 7637
    },
    {
      "epoch": 0.032636283616910365,
      "grad_norm": 0.7718676328659058,
      "learning_rate": 0.00017368825841736454,
      "loss": 0.2526,
      "step": 7638
    },
    {
      "epoch": 0.03264055650033756,
      "grad_norm": 3.3970072269439697,
      "learning_rate": 0.00017364553067851648,
      "loss": 1.9177,
      "step": 7639
    },
    {
      "epoch": 0.03264482938376475,
      "grad_norm": 4.076354026794434,
      "learning_rate": 0.00017360280293966842,
      "loss": 1.0189,
      "step": 7640
    },
    {
      "epoch": 0.03264910226719195,
      "grad_norm": 4.0519328117370605,
      "learning_rate": 0.00017356007520082038,
      "loss": 1.2952,
      "step": 7641
    },
    {
      "epoch": 0.03265337515061914,
      "grad_norm": 1.478068232536316,
      "learning_rate": 0.00017351734746197232,
      "loss": 0.361,
      "step": 7642
    },
    {
      "epoch": 0.03265764803404633,
      "grad_norm": 0.6131192445755005,
      "learning_rate": 0.00017347461972312426,
      "loss": 0.2422,
      "step": 7643
    },
    {
      "epoch": 0.03266192091747353,
      "grad_norm": 1.3058887720108032,
      "learning_rate": 0.0001734318919842762,
      "loss": 0.5514,
      "step": 7644
    },
    {
      "epoch": 0.032666193800900724,
      "grad_norm": 2.0512430667877197,
      "learning_rate": 0.00017338916424542813,
      "loss": 0.5875,
      "step": 7645
    },
    {
      "epoch": 0.032670466684327916,
      "grad_norm": 2.8932275772094727,
      "learning_rate": 0.00017334643650658007,
      "loss": 0.771,
      "step": 7646
    },
    {
      "epoch": 0.032674739567755115,
      "grad_norm": 2.24338698387146,
      "learning_rate": 0.000173303708767732,
      "loss": 0.6403,
      "step": 7647
    },
    {
      "epoch": 0.03267901245118231,
      "grad_norm": 1.2371962070465088,
      "learning_rate": 0.00017326098102888397,
      "loss": 0.5488,
      "step": 7648
    },
    {
      "epoch": 0.0326832853346095,
      "grad_norm": 1.0576344728469849,
      "learning_rate": 0.0001732182532900359,
      "loss": 0.5329,
      "step": 7649
    },
    {
      "epoch": 0.0326875582180367,
      "grad_norm": 1.143449068069458,
      "learning_rate": 0.00017317552555118785,
      "loss": 0.6183,
      "step": 7650
    },
    {
      "epoch": 0.03269183110146389,
      "grad_norm": 0.5356285572052002,
      "learning_rate": 0.0001731327978123398,
      "loss": 0.2105,
      "step": 7651
    },
    {
      "epoch": 0.03269610398489108,
      "grad_norm": 1.064360499382019,
      "learning_rate": 0.0001730900700734917,
      "loss": 0.5329,
      "step": 7652
    },
    {
      "epoch": 0.03270037686831828,
      "grad_norm": 1.7445350885391235,
      "learning_rate": 0.00017304734233464364,
      "loss": 1.2205,
      "step": 7653
    },
    {
      "epoch": 0.032704649751745474,
      "grad_norm": 1.2698696851730347,
      "learning_rate": 0.00017300461459579557,
      "loss": 0.533,
      "step": 7654
    },
    {
      "epoch": 0.032708922635172666,
      "grad_norm": 1.396433711051941,
      "learning_rate": 0.00017296188685694754,
      "loss": 0.5076,
      "step": 7655
    },
    {
      "epoch": 0.032713195518599865,
      "grad_norm": 3.3408775329589844,
      "learning_rate": 0.00017291915911809948,
      "loss": 0.6874,
      "step": 7656
    },
    {
      "epoch": 0.03271746840202706,
      "grad_norm": 0.7693222165107727,
      "learning_rate": 0.00017287643137925141,
      "loss": 0.2288,
      "step": 7657
    },
    {
      "epoch": 0.03272174128545425,
      "grad_norm": 2.330901861190796,
      "learning_rate": 0.00017283370364040335,
      "loss": 0.9126,
      "step": 7658
    },
    {
      "epoch": 0.03272601416888144,
      "grad_norm": 3.049166202545166,
      "learning_rate": 0.0001727909759015553,
      "loss": 1.1016,
      "step": 7659
    },
    {
      "epoch": 0.03273028705230864,
      "grad_norm": 0.7615450024604797,
      "learning_rate": 0.00017274824816270723,
      "loss": 0.3615,
      "step": 7660
    },
    {
      "epoch": 0.03273455993573583,
      "grad_norm": 0.5174477696418762,
      "learning_rate": 0.00017270552042385917,
      "loss": 0.1869,
      "step": 7661
    },
    {
      "epoch": 0.032738832819163025,
      "grad_norm": 1.3218339681625366,
      "learning_rate": 0.0001726627926850111,
      "loss": 0.4878,
      "step": 7662
    },
    {
      "epoch": 0.032743105702590224,
      "grad_norm": 1.2234550714492798,
      "learning_rate": 0.00017262006494616307,
      "loss": 0.5152,
      "step": 7663
    },
    {
      "epoch": 0.032747378586017416,
      "grad_norm": 2.032147169113159,
      "learning_rate": 0.000172577337207315,
      "loss": 0.7351,
      "step": 7664
    },
    {
      "epoch": 0.03275165146944461,
      "grad_norm": 1.063696265220642,
      "learning_rate": 0.00017253460946846694,
      "loss": 0.5044,
      "step": 7665
    },
    {
      "epoch": 0.03275592435287181,
      "grad_norm": 3.430908203125,
      "learning_rate": 0.00017249188172961888,
      "loss": 1.1775,
      "step": 7666
    },
    {
      "epoch": 0.032760197236299,
      "grad_norm": 2.938385486602783,
      "learning_rate": 0.00017244915399077082,
      "loss": 0.7646,
      "step": 7667
    },
    {
      "epoch": 0.03276447011972619,
      "grad_norm": 1.2832928895950317,
      "learning_rate": 0.00017240642625192276,
      "loss": 0.8894,
      "step": 7668
    },
    {
      "epoch": 0.03276874300315339,
      "grad_norm": 1.301652193069458,
      "learning_rate": 0.00017236369851307467,
      "loss": 0.4738,
      "step": 7669
    },
    {
      "epoch": 0.03277301588658058,
      "grad_norm": 3.0136070251464844,
      "learning_rate": 0.00017232097077422663,
      "loss": 0.6381,
      "step": 7670
    },
    {
      "epoch": 0.032777288770007774,
      "grad_norm": 3.7288575172424316,
      "learning_rate": 0.00017227824303537857,
      "loss": 1.307,
      "step": 7671
    },
    {
      "epoch": 0.032781561653434974,
      "grad_norm": 1.2206463813781738,
      "learning_rate": 0.0001722355152965305,
      "loss": 0.5144,
      "step": 7672
    },
    {
      "epoch": 0.032785834536862166,
      "grad_norm": 4.118960857391357,
      "learning_rate": 0.00017219278755768245,
      "loss": 2.7644,
      "step": 7673
    },
    {
      "epoch": 0.03279010742028936,
      "grad_norm": 5.2044453620910645,
      "learning_rate": 0.00017215005981883439,
      "loss": 1.313,
      "step": 7674
    },
    {
      "epoch": 0.03279438030371656,
      "grad_norm": 3.0702526569366455,
      "learning_rate": 0.00017210733207998632,
      "loss": 0.841,
      "step": 7675
    },
    {
      "epoch": 0.03279865318714375,
      "grad_norm": 2.0586798191070557,
      "learning_rate": 0.00017206460434113826,
      "loss": 0.7571,
      "step": 7676
    },
    {
      "epoch": 0.03280292607057094,
      "grad_norm": 2.3257029056549072,
      "learning_rate": 0.0001720218766022902,
      "loss": 0.6892,
      "step": 7677
    },
    {
      "epoch": 0.03280719895399814,
      "grad_norm": 0.7539405822753906,
      "learning_rate": 0.00017197914886344216,
      "loss": 0.2532,
      "step": 7678
    },
    {
      "epoch": 0.03281147183742533,
      "grad_norm": 1.236608862876892,
      "learning_rate": 0.0001719364211245941,
      "loss": 0.8921,
      "step": 7679
    },
    {
      "epoch": 0.032815744720852524,
      "grad_norm": 1.1384403705596924,
      "learning_rate": 0.00017189369338574604,
      "loss": 0.5884,
      "step": 7680
    },
    {
      "epoch": 0.03282001760427972,
      "grad_norm": 0.9657830595970154,
      "learning_rate": 0.00017185096564689798,
      "loss": 0.3394,
      "step": 7681
    },
    {
      "epoch": 0.032824290487706916,
      "grad_norm": 0.915050745010376,
      "learning_rate": 0.00017180823790804991,
      "loss": 0.2939,
      "step": 7682
    },
    {
      "epoch": 0.03282856337113411,
      "grad_norm": 4.667886257171631,
      "learning_rate": 0.00017176551016920185,
      "loss": 1.305,
      "step": 7683
    },
    {
      "epoch": 0.0328328362545613,
      "grad_norm": 2.107470750808716,
      "learning_rate": 0.0001717227824303538,
      "loss": 0.6466,
      "step": 7684
    },
    {
      "epoch": 0.0328371091379885,
      "grad_norm": 2.7732980251312256,
      "learning_rate": 0.00017168005469150573,
      "loss": 0.6433,
      "step": 7685
    },
    {
      "epoch": 0.03284138202141569,
      "grad_norm": 1.2058089971542358,
      "learning_rate": 0.00017163732695265767,
      "loss": 0.5371,
      "step": 7686
    },
    {
      "epoch": 0.03284565490484288,
      "grad_norm": 4.663760662078857,
      "learning_rate": 0.0001715945992138096,
      "loss": 2.4121,
      "step": 7687
    },
    {
      "epoch": 0.03284992778827008,
      "grad_norm": 3.1213271617889404,
      "learning_rate": 0.00017155187147496154,
      "loss": 0.9937,
      "step": 7688
    },
    {
      "epoch": 0.032854200671697274,
      "grad_norm": 2.5727455615997314,
      "learning_rate": 0.00017150914373611348,
      "loss": 0.6822,
      "step": 7689
    },
    {
      "epoch": 0.032858473555124466,
      "grad_norm": 2.075259208679199,
      "learning_rate": 0.00017146641599726542,
      "loss": 0.6131,
      "step": 7690
    },
    {
      "epoch": 0.032862746438551665,
      "grad_norm": 0.7504832148551941,
      "learning_rate": 0.00017142368825841736,
      "loss": 0.2729,
      "step": 7691
    },
    {
      "epoch": 0.03286701932197886,
      "grad_norm": 1.1976613998413086,
      "learning_rate": 0.0001713809605195693,
      "loss": 0.5262,
      "step": 7692
    },
    {
      "epoch": 0.03287129220540605,
      "grad_norm": 3.5111136436462402,
      "learning_rate": 0.00017133823278072126,
      "loss": 1.071,
      "step": 7693
    },
    {
      "epoch": 0.03287556508883325,
      "grad_norm": 0.9248823523521423,
      "learning_rate": 0.0001712955050418732,
      "loss": 0.3712,
      "step": 7694
    },
    {
      "epoch": 0.03287983797226044,
      "grad_norm": 0.9302721619606018,
      "learning_rate": 0.00017125277730302513,
      "loss": 0.4395,
      "step": 7695
    },
    {
      "epoch": 0.03288411085568763,
      "grad_norm": 4.716350078582764,
      "learning_rate": 0.00017121004956417707,
      "loss": 2.373,
      "step": 7696
    },
    {
      "epoch": 0.03288838373911483,
      "grad_norm": 1.1020318269729614,
      "learning_rate": 0.000171167321825329,
      "loss": 0.5637,
      "step": 7697
    },
    {
      "epoch": 0.032892656622542024,
      "grad_norm": 1.089784026145935,
      "learning_rate": 0.00017112459408648095,
      "loss": 0.4527,
      "step": 7698
    },
    {
      "epoch": 0.032896929505969216,
      "grad_norm": 1.153732419013977,
      "learning_rate": 0.00017108186634763289,
      "loss": 0.4877,
      "step": 7699
    },
    {
      "epoch": 0.032901202389396415,
      "grad_norm": 2.5941765308380127,
      "learning_rate": 0.00017103913860878485,
      "loss": 0.6545,
      "step": 7700
    },
    {
      "epoch": 0.03290547527282361,
      "grad_norm": 1.185742735862732,
      "learning_rate": 0.00017099641086993676,
      "loss": 0.4994,
      "step": 7701
    },
    {
      "epoch": 0.0329097481562508,
      "grad_norm": 0.5976876616477966,
      "learning_rate": 0.0001709536831310887,
      "loss": 0.1492,
      "step": 7702
    },
    {
      "epoch": 0.032914021039678,
      "grad_norm": 1.2031396627426147,
      "learning_rate": 0.00017091095539224064,
      "loss": 0.4596,
      "step": 7703
    },
    {
      "epoch": 0.03291829392310519,
      "grad_norm": 1.100563406944275,
      "learning_rate": 0.00017086822765339257,
      "loss": 0.5507,
      "step": 7704
    },
    {
      "epoch": 0.03292256680653238,
      "grad_norm": 1.0962506532669067,
      "learning_rate": 0.0001708254999145445,
      "loss": 0.5434,
      "step": 7705
    },
    {
      "epoch": 0.03292683968995958,
      "grad_norm": 0.821278989315033,
      "learning_rate": 0.00017078277217569645,
      "loss": 0.2659,
      "step": 7706
    },
    {
      "epoch": 0.032931112573386774,
      "grad_norm": 1.1021543741226196,
      "learning_rate": 0.00017074004443684842,
      "loss": 0.5453,
      "step": 7707
    },
    {
      "epoch": 0.032935385456813966,
      "grad_norm": 1.8025869131088257,
      "learning_rate": 0.00017069731669800035,
      "loss": 0.4705,
      "step": 7708
    },
    {
      "epoch": 0.03293965834024116,
      "grad_norm": 0.35686203837394714,
      "learning_rate": 0.0001706545889591523,
      "loss": 0.1205,
      "step": 7709
    },
    {
      "epoch": 0.03294393122366836,
      "grad_norm": 0.8955649137496948,
      "learning_rate": 0.00017061186122030423,
      "loss": 0.3363,
      "step": 7710
    },
    {
      "epoch": 0.03294820410709555,
      "grad_norm": 2.5060033798217773,
      "learning_rate": 0.00017056913348145617,
      "loss": 1.2677,
      "step": 7711
    },
    {
      "epoch": 0.03295247699052274,
      "grad_norm": 4.592247486114502,
      "learning_rate": 0.0001705264057426081,
      "loss": 1.3046,
      "step": 7712
    },
    {
      "epoch": 0.03295674987394994,
      "grad_norm": 2.6576757431030273,
      "learning_rate": 0.00017048367800376004,
      "loss": 0.5103,
      "step": 7713
    },
    {
      "epoch": 0.03296102275737713,
      "grad_norm": 4.151155948638916,
      "learning_rate": 0.00017044095026491198,
      "loss": 0.9316,
      "step": 7714
    },
    {
      "epoch": 0.032965295640804325,
      "grad_norm": 0.9922583103179932,
      "learning_rate": 0.00017039822252606395,
      "loss": 0.2831,
      "step": 7715
    },
    {
      "epoch": 0.032969568524231524,
      "grad_norm": 1.1292439699172974,
      "learning_rate": 0.00017035549478721588,
      "loss": 0.5515,
      "step": 7716
    },
    {
      "epoch": 0.032973841407658716,
      "grad_norm": 2.6374120712280273,
      "learning_rate": 0.00017031276704836782,
      "loss": 0.4841,
      "step": 7717
    },
    {
      "epoch": 0.03297811429108591,
      "grad_norm": 1.1707218885421753,
      "learning_rate": 0.00017027003930951973,
      "loss": 0.3056,
      "step": 7718
    },
    {
      "epoch": 0.03298238717451311,
      "grad_norm": 1.790238857269287,
      "learning_rate": 0.00017022731157067167,
      "loss": 0.6278,
      "step": 7719
    },
    {
      "epoch": 0.0329866600579403,
      "grad_norm": 1.0867538452148438,
      "learning_rate": 0.0001701845838318236,
      "loss": 0.2936,
      "step": 7720
    },
    {
      "epoch": 0.03299093294136749,
      "grad_norm": 2.095615863800049,
      "learning_rate": 0.00017014185609297554,
      "loss": 0.6517,
      "step": 7721
    },
    {
      "epoch": 0.03299520582479469,
      "grad_norm": 2.5172007083892822,
      "learning_rate": 0.0001700991283541275,
      "loss": 0.5362,
      "step": 7722
    },
    {
      "epoch": 0.03299947870822188,
      "grad_norm": 1.4320247173309326,
      "learning_rate": 0.00017005640061527945,
      "loss": 0.417,
      "step": 7723
    },
    {
      "epoch": 0.033003751591649075,
      "grad_norm": 1.1782042980194092,
      "learning_rate": 0.00017001367287643139,
      "loss": 0.4837,
      "step": 7724
    },
    {
      "epoch": 0.033008024475076274,
      "grad_norm": 1.1824270486831665,
      "learning_rate": 0.00016997094513758332,
      "loss": 0.3393,
      "step": 7725
    },
    {
      "epoch": 0.033012297358503466,
      "grad_norm": 0.8044628500938416,
      "learning_rate": 0.00016992821739873526,
      "loss": 0.1714,
      "step": 7726
    },
    {
      "epoch": 0.03301657024193066,
      "grad_norm": 2.2885305881500244,
      "learning_rate": 0.0001698854896598872,
      "loss": 1.3678,
      "step": 7727
    },
    {
      "epoch": 0.03302084312535786,
      "grad_norm": 0.8995611071586609,
      "learning_rate": 0.00016984276192103914,
      "loss": 0.3823,
      "step": 7728
    },
    {
      "epoch": 0.03302511600878505,
      "grad_norm": 1.6803985834121704,
      "learning_rate": 0.00016980003418219107,
      "loss": 0.9864,
      "step": 7729
    },
    {
      "epoch": 0.03302938889221224,
      "grad_norm": 4.8854169845581055,
      "learning_rate": 0.00016975730644334304,
      "loss": 1.2238,
      "step": 7730
    },
    {
      "epoch": 0.03303366177563944,
      "grad_norm": 3.154193878173828,
      "learning_rate": 0.00016971457870449498,
      "loss": 0.6249,
      "step": 7731
    },
    {
      "epoch": 0.03303793465906663,
      "grad_norm": 3.5735745429992676,
      "learning_rate": 0.00016967185096564692,
      "loss": 1.1034,
      "step": 7732
    },
    {
      "epoch": 0.033042207542493825,
      "grad_norm": 3.7890799045562744,
      "learning_rate": 0.00016962912322679885,
      "loss": 1.9857,
      "step": 7733
    },
    {
      "epoch": 0.03304648042592102,
      "grad_norm": 4.545900821685791,
      "learning_rate": 0.00016958639548795076,
      "loss": 1.367,
      "step": 7734
    },
    {
      "epoch": 0.033050753309348216,
      "grad_norm": 0.817603349685669,
      "learning_rate": 0.0001695436677491027,
      "loss": 0.2327,
      "step": 7735
    },
    {
      "epoch": 0.03305502619277541,
      "grad_norm": 1.974944829940796,
      "learning_rate": 0.00016950094001025464,
      "loss": 0.5661,
      "step": 7736
    },
    {
      "epoch": 0.0330592990762026,
      "grad_norm": 2.489649772644043,
      "learning_rate": 0.0001694582122714066,
      "loss": 0.6662,
      "step": 7737
    },
    {
      "epoch": 0.0330635719596298,
      "grad_norm": 3.772670269012451,
      "learning_rate": 0.00016941548453255854,
      "loss": 1.303,
      "step": 7738
    },
    {
      "epoch": 0.03306784484305699,
      "grad_norm": 2.1492505073547363,
      "learning_rate": 0.00016937275679371048,
      "loss": 0.5707,
      "step": 7739
    },
    {
      "epoch": 0.03307211772648418,
      "grad_norm": 2.4813549518585205,
      "learning_rate": 0.00016933002905486242,
      "loss": 0.399,
      "step": 7740
    },
    {
      "epoch": 0.03307639060991138,
      "grad_norm": 1.719179391860962,
      "learning_rate": 0.00016928730131601436,
      "loss": 0.581,
      "step": 7741
    },
    {
      "epoch": 0.033080663493338575,
      "grad_norm": 4.065046310424805,
      "learning_rate": 0.0001692445735771663,
      "loss": 1.4582,
      "step": 7742
    },
    {
      "epoch": 0.03308493637676577,
      "grad_norm": 3.378572702407837,
      "learning_rate": 0.00016920184583831823,
      "loss": 0.9197,
      "step": 7743
    },
    {
      "epoch": 0.033089209260192966,
      "grad_norm": 1.1477980613708496,
      "learning_rate": 0.00016915911809947017,
      "loss": 0.494,
      "step": 7744
    },
    {
      "epoch": 0.03309348214362016,
      "grad_norm": 1.568131685256958,
      "learning_rate": 0.00016911639036062213,
      "loss": 0.5492,
      "step": 7745
    },
    {
      "epoch": 0.03309775502704735,
      "grad_norm": 1.1933057308197021,
      "learning_rate": 0.00016907366262177407,
      "loss": 0.5164,
      "step": 7746
    },
    {
      "epoch": 0.03310202791047455,
      "grad_norm": 1.1227655410766602,
      "learning_rate": 0.000169030934882926,
      "loss": 0.4808,
      "step": 7747
    },
    {
      "epoch": 0.03310630079390174,
      "grad_norm": 0.6281242966651917,
      "learning_rate": 0.00016898820714407795,
      "loss": 0.1851,
      "step": 7748
    },
    {
      "epoch": 0.03311057367732893,
      "grad_norm": 2.792170524597168,
      "learning_rate": 0.00016894547940522989,
      "loss": 1.1236,
      "step": 7749
    },
    {
      "epoch": 0.03311484656075613,
      "grad_norm": 1.1754266023635864,
      "learning_rate": 0.00016890275166638182,
      "loss": 0.4983,
      "step": 7750
    },
    {
      "epoch": 0.033119119444183324,
      "grad_norm": 0.6471129059791565,
      "learning_rate": 0.00016886002392753373,
      "loss": 0.1849,
      "step": 7751
    },
    {
      "epoch": 0.03312339232761052,
      "grad_norm": 1.0860539674758911,
      "learning_rate": 0.0001688172961886857,
      "loss": 0.564,
      "step": 7752
    },
    {
      "epoch": 0.033127665211037716,
      "grad_norm": 1.1894800662994385,
      "learning_rate": 0.00016877456844983764,
      "loss": 0.4927,
      "step": 7753
    },
    {
      "epoch": 0.03313193809446491,
      "grad_norm": 2.0465426445007324,
      "learning_rate": 0.00016873184071098958,
      "loss": 0.7538,
      "step": 7754
    },
    {
      "epoch": 0.0331362109778921,
      "grad_norm": 3.3336875438690186,
      "learning_rate": 0.0001686891129721415,
      "loss": 0.8826,
      "step": 7755
    },
    {
      "epoch": 0.03314048386131929,
      "grad_norm": 2.3326961994171143,
      "learning_rate": 0.00016864638523329345,
      "loss": 0.6429,
      "step": 7756
    },
    {
      "epoch": 0.03314475674474649,
      "grad_norm": 2.9152867794036865,
      "learning_rate": 0.0001686036574944454,
      "loss": 1.0889,
      "step": 7757
    },
    {
      "epoch": 0.03314902962817368,
      "grad_norm": 1.0114730596542358,
      "learning_rate": 0.00016856092975559733,
      "loss": 0.4826,
      "step": 7758
    },
    {
      "epoch": 0.033153302511600875,
      "grad_norm": 1.016324520111084,
      "learning_rate": 0.0001685182020167493,
      "loss": 0.4826,
      "step": 7759
    },
    {
      "epoch": 0.033157575395028074,
      "grad_norm": 1.4550013542175293,
      "learning_rate": 0.00016847547427790123,
      "loss": 0.3927,
      "step": 7760
    },
    {
      "epoch": 0.033161848278455267,
      "grad_norm": 2.4088082313537598,
      "learning_rate": 0.00016843274653905317,
      "loss": 0.3711,
      "step": 7761
    },
    {
      "epoch": 0.03316612116188246,
      "grad_norm": 1.8930257558822632,
      "learning_rate": 0.0001683900188002051,
      "loss": 0.5237,
      "step": 7762
    },
    {
      "epoch": 0.03317039404530966,
      "grad_norm": 2.233743667602539,
      "learning_rate": 0.00016834729106135704,
      "loss": 0.6818,
      "step": 7763
    },
    {
      "epoch": 0.03317466692873685,
      "grad_norm": 1.1216381788253784,
      "learning_rate": 0.00016830456332250898,
      "loss": 0.5336,
      "step": 7764
    },
    {
      "epoch": 0.03317893981216404,
      "grad_norm": 2.3250794410705566,
      "learning_rate": 0.00016826183558366092,
      "loss": 0.5701,
      "step": 7765
    },
    {
      "epoch": 0.03318321269559124,
      "grad_norm": 1.5792313814163208,
      "learning_rate": 0.00016821910784481286,
      "loss": 0.5026,
      "step": 7766
    },
    {
      "epoch": 0.03318748557901843,
      "grad_norm": 1.202211856842041,
      "learning_rate": 0.0001681763801059648,
      "loss": 0.4809,
      "step": 7767
    },
    {
      "epoch": 0.033191758462445625,
      "grad_norm": 1.566177248954773,
      "learning_rate": 0.00016813365236711673,
      "loss": 0.5026,
      "step": 7768
    },
    {
      "epoch": 0.033196031345872824,
      "grad_norm": 4.399002552032471,
      "learning_rate": 0.00016809092462826867,
      "loss": 1.2204,
      "step": 7769
    },
    {
      "epoch": 0.033200304229300016,
      "grad_norm": 1.0628416538238525,
      "learning_rate": 0.0001680481968894206,
      "loss": 0.4627,
      "step": 7770
    },
    {
      "epoch": 0.03320457711272721,
      "grad_norm": 4.147861480712891,
      "learning_rate": 0.00016800546915057255,
      "loss": 1.1981,
      "step": 7771
    },
    {
      "epoch": 0.03320884999615441,
      "grad_norm": 3.7802391052246094,
      "learning_rate": 0.00016796274141172448,
      "loss": 1.1972,
      "step": 7772
    },
    {
      "epoch": 0.0332131228795816,
      "grad_norm": 3.964946746826172,
      "learning_rate": 0.00016792001367287642,
      "loss": 2.1415,
      "step": 7773
    },
    {
      "epoch": 0.03321739576300879,
      "grad_norm": 1.3306806087493896,
      "learning_rate": 0.00016787728593402839,
      "loss": 0.3822,
      "step": 7774
    },
    {
      "epoch": 0.03322166864643599,
      "grad_norm": 1.5359973907470703,
      "learning_rate": 0.00016783455819518032,
      "loss": 0.5058,
      "step": 7775
    },
    {
      "epoch": 0.03322594152986318,
      "grad_norm": 0.4069438576698303,
      "learning_rate": 0.00016779183045633226,
      "loss": 0.1195,
      "step": 7776
    },
    {
      "epoch": 0.033230214413290375,
      "grad_norm": 0.4138050675392151,
      "learning_rate": 0.0001677491027174842,
      "loss": 0.1195,
      "step": 7777
    },
    {
      "epoch": 0.033234487296717574,
      "grad_norm": 0.9674986004829407,
      "learning_rate": 0.00016770637497863614,
      "loss": 0.4261,
      "step": 7778
    },
    {
      "epoch": 0.033238760180144766,
      "grad_norm": 1.7069810628890991,
      "learning_rate": 0.00016766364723978808,
      "loss": 0.4548,
      "step": 7779
    },
    {
      "epoch": 0.03324303306357196,
      "grad_norm": 3.312798023223877,
      "learning_rate": 0.00016762091950094,
      "loss": 0.7093,
      "step": 7780
    },
    {
      "epoch": 0.03324730594699915,
      "grad_norm": 2.4459152221679688,
      "learning_rate": 0.00016757819176209195,
      "loss": 0.7726,
      "step": 7781
    },
    {
      "epoch": 0.03325157883042635,
      "grad_norm": 0.6073802709579468,
      "learning_rate": 0.00016753546402324392,
      "loss": 0.1871,
      "step": 7782
    },
    {
      "epoch": 0.03325585171385354,
      "grad_norm": 1.2246921062469482,
      "learning_rate": 0.00016749273628439585,
      "loss": 0.4516,
      "step": 7783
    },
    {
      "epoch": 0.033260124597280734,
      "grad_norm": 0.7284392714500427,
      "learning_rate": 0.00016745000854554776,
      "loss": 0.3076,
      "step": 7784
    },
    {
      "epoch": 0.03326439748070793,
      "grad_norm": 2.4626834392547607,
      "learning_rate": 0.0001674072808066997,
      "loss": 1.455,
      "step": 7785
    },
    {
      "epoch": 0.033268670364135125,
      "grad_norm": 4.983359336853027,
      "learning_rate": 0.00016736455306785164,
      "loss": 2.7675,
      "step": 7786
    },
    {
      "epoch": 0.03327294324756232,
      "grad_norm": 1.5264201164245605,
      "learning_rate": 0.00016732182532900358,
      "loss": 0.6011,
      "step": 7787
    },
    {
      "epoch": 0.033277216130989516,
      "grad_norm": 2.685370922088623,
      "learning_rate": 0.00016727909759015552,
      "loss": 0.7448,
      "step": 7788
    },
    {
      "epoch": 0.03328148901441671,
      "grad_norm": 4.748572826385498,
      "learning_rate": 0.00016723636985130748,
      "loss": 1.0684,
      "step": 7789
    },
    {
      "epoch": 0.0332857618978439,
      "grad_norm": 2.160280227661133,
      "learning_rate": 0.00016719364211245942,
      "loss": 0.4386,
      "step": 7790
    },
    {
      "epoch": 0.0332900347812711,
      "grad_norm": 1.0944783687591553,
      "learning_rate": 0.00016715091437361136,
      "loss": 0.4684,
      "step": 7791
    },
    {
      "epoch": 0.03329430766469829,
      "grad_norm": 0.8340675234794617,
      "learning_rate": 0.0001671081866347633,
      "loss": 0.3148,
      "step": 7792
    },
    {
      "epoch": 0.033298580548125484,
      "grad_norm": 0.965938150882721,
      "learning_rate": 0.00016706545889591523,
      "loss": 0.2192,
      "step": 7793
    },
    {
      "epoch": 0.03330285343155268,
      "grad_norm": 2.5716147422790527,
      "learning_rate": 0.00016702273115706717,
      "loss": 0.9529,
      "step": 7794
    },
    {
      "epoch": 0.033307126314979875,
      "grad_norm": 1.185654878616333,
      "learning_rate": 0.0001669800034182191,
      "loss": 0.3321,
      "step": 7795
    },
    {
      "epoch": 0.03331139919840707,
      "grad_norm": 1.4498332738876343,
      "learning_rate": 0.00016693727567937105,
      "loss": 0.3599,
      "step": 7796
    },
    {
      "epoch": 0.033315672081834266,
      "grad_norm": 5.095311164855957,
      "learning_rate": 0.000166894547940523,
      "loss": 1.2022,
      "step": 7797
    },
    {
      "epoch": 0.03331994496526146,
      "grad_norm": 0.49793893098831177,
      "learning_rate": 0.00016685182020167495,
      "loss": 0.1144,
      "step": 7798
    },
    {
      "epoch": 0.03332421784868865,
      "grad_norm": 4.092556953430176,
      "learning_rate": 0.00016680909246282689,
      "loss": 0.9893,
      "step": 7799
    },
    {
      "epoch": 0.03332849073211585,
      "grad_norm": 2.0893471240997314,
      "learning_rate": 0.0001667663647239788,
      "loss": 0.6444,
      "step": 7800
    },
    {
      "epoch": 0.03333276361554304,
      "grad_norm": 1.1626975536346436,
      "learning_rate": 0.00016672363698513073,
      "loss": 0.4889,
      "step": 7801
    },
    {
      "epoch": 0.033337036498970234,
      "grad_norm": 1.9001981019973755,
      "learning_rate": 0.00016668090924628267,
      "loss": 0.4825,
      "step": 7802
    },
    {
      "epoch": 0.03334130938239743,
      "grad_norm": 0.9753372669219971,
      "learning_rate": 0.0001666381815074346,
      "loss": 0.3266,
      "step": 7803
    },
    {
      "epoch": 0.033345582265824625,
      "grad_norm": 2.629455804824829,
      "learning_rate": 0.00016659545376858658,
      "loss": 0.6612,
      "step": 7804
    },
    {
      "epoch": 0.03334985514925182,
      "grad_norm": 2.678976535797119,
      "learning_rate": 0.0001665527260297385,
      "loss": 0.9589,
      "step": 7805
    },
    {
      "epoch": 0.03335412803267901,
      "grad_norm": 1.0445233583450317,
      "learning_rate": 0.00016650999829089045,
      "loss": 0.4516,
      "step": 7806
    },
    {
      "epoch": 0.03335840091610621,
      "grad_norm": 0.31275856494903564,
      "learning_rate": 0.0001664672705520424,
      "loss": 0.0831,
      "step": 7807
    },
    {
      "epoch": 0.0333626737995334,
      "grad_norm": 3.565981388092041,
      "learning_rate": 0.00016642454281319433,
      "loss": 1.8691,
      "step": 7808
    },
    {
      "epoch": 0.03336694668296059,
      "grad_norm": 1.5085011720657349,
      "learning_rate": 0.00016638181507434626,
      "loss": 0.5024,
      "step": 7809
    },
    {
      "epoch": 0.03337121956638779,
      "grad_norm": 2.09067964553833,
      "learning_rate": 0.0001663390873354982,
      "loss": 1.3651,
      "step": 7810
    },
    {
      "epoch": 0.033375492449814984,
      "grad_norm": 2.567389488220215,
      "learning_rate": 0.00016629635959665017,
      "loss": 0.8908,
      "step": 7811
    },
    {
      "epoch": 0.033379765333242176,
      "grad_norm": 3.0999066829681396,
      "learning_rate": 0.0001662536318578021,
      "loss": 0.742,
      "step": 7812
    },
    {
      "epoch": 0.033384038216669375,
      "grad_norm": 2.030841112136841,
      "learning_rate": 0.00016621090411895404,
      "loss": 1.3076,
      "step": 7813
    },
    {
      "epoch": 0.03338831110009657,
      "grad_norm": 2.3567020893096924,
      "learning_rate": 0.00016616817638010598,
      "loss": 0.678,
      "step": 7814
    },
    {
      "epoch": 0.03339258398352376,
      "grad_norm": 1.7144742012023926,
      "learning_rate": 0.00016612544864125792,
      "loss": 0.4298,
      "step": 7815
    },
    {
      "epoch": 0.03339685686695096,
      "grad_norm": 1.733969807624817,
      "learning_rate": 0.00016608272090240986,
      "loss": 0.5618,
      "step": 7816
    },
    {
      "epoch": 0.03340112975037815,
      "grad_norm": 0.9951457977294922,
      "learning_rate": 0.00016603999316356177,
      "loss": 0.5351,
      "step": 7817
    },
    {
      "epoch": 0.03340540263380534,
      "grad_norm": 1.4887077808380127,
      "learning_rate": 0.0001659972654247137,
      "loss": 1.0267,
      "step": 7818
    },
    {
      "epoch": 0.03340967551723254,
      "grad_norm": 1.9316390752792358,
      "learning_rate": 0.00016595453768586567,
      "loss": 1.0969,
      "step": 7819
    },
    {
      "epoch": 0.03341394840065973,
      "grad_norm": 1.3418989181518555,
      "learning_rate": 0.0001659118099470176,
      "loss": 0.4675,
      "step": 7820
    },
    {
      "epoch": 0.033418221284086926,
      "grad_norm": 0.7745728492736816,
      "learning_rate": 0.00016586908220816955,
      "loss": 0.2255,
      "step": 7821
    },
    {
      "epoch": 0.033422494167514125,
      "grad_norm": 2.0391733646392822,
      "learning_rate": 0.00016582635446932148,
      "loss": 0.8552,
      "step": 7822
    },
    {
      "epoch": 0.03342676705094132,
      "grad_norm": 3.2391021251678467,
      "learning_rate": 0.00016578362673047342,
      "loss": 1.0473,
      "step": 7823
    },
    {
      "epoch": 0.03343103993436851,
      "grad_norm": 3.378836154937744,
      "learning_rate": 0.00016574089899162536,
      "loss": 1.762,
      "step": 7824
    },
    {
      "epoch": 0.03343531281779571,
      "grad_norm": 2.00677490234375,
      "learning_rate": 0.0001656981712527773,
      "loss": 0.7117,
      "step": 7825
    },
    {
      "epoch": 0.0334395857012229,
      "grad_norm": 0.7326263785362244,
      "learning_rate": 0.00016565544351392926,
      "loss": 0.3043,
      "step": 7826
    },
    {
      "epoch": 0.03344385858465009,
      "grad_norm": 0.9761941432952881,
      "learning_rate": 0.0001656127157750812,
      "loss": 0.5471,
      "step": 7827
    },
    {
      "epoch": 0.03344813146807729,
      "grad_norm": 0.6384751200675964,
      "learning_rate": 0.00016556998803623314,
      "loss": 0.1771,
      "step": 7828
    },
    {
      "epoch": 0.03345240435150448,
      "grad_norm": 0.9975830912590027,
      "learning_rate": 0.00016552726029738508,
      "loss": 0.5621,
      "step": 7829
    },
    {
      "epoch": 0.033456677234931675,
      "grad_norm": 2.7337098121643066,
      "learning_rate": 0.00016548453255853701,
      "loss": 0.8405,
      "step": 7830
    },
    {
      "epoch": 0.03346095011835887,
      "grad_norm": 0.7195214629173279,
      "learning_rate": 0.00016544180481968895,
      "loss": 0.2787,
      "step": 7831
    },
    {
      "epoch": 0.03346522300178607,
      "grad_norm": 3.4613475799560547,
      "learning_rate": 0.0001653990770808409,
      "loss": 1.2446,
      "step": 7832
    },
    {
      "epoch": 0.03346949588521326,
      "grad_norm": 3.2346994876861572,
      "learning_rate": 0.0001653563493419928,
      "loss": 0.912,
      "step": 7833
    },
    {
      "epoch": 0.03347376876864045,
      "grad_norm": 2.345555543899536,
      "learning_rate": 0.00016531362160314477,
      "loss": 0.8669,
      "step": 7834
    },
    {
      "epoch": 0.03347804165206765,
      "grad_norm": 1.350533127784729,
      "learning_rate": 0.0001652708938642967,
      "loss": 0.9623,
      "step": 7835
    },
    {
      "epoch": 0.03348231453549484,
      "grad_norm": 0.9385315775871277,
      "learning_rate": 0.00016522816612544864,
      "loss": 0.5244,
      "step": 7836
    },
    {
      "epoch": 0.033486587418922034,
      "grad_norm": 0.989334225654602,
      "learning_rate": 0.00016518543838660058,
      "loss": 0.4522,
      "step": 7837
    },
    {
      "epoch": 0.03349086030234923,
      "grad_norm": 0.8658901453018188,
      "learning_rate": 0.00016514271064775252,
      "loss": 0.2822,
      "step": 7838
    },
    {
      "epoch": 0.033495133185776425,
      "grad_norm": 3.9528517723083496,
      "learning_rate": 0.00016509998290890445,
      "loss": 1.1345,
      "step": 7839
    },
    {
      "epoch": 0.03349940606920362,
      "grad_norm": 4.243390083312988,
      "learning_rate": 0.0001650572551700564,
      "loss": 1.0116,
      "step": 7840
    },
    {
      "epoch": 0.033503678952630817,
      "grad_norm": 0.7601518630981445,
      "learning_rate": 0.00016501452743120836,
      "loss": 0.2788,
      "step": 7841
    },
    {
      "epoch": 0.03350795183605801,
      "grad_norm": 0.5743559002876282,
      "learning_rate": 0.0001649717996923603,
      "loss": 0.1549,
      "step": 7842
    },
    {
      "epoch": 0.0335122247194852,
      "grad_norm": 2.7503976821899414,
      "learning_rate": 0.00016492907195351223,
      "loss": 0.6915,
      "step": 7843
    },
    {
      "epoch": 0.0335164976029124,
      "grad_norm": 1.1471248865127563,
      "learning_rate": 0.00016488634421466417,
      "loss": 0.5893,
      "step": 7844
    },
    {
      "epoch": 0.03352077048633959,
      "grad_norm": 1.1248219013214111,
      "learning_rate": 0.0001648436164758161,
      "loss": 0.5618,
      "step": 7845
    },
    {
      "epoch": 0.033525043369766784,
      "grad_norm": 0.9035534858703613,
      "learning_rate": 0.00016480088873696805,
      "loss": 0.4838,
      "step": 7846
    },
    {
      "epoch": 0.03352931625319398,
      "grad_norm": 0.9024623036384583,
      "learning_rate": 0.00016475816099811998,
      "loss": 0.483,
      "step": 7847
    },
    {
      "epoch": 0.033533589136621175,
      "grad_norm": 0.8698262572288513,
      "learning_rate": 0.00016471543325927192,
      "loss": 0.2643,
      "step": 7848
    },
    {
      "epoch": 0.03353786202004837,
      "grad_norm": 0.6647421717643738,
      "learning_rate": 0.00016467270552042386,
      "loss": 0.1777,
      "step": 7849
    },
    {
      "epoch": 0.033542134903475566,
      "grad_norm": 3.470629930496216,
      "learning_rate": 0.0001646299777815758,
      "loss": 1.0754,
      "step": 7850
    },
    {
      "epoch": 0.03354640778690276,
      "grad_norm": 1.8256889581680298,
      "learning_rate": 0.00016458725004272774,
      "loss": 0.765,
      "step": 7851
    },
    {
      "epoch": 0.03355068067032995,
      "grad_norm": 1.4028569459915161,
      "learning_rate": 0.00016454452230387967,
      "loss": 0.9742,
      "step": 7852
    },
    {
      "epoch": 0.03355495355375715,
      "grad_norm": 0.7119539380073547,
      "learning_rate": 0.0001645017945650316,
      "loss": 0.1854,
      "step": 7853
    },
    {
      "epoch": 0.03355922643718434,
      "grad_norm": 2.056344747543335,
      "learning_rate": 0.00016445906682618355,
      "loss": 0.6397,
      "step": 7854
    },
    {
      "epoch": 0.033563499320611534,
      "grad_norm": 0.8865386247634888,
      "learning_rate": 0.0001644163390873355,
      "loss": 0.4416,
      "step": 7855
    },
    {
      "epoch": 0.033567772204038726,
      "grad_norm": 1.3853774070739746,
      "learning_rate": 0.00016437361134848745,
      "loss": 0.3564,
      "step": 7856
    },
    {
      "epoch": 0.033572045087465925,
      "grad_norm": 1.7468161582946777,
      "learning_rate": 0.0001643308836096394,
      "loss": 0.5455,
      "step": 7857
    },
    {
      "epoch": 0.03357631797089312,
      "grad_norm": 1.1059447526931763,
      "learning_rate": 0.00016428815587079133,
      "loss": 0.5754,
      "step": 7858
    },
    {
      "epoch": 0.03358059085432031,
      "grad_norm": 3.041112184524536,
      "learning_rate": 0.00016424542813194327,
      "loss": 0.809,
      "step": 7859
    },
    {
      "epoch": 0.03358486373774751,
      "grad_norm": 3.4610981941223145,
      "learning_rate": 0.0001642027003930952,
      "loss": 0.952,
      "step": 7860
    },
    {
      "epoch": 0.0335891366211747,
      "grad_norm": 1.548553228378296,
      "learning_rate": 0.00016415997265424714,
      "loss": 0.614,
      "step": 7861
    },
    {
      "epoch": 0.03359340950460189,
      "grad_norm": 2.1537063121795654,
      "learning_rate": 0.00016411724491539908,
      "loss": 0.6686,
      "step": 7862
    },
    {
      "epoch": 0.03359768238802909,
      "grad_norm": 4.100955009460449,
      "learning_rate": 0.00016407451717655104,
      "loss": 1.1733,
      "step": 7863
    },
    {
      "epoch": 0.033601955271456284,
      "grad_norm": 1.5972665548324585,
      "learning_rate": 0.00016403178943770298,
      "loss": 0.6392,
      "step": 7864
    },
    {
      "epoch": 0.033606228154883476,
      "grad_norm": 3.993734121322632,
      "learning_rate": 0.00016398906169885492,
      "loss": 0.944,
      "step": 7865
    },
    {
      "epoch": 0.033610501038310675,
      "grad_norm": 1.513783574104309,
      "learning_rate": 0.00016394633396000683,
      "loss": 0.5659,
      "step": 7866
    },
    {
      "epoch": 0.03361477392173787,
      "grad_norm": 2.8449032306671143,
      "learning_rate": 0.00016390360622115877,
      "loss": 0.8155,
      "step": 7867
    },
    {
      "epoch": 0.03361904680516506,
      "grad_norm": 1.46274733543396,
      "learning_rate": 0.0001638608784823107,
      "loss": 0.9952,
      "step": 7868
    },
    {
      "epoch": 0.03362331968859226,
      "grad_norm": 2.1168899536132812,
      "learning_rate": 0.00016381815074346264,
      "loss": 0.6564,
      "step": 7869
    },
    {
      "epoch": 0.03362759257201945,
      "grad_norm": 2.762427568435669,
      "learning_rate": 0.00016377542300461458,
      "loss": 0.7868,
      "step": 7870
    },
    {
      "epoch": 0.03363186545544664,
      "grad_norm": 1.147173523902893,
      "learning_rate": 0.00016373269526576655,
      "loss": 0.4342,
      "step": 7871
    },
    {
      "epoch": 0.03363613833887384,
      "grad_norm": 0.520427405834198,
      "learning_rate": 0.00016368996752691848,
      "loss": 0.1391,
      "step": 7872
    },
    {
      "epoch": 0.033640411222301034,
      "grad_norm": 0.9347667098045349,
      "learning_rate": 0.00016364723978807042,
      "loss": 0.486,
      "step": 7873
    },
    {
      "epoch": 0.033644684105728226,
      "grad_norm": 0.49289706349372864,
      "learning_rate": 0.00016360451204922236,
      "loss": 0.1514,
      "step": 7874
    },
    {
      "epoch": 0.033648956989155425,
      "grad_norm": 2.6174368858337402,
      "learning_rate": 0.0001635617843103743,
      "loss": 0.709,
      "step": 7875
    },
    {
      "epoch": 0.03365322987258262,
      "grad_norm": 2.452570915222168,
      "learning_rate": 0.00016351905657152624,
      "loss": 0.6067,
      "step": 7876
    },
    {
      "epoch": 0.03365750275600981,
      "grad_norm": 4.728649139404297,
      "learning_rate": 0.00016347632883267817,
      "loss": 1.481,
      "step": 7877
    },
    {
      "epoch": 0.03366177563943701,
      "grad_norm": 1.8123154640197754,
      "learning_rate": 0.00016343360109383014,
      "loss": 0.753,
      "step": 7878
    },
    {
      "epoch": 0.0336660485228642,
      "grad_norm": 0.3105851709842682,
      "learning_rate": 0.00016339087335498208,
      "loss": 0.0957,
      "step": 7879
    },
    {
      "epoch": 0.03367032140629139,
      "grad_norm": 0.900059163570404,
      "learning_rate": 0.00016334814561613401,
      "loss": 0.4265,
      "step": 7880
    },
    {
      "epoch": 0.033674594289718585,
      "grad_norm": 0.9121018648147583,
      "learning_rate": 0.00016330541787728595,
      "loss": 0.4267,
      "step": 7881
    },
    {
      "epoch": 0.033678867173145784,
      "grad_norm": 2.1005890369415283,
      "learning_rate": 0.00016326269013843786,
      "loss": 0.5137,
      "step": 7882
    },
    {
      "epoch": 0.033683140056572976,
      "grad_norm": 3.4304492473602295,
      "learning_rate": 0.0001632199623995898,
      "loss": 1.2662,
      "step": 7883
    },
    {
      "epoch": 0.03368741294000017,
      "grad_norm": 2.063523769378662,
      "learning_rate": 0.00016317723466074174,
      "loss": 1.3635,
      "step": 7884
    },
    {
      "epoch": 0.03369168582342737,
      "grad_norm": 1.1350620985031128,
      "learning_rate": 0.00016313450692189368,
      "loss": 0.4753,
      "step": 7885
    },
    {
      "epoch": 0.03369595870685456,
      "grad_norm": 1.7145277261734009,
      "learning_rate": 0.00016309177918304564,
      "loss": 0.6809,
      "step": 7886
    },
    {
      "epoch": 0.03370023159028175,
      "grad_norm": 3.8368961811065674,
      "learning_rate": 0.00016304905144419758,
      "loss": 0.9777,
      "step": 7887
    },
    {
      "epoch": 0.03370450447370895,
      "grad_norm": 1.5816315412521362,
      "learning_rate": 0.00016300632370534952,
      "loss": 1.037,
      "step": 7888
    },
    {
      "epoch": 0.03370877735713614,
      "grad_norm": 1.3635997772216797,
      "learning_rate": 0.00016296359596650145,
      "loss": 0.3146,
      "step": 7889
    },
    {
      "epoch": 0.033713050240563334,
      "grad_norm": 4.461826324462891,
      "learning_rate": 0.0001629208682276534,
      "loss": 0.998,
      "step": 7890
    },
    {
      "epoch": 0.033717323123990534,
      "grad_norm": 3.6611223220825195,
      "learning_rate": 0.00016287814048880533,
      "loss": 1.4306,
      "step": 7891
    },
    {
      "epoch": 0.033721596007417726,
      "grad_norm": 0.5830824971199036,
      "learning_rate": 0.00016283541274995727,
      "loss": 0.187,
      "step": 7892
    },
    {
      "epoch": 0.03372586889084492,
      "grad_norm": 2.772829532623291,
      "learning_rate": 0.00016279268501110923,
      "loss": 0.7833,
      "step": 7893
    },
    {
      "epoch": 0.03373014177427212,
      "grad_norm": 2.95119309425354,
      "learning_rate": 0.00016274995727226117,
      "loss": 0.8359,
      "step": 7894
    },
    {
      "epoch": 0.03373441465769931,
      "grad_norm": 3.9858579635620117,
      "learning_rate": 0.0001627072295334131,
      "loss": 1.1318,
      "step": 7895
    },
    {
      "epoch": 0.0337386875411265,
      "grad_norm": 1.7464709281921387,
      "learning_rate": 0.00016266450179456505,
      "loss": 1.017,
      "step": 7896
    },
    {
      "epoch": 0.0337429604245537,
      "grad_norm": 1.4369831085205078,
      "learning_rate": 0.00016262177405571698,
      "loss": 0.5306,
      "step": 7897
    },
    {
      "epoch": 0.03374723330798089,
      "grad_norm": 0.9336775541305542,
      "learning_rate": 0.00016257904631686892,
      "loss": 0.3727,
      "step": 7898
    },
    {
      "epoch": 0.033751506191408084,
      "grad_norm": 0.6058842539787292,
      "learning_rate": 0.00016253631857802083,
      "loss": 0.2384,
      "step": 7899
    },
    {
      "epoch": 0.03375577907483528,
      "grad_norm": 0.9505929946899414,
      "learning_rate": 0.00016249359083917277,
      "loss": 0.3727,
      "step": 7900
    },
    {
      "epoch": 0.033760051958262476,
      "grad_norm": 0.9748613238334656,
      "learning_rate": 0.00016245086310032474,
      "loss": 0.3729,
      "step": 7901
    },
    {
      "epoch": 0.03376432484168967,
      "grad_norm": 2.1520848274230957,
      "learning_rate": 0.00016240813536147667,
      "loss": 0.5068,
      "step": 7902
    },
    {
      "epoch": 0.03376859772511687,
      "grad_norm": 0.9358124732971191,
      "learning_rate": 0.0001623654076226286,
      "loss": 0.3229,
      "step": 7903
    },
    {
      "epoch": 0.03377287060854406,
      "grad_norm": 3.743696689605713,
      "learning_rate": 0.00016232267988378055,
      "loss": 0.8527,
      "step": 7904
    },
    {
      "epoch": 0.03377714349197125,
      "grad_norm": 0.9929133653640747,
      "learning_rate": 0.0001622799521449325,
      "loss": 0.3223,
      "step": 7905
    },
    {
      "epoch": 0.03378141637539844,
      "grad_norm": 4.112680435180664,
      "learning_rate": 0.00016223722440608443,
      "loss": 1.0782,
      "step": 7906
    },
    {
      "epoch": 0.03378568925882564,
      "grad_norm": 3.7143495082855225,
      "learning_rate": 0.00016219449666723636,
      "loss": 0.8305,
      "step": 7907
    },
    {
      "epoch": 0.033789962142252834,
      "grad_norm": 2.131303310394287,
      "learning_rate": 0.00016215176892838833,
      "loss": 0.479,
      "step": 7908
    },
    {
      "epoch": 0.033794235025680026,
      "grad_norm": 3.200026273727417,
      "learning_rate": 0.00016210904118954027,
      "loss": 1.0246,
      "step": 7909
    },
    {
      "epoch": 0.033798507909107225,
      "grad_norm": 0.7731136083602905,
      "learning_rate": 0.0001620663134506922,
      "loss": 0.2424,
      "step": 7910
    },
    {
      "epoch": 0.03380278079253442,
      "grad_norm": 1.33882474899292,
      "learning_rate": 0.00016202358571184414,
      "loss": 0.6453,
      "step": 7911
    },
    {
      "epoch": 0.03380705367596161,
      "grad_norm": 1.1180685758590698,
      "learning_rate": 0.00016198085797299608,
      "loss": 0.2969,
      "step": 7912
    },
    {
      "epoch": 0.03381132655938881,
      "grad_norm": 0.8784021139144897,
      "learning_rate": 0.00016193813023414802,
      "loss": 0.2821,
      "step": 7913
    },
    {
      "epoch": 0.033815599442816,
      "grad_norm": 1.11961829662323,
      "learning_rate": 0.00016189540249529996,
      "loss": 0.2797,
      "step": 7914
    },
    {
      "epoch": 0.03381987232624319,
      "grad_norm": 1.4906684160232544,
      "learning_rate": 0.00016185267475645187,
      "loss": 0.4956,
      "step": 7915
    },
    {
      "epoch": 0.03382414520967039,
      "grad_norm": 4.914463520050049,
      "learning_rate": 0.00016180994701760383,
      "loss": 1.6377,
      "step": 7916
    },
    {
      "epoch": 0.033828418093097584,
      "grad_norm": 1.688999056816101,
      "learning_rate": 0.00016176721927875577,
      "loss": 0.5858,
      "step": 7917
    },
    {
      "epoch": 0.033832690976524776,
      "grad_norm": 0.9652668833732605,
      "learning_rate": 0.0001617244915399077,
      "loss": 0.426,
      "step": 7918
    },
    {
      "epoch": 0.033836963859951975,
      "grad_norm": 2.481227159500122,
      "learning_rate": 0.00016168176380105964,
      "loss": 1.2203,
      "step": 7919
    },
    {
      "epoch": 0.03384123674337917,
      "grad_norm": 2.9247169494628906,
      "learning_rate": 0.00016163903606221158,
      "loss": 1.6018,
      "step": 7920
    },
    {
      "epoch": 0.03384550962680636,
      "grad_norm": 2.997082233428955,
      "learning_rate": 0.00016159630832336352,
      "loss": 1.6371,
      "step": 7921
    },
    {
      "epoch": 0.03384978251023356,
      "grad_norm": 2.1606709957122803,
      "learning_rate": 0.00016155358058451546,
      "loss": 0.6292,
      "step": 7922
    },
    {
      "epoch": 0.03385405539366075,
      "grad_norm": 0.9913313388824463,
      "learning_rate": 0.00016151085284566742,
      "loss": 0.2079,
      "step": 7923
    },
    {
      "epoch": 0.03385832827708794,
      "grad_norm": 1.0926448106765747,
      "learning_rate": 0.00016146812510681936,
      "loss": 0.2341,
      "step": 7924
    },
    {
      "epoch": 0.03386260116051514,
      "grad_norm": 4.523883819580078,
      "learning_rate": 0.0001614253973679713,
      "loss": 1.3392,
      "step": 7925
    },
    {
      "epoch": 0.033866874043942334,
      "grad_norm": 2.204850912094116,
      "learning_rate": 0.00016138266962912324,
      "loss": 0.669,
      "step": 7926
    },
    {
      "epoch": 0.033871146927369526,
      "grad_norm": 4.857608795166016,
      "learning_rate": 0.00016133994189027517,
      "loss": 1.3179,
      "step": 7927
    },
    {
      "epoch": 0.033875419810796725,
      "grad_norm": 1.0166468620300293,
      "learning_rate": 0.0001612972141514271,
      "loss": 0.409,
      "step": 7928
    },
    {
      "epoch": 0.03387969269422392,
      "grad_norm": 0.9290545582771301,
      "learning_rate": 0.00016125448641257905,
      "loss": 0.3263,
      "step": 7929
    },
    {
      "epoch": 0.03388396557765111,
      "grad_norm": 2.2185277938842773,
      "learning_rate": 0.00016121175867373101,
      "loss": 0.6217,
      "step": 7930
    },
    {
      "epoch": 0.0338882384610783,
      "grad_norm": 3.733975410461426,
      "learning_rate": 0.00016116903093488295,
      "loss": 1.1733,
      "step": 7931
    },
    {
      "epoch": 0.0338925113445055,
      "grad_norm": 1.3686357736587524,
      "learning_rate": 0.00016112630319603486,
      "loss": 0.3038,
      "step": 7932
    },
    {
      "epoch": 0.03389678422793269,
      "grad_norm": 1.385920763015747,
      "learning_rate": 0.0001610835754571868,
      "loss": 0.4437,
      "step": 7933
    },
    {
      "epoch": 0.033901057111359885,
      "grad_norm": 1.7393296957015991,
      "learning_rate": 0.00016104084771833874,
      "loss": 0.4745,
      "step": 7934
    },
    {
      "epoch": 0.033905329994787084,
      "grad_norm": 1.1223554611206055,
      "learning_rate": 0.00016099811997949068,
      "loss": 0.3813,
      "step": 7935
    },
    {
      "epoch": 0.033909602878214276,
      "grad_norm": 3.883634090423584,
      "learning_rate": 0.00016095539224064261,
      "loss": 0.7998,
      "step": 7936
    },
    {
      "epoch": 0.03391387576164147,
      "grad_norm": 3.444415807723999,
      "learning_rate": 0.00016091266450179455,
      "loss": 1.3136,
      "step": 7937
    },
    {
      "epoch": 0.03391814864506867,
      "grad_norm": 0.9482131600379944,
      "learning_rate": 0.00016086993676294652,
      "loss": 0.2762,
      "step": 7938
    },
    {
      "epoch": 0.03392242152849586,
      "grad_norm": 1.7337888479232788,
      "learning_rate": 0.00016082720902409846,
      "loss": 0.4468,
      "step": 7939
    },
    {
      "epoch": 0.03392669441192305,
      "grad_norm": 4.947601795196533,
      "learning_rate": 0.0001607844812852504,
      "loss": 1.226,
      "step": 7940
    },
    {
      "epoch": 0.03393096729535025,
      "grad_norm": 0.6792189478874207,
      "learning_rate": 0.00016074175354640233,
      "loss": 0.208,
      "step": 7941
    },
    {
      "epoch": 0.03393524017877744,
      "grad_norm": 1.5129506587982178,
      "learning_rate": 0.00016069902580755427,
      "loss": 0.444,
      "step": 7942
    },
    {
      "epoch": 0.033939513062204635,
      "grad_norm": 1.751123309135437,
      "learning_rate": 0.0001606562980687062,
      "loss": 0.5356,
      "step": 7943
    },
    {
      "epoch": 0.033943785945631834,
      "grad_norm": 1.3973298072814941,
      "learning_rate": 0.00016061357032985814,
      "loss": 0.4513,
      "step": 7944
    },
    {
      "epoch": 0.033948058829059026,
      "grad_norm": 2.5216786861419678,
      "learning_rate": 0.0001605708425910101,
      "loss": 0.9308,
      "step": 7945
    },
    {
      "epoch": 0.03395233171248622,
      "grad_norm": 2.019583225250244,
      "learning_rate": 0.00016052811485216205,
      "loss": 0.5397,
      "step": 7946
    },
    {
      "epoch": 0.03395660459591342,
      "grad_norm": 2.1657702922821045,
      "learning_rate": 0.00016048538711331399,
      "loss": 0.5451,
      "step": 7947
    },
    {
      "epoch": 0.03396087747934061,
      "grad_norm": 2.880309581756592,
      "learning_rate": 0.0001604426593744659,
      "loss": 1.5407,
      "step": 7948
    },
    {
      "epoch": 0.0339651503627678,
      "grad_norm": 4.4728922843933105,
      "learning_rate": 0.00016039993163561783,
      "loss": 1.1024,
      "step": 7949
    },
    {
      "epoch": 0.033969423246195,
      "grad_norm": 2.208656072616577,
      "learning_rate": 0.00016035720389676977,
      "loss": 0.8628,
      "step": 7950
    },
    {
      "epoch": 0.03397369612962219,
      "grad_norm": 0.7165194749832153,
      "learning_rate": 0.0001603144761579217,
      "loss": 0.1762,
      "step": 7951
    },
    {
      "epoch": 0.033977969013049385,
      "grad_norm": 0.8143844604492188,
      "learning_rate": 0.00016027174841907365,
      "loss": 0.2241,
      "step": 7952
    },
    {
      "epoch": 0.033982241896476584,
      "grad_norm": 5.400579929351807,
      "learning_rate": 0.0001602290206802256,
      "loss": 3.7044,
      "step": 7953
    },
    {
      "epoch": 0.033986514779903776,
      "grad_norm": 2.1102354526519775,
      "learning_rate": 0.00016018629294137755,
      "loss": 0.5541,
      "step": 7954
    },
    {
      "epoch": 0.03399078766333097,
      "grad_norm": 1.7124394178390503,
      "learning_rate": 0.0001601435652025295,
      "loss": 0.4805,
      "step": 7955
    },
    {
      "epoch": 0.03399506054675816,
      "grad_norm": 3.3632335662841797,
      "learning_rate": 0.00016010083746368143,
      "loss": 1.1268,
      "step": 7956
    },
    {
      "epoch": 0.03399933343018536,
      "grad_norm": 1.091597318649292,
      "learning_rate": 0.00016005810972483336,
      "loss": 0.3441,
      "step": 7957
    },
    {
      "epoch": 0.03400360631361255,
      "grad_norm": 4.1022539138793945,
      "learning_rate": 0.0001600153819859853,
      "loss": 1.3168,
      "step": 7958
    },
    {
      "epoch": 0.03400787919703974,
      "grad_norm": 1.8975886106491089,
      "learning_rate": 0.00015997265424713724,
      "loss": 0.6553,
      "step": 7959
    },
    {
      "epoch": 0.03401215208046694,
      "grad_norm": 3.6676690578460693,
      "learning_rate": 0.0001599299265082892,
      "loss": 0.8301,
      "step": 7960
    },
    {
      "epoch": 0.034016424963894135,
      "grad_norm": 2.3006980419158936,
      "learning_rate": 0.00015988719876944114,
      "loss": 1.3584,
      "step": 7961
    },
    {
      "epoch": 0.03402069784732133,
      "grad_norm": 2.0562167167663574,
      "learning_rate": 0.00015984447103059308,
      "loss": 0.8591,
      "step": 7962
    },
    {
      "epoch": 0.034024970730748526,
      "grad_norm": 3.4160044193267822,
      "learning_rate": 0.00015980174329174502,
      "loss": 1.99,
      "step": 7963
    },
    {
      "epoch": 0.03402924361417572,
      "grad_norm": 2.1867024898529053,
      "learning_rate": 0.00015975901555289693,
      "loss": 0.7391,
      "step": 7964
    },
    {
      "epoch": 0.03403351649760291,
      "grad_norm": 1.9945207834243774,
      "learning_rate": 0.00015971628781404887,
      "loss": 1.2749,
      "step": 7965
    },
    {
      "epoch": 0.03403778938103011,
      "grad_norm": 1.8946752548217773,
      "learning_rate": 0.0001596735600752008,
      "loss": 0.6239,
      "step": 7966
    },
    {
      "epoch": 0.0340420622644573,
      "grad_norm": 1.0546526908874512,
      "learning_rate": 0.00015963083233635274,
      "loss": 0.3443,
      "step": 7967
    },
    {
      "epoch": 0.03404633514788449,
      "grad_norm": 1.59172785282135,
      "learning_rate": 0.0001595881045975047,
      "loss": 0.4026,
      "step": 7968
    },
    {
      "epoch": 0.03405060803131169,
      "grad_norm": 1.6644601821899414,
      "learning_rate": 0.00015954537685865664,
      "loss": 1.1222,
      "step": 7969
    },
    {
      "epoch": 0.034054880914738885,
      "grad_norm": 2.8493919372558594,
      "learning_rate": 0.00015950264911980858,
      "loss": 1.7927,
      "step": 7970
    },
    {
      "epoch": 0.03405915379816608,
      "grad_norm": 1.472195029258728,
      "learning_rate": 0.00015945992138096052,
      "loss": 1.05,
      "step": 7971
    },
    {
      "epoch": 0.034063426681593276,
      "grad_norm": 4.001561164855957,
      "learning_rate": 0.00015941719364211246,
      "loss": 1.1869,
      "step": 7972
    },
    {
      "epoch": 0.03406769956502047,
      "grad_norm": 2.0797717571258545,
      "learning_rate": 0.0001593744659032644,
      "loss": 0.7048,
      "step": 7973
    },
    {
      "epoch": 0.03407197244844766,
      "grad_norm": 3.1853206157684326,
      "learning_rate": 0.00015933173816441633,
      "loss": 0.9665,
      "step": 7974
    },
    {
      "epoch": 0.03407624533187486,
      "grad_norm": 1.3333085775375366,
      "learning_rate": 0.0001592890104255683,
      "loss": 0.2942,
      "step": 7975
    },
    {
      "epoch": 0.03408051821530205,
      "grad_norm": 1.5469801425933838,
      "learning_rate": 0.00015924628268672024,
      "loss": 0.3799,
      "step": 7976
    },
    {
      "epoch": 0.03408479109872924,
      "grad_norm": 2.9591705799102783,
      "learning_rate": 0.00015920355494787217,
      "loss": 0.8775,
      "step": 7977
    },
    {
      "epoch": 0.034089063982156435,
      "grad_norm": 1.3230865001678467,
      "learning_rate": 0.0001591608272090241,
      "loss": 0.9344,
      "step": 7978
    },
    {
      "epoch": 0.034093336865583634,
      "grad_norm": 5.2374653816223145,
      "learning_rate": 0.00015911809947017605,
      "loss": 1.2944,
      "step": 7979
    },
    {
      "epoch": 0.034097609749010827,
      "grad_norm": 1.6519967317581177,
      "learning_rate": 0.000159075371731328,
      "loss": 0.3643,
      "step": 7980
    },
    {
      "epoch": 0.03410188263243802,
      "grad_norm": 1.6872661113739014,
      "learning_rate": 0.0001590326439924799,
      "loss": 0.3622,
      "step": 7981
    },
    {
      "epoch": 0.03410615551586522,
      "grad_norm": 0.5191892385482788,
      "learning_rate": 0.00015898991625363184,
      "loss": 0.1995,
      "step": 7982
    },
    {
      "epoch": 0.03411042839929241,
      "grad_norm": 1.087928295135498,
      "learning_rate": 0.0001589471885147838,
      "loss": 0.3271,
      "step": 7983
    },
    {
      "epoch": 0.0341147012827196,
      "grad_norm": 1.7074483633041382,
      "learning_rate": 0.00015890446077593574,
      "loss": 0.3149,
      "step": 7984
    },
    {
      "epoch": 0.0341189741661468,
      "grad_norm": 2.0490198135375977,
      "learning_rate": 0.00015886173303708768,
      "loss": 0.5541,
      "step": 7985
    },
    {
      "epoch": 0.03412324704957399,
      "grad_norm": 0.9732787013053894,
      "learning_rate": 0.00015881900529823962,
      "loss": 0.2563,
      "step": 7986
    },
    {
      "epoch": 0.034127519933001185,
      "grad_norm": 2.6546924114227295,
      "learning_rate": 0.00015877627755939155,
      "loss": 0.7551,
      "step": 7987
    },
    {
      "epoch": 0.034131792816428384,
      "grad_norm": 1.7721736431121826,
      "learning_rate": 0.0001587335498205435,
      "loss": 0.653,
      "step": 7988
    },
    {
      "epoch": 0.034136065699855576,
      "grad_norm": 3.732525587081909,
      "learning_rate": 0.00015869082208169543,
      "loss": 1.1553,
      "step": 7989
    },
    {
      "epoch": 0.03414033858328277,
      "grad_norm": 2.843144655227661,
      "learning_rate": 0.0001586480943428474,
      "loss": 0.5756,
      "step": 7990
    },
    {
      "epoch": 0.03414461146670997,
      "grad_norm": 4.370806694030762,
      "learning_rate": 0.00015860536660399933,
      "loss": 1.089,
      "step": 7991
    },
    {
      "epoch": 0.03414888435013716,
      "grad_norm": 4.018557071685791,
      "learning_rate": 0.00015856263886515127,
      "loss": 1.0474,
      "step": 7992
    },
    {
      "epoch": 0.03415315723356435,
      "grad_norm": 2.2238752841949463,
      "learning_rate": 0.0001585199111263032,
      "loss": 0.7224,
      "step": 7993
    },
    {
      "epoch": 0.03415743011699155,
      "grad_norm": 1.4488892555236816,
      "learning_rate": 0.00015847718338745514,
      "loss": 0.4734,
      "step": 7994
    },
    {
      "epoch": 0.03416170300041874,
      "grad_norm": 1.347488522529602,
      "learning_rate": 0.00015843445564860708,
      "loss": 0.2645,
      "step": 7995
    },
    {
      "epoch": 0.034165975883845935,
      "grad_norm": 1.4235420227050781,
      "learning_rate": 0.00015839172790975902,
      "loss": 0.3927,
      "step": 7996
    },
    {
      "epoch": 0.034170248767273134,
      "grad_norm": 1.5485663414001465,
      "learning_rate": 0.00015834900017091096,
      "loss": 0.8285,
      "step": 7997
    },
    {
      "epoch": 0.034174521650700326,
      "grad_norm": 2.1730520725250244,
      "learning_rate": 0.0001583062724320629,
      "loss": 0.5071,
      "step": 7998
    },
    {
      "epoch": 0.03417879453412752,
      "grad_norm": 3.1784307956695557,
      "learning_rate": 0.00015826354469321483,
      "loss": 0.9386,
      "step": 7999
    },
    {
      "epoch": 0.03418306741755472,
      "grad_norm": 3.7095935344696045,
      "learning_rate": 0.00015822081695436677,
      "loss": 0.8168,
      "step": 8000
    },
    {
      "epoch": 0.03418734030098191,
      "grad_norm": 3.0935871601104736,
      "learning_rate": 0.0001581780892155187,
      "loss": 0.7907,
      "step": 8001
    },
    {
      "epoch": 0.0341916131844091,
      "grad_norm": 3.783843755722046,
      "learning_rate": 0.00015813536147667065,
      "loss": 0.7773,
      "step": 8002
    },
    {
      "epoch": 0.034195886067836294,
      "grad_norm": 1.994354486465454,
      "learning_rate": 0.00015809263373782259,
      "loss": 0.4365,
      "step": 8003
    },
    {
      "epoch": 0.03420015895126349,
      "grad_norm": 1.2076890468597412,
      "learning_rate": 0.00015804990599897452,
      "loss": 0.8004,
      "step": 8004
    },
    {
      "epoch": 0.034204431834690685,
      "grad_norm": 2.0662059783935547,
      "learning_rate": 0.0001580071782601265,
      "loss": 0.5051,
      "step": 8005
    },
    {
      "epoch": 0.03420870471811788,
      "grad_norm": 1.9597307443618774,
      "learning_rate": 0.00015796445052127843,
      "loss": 0.5361,
      "step": 8006
    },
    {
      "epoch": 0.034212977601545076,
      "grad_norm": 0.7251423001289368,
      "learning_rate": 0.00015792172278243036,
      "loss": 0.2758,
      "step": 8007
    },
    {
      "epoch": 0.03421725048497227,
      "grad_norm": 1.0353889465332031,
      "learning_rate": 0.0001578789950435823,
      "loss": 0.3412,
      "step": 8008
    },
    {
      "epoch": 0.03422152336839946,
      "grad_norm": 1.1780478954315186,
      "learning_rate": 0.00015783626730473424,
      "loss": 0.5416,
      "step": 8009
    },
    {
      "epoch": 0.03422579625182666,
      "grad_norm": 3.727381467819214,
      "learning_rate": 0.00015779353956588618,
      "loss": 0.9492,
      "step": 8010
    },
    {
      "epoch": 0.03423006913525385,
      "grad_norm": 3.800544261932373,
      "learning_rate": 0.00015775081182703812,
      "loss": 1.1568,
      "step": 8011
    },
    {
      "epoch": 0.034234342018681044,
      "grad_norm": 1.950456976890564,
      "learning_rate": 0.00015770808408819008,
      "loss": 0.591,
      "step": 8012
    },
    {
      "epoch": 0.03423861490210824,
      "grad_norm": 3.091761350631714,
      "learning_rate": 0.00015766535634934202,
      "loss": 0.7711,
      "step": 8013
    },
    {
      "epoch": 0.034242887785535435,
      "grad_norm": 1.0718501806259155,
      "learning_rate": 0.00015762262861049393,
      "loss": 0.4969,
      "step": 8014
    },
    {
      "epoch": 0.03424716066896263,
      "grad_norm": 1.4831815958023071,
      "learning_rate": 0.00015757990087164587,
      "loss": 0.7709,
      "step": 8015
    },
    {
      "epoch": 0.034251433552389826,
      "grad_norm": 2.443140745162964,
      "learning_rate": 0.0001575371731327978,
      "loss": 0.7775,
      "step": 8016
    },
    {
      "epoch": 0.03425570643581702,
      "grad_norm": 3.617532968521118,
      "learning_rate": 0.00015749444539394974,
      "loss": 0.7924,
      "step": 8017
    },
    {
      "epoch": 0.03425997931924421,
      "grad_norm": 4.036088466644287,
      "learning_rate": 0.00015745171765510168,
      "loss": 0.7972,
      "step": 8018
    },
    {
      "epoch": 0.03426425220267141,
      "grad_norm": 1.7336397171020508,
      "learning_rate": 0.00015740898991625362,
      "loss": 0.3357,
      "step": 8019
    },
    {
      "epoch": 0.0342685250860986,
      "grad_norm": 3.9997878074645996,
      "learning_rate": 0.00015736626217740558,
      "loss": 0.7589,
      "step": 8020
    },
    {
      "epoch": 0.034272797969525794,
      "grad_norm": 1.3418368101119995,
      "learning_rate": 0.00015732353443855752,
      "loss": 0.2443,
      "step": 8021
    },
    {
      "epoch": 0.03427707085295299,
      "grad_norm": 1.2998908758163452,
      "learning_rate": 0.00015728080669970946,
      "loss": 0.8505,
      "step": 8022
    },
    {
      "epoch": 0.034281343736380185,
      "grad_norm": 3.1147096157073975,
      "learning_rate": 0.0001572380789608614,
      "loss": 0.6437,
      "step": 8023
    },
    {
      "epoch": 0.03428561661980738,
      "grad_norm": 0.951657235622406,
      "learning_rate": 0.00015719535122201333,
      "loss": 0.425,
      "step": 8024
    },
    {
      "epoch": 0.034289889503234576,
      "grad_norm": 2.0043373107910156,
      "learning_rate": 0.00015715262348316527,
      "loss": 0.5498,
      "step": 8025
    },
    {
      "epoch": 0.03429416238666177,
      "grad_norm": 2.6500256061553955,
      "learning_rate": 0.0001571098957443172,
      "loss": 0.7598,
      "step": 8026
    },
    {
      "epoch": 0.03429843527008896,
      "grad_norm": 5.208930015563965,
      "learning_rate": 0.00015706716800546918,
      "loss": 1.3251,
      "step": 8027
    },
    {
      "epoch": 0.03430270815351615,
      "grad_norm": 1.1637537479400635,
      "learning_rate": 0.0001570244402666211,
      "loss": 0.252,
      "step": 8028
    },
    {
      "epoch": 0.03430698103694335,
      "grad_norm": 1.5368056297302246,
      "learning_rate": 0.00015698171252777305,
      "loss": 1.2168,
      "step": 8029
    },
    {
      "epoch": 0.034311253920370544,
      "grad_norm": 2.275517702102661,
      "learning_rate": 0.00015693898478892496,
      "loss": 0.5601,
      "step": 8030
    },
    {
      "epoch": 0.034315526803797736,
      "grad_norm": 2.0546751022338867,
      "learning_rate": 0.0001568962570500769,
      "loss": 0.428,
      "step": 8031
    },
    {
      "epoch": 0.034319799687224935,
      "grad_norm": 3.637843370437622,
      "learning_rate": 0.00015685352931122884,
      "loss": 0.5351,
      "step": 8032
    },
    {
      "epoch": 0.03432407257065213,
      "grad_norm": 2.047638177871704,
      "learning_rate": 0.00015681080157238078,
      "loss": 0.4172,
      "step": 8033
    },
    {
      "epoch": 0.03432834545407932,
      "grad_norm": 3.0377798080444336,
      "learning_rate": 0.0001567680738335327,
      "loss": 0.546,
      "step": 8034
    },
    {
      "epoch": 0.03433261833750652,
      "grad_norm": 0.8236461877822876,
      "learning_rate": 0.00015672534609468468,
      "loss": 0.3226,
      "step": 8035
    },
    {
      "epoch": 0.03433689122093371,
      "grad_norm": 1.0877788066864014,
      "learning_rate": 0.00015668261835583662,
      "loss": 0.2891,
      "step": 8036
    },
    {
      "epoch": 0.0343411641043609,
      "grad_norm": 3.0194661617279053,
      "learning_rate": 0.00015663989061698855,
      "loss": 1.1037,
      "step": 8037
    },
    {
      "epoch": 0.0343454369877881,
      "grad_norm": 4.054007530212402,
      "learning_rate": 0.0001565971628781405,
      "loss": 1.3182,
      "step": 8038
    },
    {
      "epoch": 0.03434970987121529,
      "grad_norm": 2.054727792739868,
      "learning_rate": 0.00015655443513929243,
      "loss": 0.535,
      "step": 8039
    },
    {
      "epoch": 0.034353982754642486,
      "grad_norm": 3.524839162826538,
      "learning_rate": 0.00015651170740044437,
      "loss": 0.8581,
      "step": 8040
    },
    {
      "epoch": 0.034358255638069685,
      "grad_norm": 0.9095001816749573,
      "learning_rate": 0.0001564689796615963,
      "loss": 0.2776,
      "step": 8041
    },
    {
      "epoch": 0.03436252852149688,
      "grad_norm": 2.5297632217407227,
      "learning_rate": 0.00015642625192274827,
      "loss": 0.7549,
      "step": 8042
    },
    {
      "epoch": 0.03436680140492407,
      "grad_norm": 3.821289300918579,
      "learning_rate": 0.0001563835241839002,
      "loss": 0.8484,
      "step": 8043
    },
    {
      "epoch": 0.03437107428835127,
      "grad_norm": 2.9536263942718506,
      "learning_rate": 0.00015634079644505215,
      "loss": 0.6615,
      "step": 8044
    },
    {
      "epoch": 0.03437534717177846,
      "grad_norm": 4.9991936683654785,
      "learning_rate": 0.00015629806870620408,
      "loss": 1.342,
      "step": 8045
    },
    {
      "epoch": 0.03437962005520565,
      "grad_norm": 3.259502410888672,
      "learning_rate": 0.00015625534096735602,
      "loss": 0.8653,
      "step": 8046
    },
    {
      "epoch": 0.03438389293863285,
      "grad_norm": 1.4947816133499146,
      "learning_rate": 0.00015621261322850793,
      "loss": 1.1468,
      "step": 8047
    },
    {
      "epoch": 0.03438816582206004,
      "grad_norm": 0.7933670878410339,
      "learning_rate": 0.00015616988548965987,
      "loss": 0.3221,
      "step": 8048
    },
    {
      "epoch": 0.034392438705487235,
      "grad_norm": 3.600106716156006,
      "learning_rate": 0.00015612715775081183,
      "loss": 0.9863,
      "step": 8049
    },
    {
      "epoch": 0.034396711588914435,
      "grad_norm": 0.8512489795684814,
      "learning_rate": 0.00015608443001196377,
      "loss": 0.3263,
      "step": 8050
    },
    {
      "epoch": 0.03440098447234163,
      "grad_norm": 1.5977219343185425,
      "learning_rate": 0.0001560417022731157,
      "loss": 0.7935,
      "step": 8051
    },
    {
      "epoch": 0.03440525735576882,
      "grad_norm": 2.6802103519439697,
      "learning_rate": 0.00015599897453426765,
      "loss": 1.0129,
      "step": 8052
    },
    {
      "epoch": 0.03440953023919601,
      "grad_norm": 2.523535966873169,
      "learning_rate": 0.00015595624679541959,
      "loss": 0.6836,
      "step": 8053
    },
    {
      "epoch": 0.03441380312262321,
      "grad_norm": 0.5169134140014648,
      "learning_rate": 0.00015591351905657152,
      "loss": 0.1954,
      "step": 8054
    },
    {
      "epoch": 0.0344180760060504,
      "grad_norm": 1.5229597091674805,
      "learning_rate": 0.00015587079131772346,
      "loss": 0.7445,
      "step": 8055
    },
    {
      "epoch": 0.034422348889477594,
      "grad_norm": 1.089001178741455,
      "learning_rate": 0.0001558280635788754,
      "loss": 0.4294,
      "step": 8056
    },
    {
      "epoch": 0.03442662177290479,
      "grad_norm": 1.1484296321868896,
      "learning_rate": 0.00015578533584002736,
      "loss": 0.3261,
      "step": 8057
    },
    {
      "epoch": 0.034430894656331985,
      "grad_norm": 1.2406773567199707,
      "learning_rate": 0.0001557426081011793,
      "loss": 0.434,
      "step": 8058
    },
    {
      "epoch": 0.03443516753975918,
      "grad_norm": 1.3408501148223877,
      "learning_rate": 0.00015569988036233124,
      "loss": 0.3726,
      "step": 8059
    },
    {
      "epoch": 0.03443944042318638,
      "grad_norm": 0.6847586035728455,
      "learning_rate": 0.00015565715262348318,
      "loss": 0.2052,
      "step": 8060
    },
    {
      "epoch": 0.03444371330661357,
      "grad_norm": 3.243985176086426,
      "learning_rate": 0.00015561442488463512,
      "loss": 0.755,
      "step": 8061
    },
    {
      "epoch": 0.03444798619004076,
      "grad_norm": 0.43659859895706177,
      "learning_rate": 0.00015557169714578705,
      "loss": 0.1377,
      "step": 8062
    },
    {
      "epoch": 0.03445225907346796,
      "grad_norm": 3.3823256492614746,
      "learning_rate": 0.00015552896940693896,
      "loss": 2.6726,
      "step": 8063
    },
    {
      "epoch": 0.03445653195689515,
      "grad_norm": 0.5735224485397339,
      "learning_rate": 0.00015548624166809093,
      "loss": 0.1763,
      "step": 8064
    },
    {
      "epoch": 0.034460804840322344,
      "grad_norm": 3.0283384323120117,
      "learning_rate": 0.00015544351392924287,
      "loss": 0.7481,
      "step": 8065
    },
    {
      "epoch": 0.03446507772374954,
      "grad_norm": 3.1166298389434814,
      "learning_rate": 0.0001554007861903948,
      "loss": 0.6917,
      "step": 8066
    },
    {
      "epoch": 0.034469350607176735,
      "grad_norm": 0.6833853721618652,
      "learning_rate": 0.00015535805845154674,
      "loss": 0.304,
      "step": 8067
    },
    {
      "epoch": 0.03447362349060393,
      "grad_norm": 1.3704547882080078,
      "learning_rate": 0.00015531533071269868,
      "loss": 0.6642,
      "step": 8068
    },
    {
      "epoch": 0.034477896374031126,
      "grad_norm": 1.8221535682678223,
      "learning_rate": 0.00015527260297385062,
      "loss": 0.5351,
      "step": 8069
    },
    {
      "epoch": 0.03448216925745832,
      "grad_norm": 0.5791404843330383,
      "learning_rate": 0.00015522987523500256,
      "loss": 0.1983,
      "step": 8070
    },
    {
      "epoch": 0.03448644214088551,
      "grad_norm": 2.367152452468872,
      "learning_rate": 0.0001551871474961545,
      "loss": 0.8233,
      "step": 8071
    },
    {
      "epoch": 0.03449071502431271,
      "grad_norm": 0.8524191379547119,
      "learning_rate": 0.00015514441975730646,
      "loss": 0.3183,
      "step": 8072
    },
    {
      "epoch": 0.0344949879077399,
      "grad_norm": 3.5449318885803223,
      "learning_rate": 0.0001551016920184584,
      "loss": 0.8607,
      "step": 8073
    },
    {
      "epoch": 0.034499260791167094,
      "grad_norm": 3.5532467365264893,
      "learning_rate": 0.00015505896427961033,
      "loss": 0.8773,
      "step": 8074
    },
    {
      "epoch": 0.03450353367459429,
      "grad_norm": 2.3195548057556152,
      "learning_rate": 0.00015501623654076227,
      "loss": 0.7857,
      "step": 8075
    },
    {
      "epoch": 0.034507806558021485,
      "grad_norm": 1.4180479049682617,
      "learning_rate": 0.0001549735088019142,
      "loss": 1.0656,
      "step": 8076
    },
    {
      "epoch": 0.03451207944144868,
      "grad_norm": 3.564298391342163,
      "learning_rate": 0.00015493078106306615,
      "loss": 1.6641,
      "step": 8077
    },
    {
      "epoch": 0.03451635232487587,
      "grad_norm": 1.340682864189148,
      "learning_rate": 0.00015488805332421809,
      "loss": 0.6427,
      "step": 8078
    },
    {
      "epoch": 0.03452062520830307,
      "grad_norm": 2.1466116905212402,
      "learning_rate": 0.00015484532558537002,
      "loss": 0.5635,
      "step": 8079
    },
    {
      "epoch": 0.03452489809173026,
      "grad_norm": 1.6256667375564575,
      "learning_rate": 0.00015480259784652196,
      "loss": 0.6376,
      "step": 8080
    },
    {
      "epoch": 0.03452917097515745,
      "grad_norm": 2.2096171379089355,
      "learning_rate": 0.0001547598701076739,
      "loss": 0.6477,
      "step": 8081
    },
    {
      "epoch": 0.03453344385858465,
      "grad_norm": 2.441253900527954,
      "learning_rate": 0.00015471714236882584,
      "loss": 0.87,
      "step": 8082
    },
    {
      "epoch": 0.034537716742011844,
      "grad_norm": 0.9511600136756897,
      "learning_rate": 0.00015467441462997778,
      "loss": 0.5188,
      "step": 8083
    },
    {
      "epoch": 0.034541989625439036,
      "grad_norm": 1.1394482851028442,
      "learning_rate": 0.0001546316868911297,
      "loss": 0.8818,
      "step": 8084
    },
    {
      "epoch": 0.034546262508866235,
      "grad_norm": 0.2301677018404007,
      "learning_rate": 0.00015458895915228165,
      "loss": 0.0546,
      "step": 8085
    },
    {
      "epoch": 0.03455053539229343,
      "grad_norm": 2.7854413986206055,
      "learning_rate": 0.0001545462314134336,
      "loss": 0.8682,
      "step": 8086
    },
    {
      "epoch": 0.03455480827572062,
      "grad_norm": 1.3189091682434082,
      "learning_rate": 0.00015450350367458555,
      "loss": 0.6098,
      "step": 8087
    },
    {
      "epoch": 0.03455908115914782,
      "grad_norm": 2.3287782669067383,
      "learning_rate": 0.0001544607759357375,
      "loss": 0.8336,
      "step": 8088
    },
    {
      "epoch": 0.03456335404257501,
      "grad_norm": 3.096552610397339,
      "learning_rate": 0.00015441804819688943,
      "loss": 1.1227,
      "step": 8089
    },
    {
      "epoch": 0.0345676269260022,
      "grad_norm": 2.6541013717651367,
      "learning_rate": 0.00015437532045804137,
      "loss": 0.8359,
      "step": 8090
    },
    {
      "epoch": 0.0345718998094294,
      "grad_norm": 1.8505017757415771,
      "learning_rate": 0.0001543325927191933,
      "loss": 0.5856,
      "step": 8091
    },
    {
      "epoch": 0.034576172692856594,
      "grad_norm": 4.338108539581299,
      "learning_rate": 0.00015428986498034524,
      "loss": 1.1255,
      "step": 8092
    },
    {
      "epoch": 0.034580445576283786,
      "grad_norm": 1.5673877000808716,
      "learning_rate": 0.00015424713724149718,
      "loss": 0.6098,
      "step": 8093
    },
    {
      "epoch": 0.034584718459710985,
      "grad_norm": 1.5508694648742676,
      "learning_rate": 0.00015420440950264915,
      "loss": 0.5659,
      "step": 8094
    },
    {
      "epoch": 0.03458899134313818,
      "grad_norm": 1.305237889289856,
      "learning_rate": 0.00015416168176380108,
      "loss": 0.5635,
      "step": 8095
    },
    {
      "epoch": 0.03459326422656537,
      "grad_norm": 3.793994188308716,
      "learning_rate": 0.000154118954024953,
      "loss": 0.9228,
      "step": 8096
    },
    {
      "epoch": 0.03459753710999257,
      "grad_norm": 1.509650707244873,
      "learning_rate": 0.00015407622628610493,
      "loss": 1.0631,
      "step": 8097
    },
    {
      "epoch": 0.03460180999341976,
      "grad_norm": 3.1289849281311035,
      "learning_rate": 0.00015403349854725687,
      "loss": 0.7744,
      "step": 8098
    },
    {
      "epoch": 0.03460608287684695,
      "grad_norm": 1.0972633361816406,
      "learning_rate": 0.0001539907708084088,
      "loss": 0.5751,
      "step": 8099
    },
    {
      "epoch": 0.03461035576027415,
      "grad_norm": 1.2069200277328491,
      "learning_rate": 0.00015394804306956075,
      "loss": 0.4181,
      "step": 8100
    },
    {
      "epoch": 0.034614628643701344,
      "grad_norm": 2.6938059329986572,
      "learning_rate": 0.0001539053153307127,
      "loss": 0.7263,
      "step": 8101
    },
    {
      "epoch": 0.034618901527128536,
      "grad_norm": 3.329723358154297,
      "learning_rate": 0.00015386258759186465,
      "loss": 0.8851,
      "step": 8102
    },
    {
      "epoch": 0.03462317441055573,
      "grad_norm": 1.3663989305496216,
      "learning_rate": 0.0001538198598530166,
      "loss": 0.5965,
      "step": 8103
    },
    {
      "epoch": 0.03462744729398293,
      "grad_norm": 3.8176991939544678,
      "learning_rate": 0.00015377713211416852,
      "loss": 0.9664,
      "step": 8104
    },
    {
      "epoch": 0.03463172017741012,
      "grad_norm": 0.930232048034668,
      "learning_rate": 0.00015373440437532046,
      "loss": 0.333,
      "step": 8105
    },
    {
      "epoch": 0.03463599306083731,
      "grad_norm": 2.6776041984558105,
      "learning_rate": 0.0001536916766364724,
      "loss": 0.7079,
      "step": 8106
    },
    {
      "epoch": 0.03464026594426451,
      "grad_norm": 3.3029401302337646,
      "learning_rate": 0.00015364894889762434,
      "loss": 1.2523,
      "step": 8107
    },
    {
      "epoch": 0.0346445388276917,
      "grad_norm": 4.2053327560424805,
      "learning_rate": 0.00015360622115877628,
      "loss": 1.1711,
      "step": 8108
    },
    {
      "epoch": 0.034648811711118895,
      "grad_norm": 1.4794048070907593,
      "learning_rate": 0.00015356349341992824,
      "loss": 0.5299,
      "step": 8109
    },
    {
      "epoch": 0.034653084594546094,
      "grad_norm": 1.3516499996185303,
      "learning_rate": 0.00015352076568108018,
      "loss": 0.5421,
      "step": 8110
    },
    {
      "epoch": 0.034657357477973286,
      "grad_norm": 0.7808311581611633,
      "learning_rate": 0.00015347803794223212,
      "loss": 0.2163,
      "step": 8111
    },
    {
      "epoch": 0.03466163036140048,
      "grad_norm": 2.167447566986084,
      "learning_rate": 0.00015343531020338403,
      "loss": 0.4812,
      "step": 8112
    },
    {
      "epoch": 0.03466590324482768,
      "grad_norm": 1.1567732095718384,
      "learning_rate": 0.00015339258246453597,
      "loss": 0.5744,
      "step": 8113
    },
    {
      "epoch": 0.03467017612825487,
      "grad_norm": 0.6488536596298218,
      "learning_rate": 0.0001533498547256879,
      "loss": 0.2383,
      "step": 8114
    },
    {
      "epoch": 0.03467444901168206,
      "grad_norm": 0.883246898651123,
      "learning_rate": 0.00015330712698683984,
      "loss": 0.2902,
      "step": 8115
    },
    {
      "epoch": 0.03467872189510926,
      "grad_norm": 0.9247557520866394,
      "learning_rate": 0.0001532643992479918,
      "loss": 0.3893,
      "step": 8116
    },
    {
      "epoch": 0.03468299477853645,
      "grad_norm": 2.178208589553833,
      "learning_rate": 0.00015322167150914374,
      "loss": 0.6033,
      "step": 8117
    },
    {
      "epoch": 0.034687267661963644,
      "grad_norm": 1.415057897567749,
      "learning_rate": 0.00015317894377029568,
      "loss": 0.5396,
      "step": 8118
    },
    {
      "epoch": 0.034691540545390843,
      "grad_norm": 2.3635783195495605,
      "learning_rate": 0.00015313621603144762,
      "loss": 0.6701,
      "step": 8119
    },
    {
      "epoch": 0.034695813428818036,
      "grad_norm": 1.397658109664917,
      "learning_rate": 0.00015309348829259956,
      "loss": 0.4956,
      "step": 8120
    },
    {
      "epoch": 0.03470008631224523,
      "grad_norm": 0.43950459361076355,
      "learning_rate": 0.0001530507605537515,
      "loss": 0.1227,
      "step": 8121
    },
    {
      "epoch": 0.03470435919567243,
      "grad_norm": 1.540270209312439,
      "learning_rate": 0.00015300803281490343,
      "loss": 0.3748,
      "step": 8122
    },
    {
      "epoch": 0.03470863207909962,
      "grad_norm": 1.7915648221969604,
      "learning_rate": 0.00015296530507605537,
      "loss": 0.5141,
      "step": 8123
    },
    {
      "epoch": 0.03471290496252681,
      "grad_norm": 5.500400066375732,
      "learning_rate": 0.00015292257733720734,
      "loss": 1.7897,
      "step": 8124
    },
    {
      "epoch": 0.03471717784595401,
      "grad_norm": 3.2539048194885254,
      "learning_rate": 0.00015287984959835927,
      "loss": 1.014,
      "step": 8125
    },
    {
      "epoch": 0.0347214507293812,
      "grad_norm": 1.5233808755874634,
      "learning_rate": 0.0001528371218595112,
      "loss": 0.6018,
      "step": 8126
    },
    {
      "epoch": 0.034725723612808394,
      "grad_norm": 4.022542953491211,
      "learning_rate": 0.00015279439412066315,
      "loss": 0.8941,
      "step": 8127
    },
    {
      "epoch": 0.034729996496235586,
      "grad_norm": 2.487835645675659,
      "learning_rate": 0.0001527516663818151,
      "loss": 0.7085,
      "step": 8128
    },
    {
      "epoch": 0.034734269379662785,
      "grad_norm": 1.4258118867874146,
      "learning_rate": 0.000152708938642967,
      "loss": 0.4788,
      "step": 8129
    },
    {
      "epoch": 0.03473854226308998,
      "grad_norm": 2.6267151832580566,
      "learning_rate": 0.00015266621090411894,
      "loss": 0.6534,
      "step": 8130
    },
    {
      "epoch": 0.03474281514651717,
      "grad_norm": 1.5638025999069214,
      "learning_rate": 0.0001526234831652709,
      "loss": 1.0058,
      "step": 8131
    },
    {
      "epoch": 0.03474708802994437,
      "grad_norm": 0.9552815556526184,
      "learning_rate": 0.00015258075542642284,
      "loss": 0.2963,
      "step": 8132
    },
    {
      "epoch": 0.03475136091337156,
      "grad_norm": 1.5572401285171509,
      "learning_rate": 0.00015253802768757478,
      "loss": 1.0037,
      "step": 8133
    },
    {
      "epoch": 0.03475563379679875,
      "grad_norm": 2.78517746925354,
      "learning_rate": 0.00015249529994872671,
      "loss": 0.9925,
      "step": 8134
    },
    {
      "epoch": 0.03475990668022595,
      "grad_norm": 1.4181965589523315,
      "learning_rate": 0.00015245257220987865,
      "loss": 0.4521,
      "step": 8135
    },
    {
      "epoch": 0.034764179563653144,
      "grad_norm": 1.54062020778656,
      "learning_rate": 0.0001524098444710306,
      "loss": 0.6857,
      "step": 8136
    },
    {
      "epoch": 0.034768452447080336,
      "grad_norm": 1.4062292575836182,
      "learning_rate": 0.00015236711673218253,
      "loss": 0.4309,
      "step": 8137
    },
    {
      "epoch": 0.034772725330507535,
      "grad_norm": 1.434025526046753,
      "learning_rate": 0.00015232438899333447,
      "loss": 0.6184,
      "step": 8138
    },
    {
      "epoch": 0.03477699821393473,
      "grad_norm": 2.0408623218536377,
      "learning_rate": 0.00015228166125448643,
      "loss": 0.9639,
      "step": 8139
    },
    {
      "epoch": 0.03478127109736192,
      "grad_norm": 2.365178346633911,
      "learning_rate": 0.00015223893351563837,
      "loss": 0.5843,
      "step": 8140
    },
    {
      "epoch": 0.03478554398078912,
      "grad_norm": 3.306382417678833,
      "learning_rate": 0.0001521962057767903,
      "loss": 1.2513,
      "step": 8141
    },
    {
      "epoch": 0.03478981686421631,
      "grad_norm": 0.8711188435554504,
      "learning_rate": 0.00015215347803794224,
      "loss": 0.3079,
      "step": 8142
    },
    {
      "epoch": 0.0347940897476435,
      "grad_norm": 5.4282941818237305,
      "learning_rate": 0.00015211075029909418,
      "loss": 1.2863,
      "step": 8143
    },
    {
      "epoch": 0.0347983626310707,
      "grad_norm": 4.433421611785889,
      "learning_rate": 0.00015206802256024612,
      "loss": 1.0619,
      "step": 8144
    },
    {
      "epoch": 0.034802635514497894,
      "grad_norm": 0.739106297492981,
      "learning_rate": 0.00015202529482139803,
      "loss": 0.2286,
      "step": 8145
    },
    {
      "epoch": 0.034806908397925086,
      "grad_norm": 1.429979681968689,
      "learning_rate": 0.00015198256708255,
      "loss": 0.4918,
      "step": 8146
    },
    {
      "epoch": 0.034811181281352285,
      "grad_norm": 0.9422167539596558,
      "learning_rate": 0.00015193983934370193,
      "loss": 0.3638,
      "step": 8147
    },
    {
      "epoch": 0.03481545416477948,
      "grad_norm": 4.077033042907715,
      "learning_rate": 0.00015189711160485387,
      "loss": 1.3227,
      "step": 8148
    },
    {
      "epoch": 0.03481972704820667,
      "grad_norm": 1.7938127517700195,
      "learning_rate": 0.0001518543838660058,
      "loss": 0.4417,
      "step": 8149
    },
    {
      "epoch": 0.03482399993163387,
      "grad_norm": 1.3085492849349976,
      "learning_rate": 0.00015181165612715775,
      "loss": 0.5291,
      "step": 8150
    },
    {
      "epoch": 0.03482827281506106,
      "grad_norm": 1.5711885690689087,
      "learning_rate": 0.00015176892838830968,
      "loss": 0.9507,
      "step": 8151
    },
    {
      "epoch": 0.03483254569848825,
      "grad_norm": 5.000288963317871,
      "learning_rate": 0.00015172620064946162,
      "loss": 1.3127,
      "step": 8152
    },
    {
      "epoch": 0.034836818581915445,
      "grad_norm": 2.9102602005004883,
      "learning_rate": 0.0001516834729106136,
      "loss": 0.932,
      "step": 8153
    },
    {
      "epoch": 0.034841091465342644,
      "grad_norm": 0.6431039571762085,
      "learning_rate": 0.00015164074517176552,
      "loss": 0.2477,
      "step": 8154
    },
    {
      "epoch": 0.034845364348769836,
      "grad_norm": 1.4379196166992188,
      "learning_rate": 0.00015159801743291746,
      "loss": 0.8707,
      "step": 8155
    },
    {
      "epoch": 0.03484963723219703,
      "grad_norm": 2.7663185596466064,
      "learning_rate": 0.0001515552896940694,
      "loss": 0.6343,
      "step": 8156
    },
    {
      "epoch": 0.03485391011562423,
      "grad_norm": 0.8389289975166321,
      "learning_rate": 0.00015151256195522134,
      "loss": 0.3089,
      "step": 8157
    },
    {
      "epoch": 0.03485818299905142,
      "grad_norm": 3.522834062576294,
      "learning_rate": 0.00015146983421637328,
      "loss": 1.1396,
      "step": 8158
    },
    {
      "epoch": 0.03486245588247861,
      "grad_norm": 0.58942049741745,
      "learning_rate": 0.00015142710647752521,
      "loss": 0.2305,
      "step": 8159
    },
    {
      "epoch": 0.03486672876590581,
      "grad_norm": 1.568734884262085,
      "learning_rate": 0.00015138437873867715,
      "loss": 0.6658,
      "step": 8160
    },
    {
      "epoch": 0.034871001649333,
      "grad_norm": 2.6778981685638428,
      "learning_rate": 0.00015134165099982912,
      "loss": 0.5848,
      "step": 8161
    },
    {
      "epoch": 0.034875274532760195,
      "grad_norm": 5.292228698730469,
      "learning_rate": 0.00015129892326098103,
      "loss": 1.2083,
      "step": 8162
    },
    {
      "epoch": 0.034879547416187394,
      "grad_norm": 1.2856234312057495,
      "learning_rate": 0.00015125619552213297,
      "loss": 0.796,
      "step": 8163
    },
    {
      "epoch": 0.034883820299614586,
      "grad_norm": 2.841520071029663,
      "learning_rate": 0.0001512134677832849,
      "loss": 0.8491,
      "step": 8164
    },
    {
      "epoch": 0.03488809318304178,
      "grad_norm": 1.8895665407180786,
      "learning_rate": 0.00015117074004443684,
      "loss": 0.6282,
      "step": 8165
    },
    {
      "epoch": 0.03489236606646898,
      "grad_norm": 1.315463900566101,
      "learning_rate": 0.00015112801230558878,
      "loss": 0.4515,
      "step": 8166
    },
    {
      "epoch": 0.03489663894989617,
      "grad_norm": 1.1720924377441406,
      "learning_rate": 0.00015108528456674072,
      "loss": 0.4046,
      "step": 8167
    },
    {
      "epoch": 0.03490091183332336,
      "grad_norm": 2.263869285583496,
      "learning_rate": 0.00015104255682789268,
      "loss": 0.6009,
      "step": 8168
    },
    {
      "epoch": 0.03490518471675056,
      "grad_norm": 2.0584328174591064,
      "learning_rate": 0.00015099982908904462,
      "loss": 0.609,
      "step": 8169
    },
    {
      "epoch": 0.03490945760017775,
      "grad_norm": 5.1762776374816895,
      "learning_rate": 0.00015095710135019656,
      "loss": 1.2813,
      "step": 8170
    },
    {
      "epoch": 0.034913730483604945,
      "grad_norm": 2.0812950134277344,
      "learning_rate": 0.0001509143736113485,
      "loss": 0.7521,
      "step": 8171
    },
    {
      "epoch": 0.034918003367032144,
      "grad_norm": 1.5603764057159424,
      "learning_rate": 0.00015087164587250043,
      "loss": 0.5546,
      "step": 8172
    },
    {
      "epoch": 0.034922276250459336,
      "grad_norm": 1.2489640712738037,
      "learning_rate": 0.00015082891813365237,
      "loss": 0.7295,
      "step": 8173
    },
    {
      "epoch": 0.03492654913388653,
      "grad_norm": 0.7891145348548889,
      "learning_rate": 0.0001507861903948043,
      "loss": 0.2424,
      "step": 8174
    },
    {
      "epoch": 0.03493082201731373,
      "grad_norm": 0.8612614870071411,
      "learning_rate": 0.00015074346265595625,
      "loss": 0.2422,
      "step": 8175
    },
    {
      "epoch": 0.03493509490074092,
      "grad_norm": 0.6839756965637207,
      "learning_rate": 0.0001507007349171082,
      "loss": 0.2859,
      "step": 8176
    },
    {
      "epoch": 0.03493936778416811,
      "grad_norm": 1.273996114730835,
      "learning_rate": 0.00015065800717826015,
      "loss": 0.7135,
      "step": 8177
    },
    {
      "epoch": 0.0349436406675953,
      "grad_norm": 2.4798710346221924,
      "learning_rate": 0.00015061527943941206,
      "loss": 0.754,
      "step": 8178
    },
    {
      "epoch": 0.0349479135510225,
      "grad_norm": 1.3229554891586304,
      "learning_rate": 0.000150572551700564,
      "loss": 0.5041,
      "step": 8179
    },
    {
      "epoch": 0.034952186434449695,
      "grad_norm": 4.912105560302734,
      "learning_rate": 0.00015052982396171594,
      "loss": 1.1062,
      "step": 8180
    },
    {
      "epoch": 0.03495645931787689,
      "grad_norm": 2.1617722511291504,
      "learning_rate": 0.00015048709622286787,
      "loss": 0.6623,
      "step": 8181
    },
    {
      "epoch": 0.034960732201304086,
      "grad_norm": 4.084399700164795,
      "learning_rate": 0.0001504443684840198,
      "loss": 1.0506,
      "step": 8182
    },
    {
      "epoch": 0.03496500508473128,
      "grad_norm": 1.6985019445419312,
      "learning_rate": 0.00015040164074517178,
      "loss": 0.6705,
      "step": 8183
    },
    {
      "epoch": 0.03496927796815847,
      "grad_norm": 0.9203706383705139,
      "learning_rate": 0.00015035891300632371,
      "loss": 0.3352,
      "step": 8184
    },
    {
      "epoch": 0.03497355085158567,
      "grad_norm": 1.3031338453292847,
      "learning_rate": 0.00015031618526747565,
      "loss": 0.5946,
      "step": 8185
    },
    {
      "epoch": 0.03497782373501286,
      "grad_norm": 4.029068470001221,
      "learning_rate": 0.0001502734575286276,
      "loss": 1.0292,
      "step": 8186
    },
    {
      "epoch": 0.03498209661844005,
      "grad_norm": 4.8187055587768555,
      "learning_rate": 0.00015023072978977953,
      "loss": 1.1874,
      "step": 8187
    },
    {
      "epoch": 0.03498636950186725,
      "grad_norm": 2.216187000274658,
      "learning_rate": 0.00015018800205093147,
      "loss": 0.5353,
      "step": 8188
    },
    {
      "epoch": 0.034990642385294445,
      "grad_norm": 2.223447322845459,
      "learning_rate": 0.0001501452743120834,
      "loss": 0.8319,
      "step": 8189
    },
    {
      "epoch": 0.03499491526872164,
      "grad_norm": 1.1253479719161987,
      "learning_rate": 0.00015010254657323534,
      "loss": 0.3812,
      "step": 8190
    },
    {
      "epoch": 0.034999188152148836,
      "grad_norm": 4.72371244430542,
      "learning_rate": 0.0001500598188343873,
      "loss": 1.0019,
      "step": 8191
    },
    {
      "epoch": 0.03500346103557603,
      "grad_norm": 1.0640267133712769,
      "learning_rate": 0.00015001709109553924,
      "loss": 0.4183,
      "step": 8192
    },
    {
      "epoch": 0.03500773391900322,
      "grad_norm": 0.5383038520812988,
      "learning_rate": 0.00014997436335669118,
      "loss": 0.2143,
      "step": 8193
    },
    {
      "epoch": 0.03501200680243042,
      "grad_norm": 5.212873458862305,
      "learning_rate": 0.00014993163561784312,
      "loss": 1.5609,
      "step": 8194
    },
    {
      "epoch": 0.03501627968585761,
      "grad_norm": 0.7899866104125977,
      "learning_rate": 0.00014988890787899503,
      "loss": 0.2934,
      "step": 8195
    },
    {
      "epoch": 0.0350205525692848,
      "grad_norm": 2.0485496520996094,
      "learning_rate": 0.00014984618014014697,
      "loss": 0.439,
      "step": 8196
    },
    {
      "epoch": 0.035024825452712,
      "grad_norm": 3.1331377029418945,
      "learning_rate": 0.0001498034524012989,
      "loss": 0.9088,
      "step": 8197
    },
    {
      "epoch": 0.035029098336139194,
      "grad_norm": 2.6635513305664062,
      "learning_rate": 0.00014976072466245087,
      "loss": 1.0492,
      "step": 8198
    },
    {
      "epoch": 0.03503337121956639,
      "grad_norm": 2.652019739151001,
      "learning_rate": 0.0001497179969236028,
      "loss": 0.9305,
      "step": 8199
    },
    {
      "epoch": 0.03503764410299358,
      "grad_norm": 1.2731738090515137,
      "learning_rate": 0.00014967526918475475,
      "loss": 0.4323,
      "step": 8200
    },
    {
      "epoch": 0.03504191698642078,
      "grad_norm": 2.536137580871582,
      "learning_rate": 0.00014963254144590668,
      "loss": 0.5394,
      "step": 8201
    },
    {
      "epoch": 0.03504618986984797,
      "grad_norm": 0.7344000935554504,
      "learning_rate": 0.00014958981370705862,
      "loss": 0.2714,
      "step": 8202
    },
    {
      "epoch": 0.03505046275327516,
      "grad_norm": 1.33319091796875,
      "learning_rate": 0.00014954708596821056,
      "loss": 0.475,
      "step": 8203
    },
    {
      "epoch": 0.03505473563670236,
      "grad_norm": 2.0357320308685303,
      "learning_rate": 0.0001495043582293625,
      "loss": 0.7296,
      "step": 8204
    },
    {
      "epoch": 0.03505900852012955,
      "grad_norm": 2.0088651180267334,
      "learning_rate": 0.00014946163049051444,
      "loss": 0.6316,
      "step": 8205
    },
    {
      "epoch": 0.035063281403556745,
      "grad_norm": 0.5729345679283142,
      "learning_rate": 0.0001494189027516664,
      "loss": 0.2143,
      "step": 8206
    },
    {
      "epoch": 0.035067554286983944,
      "grad_norm": 1.500962257385254,
      "learning_rate": 0.00014937617501281834,
      "loss": 0.6355,
      "step": 8207
    },
    {
      "epoch": 0.035071827170411136,
      "grad_norm": 0.622626543045044,
      "learning_rate": 0.00014933344727397028,
      "loss": 0.2761,
      "step": 8208
    },
    {
      "epoch": 0.03507610005383833,
      "grad_norm": 3.081399917602539,
      "learning_rate": 0.00014929071953512221,
      "loss": 2.0033,
      "step": 8209
    },
    {
      "epoch": 0.03508037293726553,
      "grad_norm": 1.0966936349868774,
      "learning_rate": 0.00014924799179627415,
      "loss": 0.3888,
      "step": 8210
    },
    {
      "epoch": 0.03508464582069272,
      "grad_norm": 1.890091896057129,
      "learning_rate": 0.00014920526405742606,
      "loss": 0.3468,
      "step": 8211
    },
    {
      "epoch": 0.03508891870411991,
      "grad_norm": 3.2330164909362793,
      "learning_rate": 0.000149162536318578,
      "loss": 0.8906,
      "step": 8212
    },
    {
      "epoch": 0.03509319158754711,
      "grad_norm": 2.441892147064209,
      "learning_rate": 0.00014911980857972997,
      "loss": 0.9769,
      "step": 8213
    },
    {
      "epoch": 0.0350974644709743,
      "grad_norm": 1.7351932525634766,
      "learning_rate": 0.0001490770808408819,
      "loss": 0.5434,
      "step": 8214
    },
    {
      "epoch": 0.035101737354401495,
      "grad_norm": 2.207958459854126,
      "learning_rate": 0.00014903435310203384,
      "loss": 0.8975,
      "step": 8215
    },
    {
      "epoch": 0.035106010237828694,
      "grad_norm": 5.0113043785095215,
      "learning_rate": 0.00014899162536318578,
      "loss": 1.3231,
      "step": 8216
    },
    {
      "epoch": 0.035110283121255886,
      "grad_norm": 0.343275249004364,
      "learning_rate": 0.00014894889762433772,
      "loss": 0.0961,
      "step": 8217
    },
    {
      "epoch": 0.03511455600468308,
      "grad_norm": 4.208591938018799,
      "learning_rate": 0.00014890616988548966,
      "loss": 0.928,
      "step": 8218
    },
    {
      "epoch": 0.03511882888811028,
      "grad_norm": 0.3854166567325592,
      "learning_rate": 0.0001488634421466416,
      "loss": 0.1444,
      "step": 8219
    },
    {
      "epoch": 0.03512310177153747,
      "grad_norm": 3.0933446884155273,
      "learning_rate": 0.00014882071440779356,
      "loss": 1.2327,
      "step": 8220
    },
    {
      "epoch": 0.03512737465496466,
      "grad_norm": 2.0478932857513428,
      "learning_rate": 0.0001487779866689455,
      "loss": 0.5993,
      "step": 8221
    },
    {
      "epoch": 0.03513164753839186,
      "grad_norm": 2.2687363624572754,
      "learning_rate": 0.00014873525893009743,
      "loss": 0.7852,
      "step": 8222
    },
    {
      "epoch": 0.03513592042181905,
      "grad_norm": 2.0391335487365723,
      "learning_rate": 0.00014869253119124937,
      "loss": 0.6756,
      "step": 8223
    },
    {
      "epoch": 0.035140193305246245,
      "grad_norm": 1.273581624031067,
      "learning_rate": 0.0001486498034524013,
      "loss": 0.5985,
      "step": 8224
    },
    {
      "epoch": 0.03514446618867344,
      "grad_norm": 1.1482704877853394,
      "learning_rate": 0.00014860707571355325,
      "loss": 0.4588,
      "step": 8225
    },
    {
      "epoch": 0.035148739072100636,
      "grad_norm": 1.7024343013763428,
      "learning_rate": 0.00014856434797470519,
      "loss": 1.2416,
      "step": 8226
    },
    {
      "epoch": 0.03515301195552783,
      "grad_norm": 2.02683424949646,
      "learning_rate": 0.0001485216202358571,
      "loss": 0.6613,
      "step": 8227
    },
    {
      "epoch": 0.03515728483895502,
      "grad_norm": 1.2253764867782593,
      "learning_rate": 0.00014847889249700906,
      "loss": 0.5618,
      "step": 8228
    },
    {
      "epoch": 0.03516155772238222,
      "grad_norm": 4.8010334968566895,
      "learning_rate": 0.000148436164758161,
      "loss": 1.0993,
      "step": 8229
    },
    {
      "epoch": 0.03516583060580941,
      "grad_norm": 3.1558947563171387,
      "learning_rate": 0.00014839343701931294,
      "loss": 0.7788,
      "step": 8230
    },
    {
      "epoch": 0.035170103489236604,
      "grad_norm": 2.4729764461517334,
      "learning_rate": 0.00014835070928046487,
      "loss": 0.4861,
      "step": 8231
    },
    {
      "epoch": 0.0351743763726638,
      "grad_norm": 1.5656448602676392,
      "learning_rate": 0.0001483079815416168,
      "loss": 0.6852,
      "step": 8232
    },
    {
      "epoch": 0.035178649256090995,
      "grad_norm": 3.6355576515197754,
      "learning_rate": 0.00014826525380276875,
      "loss": 0.9689,
      "step": 8233
    },
    {
      "epoch": 0.03518292213951819,
      "grad_norm": 1.4570528268814087,
      "learning_rate": 0.0001482225260639207,
      "loss": 0.6617,
      "step": 8234
    },
    {
      "epoch": 0.035187195022945386,
      "grad_norm": 1.0011136531829834,
      "learning_rate": 0.00014817979832507265,
      "loss": 0.3888,
      "step": 8235
    },
    {
      "epoch": 0.03519146790637258,
      "grad_norm": 1.8027803897857666,
      "learning_rate": 0.0001481370705862246,
      "loss": 0.5209,
      "step": 8236
    },
    {
      "epoch": 0.03519574078979977,
      "grad_norm": 1.271705985069275,
      "learning_rate": 0.00014809434284737653,
      "loss": 0.5613,
      "step": 8237
    },
    {
      "epoch": 0.03520001367322697,
      "grad_norm": 0.7605855464935303,
      "learning_rate": 0.00014805161510852847,
      "loss": 0.2765,
      "step": 8238
    },
    {
      "epoch": 0.03520428655665416,
      "grad_norm": 2.123541831970215,
      "learning_rate": 0.0001480088873696804,
      "loss": 0.6716,
      "step": 8239
    },
    {
      "epoch": 0.035208559440081354,
      "grad_norm": 0.5234907269477844,
      "learning_rate": 0.00014796615963083234,
      "loss": 0.2237,
      "step": 8240
    },
    {
      "epoch": 0.03521283232350855,
      "grad_norm": 3.9538700580596924,
      "learning_rate": 0.00014792343189198428,
      "loss": 1.0139,
      "step": 8241
    },
    {
      "epoch": 0.035217105206935745,
      "grad_norm": 2.1550612449645996,
      "learning_rate": 0.00014788070415313622,
      "loss": 0.6252,
      "step": 8242
    },
    {
      "epoch": 0.03522137809036294,
      "grad_norm": 4.685706615447998,
      "learning_rate": 0.00014783797641428818,
      "loss": 1.0992,
      "step": 8243
    },
    {
      "epoch": 0.035225650973790136,
      "grad_norm": 3.4012134075164795,
      "learning_rate": 0.0001477952486754401,
      "loss": 1.5804,
      "step": 8244
    },
    {
      "epoch": 0.03522992385721733,
      "grad_norm": 1.2793700695037842,
      "learning_rate": 0.00014775252093659203,
      "loss": 0.5242,
      "step": 8245
    },
    {
      "epoch": 0.03523419674064452,
      "grad_norm": 2.2290830612182617,
      "learning_rate": 0.00014770979319774397,
      "loss": 0.7937,
      "step": 8246
    },
    {
      "epoch": 0.03523846962407172,
      "grad_norm": 1.1147359609603882,
      "learning_rate": 0.0001476670654588959,
      "loss": 0.4713,
      "step": 8247
    },
    {
      "epoch": 0.03524274250749891,
      "grad_norm": 2.8905704021453857,
      "learning_rate": 0.00014762433772004784,
      "loss": 1.8729,
      "step": 8248
    },
    {
      "epoch": 0.035247015390926104,
      "grad_norm": 2.95727801322937,
      "learning_rate": 0.00014758160998119978,
      "loss": 1.0774,
      "step": 8249
    },
    {
      "epoch": 0.035251288274353296,
      "grad_norm": 0.3764476776123047,
      "learning_rate": 0.00014753888224235175,
      "loss": 0.1511,
      "step": 8250
    },
    {
      "epoch": 0.035255561157780495,
      "grad_norm": 0.7815670371055603,
      "learning_rate": 0.00014749615450350369,
      "loss": 0.3346,
      "step": 8251
    },
    {
      "epoch": 0.03525983404120769,
      "grad_norm": 1.7704715728759766,
      "learning_rate": 0.00014745342676465562,
      "loss": 1.2347,
      "step": 8252
    },
    {
      "epoch": 0.03526410692463488,
      "grad_norm": 4.065123558044434,
      "learning_rate": 0.00014741069902580756,
      "loss": 1.0402,
      "step": 8253
    },
    {
      "epoch": 0.03526837980806208,
      "grad_norm": 0.8695725202560425,
      "learning_rate": 0.0001473679712869595,
      "loss": 0.4087,
      "step": 8254
    },
    {
      "epoch": 0.03527265269148927,
      "grad_norm": 3.107877016067505,
      "learning_rate": 0.00014732524354811144,
      "loss": 0.7808,
      "step": 8255
    },
    {
      "epoch": 0.03527692557491646,
      "grad_norm": 1.2652994394302368,
      "learning_rate": 0.00014728251580926337,
      "loss": 0.5543,
      "step": 8256
    },
    {
      "epoch": 0.03528119845834366,
      "grad_norm": 1.727304458618164,
      "learning_rate": 0.0001472397880704153,
      "loss": 1.2177,
      "step": 8257
    },
    {
      "epoch": 0.035285471341770853,
      "grad_norm": 3.5531389713287354,
      "learning_rate": 0.00014719706033156728,
      "loss": 0.8681,
      "step": 8258
    },
    {
      "epoch": 0.035289744225198046,
      "grad_norm": 1.1501139402389526,
      "learning_rate": 0.00014715433259271922,
      "loss": 0.8051,
      "step": 8259
    },
    {
      "epoch": 0.035294017108625245,
      "grad_norm": 3.601184368133545,
      "learning_rate": 0.00014711160485387113,
      "loss": 1.1922,
      "step": 8260
    },
    {
      "epoch": 0.03529828999205244,
      "grad_norm": 2.804246664047241,
      "learning_rate": 0.00014706887711502306,
      "loss": 0.826,
      "step": 8261
    },
    {
      "epoch": 0.03530256287547963,
      "grad_norm": 0.7602638602256775,
      "learning_rate": 0.000147026149376175,
      "loss": 0.2759,
      "step": 8262
    },
    {
      "epoch": 0.03530683575890683,
      "grad_norm": 1.7123706340789795,
      "learning_rate": 0.00014698342163732694,
      "loss": 0.5537,
      "step": 8263
    },
    {
      "epoch": 0.03531110864233402,
      "grad_norm": 0.968148410320282,
      "learning_rate": 0.00014694069389847888,
      "loss": 0.399,
      "step": 8264
    },
    {
      "epoch": 0.03531538152576121,
      "grad_norm": 1.5479578971862793,
      "learning_rate": 0.00014689796615963084,
      "loss": 0.5657,
      "step": 8265
    },
    {
      "epoch": 0.03531965440918841,
      "grad_norm": 0.8448352813720703,
      "learning_rate": 0.00014685523842078278,
      "loss": 0.352,
      "step": 8266
    },
    {
      "epoch": 0.0353239272926156,
      "grad_norm": 1.3247090578079224,
      "learning_rate": 0.00014681251068193472,
      "loss": 0.5028,
      "step": 8267
    },
    {
      "epoch": 0.035328200176042795,
      "grad_norm": 0.6189411878585815,
      "learning_rate": 0.00014676978294308666,
      "loss": 0.2285,
      "step": 8268
    },
    {
      "epoch": 0.035332473059469995,
      "grad_norm": 0.48378127813339233,
      "learning_rate": 0.0001467270552042386,
      "loss": 0.2245,
      "step": 8269
    },
    {
      "epoch": 0.03533674594289719,
      "grad_norm": 1.214937448501587,
      "learning_rate": 0.00014668432746539053,
      "loss": 0.4293,
      "step": 8270
    },
    {
      "epoch": 0.03534101882632438,
      "grad_norm": 1.522932529449463,
      "learning_rate": 0.00014664159972654247,
      "loss": 0.5493,
      "step": 8271
    },
    {
      "epoch": 0.03534529170975158,
      "grad_norm": 0.3182483911514282,
      "learning_rate": 0.00014659887198769443,
      "loss": 0.1176,
      "step": 8272
    },
    {
      "epoch": 0.03534956459317877,
      "grad_norm": 1.1158844232559204,
      "learning_rate": 0.00014655614424884637,
      "loss": 0.7578,
      "step": 8273
    },
    {
      "epoch": 0.03535383747660596,
      "grad_norm": 3.970268726348877,
      "learning_rate": 0.0001465134165099983,
      "loss": 2.3737,
      "step": 8274
    },
    {
      "epoch": 0.035358110360033154,
      "grad_norm": 2.3178796768188477,
      "learning_rate": 0.00014647068877115025,
      "loss": 0.6546,
      "step": 8275
    },
    {
      "epoch": 0.03536238324346035,
      "grad_norm": 0.6117743849754333,
      "learning_rate": 0.00014642796103230219,
      "loss": 0.2689,
      "step": 8276
    },
    {
      "epoch": 0.035366656126887545,
      "grad_norm": 1.238526463508606,
      "learning_rate": 0.0001463852332934541,
      "loss": 0.5233,
      "step": 8277
    },
    {
      "epoch": 0.03537092901031474,
      "grad_norm": 1.5089482069015503,
      "learning_rate": 0.00014634250555460603,
      "loss": 1.0866,
      "step": 8278
    },
    {
      "epoch": 0.03537520189374194,
      "grad_norm": 0.6748366355895996,
      "learning_rate": 0.00014629977781575797,
      "loss": 0.286,
      "step": 8279
    },
    {
      "epoch": 0.03537947477716913,
      "grad_norm": 1.8500279188156128,
      "learning_rate": 0.00014625705007690994,
      "loss": 0.7451,
      "step": 8280
    },
    {
      "epoch": 0.03538374766059632,
      "grad_norm": 1.6185420751571655,
      "learning_rate": 0.00014621432233806187,
      "loss": 0.7383,
      "step": 8281
    },
    {
      "epoch": 0.03538802054402352,
      "grad_norm": 3.0350449085235596,
      "learning_rate": 0.0001461715945992138,
      "loss": 0.7662,
      "step": 8282
    },
    {
      "epoch": 0.03539229342745071,
      "grad_norm": 2.684901714324951,
      "learning_rate": 0.00014612886686036575,
      "loss": 0.8787,
      "step": 8283
    },
    {
      "epoch": 0.035396566310877904,
      "grad_norm": 1.593047022819519,
      "learning_rate": 0.0001460861391215177,
      "loss": 0.749,
      "step": 8284
    },
    {
      "epoch": 0.0354008391943051,
      "grad_norm": 1.4575961828231812,
      "learning_rate": 0.00014604341138266963,
      "loss": 1.0352,
      "step": 8285
    },
    {
      "epoch": 0.035405112077732295,
      "grad_norm": 1.5159740447998047,
      "learning_rate": 0.00014600068364382156,
      "loss": 0.7023,
      "step": 8286
    },
    {
      "epoch": 0.03540938496115949,
      "grad_norm": 4.381836891174316,
      "learning_rate": 0.00014595795590497353,
      "loss": 1.1514,
      "step": 8287
    },
    {
      "epoch": 0.035413657844586686,
      "grad_norm": 1.5603256225585938,
      "learning_rate": 0.00014591522816612547,
      "loss": 0.4311,
      "step": 8288
    },
    {
      "epoch": 0.03541793072801388,
      "grad_norm": 0.6679579019546509,
      "learning_rate": 0.0001458725004272774,
      "loss": 0.3635,
      "step": 8289
    },
    {
      "epoch": 0.03542220361144107,
      "grad_norm": 2.785388231277466,
      "learning_rate": 0.00014582977268842934,
      "loss": 0.6459,
      "step": 8290
    },
    {
      "epoch": 0.03542647649486827,
      "grad_norm": 0.7662797570228577,
      "learning_rate": 0.00014578704494958128,
      "loss": 0.278,
      "step": 8291
    },
    {
      "epoch": 0.03543074937829546,
      "grad_norm": 4.058149337768555,
      "learning_rate": 0.00014574431721073322,
      "loss": 1.1765,
      "step": 8292
    },
    {
      "epoch": 0.035435022261722654,
      "grad_norm": 1.0573697090148926,
      "learning_rate": 0.00014570158947188513,
      "loss": 0.7294,
      "step": 8293
    },
    {
      "epoch": 0.03543929514514985,
      "grad_norm": 1.5502318143844604,
      "learning_rate": 0.00014565886173303707,
      "loss": 0.6366,
      "step": 8294
    },
    {
      "epoch": 0.035443568028577045,
      "grad_norm": 2.6881768703460693,
      "learning_rate": 0.00014561613399418903,
      "loss": 0.9311,
      "step": 8295
    },
    {
      "epoch": 0.03544784091200424,
      "grad_norm": 1.0687298774719238,
      "learning_rate": 0.00014557340625534097,
      "loss": 0.7269,
      "step": 8296
    },
    {
      "epoch": 0.035452113795431436,
      "grad_norm": 1.3289793729782104,
      "learning_rate": 0.0001455306785164929,
      "loss": 0.6226,
      "step": 8297
    },
    {
      "epoch": 0.03545638667885863,
      "grad_norm": 1.8757344484329224,
      "learning_rate": 0.00014548795077764485,
      "loss": 0.5902,
      "step": 8298
    },
    {
      "epoch": 0.03546065956228582,
      "grad_norm": 1.3064485788345337,
      "learning_rate": 0.00014544522303879678,
      "loss": 0.4875,
      "step": 8299
    },
    {
      "epoch": 0.03546493244571301,
      "grad_norm": 1.3252689838409424,
      "learning_rate": 0.00014540249529994872,
      "loss": 0.604,
      "step": 8300
    },
    {
      "epoch": 0.03546920532914021,
      "grad_norm": 1.1047581434249878,
      "learning_rate": 0.00014535976756110066,
      "loss": 0.7266,
      "step": 8301
    },
    {
      "epoch": 0.035473478212567404,
      "grad_norm": 3.991410255432129,
      "learning_rate": 0.00014531703982225262,
      "loss": 0.876,
      "step": 8302
    },
    {
      "epoch": 0.035477751095994596,
      "grad_norm": 1.435654640197754,
      "learning_rate": 0.00014527431208340456,
      "loss": 0.7018,
      "step": 8303
    },
    {
      "epoch": 0.035482023979421795,
      "grad_norm": 0.6296265721321106,
      "learning_rate": 0.0001452315843445565,
      "loss": 0.1608,
      "step": 8304
    },
    {
      "epoch": 0.03548629686284899,
      "grad_norm": 2.688688278198242,
      "learning_rate": 0.00014518885660570844,
      "loss": 0.7797,
      "step": 8305
    },
    {
      "epoch": 0.03549056974627618,
      "grad_norm": 3.06008243560791,
      "learning_rate": 0.00014514612886686038,
      "loss": 1.3466,
      "step": 8306
    },
    {
      "epoch": 0.03549484262970338,
      "grad_norm": 2.931410789489746,
      "learning_rate": 0.0001451034011280123,
      "loss": 0.8871,
      "step": 8307
    },
    {
      "epoch": 0.03549911551313057,
      "grad_norm": 2.7202577590942383,
      "learning_rate": 0.00014506067338916425,
      "loss": 0.8588,
      "step": 8308
    },
    {
      "epoch": 0.03550338839655776,
      "grad_norm": 0.5036467909812927,
      "learning_rate": 0.0001450179456503162,
      "loss": 0.2223,
      "step": 8309
    },
    {
      "epoch": 0.03550766127998496,
      "grad_norm": 1.3074281215667725,
      "learning_rate": 0.00014497521791146813,
      "loss": 0.6071,
      "step": 8310
    },
    {
      "epoch": 0.035511934163412154,
      "grad_norm": 1.717614769935608,
      "learning_rate": 0.00014493249017262006,
      "loss": 0.4939,
      "step": 8311
    },
    {
      "epoch": 0.035516207046839346,
      "grad_norm": 0.5664821267127991,
      "learning_rate": 0.000144889762433772,
      "loss": 0.1074,
      "step": 8312
    },
    {
      "epoch": 0.035520479930266545,
      "grad_norm": 0.47279223799705505,
      "learning_rate": 0.00014484703469492394,
      "loss": 0.0889,
      "step": 8313
    },
    {
      "epoch": 0.03552475281369374,
      "grad_norm": 1.4748495817184448,
      "learning_rate": 0.00014480430695607588,
      "loss": 0.7132,
      "step": 8314
    },
    {
      "epoch": 0.03552902569712093,
      "grad_norm": 2.1099863052368164,
      "learning_rate": 0.00014476157921722782,
      "loss": 0.6178,
      "step": 8315
    },
    {
      "epoch": 0.03553329858054813,
      "grad_norm": 3.3607821464538574,
      "learning_rate": 0.00014471885147837975,
      "loss": 0.8773,
      "step": 8316
    },
    {
      "epoch": 0.03553757146397532,
      "grad_norm": 0.7938568592071533,
      "learning_rate": 0.00014467612373953172,
      "loss": 0.2896,
      "step": 8317
    },
    {
      "epoch": 0.03554184434740251,
      "grad_norm": 4.476802349090576,
      "learning_rate": 0.00014463339600068366,
      "loss": 1.5386,
      "step": 8318
    },
    {
      "epoch": 0.03554611723082971,
      "grad_norm": 1.4051311016082764,
      "learning_rate": 0.0001445906682618356,
      "loss": 0.6858,
      "step": 8319
    },
    {
      "epoch": 0.035550390114256904,
      "grad_norm": 0.6972342729568481,
      "learning_rate": 0.00014454794052298753,
      "loss": 0.3095,
      "step": 8320
    },
    {
      "epoch": 0.035554662997684096,
      "grad_norm": 0.7766733765602112,
      "learning_rate": 0.00014450521278413947,
      "loss": 0.3097,
      "step": 8321
    },
    {
      "epoch": 0.035558935881111295,
      "grad_norm": 0.8195583820343018,
      "learning_rate": 0.0001444624850452914,
      "loss": 0.29,
      "step": 8322
    },
    {
      "epoch": 0.03556320876453849,
      "grad_norm": 2.209465742111206,
      "learning_rate": 0.00014441975730644335,
      "loss": 0.5982,
      "step": 8323
    },
    {
      "epoch": 0.03556748164796568,
      "grad_norm": 1.3121092319488525,
      "learning_rate": 0.0001443770295675953,
      "loss": 0.6399,
      "step": 8324
    },
    {
      "epoch": 0.03557175453139287,
      "grad_norm": 0.6769144535064697,
      "learning_rate": 0.00014433430182874725,
      "loss": 0.2717,
      "step": 8325
    },
    {
      "epoch": 0.03557602741482007,
      "grad_norm": 1.1150134801864624,
      "learning_rate": 0.00014429157408989916,
      "loss": 0.7117,
      "step": 8326
    },
    {
      "epoch": 0.03558030029824726,
      "grad_norm": 2.972661256790161,
      "learning_rate": 0.0001442488463510511,
      "loss": 0.7709,
      "step": 8327
    },
    {
      "epoch": 0.035584573181674455,
      "grad_norm": 2.820812225341797,
      "learning_rate": 0.00014420611861220303,
      "loss": 0.6264,
      "step": 8328
    },
    {
      "epoch": 0.035588846065101654,
      "grad_norm": 1.2196460962295532,
      "learning_rate": 0.00014416339087335497,
      "loss": 0.5955,
      "step": 8329
    },
    {
      "epoch": 0.035593118948528846,
      "grad_norm": 1.6581252813339233,
      "learning_rate": 0.0001441206631345069,
      "loss": 0.4504,
      "step": 8330
    },
    {
      "epoch": 0.03559739183195604,
      "grad_norm": 1.14824378490448,
      "learning_rate": 0.00014407793539565885,
      "loss": 0.5084,
      "step": 8331
    },
    {
      "epoch": 0.03560166471538324,
      "grad_norm": 1.277030110359192,
      "learning_rate": 0.0001440352076568108,
      "loss": 0.5865,
      "step": 8332
    },
    {
      "epoch": 0.03560593759881043,
      "grad_norm": 1.1490213871002197,
      "learning_rate": 0.00014399247991796275,
      "loss": 0.5442,
      "step": 8333
    },
    {
      "epoch": 0.03561021048223762,
      "grad_norm": 0.6475090384483337,
      "learning_rate": 0.0001439497521791147,
      "loss": 0.2639,
      "step": 8334
    },
    {
      "epoch": 0.03561448336566482,
      "grad_norm": 3.2267632484436035,
      "learning_rate": 0.00014390702444026663,
      "loss": 1.0254,
      "step": 8335
    },
    {
      "epoch": 0.03561875624909201,
      "grad_norm": 2.3655104637145996,
      "learning_rate": 0.00014386429670141856,
      "loss": 0.7418,
      "step": 8336
    },
    {
      "epoch": 0.035623029132519204,
      "grad_norm": 2.331055164337158,
      "learning_rate": 0.0001438215689625705,
      "loss": 0.7247,
      "step": 8337
    },
    {
      "epoch": 0.035627302015946404,
      "grad_norm": 0.6939610838890076,
      "learning_rate": 0.00014377884122372244,
      "loss": 0.3444,
      "step": 8338
    },
    {
      "epoch": 0.035631574899373596,
      "grad_norm": 1.3207801580429077,
      "learning_rate": 0.0001437361134848744,
      "loss": 0.6285,
      "step": 8339
    },
    {
      "epoch": 0.03563584778280079,
      "grad_norm": 4.339291572570801,
      "learning_rate": 0.00014369338574602634,
      "loss": 0.8857,
      "step": 8340
    },
    {
      "epoch": 0.03564012066622799,
      "grad_norm": 2.236158847808838,
      "learning_rate": 0.00014365065800717828,
      "loss": 0.5246,
      "step": 8341
    },
    {
      "epoch": 0.03564439354965518,
      "grad_norm": 1.6458766460418701,
      "learning_rate": 0.0001436079302683302,
      "loss": 0.4191,
      "step": 8342
    },
    {
      "epoch": 0.03564866643308237,
      "grad_norm": 0.38954129815101624,
      "learning_rate": 0.00014356520252948213,
      "loss": 0.1078,
      "step": 8343
    },
    {
      "epoch": 0.03565293931650957,
      "grad_norm": 1.0460928678512573,
      "learning_rate": 0.00014352247479063407,
      "loss": 0.4825,
      "step": 8344
    },
    {
      "epoch": 0.03565721219993676,
      "grad_norm": 1.5707639455795288,
      "learning_rate": 0.000143479747051786,
      "loss": 0.6517,
      "step": 8345
    },
    {
      "epoch": 0.035661485083363954,
      "grad_norm": 4.821016311645508,
      "learning_rate": 0.00014343701931293794,
      "loss": 1.4476,
      "step": 8346
    },
    {
      "epoch": 0.03566575796679115,
      "grad_norm": 4.231935024261475,
      "learning_rate": 0.0001433942915740899,
      "loss": 1.1093,
      "step": 8347
    },
    {
      "epoch": 0.035670030850218346,
      "grad_norm": 0.68806391954422,
      "learning_rate": 0.00014335156383524185,
      "loss": 0.2644,
      "step": 8348
    },
    {
      "epoch": 0.03567430373364554,
      "grad_norm": 1.1303670406341553,
      "learning_rate": 0.00014330883609639378,
      "loss": 0.4884,
      "step": 8349
    },
    {
      "epoch": 0.03567857661707273,
      "grad_norm": 3.294635534286499,
      "learning_rate": 0.00014326610835754572,
      "loss": 0.756,
      "step": 8350
    },
    {
      "epoch": 0.03568284950049993,
      "grad_norm": 3.977576971054077,
      "learning_rate": 0.00014322338061869766,
      "loss": 1.0264,
      "step": 8351
    },
    {
      "epoch": 0.03568712238392712,
      "grad_norm": 0.5752542614936829,
      "learning_rate": 0.0001431806528798496,
      "loss": 0.2693,
      "step": 8352
    },
    {
      "epoch": 0.03569139526735431,
      "grad_norm": 1.3110114336013794,
      "learning_rate": 0.00014313792514100153,
      "loss": 0.6052,
      "step": 8353
    },
    {
      "epoch": 0.03569566815078151,
      "grad_norm": 1.0513228178024292,
      "learning_rate": 0.0001430951974021535,
      "loss": 0.4803,
      "step": 8354
    },
    {
      "epoch": 0.035699941034208704,
      "grad_norm": 3.714719295501709,
      "learning_rate": 0.00014305246966330544,
      "loss": 1.3108,
      "step": 8355
    },
    {
      "epoch": 0.035704213917635896,
      "grad_norm": 1.3855440616607666,
      "learning_rate": 0.00014300974192445738,
      "loss": 0.6583,
      "step": 8356
    },
    {
      "epoch": 0.035708486801063095,
      "grad_norm": 0.5979363322257996,
      "learning_rate": 0.0001429670141856093,
      "loss": 0.2163,
      "step": 8357
    },
    {
      "epoch": 0.03571275968449029,
      "grad_norm": 0.6549630165100098,
      "learning_rate": 0.00014292428644676125,
      "loss": 0.2535,
      "step": 8358
    },
    {
      "epoch": 0.03571703256791748,
      "grad_norm": 0.6051332354545593,
      "learning_rate": 0.00014288155870791316,
      "loss": 0.2223,
      "step": 8359
    },
    {
      "epoch": 0.03572130545134468,
      "grad_norm": 9.286688804626465,
      "learning_rate": 0.0001428388309690651,
      "loss": 4.1117,
      "step": 8360
    },
    {
      "epoch": 0.03572557833477187,
      "grad_norm": 1.4952266216278076,
      "learning_rate": 0.00014279610323021704,
      "loss": 0.557,
      "step": 8361
    },
    {
      "epoch": 0.03572985121819906,
      "grad_norm": 0.6774295568466187,
      "learning_rate": 0.000142753375491369,
      "loss": 0.3353,
      "step": 8362
    },
    {
      "epoch": 0.03573412410162626,
      "grad_norm": 3.4717020988464355,
      "learning_rate": 0.00014271064775252094,
      "loss": 0.7591,
      "step": 8363
    },
    {
      "epoch": 0.035738396985053454,
      "grad_norm": 1.9371848106384277,
      "learning_rate": 0.00014266792001367288,
      "loss": 0.7191,
      "step": 8364
    },
    {
      "epoch": 0.035742669868480646,
      "grad_norm": 0.6524847149848938,
      "learning_rate": 0.00014262519227482482,
      "loss": 0.3426,
      "step": 8365
    },
    {
      "epoch": 0.035746942751907845,
      "grad_norm": 0.5028567314147949,
      "learning_rate": 0.00014258246453597675,
      "loss": 0.2081,
      "step": 8366
    },
    {
      "epoch": 0.03575121563533504,
      "grad_norm": 0.5186127424240112,
      "learning_rate": 0.0001425397367971287,
      "loss": 0.2222,
      "step": 8367
    },
    {
      "epoch": 0.03575548851876223,
      "grad_norm": 0.6140841245651245,
      "learning_rate": 0.00014249700905828063,
      "loss": 0.2106,
      "step": 8368
    },
    {
      "epoch": 0.03575976140218943,
      "grad_norm": 1.9135743379592896,
      "learning_rate": 0.0001424542813194326,
      "loss": 0.6947,
      "step": 8369
    },
    {
      "epoch": 0.03576403428561662,
      "grad_norm": 0.7178189158439636,
      "learning_rate": 0.00014241155358058453,
      "loss": 0.1866,
      "step": 8370
    },
    {
      "epoch": 0.03576830716904381,
      "grad_norm": 0.6536548137664795,
      "learning_rate": 0.00014236882584173647,
      "loss": 0.2406,
      "step": 8371
    },
    {
      "epoch": 0.03577258005247101,
      "grad_norm": 1.2773571014404297,
      "learning_rate": 0.0001423260981028884,
      "loss": 0.3752,
      "step": 8372
    },
    {
      "epoch": 0.035776852935898204,
      "grad_norm": 2.563645839691162,
      "learning_rate": 0.00014228337036404035,
      "loss": 0.7048,
      "step": 8373
    },
    {
      "epoch": 0.035781125819325396,
      "grad_norm": 1.357308030128479,
      "learning_rate": 0.00014224064262519228,
      "loss": 0.5806,
      "step": 8374
    },
    {
      "epoch": 0.03578539870275259,
      "grad_norm": 1.1866354942321777,
      "learning_rate": 0.0001421979148863442,
      "loss": 0.3382,
      "step": 8375
    },
    {
      "epoch": 0.03578967158617979,
      "grad_norm": 1.8969193696975708,
      "learning_rate": 0.00014215518714749613,
      "loss": 1.3317,
      "step": 8376
    },
    {
      "epoch": 0.03579394446960698,
      "grad_norm": 0.4360004961490631,
      "learning_rate": 0.0001421124594086481,
      "loss": 0.1583,
      "step": 8377
    },
    {
      "epoch": 0.03579821735303417,
      "grad_norm": 4.4637861251831055,
      "learning_rate": 0.00014206973166980004,
      "loss": 0.8801,
      "step": 8378
    },
    {
      "epoch": 0.03580249023646137,
      "grad_norm": 4.847091197967529,
      "learning_rate": 0.00014202700393095197,
      "loss": 1.1728,
      "step": 8379
    },
    {
      "epoch": 0.03580676311988856,
      "grad_norm": 2.9444103240966797,
      "learning_rate": 0.0001419842761921039,
      "loss": 0.8239,
      "step": 8380
    },
    {
      "epoch": 0.035811036003315755,
      "grad_norm": 2.4817354679107666,
      "learning_rate": 0.00014194154845325585,
      "loss": 0.6748,
      "step": 8381
    },
    {
      "epoch": 0.035815308886742954,
      "grad_norm": 1.3687152862548828,
      "learning_rate": 0.0001418988207144078,
      "loss": 0.5601,
      "step": 8382
    },
    {
      "epoch": 0.035819581770170146,
      "grad_norm": 4.448215961456299,
      "learning_rate": 0.00014185609297555972,
      "loss": 1.2888,
      "step": 8383
    },
    {
      "epoch": 0.03582385465359734,
      "grad_norm": 0.9108883738517761,
      "learning_rate": 0.0001418133652367117,
      "loss": 0.3261,
      "step": 8384
    },
    {
      "epoch": 0.03582812753702454,
      "grad_norm": 2.8847577571868896,
      "learning_rate": 0.00014177063749786363,
      "loss": 0.8797,
      "step": 8385
    },
    {
      "epoch": 0.03583240042045173,
      "grad_norm": 1.562691569328308,
      "learning_rate": 0.00014172790975901557,
      "loss": 0.4948,
      "step": 8386
    },
    {
      "epoch": 0.03583667330387892,
      "grad_norm": 1.3081914186477661,
      "learning_rate": 0.0001416851820201675,
      "loss": 0.8843,
      "step": 8387
    },
    {
      "epoch": 0.03584094618730612,
      "grad_norm": 1.1769264936447144,
      "learning_rate": 0.00014164245428131944,
      "loss": 0.4512,
      "step": 8388
    },
    {
      "epoch": 0.03584521907073331,
      "grad_norm": 1.3199479579925537,
      "learning_rate": 0.00014159972654247138,
      "loss": 0.5108,
      "step": 8389
    },
    {
      "epoch": 0.035849491954160505,
      "grad_norm": 1.2973812818527222,
      "learning_rate": 0.00014155699880362332,
      "loss": 0.5046,
      "step": 8390
    },
    {
      "epoch": 0.035853764837587704,
      "grad_norm": 0.5402851104736328,
      "learning_rate": 0.00014151427106477528,
      "loss": 0.2758,
      "step": 8391
    },
    {
      "epoch": 0.035858037721014896,
      "grad_norm": 1.2335127592086792,
      "learning_rate": 0.0001414715433259272,
      "loss": 0.5097,
      "step": 8392
    },
    {
      "epoch": 0.03586231060444209,
      "grad_norm": 0.7151185870170593,
      "learning_rate": 0.00014142881558707913,
      "loss": 0.4014,
      "step": 8393
    },
    {
      "epoch": 0.03586658348786929,
      "grad_norm": 4.395257949829102,
      "learning_rate": 0.00014138608784823107,
      "loss": 1.0419,
      "step": 8394
    },
    {
      "epoch": 0.03587085637129648,
      "grad_norm": 2.3924872875213623,
      "learning_rate": 0.000141343360109383,
      "loss": 0.6109,
      "step": 8395
    },
    {
      "epoch": 0.03587512925472367,
      "grad_norm": 3.631957530975342,
      "learning_rate": 0.00014130063237053494,
      "loss": 1.2136,
      "step": 8396
    },
    {
      "epoch": 0.03587940213815087,
      "grad_norm": 1.1083002090454102,
      "learning_rate": 0.00014125790463168688,
      "loss": 0.3808,
      "step": 8397
    },
    {
      "epoch": 0.03588367502157806,
      "grad_norm": 3.2246859073638916,
      "learning_rate": 0.00014121517689283882,
      "loss": 0.8833,
      "step": 8398
    },
    {
      "epoch": 0.035887947905005255,
      "grad_norm": 1.2318607568740845,
      "learning_rate": 0.00014117244915399078,
      "loss": 0.4436,
      "step": 8399
    },
    {
      "epoch": 0.03589222078843245,
      "grad_norm": 1.2475130558013916,
      "learning_rate": 0.00014112972141514272,
      "loss": 0.4436,
      "step": 8400
    },
    {
      "epoch": 0.035896493671859646,
      "grad_norm": 4.798763275146484,
      "learning_rate": 0.00014108699367629466,
      "loss": 0.9843,
      "step": 8401
    },
    {
      "epoch": 0.03590076655528684,
      "grad_norm": 4.764499664306641,
      "learning_rate": 0.0001410442659374466,
      "loss": 0.9489,
      "step": 8402
    },
    {
      "epoch": 0.03590503943871403,
      "grad_norm": 1.2875280380249023,
      "learning_rate": 0.00014100153819859854,
      "loss": 0.5158,
      "step": 8403
    },
    {
      "epoch": 0.03590931232214123,
      "grad_norm": 4.693963050842285,
      "learning_rate": 0.00014095881045975047,
      "loss": 0.8607,
      "step": 8404
    },
    {
      "epoch": 0.03591358520556842,
      "grad_norm": 1.40096914768219,
      "learning_rate": 0.0001409160827209024,
      "loss": 0.5828,
      "step": 8405
    },
    {
      "epoch": 0.03591785808899561,
      "grad_norm": 1.2710635662078857,
      "learning_rate": 0.00014087335498205438,
      "loss": 0.4378,
      "step": 8406
    },
    {
      "epoch": 0.03592213097242281,
      "grad_norm": 0.21279214322566986,
      "learning_rate": 0.00014083062724320631,
      "loss": 0.0426,
      "step": 8407
    },
    {
      "epoch": 0.035926403855850005,
      "grad_norm": 1.5874894857406616,
      "learning_rate": 0.00014078789950435822,
      "loss": 0.3756,
      "step": 8408
    },
    {
      "epoch": 0.0359306767392772,
      "grad_norm": 1.3887999057769775,
      "learning_rate": 0.00014074517176551016,
      "loss": 0.8795,
      "step": 8409
    },
    {
      "epoch": 0.035934949622704396,
      "grad_norm": 1.4088701009750366,
      "learning_rate": 0.0001407024440266621,
      "loss": 0.4882,
      "step": 8410
    },
    {
      "epoch": 0.03593922250613159,
      "grad_norm": 2.9123737812042236,
      "learning_rate": 0.00014065971628781404,
      "loss": 0.7279,
      "step": 8411
    },
    {
      "epoch": 0.03594349538955878,
      "grad_norm": 0.8044232130050659,
      "learning_rate": 0.00014061698854896598,
      "loss": 0.3633,
      "step": 8412
    },
    {
      "epoch": 0.03594776827298598,
      "grad_norm": 1.1358925104141235,
      "learning_rate": 0.00014057426081011791,
      "loss": 0.434,
      "step": 8413
    },
    {
      "epoch": 0.03595204115641317,
      "grad_norm": 0.4277876615524292,
      "learning_rate": 0.00014053153307126988,
      "loss": 0.1699,
      "step": 8414
    },
    {
      "epoch": 0.03595631403984036,
      "grad_norm": 1.4655734300613403,
      "learning_rate": 0.00014048880533242182,
      "loss": 0.5677,
      "step": 8415
    },
    {
      "epoch": 0.03596058692326756,
      "grad_norm": 3.7894105911254883,
      "learning_rate": 0.00014044607759357375,
      "loss": 1.3833,
      "step": 8416
    },
    {
      "epoch": 0.035964859806694754,
      "grad_norm": 1.0339162349700928,
      "learning_rate": 0.0001404033498547257,
      "loss": 0.3808,
      "step": 8417
    },
    {
      "epoch": 0.03596913269012195,
      "grad_norm": 3.7332253456115723,
      "learning_rate": 0.00014036062211587763,
      "loss": 1.0162,
      "step": 8418
    },
    {
      "epoch": 0.035973405573549146,
      "grad_norm": 1.9968640804290771,
      "learning_rate": 0.00014031789437702957,
      "loss": 0.524,
      "step": 8419
    },
    {
      "epoch": 0.03597767845697634,
      "grad_norm": 1.4551151990890503,
      "learning_rate": 0.0001402751666381815,
      "loss": 0.5622,
      "step": 8420
    },
    {
      "epoch": 0.03598195134040353,
      "grad_norm": 3.3498523235321045,
      "learning_rate": 0.00014023243889933347,
      "loss": 2.172,
      "step": 8421
    },
    {
      "epoch": 0.03598622422383072,
      "grad_norm": 3.666621685028076,
      "learning_rate": 0.0001401897111604854,
      "loss": 1.2863,
      "step": 8422
    },
    {
      "epoch": 0.03599049710725792,
      "grad_norm": 3.804121255874634,
      "learning_rate": 0.00014014698342163735,
      "loss": 1.0513,
      "step": 8423
    },
    {
      "epoch": 0.03599476999068511,
      "grad_norm": 4.508955955505371,
      "learning_rate": 0.00014010425568278928,
      "loss": 1.0229,
      "step": 8424
    },
    {
      "epoch": 0.035999042874112305,
      "grad_norm": 3.2697627544403076,
      "learning_rate": 0.0001400615279439412,
      "loss": 0.5751,
      "step": 8425
    },
    {
      "epoch": 0.036003315757539504,
      "grad_norm": 1.378653645515442,
      "learning_rate": 0.00014001880020509313,
      "loss": 0.8752,
      "step": 8426
    },
    {
      "epoch": 0.036007588640966696,
      "grad_norm": 2.241399049758911,
      "learning_rate": 0.00013997607246624507,
      "loss": 0.723,
      "step": 8427
    },
    {
      "epoch": 0.03601186152439389,
      "grad_norm": 3.4297845363616943,
      "learning_rate": 0.000139933344727397,
      "loss": 0.8073,
      "step": 8428
    },
    {
      "epoch": 0.03601613440782109,
      "grad_norm": 2.0004208087921143,
      "learning_rate": 0.00013989061698854897,
      "loss": 1.3751,
      "step": 8429
    },
    {
      "epoch": 0.03602040729124828,
      "grad_norm": 2.3113386631011963,
      "learning_rate": 0.0001398478892497009,
      "loss": 0.7862,
      "step": 8430
    },
    {
      "epoch": 0.03602468017467547,
      "grad_norm": 0.5917963981628418,
      "learning_rate": 0.00013980516151085285,
      "loss": 0.251,
      "step": 8431
    },
    {
      "epoch": 0.03602895305810267,
      "grad_norm": 2.34468150138855,
      "learning_rate": 0.0001397624337720048,
      "loss": 0.8886,
      "step": 8432
    },
    {
      "epoch": 0.03603322594152986,
      "grad_norm": 4.072716236114502,
      "learning_rate": 0.00013971970603315672,
      "loss": 1.0177,
      "step": 8433
    },
    {
      "epoch": 0.036037498824957055,
      "grad_norm": 2.7731618881225586,
      "learning_rate": 0.00013967697829430866,
      "loss": 0.6068,
      "step": 8434
    },
    {
      "epoch": 0.036041771708384254,
      "grad_norm": 4.910760402679443,
      "learning_rate": 0.0001396342505554606,
      "loss": 1.0386,
      "step": 8435
    },
    {
      "epoch": 0.036046044591811446,
      "grad_norm": 4.479185581207275,
      "learning_rate": 0.00013959152281661257,
      "loss": 1.043,
      "step": 8436
    },
    {
      "epoch": 0.03605031747523864,
      "grad_norm": 0.6521368026733398,
      "learning_rate": 0.0001395487950777645,
      "loss": 0.2879,
      "step": 8437
    },
    {
      "epoch": 0.03605459035866584,
      "grad_norm": 3.571446418762207,
      "learning_rate": 0.00013950606733891644,
      "loss": 0.8099,
      "step": 8438
    },
    {
      "epoch": 0.03605886324209303,
      "grad_norm": 4.447733402252197,
      "learning_rate": 0.00013946333960006838,
      "loss": 0.6769,
      "step": 8439
    },
    {
      "epoch": 0.03606313612552022,
      "grad_norm": 3.8852055072784424,
      "learning_rate": 0.00013942061186122032,
      "loss": 3.0232,
      "step": 8440
    },
    {
      "epoch": 0.03606740900894742,
      "grad_norm": 3.1109182834625244,
      "learning_rate": 0.00013937788412237223,
      "loss": 0.8879,
      "step": 8441
    },
    {
      "epoch": 0.03607168189237461,
      "grad_norm": 1.248226523399353,
      "learning_rate": 0.00013933515638352417,
      "loss": 0.8402,
      "step": 8442
    },
    {
      "epoch": 0.036075954775801805,
      "grad_norm": 3.0410735607147217,
      "learning_rate": 0.0001392924286446761,
      "loss": 1.96,
      "step": 8443
    },
    {
      "epoch": 0.036080227659229004,
      "grad_norm": 1.7244148254394531,
      "learning_rate": 0.00013924970090582807,
      "loss": 0.6015,
      "step": 8444
    },
    {
      "epoch": 0.036084500542656196,
      "grad_norm": 4.143239974975586,
      "learning_rate": 0.00013920697316698,
      "loss": 2.6929,
      "step": 8445
    },
    {
      "epoch": 0.03608877342608339,
      "grad_norm": 1.954383373260498,
      "learning_rate": 0.00013916424542813194,
      "loss": 0.4936,
      "step": 8446
    },
    {
      "epoch": 0.03609304630951058,
      "grad_norm": 0.8415525555610657,
      "learning_rate": 0.00013912151768928388,
      "loss": 0.3477,
      "step": 8447
    },
    {
      "epoch": 0.03609731919293778,
      "grad_norm": 1.2432498931884766,
      "learning_rate": 0.00013907878995043582,
      "loss": 0.5449,
      "step": 8448
    },
    {
      "epoch": 0.03610159207636497,
      "grad_norm": 2.708141565322876,
      "learning_rate": 0.00013903606221158776,
      "loss": 0.5864,
      "step": 8449
    },
    {
      "epoch": 0.036105864959792164,
      "grad_norm": 1.5025395154953003,
      "learning_rate": 0.0001389933344727397,
      "loss": 0.34,
      "step": 8450
    },
    {
      "epoch": 0.03611013784321936,
      "grad_norm": 3.162667751312256,
      "learning_rate": 0.00013895060673389166,
      "loss": 1.0278,
      "step": 8451
    },
    {
      "epoch": 0.036114410726646555,
      "grad_norm": 1.863106608390808,
      "learning_rate": 0.0001389078789950436,
      "loss": 0.6215,
      "step": 8452
    },
    {
      "epoch": 0.03611868361007375,
      "grad_norm": 3.589446783065796,
      "learning_rate": 0.00013886515125619554,
      "loss": 2.6586,
      "step": 8453
    },
    {
      "epoch": 0.036122956493500946,
      "grad_norm": 1.2048009634017944,
      "learning_rate": 0.00013882242351734747,
      "loss": 0.5463,
      "step": 8454
    },
    {
      "epoch": 0.03612722937692814,
      "grad_norm": 3.451766014099121,
      "learning_rate": 0.0001387796957784994,
      "loss": 1.1412,
      "step": 8455
    },
    {
      "epoch": 0.03613150226035533,
      "grad_norm": 3.5280778408050537,
      "learning_rate": 0.00013873696803965135,
      "loss": 0.9403,
      "step": 8456
    },
    {
      "epoch": 0.03613577514378253,
      "grad_norm": 0.9562257528305054,
      "learning_rate": 0.0001386942403008033,
      "loss": 0.3976,
      "step": 8457
    },
    {
      "epoch": 0.03614004802720972,
      "grad_norm": 1.3900092840194702,
      "learning_rate": 0.00013865151256195523,
      "loss": 0.6393,
      "step": 8458
    },
    {
      "epoch": 0.036144320910636914,
      "grad_norm": 2.0835318565368652,
      "learning_rate": 0.00013860878482310716,
      "loss": 0.7155,
      "step": 8459
    },
    {
      "epoch": 0.03614859379406411,
      "grad_norm": 1.0874043703079224,
      "learning_rate": 0.0001385660570842591,
      "loss": 0.8297,
      "step": 8460
    },
    {
      "epoch": 0.036152866677491305,
      "grad_norm": 2.179961681365967,
      "learning_rate": 0.00013852332934541104,
      "loss": 0.6001,
      "step": 8461
    },
    {
      "epoch": 0.0361571395609185,
      "grad_norm": 0.2615334093570709,
      "learning_rate": 0.00013848060160656298,
      "loss": 0.0864,
      "step": 8462
    },
    {
      "epoch": 0.036161412444345696,
      "grad_norm": 2.6296398639678955,
      "learning_rate": 0.00013843787386771491,
      "loss": 0.5236,
      "step": 8463
    },
    {
      "epoch": 0.03616568532777289,
      "grad_norm": 1.2878391742706299,
      "learning_rate": 0.00013839514612886685,
      "loss": 0.5798,
      "step": 8464
    },
    {
      "epoch": 0.03616995821120008,
      "grad_norm": 1.448792576789856,
      "learning_rate": 0.0001383524183900188,
      "loss": 1.186,
      "step": 8465
    },
    {
      "epoch": 0.03617423109462728,
      "grad_norm": 3.3520798683166504,
      "learning_rate": 0.00013830969065117076,
      "loss": 0.7765,
      "step": 8466
    },
    {
      "epoch": 0.03617850397805447,
      "grad_norm": 4.646409034729004,
      "learning_rate": 0.0001382669629123227,
      "loss": 0.8617,
      "step": 8467
    },
    {
      "epoch": 0.036182776861481664,
      "grad_norm": 0.8402748107910156,
      "learning_rate": 0.00013822423517347463,
      "loss": 0.3479,
      "step": 8468
    },
    {
      "epoch": 0.03618704974490886,
      "grad_norm": 1.4398871660232544,
      "learning_rate": 0.00013818150743462657,
      "loss": 1.1639,
      "step": 8469
    },
    {
      "epoch": 0.036191322628336055,
      "grad_norm": 0.8156397342681885,
      "learning_rate": 0.0001381387796957785,
      "loss": 0.3483,
      "step": 8470
    },
    {
      "epoch": 0.03619559551176325,
      "grad_norm": 2.5184648036956787,
      "learning_rate": 0.00013809605195693044,
      "loss": 0.4631,
      "step": 8471
    },
    {
      "epoch": 0.03619986839519044,
      "grad_norm": 3.123593330383301,
      "learning_rate": 0.00013805332421808238,
      "loss": 0.9534,
      "step": 8472
    },
    {
      "epoch": 0.03620414127861764,
      "grad_norm": 3.323469638824463,
      "learning_rate": 0.00013801059647923435,
      "loss": 1.4035,
      "step": 8473
    },
    {
      "epoch": 0.03620841416204483,
      "grad_norm": 0.6146432161331177,
      "learning_rate": 0.00013796786874038626,
      "loss": 0.2709,
      "step": 8474
    },
    {
      "epoch": 0.03621268704547202,
      "grad_norm": 3.4414031505584717,
      "learning_rate": 0.0001379251410015382,
      "loss": 1.0279,
      "step": 8475
    },
    {
      "epoch": 0.03621695992889922,
      "grad_norm": 1.428114652633667,
      "learning_rate": 0.00013788241326269013,
      "loss": 1.1531,
      "step": 8476
    },
    {
      "epoch": 0.036221232812326414,
      "grad_norm": 5.097144603729248,
      "learning_rate": 0.00013783968552384207,
      "loss": 1.3886,
      "step": 8477
    },
    {
      "epoch": 0.036225505695753606,
      "grad_norm": 3.6458945274353027,
      "learning_rate": 0.000137796957784994,
      "loss": 1.1115,
      "step": 8478
    },
    {
      "epoch": 0.036229778579180805,
      "grad_norm": 0.742056667804718,
      "learning_rate": 0.00013775423004614595,
      "loss": 0.2565,
      "step": 8479
    },
    {
      "epoch": 0.036234051462608,
      "grad_norm": 2.4019272327423096,
      "learning_rate": 0.00013771150230729788,
      "loss": 0.73,
      "step": 8480
    },
    {
      "epoch": 0.03623832434603519,
      "grad_norm": 0.6291928887367249,
      "learning_rate": 0.00013766877456844985,
      "loss": 0.2381,
      "step": 8481
    },
    {
      "epoch": 0.03624259722946239,
      "grad_norm": 2.9924118518829346,
      "learning_rate": 0.0001376260468296018,
      "loss": 0.8754,
      "step": 8482
    },
    {
      "epoch": 0.03624687011288958,
      "grad_norm": 3.3853986263275146,
      "learning_rate": 0.00013758331909075373,
      "loss": 0.9178,
      "step": 8483
    },
    {
      "epoch": 0.03625114299631677,
      "grad_norm": 1.0064723491668701,
      "learning_rate": 0.00013754059135190566,
      "loss": 0.8077,
      "step": 8484
    },
    {
      "epoch": 0.03625541587974397,
      "grad_norm": 3.1967687606811523,
      "learning_rate": 0.0001374978636130576,
      "loss": 0.7862,
      "step": 8485
    },
    {
      "epoch": 0.03625968876317116,
      "grad_norm": 0.26055869460105896,
      "learning_rate": 0.00013745513587420954,
      "loss": 0.0864,
      "step": 8486
    },
    {
      "epoch": 0.036263961646598356,
      "grad_norm": 2.166255474090576,
      "learning_rate": 0.00013741240813536148,
      "loss": 0.5774,
      "step": 8487
    },
    {
      "epoch": 0.036268234530025555,
      "grad_norm": 4.018962383270264,
      "learning_rate": 0.00013736968039651344,
      "loss": 1.0519,
      "step": 8488
    },
    {
      "epoch": 0.03627250741345275,
      "grad_norm": 0.9888399243354797,
      "learning_rate": 0.00013732695265766538,
      "loss": 0.7623,
      "step": 8489
    },
    {
      "epoch": 0.03627678029687994,
      "grad_norm": 3.3228015899658203,
      "learning_rate": 0.0001372842249188173,
      "loss": 0.7903,
      "step": 8490
    },
    {
      "epoch": 0.03628105318030714,
      "grad_norm": 4.565022945404053,
      "learning_rate": 0.00013724149717996923,
      "loss": 0.8113,
      "step": 8491
    },
    {
      "epoch": 0.03628532606373433,
      "grad_norm": 2.536531448364258,
      "learning_rate": 0.00013719876944112117,
      "loss": 1.6769,
      "step": 8492
    },
    {
      "epoch": 0.03628959894716152,
      "grad_norm": 1.0126354694366455,
      "learning_rate": 0.0001371560417022731,
      "loss": 0.77,
      "step": 8493
    },
    {
      "epoch": 0.03629387183058872,
      "grad_norm": 0.49425122141838074,
      "learning_rate": 0.00013711331396342504,
      "loss": 0.2233,
      "step": 8494
    },
    {
      "epoch": 0.03629814471401591,
      "grad_norm": 3.1614463329315186,
      "learning_rate": 0.00013707058622457698,
      "loss": 0.9316,
      "step": 8495
    },
    {
      "epoch": 0.036302417597443105,
      "grad_norm": 3.110398769378662,
      "learning_rate": 0.00013702785848572894,
      "loss": 0.83,
      "step": 8496
    },
    {
      "epoch": 0.0363066904808703,
      "grad_norm": 3.2237303256988525,
      "learning_rate": 0.00013698513074688088,
      "loss": 0.9034,
      "step": 8497
    },
    {
      "epoch": 0.0363109633642975,
      "grad_norm": 1.3395448923110962,
      "learning_rate": 0.00013694240300803282,
      "loss": 0.6617,
      "step": 8498
    },
    {
      "epoch": 0.03631523624772469,
      "grad_norm": 0.9443395137786865,
      "learning_rate": 0.00013689967526918476,
      "loss": 0.4518,
      "step": 8499
    },
    {
      "epoch": 0.03631950913115188,
      "grad_norm": 4.999984264373779,
      "learning_rate": 0.0001368569475303367,
      "loss": 1.2508,
      "step": 8500
    },
    {
      "epoch": 0.03632378201457908,
      "grad_norm": 1.4285717010498047,
      "learning_rate": 0.00013681421979148863,
      "loss": 0.6334,
      "step": 8501
    },
    {
      "epoch": 0.03632805489800627,
      "grad_norm": 0.6370730400085449,
      "learning_rate": 0.00013677149205264057,
      "loss": 0.2875,
      "step": 8502
    },
    {
      "epoch": 0.036332327781433464,
      "grad_norm": 1.847131371498108,
      "learning_rate": 0.00013672876431379254,
      "loss": 0.5576,
      "step": 8503
    },
    {
      "epoch": 0.03633660066486066,
      "grad_norm": 2.192254066467285,
      "learning_rate": 0.00013668603657494447,
      "loss": 0.6516,
      "step": 8504
    },
    {
      "epoch": 0.036340873548287855,
      "grad_norm": 0.2605491876602173,
      "learning_rate": 0.0001366433088360964,
      "loss": 0.0825,
      "step": 8505
    },
    {
      "epoch": 0.03634514643171505,
      "grad_norm": 4.278287887573242,
      "learning_rate": 0.00013660058109724835,
      "loss": 1.0784,
      "step": 8506
    },
    {
      "epoch": 0.036349419315142247,
      "grad_norm": 4.274540901184082,
      "learning_rate": 0.00013655785335840026,
      "loss": 1.0823,
      "step": 8507
    },
    {
      "epoch": 0.03635369219856944,
      "grad_norm": 0.6276003122329712,
      "learning_rate": 0.0001365151256195522,
      "loss": 0.2875,
      "step": 8508
    },
    {
      "epoch": 0.03635796508199663,
      "grad_norm": 2.523798704147339,
      "learning_rate": 0.00013647239788070414,
      "loss": 0.9646,
      "step": 8509
    },
    {
      "epoch": 0.03636223796542383,
      "grad_norm": 1.3146154880523682,
      "learning_rate": 0.0001364296701418561,
      "loss": 0.6594,
      "step": 8510
    },
    {
      "epoch": 0.03636651084885102,
      "grad_norm": 1.1507145166397095,
      "learning_rate": 0.00013638694240300804,
      "loss": 0.4605,
      "step": 8511
    },
    {
      "epoch": 0.036370783732278214,
      "grad_norm": 1.290827989578247,
      "learning_rate": 0.00013634421466415998,
      "loss": 0.6313,
      "step": 8512
    },
    {
      "epoch": 0.03637505661570541,
      "grad_norm": 0.5911747217178345,
      "learning_rate": 0.00013630148692531191,
      "loss": 0.2868,
      "step": 8513
    },
    {
      "epoch": 0.036379329499132605,
      "grad_norm": 4.104210376739502,
      "learning_rate": 0.00013625875918646385,
      "loss": 1.2696,
      "step": 8514
    },
    {
      "epoch": 0.0363836023825598,
      "grad_norm": 1.9570602178573608,
      "learning_rate": 0.0001362160314476158,
      "loss": 0.5264,
      "step": 8515
    },
    {
      "epoch": 0.036387875265986996,
      "grad_norm": 1.022563099861145,
      "learning_rate": 0.00013617330370876773,
      "loss": 0.4588,
      "step": 8516
    },
    {
      "epoch": 0.03639214814941419,
      "grad_norm": 3.2540364265441895,
      "learning_rate": 0.00013613057596991967,
      "loss": 0.7011,
      "step": 8517
    },
    {
      "epoch": 0.03639642103284138,
      "grad_norm": 1.0063016414642334,
      "learning_rate": 0.00013608784823107163,
      "loss": 0.6729,
      "step": 8518
    },
    {
      "epoch": 0.03640069391626858,
      "grad_norm": 2.7477166652679443,
      "learning_rate": 0.00013604512049222357,
      "loss": 0.8855,
      "step": 8519
    },
    {
      "epoch": 0.03640496679969577,
      "grad_norm": 1.8046916723251343,
      "learning_rate": 0.0001360023927533755,
      "loss": 0.5615,
      "step": 8520
    },
    {
      "epoch": 0.036409239683122964,
      "grad_norm": 1.0830219984054565,
      "learning_rate": 0.00013595966501452744,
      "loss": 0.4296,
      "step": 8521
    },
    {
      "epoch": 0.036413512566550156,
      "grad_norm": 1.242557168006897,
      "learning_rate": 0.00013591693727567938,
      "loss": 0.5702,
      "step": 8522
    },
    {
      "epoch": 0.036417785449977355,
      "grad_norm": 4.136927127838135,
      "learning_rate": 0.0001358742095368313,
      "loss": 0.8329,
      "step": 8523
    },
    {
      "epoch": 0.03642205833340455,
      "grad_norm": 1.3630565404891968,
      "learning_rate": 0.00013583148179798323,
      "loss": 0.6923,
      "step": 8524
    },
    {
      "epoch": 0.03642633121683174,
      "grad_norm": 4.581576347351074,
      "learning_rate": 0.0001357887540591352,
      "loss": 0.8253,
      "step": 8525
    },
    {
      "epoch": 0.03643060410025894,
      "grad_norm": 3.584364891052246,
      "learning_rate": 0.00013574602632028713,
      "loss": 1.0277,
      "step": 8526
    },
    {
      "epoch": 0.03643487698368613,
      "grad_norm": 3.060336112976074,
      "learning_rate": 0.00013570329858143907,
      "loss": 0.857,
      "step": 8527
    },
    {
      "epoch": 0.03643914986711332,
      "grad_norm": 4.542077541351318,
      "learning_rate": 0.000135660570842591,
      "loss": 0.7771,
      "step": 8528
    },
    {
      "epoch": 0.03644342275054052,
      "grad_norm": 1.266852617263794,
      "learning_rate": 0.00013561784310374295,
      "loss": 0.5834,
      "step": 8529
    },
    {
      "epoch": 0.036447695633967714,
      "grad_norm": 1.536440134048462,
      "learning_rate": 0.00013557511536489489,
      "loss": 0.3565,
      "step": 8530
    },
    {
      "epoch": 0.036451968517394906,
      "grad_norm": 1.4049745798110962,
      "learning_rate": 0.00013553238762604682,
      "loss": 0.5574,
      "step": 8531
    },
    {
      "epoch": 0.036456241400822105,
      "grad_norm": 0.7032848000526428,
      "learning_rate": 0.00013548965988719876,
      "loss": 0.293,
      "step": 8532
    },
    {
      "epoch": 0.0364605142842493,
      "grad_norm": 2.2400119304656982,
      "learning_rate": 0.00013544693214835073,
      "loss": 0.7264,
      "step": 8533
    },
    {
      "epoch": 0.03646478716767649,
      "grad_norm": 2.171786069869995,
      "learning_rate": 0.00013540420440950266,
      "loss": 0.5227,
      "step": 8534
    },
    {
      "epoch": 0.03646906005110369,
      "grad_norm": 3.4026503562927246,
      "learning_rate": 0.0001353614766706546,
      "loss": 1.19,
      "step": 8535
    },
    {
      "epoch": 0.03647333293453088,
      "grad_norm": 3.1329805850982666,
      "learning_rate": 0.00013531874893180654,
      "loss": 0.7398,
      "step": 8536
    },
    {
      "epoch": 0.03647760581795807,
      "grad_norm": 0.7869536876678467,
      "learning_rate": 0.00013527602119295848,
      "loss": 0.3183,
      "step": 8537
    },
    {
      "epoch": 0.03648187870138527,
      "grad_norm": 2.1430470943450928,
      "learning_rate": 0.00013523329345411042,
      "loss": 0.5,
      "step": 8538
    },
    {
      "epoch": 0.036486151584812464,
      "grad_norm": 1.6657379865646362,
      "learning_rate": 0.00013519056571526235,
      "loss": 1.1403,
      "step": 8539
    },
    {
      "epoch": 0.036490424468239656,
      "grad_norm": 0.7479529976844788,
      "learning_rate": 0.0001351478379764143,
      "loss": 0.4014,
      "step": 8540
    },
    {
      "epoch": 0.036494697351666855,
      "grad_norm": 0.7989320158958435,
      "learning_rate": 0.00013510511023756623,
      "loss": 0.3186,
      "step": 8541
    },
    {
      "epoch": 0.03649897023509405,
      "grad_norm": 1.9230903387069702,
      "learning_rate": 0.00013506238249871817,
      "loss": 0.6171,
      "step": 8542
    },
    {
      "epoch": 0.03650324311852124,
      "grad_norm": 0.7574318647384644,
      "learning_rate": 0.0001350196547598701,
      "loss": 0.2759,
      "step": 8543
    },
    {
      "epoch": 0.03650751600194844,
      "grad_norm": 4.8644514083862305,
      "learning_rate": 0.00013497692702102204,
      "loss": 1.0125,
      "step": 8544
    },
    {
      "epoch": 0.03651178888537563,
      "grad_norm": 1.9242182970046997,
      "learning_rate": 0.00013493419928217398,
      "loss": 0.5971,
      "step": 8545
    },
    {
      "epoch": 0.03651606176880282,
      "grad_norm": 1.3089228868484497,
      "learning_rate": 0.00013489147154332592,
      "loss": 0.5636,
      "step": 8546
    },
    {
      "epoch": 0.036520334652230015,
      "grad_norm": 1.2227437496185303,
      "learning_rate": 0.00013484874380447786,
      "loss": 0.475,
      "step": 8547
    },
    {
      "epoch": 0.036524607535657214,
      "grad_norm": 1.9357131719589233,
      "learning_rate": 0.00013480601606562982,
      "loss": 0.6523,
      "step": 8548
    },
    {
      "epoch": 0.036528880419084406,
      "grad_norm": 1.5009946823120117,
      "learning_rate": 0.00013476328832678176,
      "loss": 0.71,
      "step": 8549
    },
    {
      "epoch": 0.0365331533025116,
      "grad_norm": 1.5070265531539917,
      "learning_rate": 0.0001347205605879337,
      "loss": 0.7278,
      "step": 8550
    },
    {
      "epoch": 0.0365374261859388,
      "grad_norm": 1.1661577224731445,
      "learning_rate": 0.00013467783284908563,
      "loss": 0.3758,
      "step": 8551
    },
    {
      "epoch": 0.03654169906936599,
      "grad_norm": 2.5689492225646973,
      "learning_rate": 0.00013463510511023757,
      "loss": 0.8777,
      "step": 8552
    },
    {
      "epoch": 0.03654597195279318,
      "grad_norm": 0.5874930024147034,
      "learning_rate": 0.0001345923773713895,
      "loss": 0.2409,
      "step": 8553
    },
    {
      "epoch": 0.03655024483622038,
      "grad_norm": 3.0812127590179443,
      "learning_rate": 0.00013454964963254145,
      "loss": 0.8342,
      "step": 8554
    },
    {
      "epoch": 0.03655451771964757,
      "grad_norm": 0.30871322751045227,
      "learning_rate": 0.0001345069218936934,
      "loss": 0.1131,
      "step": 8555
    },
    {
      "epoch": 0.036558790603074764,
      "grad_norm": 1.1404924392700195,
      "learning_rate": 0.00013446419415484532,
      "loss": 0.4293,
      "step": 8556
    },
    {
      "epoch": 0.036563063486501964,
      "grad_norm": 1.1806496381759644,
      "learning_rate": 0.00013442146641599726,
      "loss": 0.4694,
      "step": 8557
    },
    {
      "epoch": 0.036567336369929156,
      "grad_norm": 1.0728882551193237,
      "learning_rate": 0.0001343787386771492,
      "loss": 0.4511,
      "step": 8558
    },
    {
      "epoch": 0.03657160925335635,
      "grad_norm": 0.27106547355651855,
      "learning_rate": 0.00013433601093830114,
      "loss": 0.0989,
      "step": 8559
    },
    {
      "epoch": 0.03657588213678355,
      "grad_norm": 5.356358528137207,
      "learning_rate": 0.00013429328319945307,
      "loss": 1.2327,
      "step": 8560
    },
    {
      "epoch": 0.03658015502021074,
      "grad_norm": 1.0608922243118286,
      "learning_rate": 0.000134250555460605,
      "loss": 0.7256,
      "step": 8561
    },
    {
      "epoch": 0.03658442790363793,
      "grad_norm": 1.0579217672348022,
      "learning_rate": 0.00013420782772175698,
      "loss": 0.7275,
      "step": 8562
    },
    {
      "epoch": 0.03658870078706513,
      "grad_norm": 0.4896771311759949,
      "learning_rate": 0.00013416509998290892,
      "loss": 0.227,
      "step": 8563
    },
    {
      "epoch": 0.03659297367049232,
      "grad_norm": 4.453180313110352,
      "learning_rate": 0.00013412237224406085,
      "loss": 0.9146,
      "step": 8564
    },
    {
      "epoch": 0.036597246553919514,
      "grad_norm": 1.5446621179580688,
      "learning_rate": 0.0001340796445052128,
      "loss": 0.6369,
      "step": 8565
    },
    {
      "epoch": 0.03660151943734671,
      "grad_norm": 0.7830901145935059,
      "learning_rate": 0.00013403691676636473,
      "loss": 0.3442,
      "step": 8566
    },
    {
      "epoch": 0.036605792320773906,
      "grad_norm": 1.762911081314087,
      "learning_rate": 0.00013399418902751667,
      "loss": 0.4921,
      "step": 8567
    },
    {
      "epoch": 0.0366100652042011,
      "grad_norm": 3.7567155361175537,
      "learning_rate": 0.0001339514612886686,
      "loss": 2.3678,
      "step": 8568
    },
    {
      "epoch": 0.0366143380876283,
      "grad_norm": 0.5045511722564697,
      "learning_rate": 0.00013390873354982054,
      "loss": 0.2412,
      "step": 8569
    },
    {
      "epoch": 0.03661861097105549,
      "grad_norm": 1.6932390928268433,
      "learning_rate": 0.0001338660058109725,
      "loss": 0.8519,
      "step": 8570
    },
    {
      "epoch": 0.03662288385448268,
      "grad_norm": 2.0858042240142822,
      "learning_rate": 0.00013382327807212445,
      "loss": 0.6523,
      "step": 8571
    },
    {
      "epoch": 0.03662715673790987,
      "grad_norm": 1.3510994911193848,
      "learning_rate": 0.00013378055033327638,
      "loss": 0.6888,
      "step": 8572
    },
    {
      "epoch": 0.03663142962133707,
      "grad_norm": 2.1765472888946533,
      "learning_rate": 0.0001337378225944283,
      "loss": 0.7551,
      "step": 8573
    },
    {
      "epoch": 0.036635702504764264,
      "grad_norm": 1.934821605682373,
      "learning_rate": 0.00013369509485558023,
      "loss": 0.6927,
      "step": 8574
    },
    {
      "epoch": 0.036639975388191456,
      "grad_norm": 0.9590202569961548,
      "learning_rate": 0.00013365236711673217,
      "loss": 0.3973,
      "step": 8575
    },
    {
      "epoch": 0.036644248271618655,
      "grad_norm": 1.2448614835739136,
      "learning_rate": 0.0001336096393778841,
      "loss": 0.5733,
      "step": 8576
    },
    {
      "epoch": 0.03664852115504585,
      "grad_norm": 3.2998459339141846,
      "learning_rate": 0.00013356691163903607,
      "loss": 1.3017,
      "step": 8577
    },
    {
      "epoch": 0.03665279403847304,
      "grad_norm": 0.7101011276245117,
      "learning_rate": 0.000133524183900188,
      "loss": 0.278,
      "step": 8578
    },
    {
      "epoch": 0.03665706692190024,
      "grad_norm": 0.48513180017471313,
      "learning_rate": 0.00013348145616133995,
      "loss": 0.206,
      "step": 8579
    },
    {
      "epoch": 0.03666133980532743,
      "grad_norm": 0.7382453083992004,
      "learning_rate": 0.00013343872842249189,
      "loss": 0.2759,
      "step": 8580
    },
    {
      "epoch": 0.03666561268875462,
      "grad_norm": 1.5125455856323242,
      "learning_rate": 0.00013339600068364382,
      "loss": 0.6513,
      "step": 8581
    },
    {
      "epoch": 0.03666988557218182,
      "grad_norm": 0.7601723074913025,
      "learning_rate": 0.00013335327294479576,
      "loss": 0.3187,
      "step": 8582
    },
    {
      "epoch": 0.036674158455609014,
      "grad_norm": 1.0692270994186401,
      "learning_rate": 0.0001333105452059477,
      "loss": 0.7051,
      "step": 8583
    },
    {
      "epoch": 0.036678431339036206,
      "grad_norm": 0.7811472415924072,
      "learning_rate": 0.00013326781746709964,
      "loss": 0.3184,
      "step": 8584
    },
    {
      "epoch": 0.036682704222463405,
      "grad_norm": 1.7244478464126587,
      "learning_rate": 0.0001332250897282516,
      "loss": 0.5943,
      "step": 8585
    },
    {
      "epoch": 0.0366869771058906,
      "grad_norm": 1.975907564163208,
      "learning_rate": 0.00013318236198940354,
      "loss": 0.4546,
      "step": 8586
    },
    {
      "epoch": 0.03669124998931779,
      "grad_norm": 0.7728639245033264,
      "learning_rate": 0.00013313963425055548,
      "loss": 0.3348,
      "step": 8587
    },
    {
      "epoch": 0.03669552287274499,
      "grad_norm": 1.3056727647781372,
      "learning_rate": 0.00013309690651170742,
      "loss": 0.681,
      "step": 8588
    },
    {
      "epoch": 0.03669979575617218,
      "grad_norm": 3.423337697982788,
      "learning_rate": 0.00013305417877285933,
      "loss": 0.9366,
      "step": 8589
    },
    {
      "epoch": 0.03670406863959937,
      "grad_norm": 1.0686794519424438,
      "learning_rate": 0.00013301145103401126,
      "loss": 0.6802,
      "step": 8590
    },
    {
      "epoch": 0.03670834152302657,
      "grad_norm": 1.164489507675171,
      "learning_rate": 0.0001329687232951632,
      "loss": 0.4439,
      "step": 8591
    },
    {
      "epoch": 0.036712614406453764,
      "grad_norm": 0.7131406664848328,
      "learning_rate": 0.00013292599555631517,
      "loss": 0.3274,
      "step": 8592
    },
    {
      "epoch": 0.036716887289880956,
      "grad_norm": 2.626742362976074,
      "learning_rate": 0.0001328832678174671,
      "loss": 0.8298,
      "step": 8593
    },
    {
      "epoch": 0.036721160173308155,
      "grad_norm": 1.9887882471084595,
      "learning_rate": 0.00013284054007861904,
      "loss": 0.5516,
      "step": 8594
    },
    {
      "epoch": 0.03672543305673535,
      "grad_norm": 2.9131979942321777,
      "learning_rate": 0.00013279781233977098,
      "loss": 1.0768,
      "step": 8595
    },
    {
      "epoch": 0.03672970594016254,
      "grad_norm": 0.7673722505569458,
      "learning_rate": 0.00013275508460092292,
      "loss": 0.3345,
      "step": 8596
    },
    {
      "epoch": 0.03673397882358973,
      "grad_norm": 1.5273891687393188,
      "learning_rate": 0.00013271235686207486,
      "loss": 0.5741,
      "step": 8597
    },
    {
      "epoch": 0.03673825170701693,
      "grad_norm": 2.1078827381134033,
      "learning_rate": 0.0001326696291232268,
      "loss": 0.6117,
      "step": 8598
    },
    {
      "epoch": 0.03674252459044412,
      "grad_norm": 0.7622923254966736,
      "learning_rate": 0.00013262690138437873,
      "loss": 0.2892,
      "step": 8599
    },
    {
      "epoch": 0.036746797473871315,
      "grad_norm": 2.824456214904785,
      "learning_rate": 0.0001325841736455307,
      "loss": 0.8562,
      "step": 8600
    },
    {
      "epoch": 0.036751070357298514,
      "grad_norm": 0.752754807472229,
      "learning_rate": 0.00013254144590668263,
      "loss": 0.276,
      "step": 8601
    },
    {
      "epoch": 0.036755343240725706,
      "grad_norm": 3.2072184085845947,
      "learning_rate": 0.00013249871816783457,
      "loss": 0.863,
      "step": 8602
    },
    {
      "epoch": 0.0367596161241529,
      "grad_norm": 3.061487913131714,
      "learning_rate": 0.0001324559904289865,
      "loss": 0.6559,
      "step": 8603
    },
    {
      "epoch": 0.0367638890075801,
      "grad_norm": 2.6771461963653564,
      "learning_rate": 0.00013241326269013845,
      "loss": 1.7762,
      "step": 8604
    },
    {
      "epoch": 0.03676816189100729,
      "grad_norm": 1.627196192741394,
      "learning_rate": 0.00013237053495129036,
      "loss": 0.5405,
      "step": 8605
    },
    {
      "epoch": 0.03677243477443448,
      "grad_norm": 1.115468144416809,
      "learning_rate": 0.0001323278072124423,
      "loss": 0.6512,
      "step": 8606
    },
    {
      "epoch": 0.03677670765786168,
      "grad_norm": 2.6468753814697266,
      "learning_rate": 0.00013228507947359426,
      "loss": 1.7403,
      "step": 8607
    },
    {
      "epoch": 0.03678098054128887,
      "grad_norm": 0.8036180734634399,
      "learning_rate": 0.0001322423517347462,
      "loss": 0.3633,
      "step": 8608
    },
    {
      "epoch": 0.036785253424716065,
      "grad_norm": 0.6465649008750916,
      "learning_rate": 0.00013219962399589814,
      "loss": 0.2504,
      "step": 8609
    },
    {
      "epoch": 0.036789526308143264,
      "grad_norm": 1.9317635297775269,
      "learning_rate": 0.00013215689625705008,
      "loss": 0.6124,
      "step": 8610
    },
    {
      "epoch": 0.036793799191570456,
      "grad_norm": 1.1001843214035034,
      "learning_rate": 0.000132114168518202,
      "loss": 0.616,
      "step": 8611
    },
    {
      "epoch": 0.03679807207499765,
      "grad_norm": 5.155322074890137,
      "learning_rate": 0.00013207144077935395,
      "loss": 1.1798,
      "step": 8612
    },
    {
      "epoch": 0.03680234495842485,
      "grad_norm": 0.5915022492408752,
      "learning_rate": 0.0001320287130405059,
      "loss": 0.2374,
      "step": 8613
    },
    {
      "epoch": 0.03680661784185204,
      "grad_norm": 0.6436662077903748,
      "learning_rate": 0.00013198598530165785,
      "loss": 0.2504,
      "step": 8614
    },
    {
      "epoch": 0.03681089072527923,
      "grad_norm": 2.091754198074341,
      "learning_rate": 0.0001319432575628098,
      "loss": 0.5575,
      "step": 8615
    },
    {
      "epoch": 0.03681516360870643,
      "grad_norm": 0.5368327498435974,
      "learning_rate": 0.00013190052982396173,
      "loss": 0.2096,
      "step": 8616
    },
    {
      "epoch": 0.03681943649213362,
      "grad_norm": 4.8039727210998535,
      "learning_rate": 0.00013185780208511367,
      "loss": 1.0807,
      "step": 8617
    },
    {
      "epoch": 0.036823709375560815,
      "grad_norm": 1.3880796432495117,
      "learning_rate": 0.0001318150743462656,
      "loss": 0.7008,
      "step": 8618
    },
    {
      "epoch": 0.036827982258988014,
      "grad_norm": 2.501077890396118,
      "learning_rate": 0.00013177234660741754,
      "loss": 1.6473,
      "step": 8619
    },
    {
      "epoch": 0.036832255142415206,
      "grad_norm": 2.0350756645202637,
      "learning_rate": 0.00013172961886856948,
      "loss": 0.7523,
      "step": 8620
    },
    {
      "epoch": 0.0368365280258424,
      "grad_norm": 1.9729182720184326,
      "learning_rate": 0.00013168689112972142,
      "loss": 0.5351,
      "step": 8621
    },
    {
      "epoch": 0.03684080090926959,
      "grad_norm": 0.5106005668640137,
      "learning_rate": 0.00013164416339087336,
      "loss": 0.21,
      "step": 8622
    },
    {
      "epoch": 0.03684507379269679,
      "grad_norm": 0.6144275069236755,
      "learning_rate": 0.0001316014356520253,
      "loss": 0.2384,
      "step": 8623
    },
    {
      "epoch": 0.03684934667612398,
      "grad_norm": 0.9182731509208679,
      "learning_rate": 0.00013155870791317723,
      "loss": 0.4142,
      "step": 8624
    },
    {
      "epoch": 0.03685361955955117,
      "grad_norm": 1.3411378860473633,
      "learning_rate": 0.00013151598017432917,
      "loss": 0.7137,
      "step": 8625
    },
    {
      "epoch": 0.03685789244297837,
      "grad_norm": 3.0617008209228516,
      "learning_rate": 0.0001314732524354811,
      "loss": 1.0072,
      "step": 8626
    },
    {
      "epoch": 0.036862165326405565,
      "grad_norm": 1.1983253955841064,
      "learning_rate": 0.00013143052469663305,
      "loss": 0.6869,
      "step": 8627
    },
    {
      "epoch": 0.03686643820983276,
      "grad_norm": 2.454026222229004,
      "learning_rate": 0.00013138779695778498,
      "loss": 1.641,
      "step": 8628
    },
    {
      "epoch": 0.036870711093259956,
      "grad_norm": 1.252071738243103,
      "learning_rate": 0.00013134506921893695,
      "loss": 0.6672,
      "step": 8629
    },
    {
      "epoch": 0.03687498397668715,
      "grad_norm": 0.6633086204528809,
      "learning_rate": 0.00013130234148008889,
      "loss": 0.3043,
      "step": 8630
    },
    {
      "epoch": 0.03687925686011434,
      "grad_norm": 1.2058228254318237,
      "learning_rate": 0.00013125961374124082,
      "loss": 0.6711,
      "step": 8631
    },
    {
      "epoch": 0.03688352974354154,
      "grad_norm": 3.4594531059265137,
      "learning_rate": 0.00013121688600239276,
      "loss": 1.0577,
      "step": 8632
    },
    {
      "epoch": 0.03688780262696873,
      "grad_norm": 3.031541347503662,
      "learning_rate": 0.0001311741582635447,
      "loss": 0.6176,
      "step": 8633
    },
    {
      "epoch": 0.03689207551039592,
      "grad_norm": 1.8913697004318237,
      "learning_rate": 0.00013113143052469664,
      "loss": 0.4303,
      "step": 8634
    },
    {
      "epoch": 0.03689634839382312,
      "grad_norm": 4.330066204071045,
      "learning_rate": 0.00013108870278584858,
      "loss": 1.2576,
      "step": 8635
    },
    {
      "epoch": 0.036900621277250314,
      "grad_norm": 0.6688066720962524,
      "learning_rate": 0.0001310459750470005,
      "loss": 0.264,
      "step": 8636
    },
    {
      "epoch": 0.03690489416067751,
      "grad_norm": 1.3643501996994019,
      "learning_rate": 0.00013100324730815248,
      "loss": 1.1071,
      "step": 8637
    },
    {
      "epoch": 0.036909167044104706,
      "grad_norm": 0.9495531916618347,
      "learning_rate": 0.0001309605195693044,
      "loss": 0.3754,
      "step": 8638
    },
    {
      "epoch": 0.0369134399275319,
      "grad_norm": 1.8984915018081665,
      "learning_rate": 0.00013091779183045633,
      "loss": 0.5211,
      "step": 8639
    },
    {
      "epoch": 0.03691771281095909,
      "grad_norm": 1.1738779544830322,
      "learning_rate": 0.00013087506409160826,
      "loss": 0.67,
      "step": 8640
    },
    {
      "epoch": 0.03692198569438629,
      "grad_norm": 1.081286072731018,
      "learning_rate": 0.0001308323363527602,
      "loss": 0.67,
      "step": 8641
    },
    {
      "epoch": 0.03692625857781348,
      "grad_norm": 1.0545934438705444,
      "learning_rate": 0.00013078960861391214,
      "loss": 0.6073,
      "step": 8642
    },
    {
      "epoch": 0.03693053146124067,
      "grad_norm": 2.4382524490356445,
      "learning_rate": 0.00013074688087506408,
      "loss": 0.8535,
      "step": 8643
    },
    {
      "epoch": 0.036934804344667865,
      "grad_norm": 4.748663902282715,
      "learning_rate": 0.00013070415313621604,
      "loss": 0.93,
      "step": 8644
    },
    {
      "epoch": 0.036939077228095064,
      "grad_norm": 0.43949824571609497,
      "learning_rate": 0.00013066142539736798,
      "loss": 0.1787,
      "step": 8645
    },
    {
      "epoch": 0.036943350111522257,
      "grad_norm": 0.7962439656257629,
      "learning_rate": 0.00013061869765851992,
      "loss": 0.2714,
      "step": 8646
    },
    {
      "epoch": 0.03694762299494945,
      "grad_norm": 1.3852256536483765,
      "learning_rate": 0.00013057596991967186,
      "loss": 1.1042,
      "step": 8647
    },
    {
      "epoch": 0.03695189587837665,
      "grad_norm": 0.8880992531776428,
      "learning_rate": 0.0001305332421808238,
      "loss": 0.3519,
      "step": 8648
    },
    {
      "epoch": 0.03695616876180384,
      "grad_norm": 3.638155221939087,
      "learning_rate": 0.00013049051444197573,
      "loss": 1.1909,
      "step": 8649
    },
    {
      "epoch": 0.03696044164523103,
      "grad_norm": 0.43861883878707886,
      "learning_rate": 0.00013044778670312767,
      "loss": 0.1744,
      "step": 8650
    },
    {
      "epoch": 0.03696471452865823,
      "grad_norm": 1.3262063264846802,
      "learning_rate": 0.0001304050589642796,
      "loss": 0.5831,
      "step": 8651
    },
    {
      "epoch": 0.03696898741208542,
      "grad_norm": 1.798552393913269,
      "learning_rate": 0.00013036233122543157,
      "loss": 0.5604,
      "step": 8652
    },
    {
      "epoch": 0.036973260295512615,
      "grad_norm": 1.8320503234863281,
      "learning_rate": 0.0001303196034865835,
      "loss": 0.4546,
      "step": 8653
    },
    {
      "epoch": 0.036977533178939814,
      "grad_norm": 1.7385048866271973,
      "learning_rate": 0.00013027687574773545,
      "loss": 0.4704,
      "step": 8654
    },
    {
      "epoch": 0.036981806062367006,
      "grad_norm": 1.034855604171753,
      "learning_rate": 0.00013023414800888736,
      "loss": 0.5887,
      "step": 8655
    },
    {
      "epoch": 0.0369860789457942,
      "grad_norm": 2.0117814540863037,
      "learning_rate": 0.0001301914202700393,
      "loss": 0.5878,
      "step": 8656
    },
    {
      "epoch": 0.0369903518292214,
      "grad_norm": 0.49495306611061096,
      "learning_rate": 0.00013014869253119124,
      "loss": 0.225,
      "step": 8657
    },
    {
      "epoch": 0.03699462471264859,
      "grad_norm": 1.0110565423965454,
      "learning_rate": 0.00013010596479234317,
      "loss": 0.5753,
      "step": 8658
    },
    {
      "epoch": 0.03699889759607578,
      "grad_norm": 3.56640887260437,
      "learning_rate": 0.00013006323705349514,
      "loss": 0.9756,
      "step": 8659
    },
    {
      "epoch": 0.03700317047950298,
      "grad_norm": 0.8563812971115112,
      "learning_rate": 0.00013002050931464708,
      "loss": 0.3476,
      "step": 8660
    },
    {
      "epoch": 0.03700744336293017,
      "grad_norm": 1.4150692224502563,
      "learning_rate": 0.00012997778157579901,
      "loss": 0.6855,
      "step": 8661
    },
    {
      "epoch": 0.037011716246357365,
      "grad_norm": 0.5372281074523926,
      "learning_rate": 0.00012993505383695095,
      "loss": 0.2535,
      "step": 8662
    },
    {
      "epoch": 0.037015989129784564,
      "grad_norm": 0.8550443649291992,
      "learning_rate": 0.0001298923260981029,
      "loss": 0.3477,
      "step": 8663
    },
    {
      "epoch": 0.037020262013211756,
      "grad_norm": 2.9395461082458496,
      "learning_rate": 0.00012984959835925483,
      "loss": 0.8169,
      "step": 8664
    },
    {
      "epoch": 0.03702453489663895,
      "grad_norm": 1.6852576732635498,
      "learning_rate": 0.00012980687062040677,
      "loss": 0.5874,
      "step": 8665
    },
    {
      "epoch": 0.03702880778006615,
      "grad_norm": 1.589362621307373,
      "learning_rate": 0.00012976414288155873,
      "loss": 0.6913,
      "step": 8666
    },
    {
      "epoch": 0.03703308066349334,
      "grad_norm": 2.097172737121582,
      "learning_rate": 0.00012972141514271067,
      "loss": 0.5427,
      "step": 8667
    },
    {
      "epoch": 0.03703735354692053,
      "grad_norm": 1.7382564544677734,
      "learning_rate": 0.0001296786874038626,
      "loss": 0.51,
      "step": 8668
    },
    {
      "epoch": 0.037041626430347724,
      "grad_norm": 4.6253662109375,
      "learning_rate": 0.00012963595966501454,
      "loss": 0.828,
      "step": 8669
    },
    {
      "epoch": 0.03704589931377492,
      "grad_norm": 3.487776041030884,
      "learning_rate": 0.00012959323192616648,
      "loss": 0.8968,
      "step": 8670
    },
    {
      "epoch": 0.037050172197202115,
      "grad_norm": 3.090527057647705,
      "learning_rate": 0.0001295505041873184,
      "loss": 1.0638,
      "step": 8671
    },
    {
      "epoch": 0.03705444508062931,
      "grad_norm": 0.9418544173240662,
      "learning_rate": 0.00012950777644847033,
      "loss": 0.4808,
      "step": 8672
    },
    {
      "epoch": 0.037058717964056506,
      "grad_norm": 3.4692206382751465,
      "learning_rate": 0.00012946504870962227,
      "loss": 0.8305,
      "step": 8673
    },
    {
      "epoch": 0.0370629908474837,
      "grad_norm": 0.9729271531105042,
      "learning_rate": 0.00012942232097077423,
      "loss": 0.5306,
      "step": 8674
    },
    {
      "epoch": 0.03706726373091089,
      "grad_norm": 2.1661367416381836,
      "learning_rate": 0.00012937959323192617,
      "loss": 0.7885,
      "step": 8675
    },
    {
      "epoch": 0.03707153661433809,
      "grad_norm": 4.338129997253418,
      "learning_rate": 0.0001293368654930781,
      "loss": 1.0345,
      "step": 8676
    },
    {
      "epoch": 0.03707580949776528,
      "grad_norm": 4.624148845672607,
      "learning_rate": 0.00012929413775423005,
      "loss": 0.8197,
      "step": 8677
    },
    {
      "epoch": 0.037080082381192474,
      "grad_norm": 4.452178001403809,
      "learning_rate": 0.00012925141001538198,
      "loss": 0.9244,
      "step": 8678
    },
    {
      "epoch": 0.03708435526461967,
      "grad_norm": 2.551513195037842,
      "learning_rate": 0.00012920868227653392,
      "loss": 0.8281,
      "step": 8679
    },
    {
      "epoch": 0.037088628148046865,
      "grad_norm": 3.0120294094085693,
      "learning_rate": 0.00012916595453768586,
      "loss": 0.5718,
      "step": 8680
    },
    {
      "epoch": 0.03709290103147406,
      "grad_norm": 0.8107185363769531,
      "learning_rate": 0.00012912322679883782,
      "loss": 0.3056,
      "step": 8681
    },
    {
      "epoch": 0.037097173914901256,
      "grad_norm": 1.1265450716018677,
      "learning_rate": 0.00012908049905998976,
      "loss": 0.4591,
      "step": 8682
    },
    {
      "epoch": 0.03710144679832845,
      "grad_norm": 2.030601978302002,
      "learning_rate": 0.0001290377713211417,
      "loss": 0.5655,
      "step": 8683
    },
    {
      "epoch": 0.03710571968175564,
      "grad_norm": 0.44939008355140686,
      "learning_rate": 0.00012899504358229364,
      "loss": 0.1345,
      "step": 8684
    },
    {
      "epoch": 0.03710999256518284,
      "grad_norm": 0.9580003619194031,
      "learning_rate": 0.00012895231584344558,
      "loss": 0.4757,
      "step": 8685
    },
    {
      "epoch": 0.03711426544861003,
      "grad_norm": 4.542891025543213,
      "learning_rate": 0.00012890958810459751,
      "loss": 0.7259,
      "step": 8686
    },
    {
      "epoch": 0.037118538332037224,
      "grad_norm": 4.4967803955078125,
      "learning_rate": 0.00012886686036574945,
      "loss": 1.3131,
      "step": 8687
    },
    {
      "epoch": 0.03712281121546442,
      "grad_norm": 2.857409954071045,
      "learning_rate": 0.00012882413262690136,
      "loss": 0.7316,
      "step": 8688
    },
    {
      "epoch": 0.037127084098891615,
      "grad_norm": 1.3294199705123901,
      "learning_rate": 0.00012878140488805333,
      "loss": 0.7255,
      "step": 8689
    },
    {
      "epoch": 0.03713135698231881,
      "grad_norm": 0.7912008166313171,
      "learning_rate": 0.00012873867714920527,
      "loss": 0.2759,
      "step": 8690
    },
    {
      "epoch": 0.037135629865746006,
      "grad_norm": 1.371495008468628,
      "learning_rate": 0.0001286959494103572,
      "loss": 1.1141,
      "step": 8691
    },
    {
      "epoch": 0.0371399027491732,
      "grad_norm": 1.4012537002563477,
      "learning_rate": 0.00012865322167150914,
      "loss": 0.3543,
      "step": 8692
    },
    {
      "epoch": 0.03714417563260039,
      "grad_norm": 0.9854257106781006,
      "learning_rate": 0.00012861049393266108,
      "loss": 0.4665,
      "step": 8693
    },
    {
      "epoch": 0.03714844851602758,
      "grad_norm": 1.3490269184112549,
      "learning_rate": 0.00012856776619381302,
      "loss": 0.7635,
      "step": 8694
    },
    {
      "epoch": 0.03715272139945478,
      "grad_norm": 4.536164283752441,
      "learning_rate": 0.00012852503845496495,
      "loss": 1.2362,
      "step": 8695
    },
    {
      "epoch": 0.037156994282881974,
      "grad_norm": 0.593012273311615,
      "learning_rate": 0.00012848231071611692,
      "loss": 0.2685,
      "step": 8696
    },
    {
      "epoch": 0.037161267166309166,
      "grad_norm": 2.6354286670684814,
      "learning_rate": 0.00012843958297726886,
      "loss": 0.7684,
      "step": 8697
    },
    {
      "epoch": 0.037165540049736365,
      "grad_norm": 4.512338638305664,
      "learning_rate": 0.0001283968552384208,
      "loss": 1.1462,
      "step": 8698
    },
    {
      "epoch": 0.03716981293316356,
      "grad_norm": 4.12320613861084,
      "learning_rate": 0.00012835412749957273,
      "loss": 1.2302,
      "step": 8699
    },
    {
      "epoch": 0.03717408581659075,
      "grad_norm": 3.3828227519989014,
      "learning_rate": 0.00012831139976072467,
      "loss": 1.0437,
      "step": 8700
    },
    {
      "epoch": 0.03717835870001795,
      "grad_norm": 2.0199241638183594,
      "learning_rate": 0.0001282686720218766,
      "loss": 0.739,
      "step": 8701
    },
    {
      "epoch": 0.03718263158344514,
      "grad_norm": 3.4291462898254395,
      "learning_rate": 0.00012822594428302855,
      "loss": 0.7184,
      "step": 8702
    },
    {
      "epoch": 0.03718690446687233,
      "grad_norm": 1.2932595014572144,
      "learning_rate": 0.00012818321654418048,
      "loss": 0.7026,
      "step": 8703
    },
    {
      "epoch": 0.03719117735029953,
      "grad_norm": 1.0096766948699951,
      "learning_rate": 0.00012814048880533242,
      "loss": 0.4655,
      "step": 8704
    },
    {
      "epoch": 0.03719545023372672,
      "grad_norm": 1.1769570112228394,
      "learning_rate": 0.00012809776106648436,
      "loss": 0.4511,
      "step": 8705
    },
    {
      "epoch": 0.037199723117153916,
      "grad_norm": 1.4192733764648438,
      "learning_rate": 0.0001280550333276363,
      "loss": 1.14,
      "step": 8706
    },
    {
      "epoch": 0.037203996000581115,
      "grad_norm": 2.410721778869629,
      "learning_rate": 0.00012801230558878824,
      "loss": 0.788,
      "step": 8707
    },
    {
      "epoch": 0.03720826888400831,
      "grad_norm": 3.350666046142578,
      "learning_rate": 0.00012796957784994017,
      "loss": 0.9516,
      "step": 8708
    },
    {
      "epoch": 0.0372125417674355,
      "grad_norm": 1.2773863077163696,
      "learning_rate": 0.0001279268501110921,
      "loss": 0.4297,
      "step": 8709
    },
    {
      "epoch": 0.0372168146508627,
      "grad_norm": 3.751127243041992,
      "learning_rate": 0.00012788412237224405,
      "loss": 2.3275,
      "step": 8710
    },
    {
      "epoch": 0.03722108753428989,
      "grad_norm": 0.5328958034515381,
      "learning_rate": 0.00012784139463339601,
      "loss": 0.1342,
      "step": 8711
    },
    {
      "epoch": 0.03722536041771708,
      "grad_norm": 1.792973518371582,
      "learning_rate": 0.00012779866689454795,
      "loss": 0.5742,
      "step": 8712
    },
    {
      "epoch": 0.03722963330114428,
      "grad_norm": 2.1723694801330566,
      "learning_rate": 0.0001277559391556999,
      "loss": 0.5334,
      "step": 8713
    },
    {
      "epoch": 0.03723390618457147,
      "grad_norm": 1.2709630727767944,
      "learning_rate": 0.00012771321141685183,
      "loss": 0.7028,
      "step": 8714
    },
    {
      "epoch": 0.037238179067998665,
      "grad_norm": 3.3865671157836914,
      "learning_rate": 0.00012767048367800377,
      "loss": 0.6812,
      "step": 8715
    },
    {
      "epoch": 0.037242451951425865,
      "grad_norm": 1.318618655204773,
      "learning_rate": 0.0001276277559391557,
      "loss": 0.5205,
      "step": 8716
    },
    {
      "epoch": 0.03724672483485306,
      "grad_norm": 2.8216962814331055,
      "learning_rate": 0.00012758502820030764,
      "loss": 0.7094,
      "step": 8717
    },
    {
      "epoch": 0.03725099771828025,
      "grad_norm": 2.823387861251831,
      "learning_rate": 0.0001275423004614596,
      "loss": 0.6536,
      "step": 8718
    },
    {
      "epoch": 0.03725527060170744,
      "grad_norm": 0.9713091254234314,
      "learning_rate": 0.00012749957272261154,
      "loss": 0.461,
      "step": 8719
    },
    {
      "epoch": 0.03725954348513464,
      "grad_norm": 3.279576063156128,
      "learning_rate": 0.00012745684498376345,
      "loss": 0.6148,
      "step": 8720
    },
    {
      "epoch": 0.03726381636856183,
      "grad_norm": 0.7691527605056763,
      "learning_rate": 0.0001274141172449154,
      "loss": 0.2325,
      "step": 8721
    },
    {
      "epoch": 0.037268089251989024,
      "grad_norm": 0.602685809135437,
      "learning_rate": 0.00012737138950606733,
      "loss": 0.1946,
      "step": 8722
    },
    {
      "epoch": 0.03727236213541622,
      "grad_norm": 1.2190263271331787,
      "learning_rate": 0.00012732866176721927,
      "loss": 0.4694,
      "step": 8723
    },
    {
      "epoch": 0.037276635018843415,
      "grad_norm": 0.6733080148696899,
      "learning_rate": 0.0001272859340283712,
      "loss": 0.3046,
      "step": 8724
    },
    {
      "epoch": 0.03728090790227061,
      "grad_norm": 3.155078887939453,
      "learning_rate": 0.00012724320628952314,
      "loss": 0.8656,
      "step": 8725
    },
    {
      "epoch": 0.037285180785697807,
      "grad_norm": 1.2432194948196411,
      "learning_rate": 0.0001272004785506751,
      "loss": 0.6892,
      "step": 8726
    },
    {
      "epoch": 0.037289453669125,
      "grad_norm": 1.0679795742034912,
      "learning_rate": 0.00012715775081182705,
      "loss": 0.4294,
      "step": 8727
    },
    {
      "epoch": 0.03729372655255219,
      "grad_norm": 1.2868915796279907,
      "learning_rate": 0.00012711502307297898,
      "loss": 0.8766,
      "step": 8728
    },
    {
      "epoch": 0.03729799943597939,
      "grad_norm": 3.4314186573028564,
      "learning_rate": 0.00012707229533413092,
      "loss": 1.1132,
      "step": 8729
    },
    {
      "epoch": 0.03730227231940658,
      "grad_norm": 0.9586467146873474,
      "learning_rate": 0.00012702956759528286,
      "loss": 0.3814,
      "step": 8730
    },
    {
      "epoch": 0.037306545202833774,
      "grad_norm": 1.4491117000579834,
      "learning_rate": 0.0001269868398564348,
      "loss": 0.3406,
      "step": 8731
    },
    {
      "epoch": 0.03731081808626097,
      "grad_norm": 1.17678964138031,
      "learning_rate": 0.00012694411211758674,
      "loss": 0.6641,
      "step": 8732
    },
    {
      "epoch": 0.037315090969688165,
      "grad_norm": 4.771146297454834,
      "learning_rate": 0.0001269013843787387,
      "loss": 0.949,
      "step": 8733
    },
    {
      "epoch": 0.03731936385311536,
      "grad_norm": 0.32947978377342224,
      "learning_rate": 0.00012685865663989064,
      "loss": 0.0889,
      "step": 8734
    },
    {
      "epoch": 0.037323636736542556,
      "grad_norm": 1.5037864446640015,
      "learning_rate": 0.00012681592890104258,
      "loss": 0.6355,
      "step": 8735
    },
    {
      "epoch": 0.03732790961996975,
      "grad_norm": 2.768138885498047,
      "learning_rate": 0.00012677320116219451,
      "loss": 0.8577,
      "step": 8736
    },
    {
      "epoch": 0.03733218250339694,
      "grad_norm": 0.7115994095802307,
      "learning_rate": 0.00012673047342334643,
      "loss": 0.2235,
      "step": 8737
    },
    {
      "epoch": 0.03733645538682414,
      "grad_norm": 1.1835200786590576,
      "learning_rate": 0.00012668774568449836,
      "loss": 0.4265,
      "step": 8738
    },
    {
      "epoch": 0.03734072827025133,
      "grad_norm": 2.81093168258667,
      "learning_rate": 0.0001266450179456503,
      "loss": 0.6053,
      "step": 8739
    },
    {
      "epoch": 0.037345001153678524,
      "grad_norm": 0.9912075996398926,
      "learning_rate": 0.00012660229020680224,
      "loss": 0.3885,
      "step": 8740
    },
    {
      "epoch": 0.03734927403710572,
      "grad_norm": 1.1757856607437134,
      "learning_rate": 0.0001265595624679542,
      "loss": 0.5081,
      "step": 8741
    },
    {
      "epoch": 0.037353546920532915,
      "grad_norm": 3.943932294845581,
      "learning_rate": 0.00012651683472910614,
      "loss": 1.2819,
      "step": 8742
    },
    {
      "epoch": 0.03735781980396011,
      "grad_norm": 2.524750232696533,
      "learning_rate": 0.00012647410699025808,
      "loss": 0.6883,
      "step": 8743
    },
    {
      "epoch": 0.0373620926873873,
      "grad_norm": 2.1718297004699707,
      "learning_rate": 0.00012643137925141002,
      "loss": 0.6388,
      "step": 8744
    },
    {
      "epoch": 0.0373663655708145,
      "grad_norm": 1.0627559423446655,
      "learning_rate": 0.00012638865151256195,
      "loss": 0.6305,
      "step": 8745
    },
    {
      "epoch": 0.03737063845424169,
      "grad_norm": 1.3977912664413452,
      "learning_rate": 0.0001263459237737139,
      "loss": 1.1305,
      "step": 8746
    },
    {
      "epoch": 0.03737491133766888,
      "grad_norm": 1.1555570363998413,
      "learning_rate": 0.00012630319603486583,
      "loss": 0.818,
      "step": 8747
    },
    {
      "epoch": 0.03737918422109608,
      "grad_norm": 0.9439635872840881,
      "learning_rate": 0.0001262604682960178,
      "loss": 0.3813,
      "step": 8748
    },
    {
      "epoch": 0.037383457104523274,
      "grad_norm": 0.9814180731773376,
      "learning_rate": 0.00012621774055716973,
      "loss": 0.533,
      "step": 8749
    },
    {
      "epoch": 0.037387729987950466,
      "grad_norm": 0.7322595715522766,
      "learning_rate": 0.00012617501281832167,
      "loss": 0.2321,
      "step": 8750
    },
    {
      "epoch": 0.037392002871377665,
      "grad_norm": 1.410679817199707,
      "learning_rate": 0.0001261322850794736,
      "loss": 1.13,
      "step": 8751
    },
    {
      "epoch": 0.03739627575480486,
      "grad_norm": 1.4115118980407715,
      "learning_rate": 0.00012608955734062555,
      "loss": 1.1305,
      "step": 8752
    },
    {
      "epoch": 0.03740054863823205,
      "grad_norm": 0.7294877767562866,
      "learning_rate": 0.00012604682960177746,
      "loss": 0.3088,
      "step": 8753
    },
    {
      "epoch": 0.03740482152165925,
      "grad_norm": 0.9831414818763733,
      "learning_rate": 0.0001260041018629294,
      "loss": 0.404,
      "step": 8754
    },
    {
      "epoch": 0.03740909440508644,
      "grad_norm": 2.0430307388305664,
      "learning_rate": 0.00012596137412408133,
      "loss": 0.5212,
      "step": 8755
    },
    {
      "epoch": 0.03741336728851363,
      "grad_norm": 2.107633590698242,
      "learning_rate": 0.0001259186463852333,
      "loss": 0.579,
      "step": 8756
    },
    {
      "epoch": 0.03741764017194083,
      "grad_norm": 3.1989974975585938,
      "learning_rate": 0.00012587591864638524,
      "loss": 0.5601,
      "step": 8757
    },
    {
      "epoch": 0.037421913055368024,
      "grad_norm": 2.2556815147399902,
      "learning_rate": 0.00012583319090753717,
      "loss": 0.727,
      "step": 8758
    },
    {
      "epoch": 0.037426185938795216,
      "grad_norm": 0.7213073372840881,
      "learning_rate": 0.0001257904631686891,
      "loss": 0.2929,
      "step": 8759
    },
    {
      "epoch": 0.037430458822222415,
      "grad_norm": 0.7775305509567261,
      "learning_rate": 0.00012574773542984105,
      "loss": 0.3417,
      "step": 8760
    },
    {
      "epoch": 0.03743473170564961,
      "grad_norm": 1.439274549484253,
      "learning_rate": 0.000125705007690993,
      "loss": 0.3012,
      "step": 8761
    },
    {
      "epoch": 0.0374390045890768,
      "grad_norm": 3.8428902626037598,
      "learning_rate": 0.00012566227995214493,
      "loss": 1.1788,
      "step": 8762
    },
    {
      "epoch": 0.037443277472504,
      "grad_norm": 3.5569639205932617,
      "learning_rate": 0.0001256195522132969,
      "loss": 1.196,
      "step": 8763
    },
    {
      "epoch": 0.03744755035593119,
      "grad_norm": 0.8454703092575073,
      "learning_rate": 0.00012557682447444883,
      "loss": 0.4477,
      "step": 8764
    },
    {
      "epoch": 0.03745182323935838,
      "grad_norm": 2.009789228439331,
      "learning_rate": 0.00012553409673560077,
      "loss": 0.4867,
      "step": 8765
    },
    {
      "epoch": 0.03745609612278558,
      "grad_norm": 1.0752474069595337,
      "learning_rate": 0.0001254913689967527,
      "loss": 0.641,
      "step": 8766
    },
    {
      "epoch": 0.037460369006212774,
      "grad_norm": 1.7281291484832764,
      "learning_rate": 0.00012544864125790464,
      "loss": 0.5478,
      "step": 8767
    },
    {
      "epoch": 0.037464641889639966,
      "grad_norm": 0.9674127101898193,
      "learning_rate": 0.00012540591351905658,
      "loss": 0.3969,
      "step": 8768
    },
    {
      "epoch": 0.03746891477306716,
      "grad_norm": 2.069611072540283,
      "learning_rate": 0.00012536318578020852,
      "loss": 0.5471,
      "step": 8769
    },
    {
      "epoch": 0.03747318765649436,
      "grad_norm": 3.228515148162842,
      "learning_rate": 0.00012532045804136043,
      "loss": 0.5697,
      "step": 8770
    },
    {
      "epoch": 0.03747746053992155,
      "grad_norm": 2.2705605030059814,
      "learning_rate": 0.0001252777303025124,
      "loss": 0.7224,
      "step": 8771
    },
    {
      "epoch": 0.03748173342334874,
      "grad_norm": 4.053878307342529,
      "learning_rate": 0.00012523500256366433,
      "loss": 1.2444,
      "step": 8772
    },
    {
      "epoch": 0.03748600630677594,
      "grad_norm": 2.717841386795044,
      "learning_rate": 0.00012519227482481627,
      "loss": 0.7856,
      "step": 8773
    },
    {
      "epoch": 0.03749027919020313,
      "grad_norm": 0.9238390922546387,
      "learning_rate": 0.0001251495470859682,
      "loss": 0.5362,
      "step": 8774
    },
    {
      "epoch": 0.037494552073630324,
      "grad_norm": 0.742038905620575,
      "learning_rate": 0.00012510681934712014,
      "loss": 0.2768,
      "step": 8775
    },
    {
      "epoch": 0.037498824957057524,
      "grad_norm": 0.7280123829841614,
      "learning_rate": 0.00012506409160827208,
      "loss": 0.2645,
      "step": 8776
    },
    {
      "epoch": 0.037503097840484716,
      "grad_norm": 0.9086929559707642,
      "learning_rate": 0.00012502136386942402,
      "loss": 0.5127,
      "step": 8777
    },
    {
      "epoch": 0.03750737072391191,
      "grad_norm": 4.700216293334961,
      "learning_rate": 0.00012497863613057599,
      "loss": 0.9576,
      "step": 8778
    },
    {
      "epoch": 0.03751164360733911,
      "grad_norm": 1.4255845546722412,
      "learning_rate": 0.00012493590839172792,
      "loss": 1.0411,
      "step": 8779
    },
    {
      "epoch": 0.0375159164907663,
      "grad_norm": 3.3074893951416016,
      "learning_rate": 0.00012489318065287986,
      "loss": 0.9504,
      "step": 8780
    },
    {
      "epoch": 0.03752018937419349,
      "grad_norm": 1.6631866693496704,
      "learning_rate": 0.0001248504529140318,
      "loss": 0.5147,
      "step": 8781
    },
    {
      "epoch": 0.03752446225762069,
      "grad_norm": 2.654287099838257,
      "learning_rate": 0.00012480772517518374,
      "loss": 0.7143,
      "step": 8782
    },
    {
      "epoch": 0.03752873514104788,
      "grad_norm": 1.1003814935684204,
      "learning_rate": 0.00012476499743633567,
      "loss": 0.6688,
      "step": 8783
    },
    {
      "epoch": 0.037533008024475074,
      "grad_norm": 1.019478678703308,
      "learning_rate": 0.0001247222696974876,
      "loss": 0.4886,
      "step": 8784
    },
    {
      "epoch": 0.03753728090790227,
      "grad_norm": 0.8100730776786804,
      "learning_rate": 0.00012467954195863955,
      "loss": 0.3495,
      "step": 8785
    },
    {
      "epoch": 0.037541553791329466,
      "grad_norm": 1.7752938270568848,
      "learning_rate": 0.0001246368142197915,
      "loss": 0.4988,
      "step": 8786
    },
    {
      "epoch": 0.03754582667475666,
      "grad_norm": 1.015107274055481,
      "learning_rate": 0.00012459408648094343,
      "loss": 0.4294,
      "step": 8787
    },
    {
      "epoch": 0.03755009955818386,
      "grad_norm": 3.26424503326416,
      "learning_rate": 0.00012455135874209536,
      "loss": 0.5833,
      "step": 8788
    },
    {
      "epoch": 0.03755437244161105,
      "grad_norm": 2.057406425476074,
      "learning_rate": 0.00012450863100324733,
      "loss": 0.5438,
      "step": 8789
    },
    {
      "epoch": 0.03755864532503824,
      "grad_norm": 1.8719035387039185,
      "learning_rate": 0.00012446590326439924,
      "loss": 0.5274,
      "step": 8790
    },
    {
      "epoch": 0.03756291820846544,
      "grad_norm": 1.4380404949188232,
      "learning_rate": 0.00012442317552555118,
      "loss": 1.0197,
      "step": 8791
    },
    {
      "epoch": 0.03756719109189263,
      "grad_norm": 0.9353579878807068,
      "learning_rate": 0.00012438044778670311,
      "loss": 0.398,
      "step": 8792
    },
    {
      "epoch": 0.037571463975319824,
      "grad_norm": 0.3642452657222748,
      "learning_rate": 0.00012433772004785508,
      "loss": 0.1075,
      "step": 8793
    },
    {
      "epoch": 0.037575736858747016,
      "grad_norm": 0.6915863156318665,
      "learning_rate": 0.00012429499230900702,
      "loss": 0.2779,
      "step": 8794
    },
    {
      "epoch": 0.037580009742174215,
      "grad_norm": 4.6803059577941895,
      "learning_rate": 0.00012425226457015896,
      "loss": 0.862,
      "step": 8795
    },
    {
      "epoch": 0.03758428262560141,
      "grad_norm": 2.764183759689331,
      "learning_rate": 0.0001242095368313109,
      "loss": 0.6803,
      "step": 8796
    },
    {
      "epoch": 0.0375885555090286,
      "grad_norm": 1.4323692321777344,
      "learning_rate": 0.00012416680909246283,
      "loss": 0.2889,
      "step": 8797
    },
    {
      "epoch": 0.0375928283924558,
      "grad_norm": 0.7048220038414001,
      "learning_rate": 0.00012412408135361477,
      "loss": 0.3262,
      "step": 8798
    },
    {
      "epoch": 0.03759710127588299,
      "grad_norm": 0.9402437806129456,
      "learning_rate": 0.0001240813536147667,
      "loss": 0.5085,
      "step": 8799
    },
    {
      "epoch": 0.03760137415931018,
      "grad_norm": 1.0784968137741089,
      "learning_rate": 0.00012403862587591864,
      "loss": 0.4583,
      "step": 8800
    },
    {
      "epoch": 0.03760564704273738,
      "grad_norm": 3.4872429370880127,
      "learning_rate": 0.00012399589813707058,
      "loss": 1.128,
      "step": 8801
    },
    {
      "epoch": 0.037609919926164574,
      "grad_norm": 1.1289526224136353,
      "learning_rate": 0.00012395317039822252,
      "loss": 0.6682,
      "step": 8802
    },
    {
      "epoch": 0.037614192809591766,
      "grad_norm": 0.783771812915802,
      "learning_rate": 0.00012391044265937446,
      "loss": 0.293,
      "step": 8803
    },
    {
      "epoch": 0.037618465693018965,
      "grad_norm": 1.7264456748962402,
      "learning_rate": 0.00012386771492052642,
      "loss": 0.4901,
      "step": 8804
    },
    {
      "epoch": 0.03762273857644616,
      "grad_norm": 3.405360221862793,
      "learning_rate": 0.00012382498718167836,
      "loss": 1.1882,
      "step": 8805
    },
    {
      "epoch": 0.03762701145987335,
      "grad_norm": 0.8080421686172485,
      "learning_rate": 0.0001237822594428303,
      "loss": 0.3702,
      "step": 8806
    },
    {
      "epoch": 0.03763128434330055,
      "grad_norm": 0.5887841582298279,
      "learning_rate": 0.0001237395317039822,
      "loss": 0.238,
      "step": 8807
    },
    {
      "epoch": 0.03763555722672774,
      "grad_norm": 0.7302847504615784,
      "learning_rate": 0.00012369680396513417,
      "loss": 0.276,
      "step": 8808
    },
    {
      "epoch": 0.03763983011015493,
      "grad_norm": 1.0068359375,
      "learning_rate": 0.0001236540762262861,
      "loss": 0.434,
      "step": 8809
    },
    {
      "epoch": 0.03764410299358213,
      "grad_norm": 2.045274496078491,
      "learning_rate": 0.00012361134848743805,
      "loss": 0.5023,
      "step": 8810
    },
    {
      "epoch": 0.037648375877009324,
      "grad_norm": 3.529031753540039,
      "learning_rate": 0.00012356862074859,
      "loss": 1.1028,
      "step": 8811
    },
    {
      "epoch": 0.037652648760436516,
      "grad_norm": 2.1374926567077637,
      "learning_rate": 0.00012352589300974193,
      "loss": 0.5282,
      "step": 8812
    },
    {
      "epoch": 0.037656921643863715,
      "grad_norm": 2.25858736038208,
      "learning_rate": 0.00012348316527089386,
      "loss": 0.6558,
      "step": 8813
    },
    {
      "epoch": 0.03766119452729091,
      "grad_norm": 5.414379596710205,
      "learning_rate": 0.0001234404375320458,
      "loss": 1.3679,
      "step": 8814
    },
    {
      "epoch": 0.0376654674107181,
      "grad_norm": 5.31159782409668,
      "learning_rate": 0.00012339770979319774,
      "loss": 1.2829,
      "step": 8815
    },
    {
      "epoch": 0.0376697402941453,
      "grad_norm": 1.0613380670547485,
      "learning_rate": 0.00012335498205434968,
      "loss": 0.4344,
      "step": 8816
    },
    {
      "epoch": 0.03767401317757249,
      "grad_norm": 1.058039665222168,
      "learning_rate": 0.00012331225431550162,
      "loss": 0.6223,
      "step": 8817
    },
    {
      "epoch": 0.03767828606099968,
      "grad_norm": 0.7136737108230591,
      "learning_rate": 0.00012326952657665355,
      "loss": 0.3613,
      "step": 8818
    },
    {
      "epoch": 0.037682558944426875,
      "grad_norm": 0.2935187816619873,
      "learning_rate": 0.00012322679883780552,
      "loss": 0.081,
      "step": 8819
    },
    {
      "epoch": 0.037686831827854074,
      "grad_norm": 1.3197789192199707,
      "learning_rate": 0.00012318407109895746,
      "loss": 0.9039,
      "step": 8820
    },
    {
      "epoch": 0.037691104711281266,
      "grad_norm": 0.7234612107276917,
      "learning_rate": 0.0001231413433601094,
      "loss": 0.3035,
      "step": 8821
    },
    {
      "epoch": 0.03769537759470846,
      "grad_norm": 2.9584007263183594,
      "learning_rate": 0.00012309861562126133,
      "loss": 0.899,
      "step": 8822
    },
    {
      "epoch": 0.03769965047813566,
      "grad_norm": 1.4801230430603027,
      "learning_rate": 0.00012305588788241327,
      "loss": 1.0598,
      "step": 8823
    },
    {
      "epoch": 0.03770392336156285,
      "grad_norm": 3.4606003761291504,
      "learning_rate": 0.0001230131601435652,
      "loss": 1.0009,
      "step": 8824
    },
    {
      "epoch": 0.03770819624499004,
      "grad_norm": 5.117558479309082,
      "learning_rate": 0.00012297043240471714,
      "loss": 1.2521,
      "step": 8825
    },
    {
      "epoch": 0.03771246912841724,
      "grad_norm": 2.8971266746520996,
      "learning_rate": 0.00012292770466586908,
      "loss": 0.8394,
      "step": 8826
    },
    {
      "epoch": 0.03771674201184443,
      "grad_norm": 1.7103885412216187,
      "learning_rate": 0.00012288497692702102,
      "loss": 0.4612,
      "step": 8827
    },
    {
      "epoch": 0.037721014895271625,
      "grad_norm": 2.0978705883026123,
      "learning_rate": 0.00012284224918817296,
      "loss": 0.4968,
      "step": 8828
    },
    {
      "epoch": 0.037725287778698824,
      "grad_norm": 0.6767578721046448,
      "learning_rate": 0.0001227995214493249,
      "loss": 0.2322,
      "step": 8829
    },
    {
      "epoch": 0.037729560662126016,
      "grad_norm": 1.3462882041931152,
      "learning_rate": 0.00012275679371047686,
      "loss": 0.6514,
      "step": 8830
    },
    {
      "epoch": 0.03773383354555321,
      "grad_norm": 1.2158241271972656,
      "learning_rate": 0.00012271406597162877,
      "loss": 0.8501,
      "step": 8831
    },
    {
      "epoch": 0.03773810642898041,
      "grad_norm": 1.961948275566101,
      "learning_rate": 0.0001226713382327807,
      "loss": 0.4656,
      "step": 8832
    },
    {
      "epoch": 0.0377423793124076,
      "grad_norm": 3.4167540073394775,
      "learning_rate": 0.00012262861049393265,
      "loss": 0.8666,
      "step": 8833
    },
    {
      "epoch": 0.03774665219583479,
      "grad_norm": 0.24339480698108673,
      "learning_rate": 0.0001225858827550846,
      "loss": 0.0641,
      "step": 8834
    },
    {
      "epoch": 0.03775092507926199,
      "grad_norm": 1.020323395729065,
      "learning_rate": 0.00012254315501623655,
      "loss": 0.6207,
      "step": 8835
    },
    {
      "epoch": 0.03775519796268918,
      "grad_norm": 1.87497878074646,
      "learning_rate": 0.0001225004272773885,
      "loss": 0.5212,
      "step": 8836
    },
    {
      "epoch": 0.037759470846116375,
      "grad_norm": 0.6155610680580139,
      "learning_rate": 0.00012245769953854043,
      "loss": 0.2165,
      "step": 8837
    },
    {
      "epoch": 0.037763743729543574,
      "grad_norm": 1.166599154472351,
      "learning_rate": 0.00012241497179969236,
      "loss": 0.8285,
      "step": 8838
    },
    {
      "epoch": 0.037768016612970766,
      "grad_norm": 4.312163829803467,
      "learning_rate": 0.0001223722440608443,
      "loss": 1.123,
      "step": 8839
    },
    {
      "epoch": 0.03777228949639796,
      "grad_norm": 0.9540718793869019,
      "learning_rate": 0.00012232951632199624,
      "loss": 0.3521,
      "step": 8840
    },
    {
      "epoch": 0.03777656237982516,
      "grad_norm": 2.5335283279418945,
      "learning_rate": 0.00012228678858314818,
      "loss": 0.7401,
      "step": 8841
    },
    {
      "epoch": 0.03778083526325235,
      "grad_norm": 1.1650195121765137,
      "learning_rate": 0.00012224406084430012,
      "loss": 0.8266,
      "step": 8842
    },
    {
      "epoch": 0.03778510814667954,
      "grad_norm": 0.9762405753135681,
      "learning_rate": 0.00012220133310545205,
      "loss": 0.5894,
      "step": 8843
    },
    {
      "epoch": 0.03778938103010673,
      "grad_norm": 2.186774492263794,
      "learning_rate": 0.000122158605366604,
      "loss": 0.7183,
      "step": 8844
    },
    {
      "epoch": 0.03779365391353393,
      "grad_norm": 0.7896563410758972,
      "learning_rate": 0.00012211587762775596,
      "loss": 0.305,
      "step": 8845
    },
    {
      "epoch": 0.037797926796961125,
      "grad_norm": 0.9692503809928894,
      "learning_rate": 0.0001220731498889079,
      "loss": 0.5892,
      "step": 8846
    },
    {
      "epoch": 0.03780219968038832,
      "grad_norm": 0.8308500051498413,
      "learning_rate": 0.00012203042215005983,
      "loss": 0.3329,
      "step": 8847
    },
    {
      "epoch": 0.037806472563815516,
      "grad_norm": 0.9586238265037537,
      "learning_rate": 0.00012198769441121176,
      "loss": 0.4802,
      "step": 8848
    },
    {
      "epoch": 0.03781074544724271,
      "grad_norm": 3.7480764389038086,
      "learning_rate": 0.0001219449666723637,
      "loss": 1.1157,
      "step": 8849
    },
    {
      "epoch": 0.0378150183306699,
      "grad_norm": 1.565914511680603,
      "learning_rate": 0.00012190223893351563,
      "loss": 0.4496,
      "step": 8850
    },
    {
      "epoch": 0.0378192912140971,
      "grad_norm": 1.0286096334457397,
      "learning_rate": 0.00012185951119466758,
      "loss": 0.6532,
      "step": 8851
    },
    {
      "epoch": 0.03782356409752429,
      "grad_norm": 1.8353172540664673,
      "learning_rate": 0.00012181678345581952,
      "loss": 0.5171,
      "step": 8852
    },
    {
      "epoch": 0.03782783698095148,
      "grad_norm": 4.142068386077881,
      "learning_rate": 0.00012177405571697146,
      "loss": 1.3623,
      "step": 8853
    },
    {
      "epoch": 0.03783210986437868,
      "grad_norm": 0.7863189578056335,
      "learning_rate": 0.00012173132797812341,
      "loss": 0.3517,
      "step": 8854
    },
    {
      "epoch": 0.037836382747805875,
      "grad_norm": 3.925686836242676,
      "learning_rate": 0.00012168860023927535,
      "loss": 1.1951,
      "step": 8855
    },
    {
      "epoch": 0.03784065563123307,
      "grad_norm": 0.8029037714004517,
      "learning_rate": 0.00012164587250042727,
      "loss": 0.3519,
      "step": 8856
    },
    {
      "epoch": 0.037844928514660266,
      "grad_norm": 0.6578746438026428,
      "learning_rate": 0.00012160314476157921,
      "loss": 0.2153,
      "step": 8857
    },
    {
      "epoch": 0.03784920139808746,
      "grad_norm": 4.658682346343994,
      "learning_rate": 0.00012156041702273116,
      "loss": 0.9091,
      "step": 8858
    },
    {
      "epoch": 0.03785347428151465,
      "grad_norm": 0.9693129658699036,
      "learning_rate": 0.0001215176892838831,
      "loss": 0.5654,
      "step": 8859
    },
    {
      "epoch": 0.03785774716494185,
      "grad_norm": 1.3954930305480957,
      "learning_rate": 0.00012147496154503504,
      "loss": 0.6013,
      "step": 8860
    },
    {
      "epoch": 0.03786202004836904,
      "grad_norm": 2.1229207515716553,
      "learning_rate": 0.00012143223380618698,
      "loss": 0.6314,
      "step": 8861
    },
    {
      "epoch": 0.03786629293179623,
      "grad_norm": 3.4301605224609375,
      "learning_rate": 0.00012138950606733893,
      "loss": 0.7263,
      "step": 8862
    },
    {
      "epoch": 0.03787056581522343,
      "grad_norm": 0.9735657572746277,
      "learning_rate": 0.00012134677832849086,
      "loss": 0.556,
      "step": 8863
    },
    {
      "epoch": 0.037874838698650624,
      "grad_norm": 2.617913246154785,
      "learning_rate": 0.00012130405058964279,
      "loss": 0.7132,
      "step": 8864
    },
    {
      "epoch": 0.03787911158207782,
      "grad_norm": 1.8712749481201172,
      "learning_rate": 0.00012126132285079474,
      "loss": 0.4467,
      "step": 8865
    },
    {
      "epoch": 0.03788338446550501,
      "grad_norm": 3.4065263271331787,
      "learning_rate": 0.00012121859511194668,
      "loss": 0.8009,
      "step": 8866
    },
    {
      "epoch": 0.03788765734893221,
      "grad_norm": 0.8287439346313477,
      "learning_rate": 0.00012117586737309862,
      "loss": 0.4162,
      "step": 8867
    },
    {
      "epoch": 0.0378919302323594,
      "grad_norm": 1.016601324081421,
      "learning_rate": 0.00012113313963425055,
      "loss": 0.6297,
      "step": 8868
    },
    {
      "epoch": 0.03789620311578659,
      "grad_norm": 0.9131786823272705,
      "learning_rate": 0.0001210904118954025,
      "loss": 0.4963,
      "step": 8869
    },
    {
      "epoch": 0.03790047599921379,
      "grad_norm": 0.3612350821495056,
      "learning_rate": 0.00012104768415655444,
      "loss": 0.111,
      "step": 8870
    },
    {
      "epoch": 0.03790474888264098,
      "grad_norm": 0.6433851718902588,
      "learning_rate": 0.00012100495641770638,
      "loss": 0.238,
      "step": 8871
    },
    {
      "epoch": 0.037909021766068175,
      "grad_norm": 1.8240307569503784,
      "learning_rate": 0.00012096222867885832,
      "loss": 0.4642,
      "step": 8872
    },
    {
      "epoch": 0.037913294649495374,
      "grad_norm": 3.4086790084838867,
      "learning_rate": 0.00012091950094001026,
      "loss": 0.966,
      "step": 8873
    },
    {
      "epoch": 0.037917567532922566,
      "grad_norm": 2.1462066173553467,
      "learning_rate": 0.0001208767732011622,
      "loss": 0.5902,
      "step": 8874
    },
    {
      "epoch": 0.03792184041634976,
      "grad_norm": 4.994144439697266,
      "learning_rate": 0.00012083404546231413,
      "loss": 1.1422,
      "step": 8875
    },
    {
      "epoch": 0.03792611329977696,
      "grad_norm": 0.9178332090377808,
      "learning_rate": 0.00012079131772346607,
      "loss": 0.4139,
      "step": 8876
    },
    {
      "epoch": 0.03793038618320415,
      "grad_norm": 0.8900607228279114,
      "learning_rate": 0.00012074858998461802,
      "loss": 0.3385,
      "step": 8877
    },
    {
      "epoch": 0.03793465906663134,
      "grad_norm": 2.5618064403533936,
      "learning_rate": 0.00012070586224576996,
      "loss": 0.7307,
      "step": 8878
    },
    {
      "epoch": 0.03793893195005854,
      "grad_norm": 0.9775904417037964,
      "learning_rate": 0.0001206631345069219,
      "loss": 0.5555,
      "step": 8879
    },
    {
      "epoch": 0.03794320483348573,
      "grad_norm": 2.1426098346710205,
      "learning_rate": 0.00012062040676807385,
      "loss": 0.7638,
      "step": 8880
    },
    {
      "epoch": 0.037947477716912925,
      "grad_norm": 0.7700807452201843,
      "learning_rate": 0.00012057767902922577,
      "loss": 0.3035,
      "step": 8881
    },
    {
      "epoch": 0.037951750600340124,
      "grad_norm": 0.8912177681922913,
      "learning_rate": 0.00012053495129037771,
      "loss": 0.4752,
      "step": 8882
    },
    {
      "epoch": 0.037956023483767316,
      "grad_norm": 2.104688882827759,
      "learning_rate": 0.00012049222355152965,
      "loss": 0.6082,
      "step": 8883
    },
    {
      "epoch": 0.03796029636719451,
      "grad_norm": 1.5908101797103882,
      "learning_rate": 0.0001204494958126816,
      "loss": 0.8016,
      "step": 8884
    },
    {
      "epoch": 0.03796456925062171,
      "grad_norm": 1.4549888372421265,
      "learning_rate": 0.00012040676807383354,
      "loss": 0.332,
      "step": 8885
    },
    {
      "epoch": 0.0379688421340489,
      "grad_norm": 0.679396390914917,
      "learning_rate": 0.00012036404033498548,
      "loss": 0.2933,
      "step": 8886
    },
    {
      "epoch": 0.03797311501747609,
      "grad_norm": 1.5808085203170776,
      "learning_rate": 0.00012032131259613741,
      "loss": 0.4554,
      "step": 8887
    },
    {
      "epoch": 0.03797738790090329,
      "grad_norm": 3.0136866569519043,
      "learning_rate": 0.00012027858485728936,
      "loss": 1.9568,
      "step": 8888
    },
    {
      "epoch": 0.03798166078433048,
      "grad_norm": 4.162269592285156,
      "learning_rate": 0.00012023585711844129,
      "loss": 2.6443,
      "step": 8889
    },
    {
      "epoch": 0.037985933667757675,
      "grad_norm": 3.161478042602539,
      "learning_rate": 0.00012019312937959323,
      "loss": 1.0277,
      "step": 8890
    },
    {
      "epoch": 0.03799020655118487,
      "grad_norm": 0.9468551278114319,
      "learning_rate": 0.00012015040164074518,
      "loss": 0.5355,
      "step": 8891
    },
    {
      "epoch": 0.037994479434612066,
      "grad_norm": 3.340487003326416,
      "learning_rate": 0.00012010767390189712,
      "loss": 1.013,
      "step": 8892
    },
    {
      "epoch": 0.03799875231803926,
      "grad_norm": 1.0256867408752441,
      "learning_rate": 0.00012006494616304905,
      "loss": 0.4513,
      "step": 8893
    },
    {
      "epoch": 0.03800302520146645,
      "grad_norm": 4.291538715362549,
      "learning_rate": 0.00012002221842420099,
      "loss": 1.1805,
      "step": 8894
    },
    {
      "epoch": 0.03800729808489365,
      "grad_norm": 0.9702171087265015,
      "learning_rate": 0.00011997949068535294,
      "loss": 0.6026,
      "step": 8895
    },
    {
      "epoch": 0.03801157096832084,
      "grad_norm": 1.154483675956726,
      "learning_rate": 0.00011993676294650488,
      "loss": 0.8526,
      "step": 8896
    },
    {
      "epoch": 0.038015843851748034,
      "grad_norm": 3.7153689861297607,
      "learning_rate": 0.0001198940352076568,
      "loss": 2.8526,
      "step": 8897
    },
    {
      "epoch": 0.03802011673517523,
      "grad_norm": 0.7296510934829712,
      "learning_rate": 0.00011985130746880874,
      "loss": 0.363,
      "step": 8898
    },
    {
      "epoch": 0.038024389618602425,
      "grad_norm": 1.6667029857635498,
      "learning_rate": 0.0001198085797299607,
      "loss": 0.3856,
      "step": 8899
    },
    {
      "epoch": 0.03802866250202962,
      "grad_norm": 0.49322623014450073,
      "learning_rate": 0.00011976585199111263,
      "loss": 0.1295,
      "step": 8900
    },
    {
      "epoch": 0.038032935385456816,
      "grad_norm": 0.7021450400352478,
      "learning_rate": 0.00011972312425226457,
      "loss": 0.357,
      "step": 8901
    },
    {
      "epoch": 0.03803720826888401,
      "grad_norm": 1.996392011642456,
      "learning_rate": 0.00011968039651341651,
      "loss": 0.536,
      "step": 8902
    },
    {
      "epoch": 0.0380414811523112,
      "grad_norm": 2.1076290607452393,
      "learning_rate": 0.00011963766877456846,
      "loss": 0.5191,
      "step": 8903
    },
    {
      "epoch": 0.0380457540357384,
      "grad_norm": 0.8568244576454163,
      "learning_rate": 0.0001195949410357204,
      "loss": 0.364,
      "step": 8904
    },
    {
      "epoch": 0.03805002691916559,
      "grad_norm": 0.8001219630241394,
      "learning_rate": 0.00011955221329687232,
      "loss": 0.3348,
      "step": 8905
    },
    {
      "epoch": 0.038054299802592784,
      "grad_norm": 0.9644392728805542,
      "learning_rate": 0.00011950948555802427,
      "loss": 0.6128,
      "step": 8906
    },
    {
      "epoch": 0.03805857268601998,
      "grad_norm": 0.9503124952316284,
      "learning_rate": 0.00011946675781917621,
      "loss": 0.3809,
      "step": 8907
    },
    {
      "epoch": 0.038062845569447175,
      "grad_norm": 3.6634225845336914,
      "learning_rate": 0.00011942403008032815,
      "loss": 0.8902,
      "step": 8908
    },
    {
      "epoch": 0.03806711845287437,
      "grad_norm": 2.19416880607605,
      "learning_rate": 0.00011938130234148009,
      "loss": 0.8892,
      "step": 8909
    },
    {
      "epoch": 0.038071391336301566,
      "grad_norm": 1.732560396194458,
      "learning_rate": 0.00011933857460263204,
      "loss": 0.4582,
      "step": 8910
    },
    {
      "epoch": 0.03807566421972876,
      "grad_norm": 1.558471918106079,
      "learning_rate": 0.00011929584686378398,
      "loss": 1.1665,
      "step": 8911
    },
    {
      "epoch": 0.03807993710315595,
      "grad_norm": 0.566504955291748,
      "learning_rate": 0.00011925311912493591,
      "loss": 0.2687,
      "step": 8912
    },
    {
      "epoch": 0.03808420998658315,
      "grad_norm": 0.7938047647476196,
      "learning_rate": 0.00011921039138608785,
      "loss": 0.3189,
      "step": 8913
    },
    {
      "epoch": 0.03808848287001034,
      "grad_norm": 1.8912519216537476,
      "learning_rate": 0.00011916766364723979,
      "loss": 0.6739,
      "step": 8914
    },
    {
      "epoch": 0.038092755753437534,
      "grad_norm": 0.9189940094947815,
      "learning_rate": 0.00011912493590839173,
      "loss": 0.3662,
      "step": 8915
    },
    {
      "epoch": 0.038097028636864726,
      "grad_norm": 0.8243793249130249,
      "learning_rate": 0.00011908220816954366,
      "loss": 0.4797,
      "step": 8916
    },
    {
      "epoch": 0.038101301520291925,
      "grad_norm": 4.215463161468506,
      "learning_rate": 0.00011903948043069562,
      "loss": 1.4413,
      "step": 8917
    },
    {
      "epoch": 0.03810557440371912,
      "grad_norm": 0.9431774616241455,
      "learning_rate": 0.00011899675269184755,
      "loss": 0.6045,
      "step": 8918
    },
    {
      "epoch": 0.03810984728714631,
      "grad_norm": 1.1130790710449219,
      "learning_rate": 0.00011895402495299949,
      "loss": 0.8814,
      "step": 8919
    },
    {
      "epoch": 0.03811412017057351,
      "grad_norm": 0.8194200396537781,
      "learning_rate": 0.00011891129721415143,
      "loss": 0.4818,
      "step": 8920
    },
    {
      "epoch": 0.0381183930540007,
      "grad_norm": 4.609435081481934,
      "learning_rate": 0.00011886856947530338,
      "loss": 1.1454,
      "step": 8921
    },
    {
      "epoch": 0.03812266593742789,
      "grad_norm": 1.5430998802185059,
      "learning_rate": 0.0001188258417364553,
      "loss": 1.1938,
      "step": 8922
    },
    {
      "epoch": 0.03812693882085509,
      "grad_norm": 0.8592602610588074,
      "learning_rate": 0.00011878311399760724,
      "loss": 0.5194,
      "step": 8923
    },
    {
      "epoch": 0.03813121170428228,
      "grad_norm": 0.8231372833251953,
      "learning_rate": 0.00011874038625875918,
      "loss": 0.2932,
      "step": 8924
    },
    {
      "epoch": 0.038135484587709476,
      "grad_norm": 1.9822356700897217,
      "learning_rate": 0.00011869765851991113,
      "loss": 0.5797,
      "step": 8925
    },
    {
      "epoch": 0.038139757471136675,
      "grad_norm": 3.987236738204956,
      "learning_rate": 0.00011865493078106307,
      "loss": 1.2144,
      "step": 8926
    },
    {
      "epoch": 0.03814403035456387,
      "grad_norm": 4.825875282287598,
      "learning_rate": 0.00011861220304221501,
      "loss": 1.226,
      "step": 8927
    },
    {
      "epoch": 0.03814830323799106,
      "grad_norm": 1.4674592018127441,
      "learning_rate": 0.00011856947530336695,
      "loss": 0.3368,
      "step": 8928
    },
    {
      "epoch": 0.03815257612141826,
      "grad_norm": 1.0528608560562134,
      "learning_rate": 0.0001185267475645189,
      "loss": 0.4187,
      "step": 8929
    },
    {
      "epoch": 0.03815684900484545,
      "grad_norm": 0.8174424767494202,
      "learning_rate": 0.00011848401982567082,
      "loss": 0.3187,
      "step": 8930
    },
    {
      "epoch": 0.03816112188827264,
      "grad_norm": 0.8400221467018127,
      "learning_rate": 0.00011844129208682276,
      "loss": 0.4089,
      "step": 8931
    },
    {
      "epoch": 0.03816539477169984,
      "grad_norm": 1.5066156387329102,
      "learning_rate": 0.00011839856434797471,
      "loss": 1.1131,
      "step": 8932
    },
    {
      "epoch": 0.03816966765512703,
      "grad_norm": 4.653923988342285,
      "learning_rate": 0.00011835583660912665,
      "loss": 1.0872,
      "step": 8933
    },
    {
      "epoch": 0.038173940538554225,
      "grad_norm": 3.1691091060638428,
      "learning_rate": 0.00011831310887027859,
      "loss": 0.9464,
      "step": 8934
    },
    {
      "epoch": 0.038178213421981425,
      "grad_norm": 0.8393847346305847,
      "learning_rate": 0.00011827038113143052,
      "loss": 0.4881,
      "step": 8935
    },
    {
      "epoch": 0.03818248630540862,
      "grad_norm": 3.4887378215789795,
      "learning_rate": 0.00011822765339258248,
      "loss": 0.9029,
      "step": 8936
    },
    {
      "epoch": 0.03818675918883581,
      "grad_norm": 0.9517399072647095,
      "learning_rate": 0.00011818492565373441,
      "loss": 0.6129,
      "step": 8937
    },
    {
      "epoch": 0.03819103207226301,
      "grad_norm": 0.5482228398323059,
      "learning_rate": 0.00011814219791488634,
      "loss": 0.2693,
      "step": 8938
    },
    {
      "epoch": 0.0381953049556902,
      "grad_norm": 0.9513294696807861,
      "learning_rate": 0.00011809947017603828,
      "loss": 0.613,
      "step": 8939
    },
    {
      "epoch": 0.03819957783911739,
      "grad_norm": 1.1020469665527344,
      "learning_rate": 0.00011805674243719023,
      "loss": 0.863,
      "step": 8940
    },
    {
      "epoch": 0.038203850722544584,
      "grad_norm": 0.8381098508834839,
      "learning_rate": 0.00011801401469834217,
      "loss": 0.4863,
      "step": 8941
    },
    {
      "epoch": 0.03820812360597178,
      "grad_norm": 0.8292186856269836,
      "learning_rate": 0.0001179712869594941,
      "loss": 0.3045,
      "step": 8942
    },
    {
      "epoch": 0.038212396489398975,
      "grad_norm": 1.8434655666351318,
      "learning_rate": 0.00011792855922064605,
      "loss": 0.4996,
      "step": 8943
    },
    {
      "epoch": 0.03821666937282617,
      "grad_norm": 2.8992300033569336,
      "learning_rate": 0.00011788583148179799,
      "loss": 0.9096,
      "step": 8944
    },
    {
      "epoch": 0.03822094225625337,
      "grad_norm": 4.541959285736084,
      "learning_rate": 0.00011784310374294993,
      "loss": 1.0975,
      "step": 8945
    },
    {
      "epoch": 0.03822521513968056,
      "grad_norm": 0.36990559101104736,
      "learning_rate": 0.00011780037600410185,
      "loss": 0.1077,
      "step": 8946
    },
    {
      "epoch": 0.03822948802310775,
      "grad_norm": 0.9766290187835693,
      "learning_rate": 0.0001177576482652538,
      "loss": 0.6373,
      "step": 8947
    },
    {
      "epoch": 0.03823376090653495,
      "grad_norm": 3.7714552879333496,
      "learning_rate": 0.00011771492052640574,
      "loss": 1.0492,
      "step": 8948
    },
    {
      "epoch": 0.03823803378996214,
      "grad_norm": 1.151957392692566,
      "learning_rate": 0.00011767219278755768,
      "loss": 0.9052,
      "step": 8949
    },
    {
      "epoch": 0.038242306673389334,
      "grad_norm": 0.7109774947166443,
      "learning_rate": 0.00011762946504870962,
      "loss": 0.3444,
      "step": 8950
    },
    {
      "epoch": 0.03824657955681653,
      "grad_norm": 1.0207198858261108,
      "learning_rate": 0.00011758673730986157,
      "loss": 0.4296,
      "step": 8951
    },
    {
      "epoch": 0.038250852440243725,
      "grad_norm": 4.177462100982666,
      "learning_rate": 0.00011754400957101351,
      "loss": 1.1014,
      "step": 8952
    },
    {
      "epoch": 0.03825512532367092,
      "grad_norm": 2.9246106147766113,
      "learning_rate": 0.00011750128183216545,
      "loss": 1.9033,
      "step": 8953
    },
    {
      "epoch": 0.038259398207098116,
      "grad_norm": 0.9429371953010559,
      "learning_rate": 0.00011745855409331738,
      "loss": 0.6038,
      "step": 8954
    },
    {
      "epoch": 0.03826367109052531,
      "grad_norm": 2.1229288578033447,
      "learning_rate": 0.00011741582635446932,
      "loss": 0.5404,
      "step": 8955
    },
    {
      "epoch": 0.0382679439739525,
      "grad_norm": 1.178406834602356,
      "learning_rate": 0.00011737309861562126,
      "loss": 0.5033,
      "step": 8956
    },
    {
      "epoch": 0.0382722168573797,
      "grad_norm": 2.0059399604797363,
      "learning_rate": 0.0001173303708767732,
      "loss": 0.6951,
      "step": 8957
    },
    {
      "epoch": 0.03827648974080689,
      "grad_norm": 2.425696849822998,
      "learning_rate": 0.00011728764313792515,
      "loss": 0.7173,
      "step": 8958
    },
    {
      "epoch": 0.038280762624234084,
      "grad_norm": 4.849392414093018,
      "learning_rate": 0.00011724491539907709,
      "loss": 1.0481,
      "step": 8959
    },
    {
      "epoch": 0.03828503550766128,
      "grad_norm": 0.5614831447601318,
      "learning_rate": 0.00011720218766022902,
      "loss": 0.2292,
      "step": 8960
    },
    {
      "epoch": 0.038289308391088475,
      "grad_norm": 0.996935248374939,
      "learning_rate": 0.00011715945992138096,
      "loss": 0.469,
      "step": 8961
    },
    {
      "epoch": 0.03829358127451567,
      "grad_norm": 4.059614181518555,
      "learning_rate": 0.00011711673218253291,
      "loss": 1.3363,
      "step": 8962
    },
    {
      "epoch": 0.038297854157942866,
      "grad_norm": 0.9304614067077637,
      "learning_rate": 0.00011707400444368484,
      "loss": 0.3756,
      "step": 8963
    },
    {
      "epoch": 0.03830212704137006,
      "grad_norm": 1.5299166440963745,
      "learning_rate": 0.00011703127670483678,
      "loss": 1.1433,
      "step": 8964
    },
    {
      "epoch": 0.03830639992479725,
      "grad_norm": 0.9354250431060791,
      "learning_rate": 0.00011698854896598871,
      "loss": 0.4513,
      "step": 8965
    },
    {
      "epoch": 0.03831067280822444,
      "grad_norm": 2.86286997795105,
      "learning_rate": 0.00011694582122714067,
      "loss": 0.8298,
      "step": 8966
    },
    {
      "epoch": 0.03831494569165164,
      "grad_norm": 0.9311984777450562,
      "learning_rate": 0.0001169030934882926,
      "loss": 0.4513,
      "step": 8967
    },
    {
      "epoch": 0.038319218575078834,
      "grad_norm": 0.7505837678909302,
      "learning_rate": 0.00011686036574944454,
      "loss": 0.3351,
      "step": 8968
    },
    {
      "epoch": 0.038323491458506026,
      "grad_norm": 2.8376998901367188,
      "learning_rate": 0.00011681763801059649,
      "loss": 1.8187,
      "step": 8969
    },
    {
      "epoch": 0.038327764341933225,
      "grad_norm": 1.1306414604187012,
      "learning_rate": 0.00011677491027174843,
      "loss": 0.4604,
      "step": 8970
    },
    {
      "epoch": 0.03833203722536042,
      "grad_norm": 0.6552903652191162,
      "learning_rate": 0.00011673218253290035,
      "loss": 0.2929,
      "step": 8971
    },
    {
      "epoch": 0.03833631010878761,
      "grad_norm": 1.5942779779434204,
      "learning_rate": 0.00011668945479405229,
      "loss": 0.8301,
      "step": 8972
    },
    {
      "epoch": 0.03834058299221481,
      "grad_norm": 1.8203059434890747,
      "learning_rate": 0.00011664672705520424,
      "loss": 0.6376,
      "step": 8973
    },
    {
      "epoch": 0.038344855875642,
      "grad_norm": 0.5664269924163818,
      "learning_rate": 0.00011660399931635618,
      "loss": 0.2292,
      "step": 8974
    },
    {
      "epoch": 0.03834912875906919,
      "grad_norm": 0.7408372163772583,
      "learning_rate": 0.00011656127157750812,
      "loss": 0.2615,
      "step": 8975
    },
    {
      "epoch": 0.03835340164249639,
      "grad_norm": 0.8416252732276917,
      "learning_rate": 0.00011651854383866006,
      "loss": 0.5278,
      "step": 8976
    },
    {
      "epoch": 0.038357674525923584,
      "grad_norm": 2.1144540309906006,
      "learning_rate": 0.00011647581609981201,
      "loss": 0.7992,
      "step": 8977
    },
    {
      "epoch": 0.038361947409350776,
      "grad_norm": 0.718533456325531,
      "learning_rate": 0.00011643308836096395,
      "loss": 0.3347,
      "step": 8978
    },
    {
      "epoch": 0.038366220292777975,
      "grad_norm": 0.8225508332252502,
      "learning_rate": 0.00011639036062211587,
      "loss": 0.5099,
      "step": 8979
    },
    {
      "epoch": 0.03837049317620517,
      "grad_norm": 4.307285785675049,
      "learning_rate": 0.00011634763288326781,
      "loss": 1.1069,
      "step": 8980
    },
    {
      "epoch": 0.03837476605963236,
      "grad_norm": 1.9290449619293213,
      "learning_rate": 0.00011630490514441976,
      "loss": 0.638,
      "step": 8981
    },
    {
      "epoch": 0.03837903894305956,
      "grad_norm": 0.8206793069839478,
      "learning_rate": 0.0001162621774055717,
      "loss": 0.509,
      "step": 8982
    },
    {
      "epoch": 0.03838331182648675,
      "grad_norm": 0.5670198202133179,
      "learning_rate": 0.00011621944966672364,
      "loss": 0.2288,
      "step": 8983
    },
    {
      "epoch": 0.03838758470991394,
      "grad_norm": 3.090956926345825,
      "learning_rate": 0.00011617672192787559,
      "loss": 0.9773,
      "step": 8984
    },
    {
      "epoch": 0.03839185759334114,
      "grad_norm": 1.4349287748336792,
      "learning_rate": 0.00011613399418902752,
      "loss": 0.5705,
      "step": 8985
    },
    {
      "epoch": 0.038396130476768334,
      "grad_norm": 0.5676262974739075,
      "learning_rate": 0.00011609126645017946,
      "loss": 0.2288,
      "step": 8986
    },
    {
      "epoch": 0.038400403360195526,
      "grad_norm": 3.1646862030029297,
      "learning_rate": 0.0001160485387113314,
      "loss": 0.7369,
      "step": 8987
    },
    {
      "epoch": 0.038404676243622725,
      "grad_norm": 4.185573577880859,
      "learning_rate": 0.00011600581097248334,
      "loss": 1.3658,
      "step": 8988
    },
    {
      "epoch": 0.03840894912704992,
      "grad_norm": 2.813877820968628,
      "learning_rate": 0.00011596308323363528,
      "loss": 1.8004,
      "step": 8989
    },
    {
      "epoch": 0.03841322201047711,
      "grad_norm": 0.8092959523200989,
      "learning_rate": 0.00011592035549478721,
      "loss": 0.3056,
      "step": 8990
    },
    {
      "epoch": 0.0384174948939043,
      "grad_norm": 4.1001057624816895,
      "learning_rate": 0.00011587762775593915,
      "loss": 1.0225,
      "step": 8991
    },
    {
      "epoch": 0.0384217677773315,
      "grad_norm": 1.098331332206726,
      "learning_rate": 0.0001158349000170911,
      "loss": 0.4602,
      "step": 8992
    },
    {
      "epoch": 0.03842604066075869,
      "grad_norm": 0.8136604428291321,
      "learning_rate": 0.00011579217227824304,
      "loss": 0.2818,
      "step": 8993
    },
    {
      "epoch": 0.038430313544185885,
      "grad_norm": 4.182470321655273,
      "learning_rate": 0.00011574944453939498,
      "loss": 1.3515,
      "step": 8994
    },
    {
      "epoch": 0.038434586427613084,
      "grad_norm": 4.52690315246582,
      "learning_rate": 0.00011570671680054693,
      "loss": 0.9026,
      "step": 8995
    },
    {
      "epoch": 0.038438859311040276,
      "grad_norm": 0.9303402304649353,
      "learning_rate": 0.00011566398906169885,
      "loss": 0.4589,
      "step": 8996
    },
    {
      "epoch": 0.03844313219446747,
      "grad_norm": 0.9238444566726685,
      "learning_rate": 0.00011562126132285079,
      "loss": 0.4183,
      "step": 8997
    },
    {
      "epoch": 0.03844740507789467,
      "grad_norm": 1.8445039987564087,
      "learning_rate": 0.00011557853358400273,
      "loss": 0.5943,
      "step": 8998
    },
    {
      "epoch": 0.03845167796132186,
      "grad_norm": 0.9153940081596375,
      "learning_rate": 0.00011553580584515468,
      "loss": 0.4182,
      "step": 8999
    },
    {
      "epoch": 0.03845595084474905,
      "grad_norm": 0.9982237815856934,
      "learning_rate": 0.00011549307810630662,
      "loss": 0.4039,
      "step": 9000
    },
    {
      "epoch": 0.03846022372817625,
      "grad_norm": 3.0889623165130615,
      "learning_rate": 0.00011545035036745856,
      "loss": 0.9684,
      "step": 9001
    },
    {
      "epoch": 0.03846449661160344,
      "grad_norm": 1.0853034257888794,
      "learning_rate": 0.0001154076226286105,
      "loss": 0.4601,
      "step": 9002
    },
    {
      "epoch": 0.038468769495030634,
      "grad_norm": 2.360654354095459,
      "learning_rate": 0.00011536489488976245,
      "loss": 0.6774,
      "step": 9003
    },
    {
      "epoch": 0.038473042378457833,
      "grad_norm": 0.8130626082420349,
      "learning_rate": 0.00011532216715091437,
      "loss": 0.3702,
      "step": 9004
    },
    {
      "epoch": 0.038477315261885026,
      "grad_norm": 0.8393853902816772,
      "learning_rate": 0.00011527943941206631,
      "loss": 0.2948,
      "step": 9005
    },
    {
      "epoch": 0.03848158814531222,
      "grad_norm": 1.6957982778549194,
      "learning_rate": 0.00011523671167321825,
      "loss": 0.476,
      "step": 9006
    },
    {
      "epoch": 0.03848586102873942,
      "grad_norm": 0.9991510510444641,
      "learning_rate": 0.0001151939839343702,
      "loss": 0.5136,
      "step": 9007
    },
    {
      "epoch": 0.03849013391216661,
      "grad_norm": 4.46533203125,
      "learning_rate": 0.00011515125619552214,
      "loss": 1.0985,
      "step": 9008
    },
    {
      "epoch": 0.0384944067955938,
      "grad_norm": 3.9930579662323,
      "learning_rate": 0.00011510852845667407,
      "loss": 2.3654,
      "step": 9009
    },
    {
      "epoch": 0.038498679679021,
      "grad_norm": 0.7522903084754944,
      "learning_rate": 0.00011506580071782603,
      "loss": 0.3697,
      "step": 9010
    },
    {
      "epoch": 0.03850295256244819,
      "grad_norm": 1.0400118827819824,
      "learning_rate": 0.00011502307297897796,
      "loss": 0.4923,
      "step": 9011
    },
    {
      "epoch": 0.038507225445875384,
      "grad_norm": 0.6730190515518188,
      "learning_rate": 0.00011498034524012989,
      "loss": 0.2936,
      "step": 9012
    },
    {
      "epoch": 0.03851149832930258,
      "grad_norm": 4.122203350067139,
      "learning_rate": 0.00011493761750128183,
      "loss": 0.7978,
      "step": 9013
    },
    {
      "epoch": 0.038515771212729775,
      "grad_norm": 1.4157408475875854,
      "learning_rate": 0.00011489488976243378,
      "loss": 0.4911,
      "step": 9014
    },
    {
      "epoch": 0.03852004409615697,
      "grad_norm": 2.9994287490844727,
      "learning_rate": 0.00011485216202358571,
      "loss": 0.8741,
      "step": 9015
    },
    {
      "epoch": 0.03852431697958416,
      "grad_norm": 1.4331467151641846,
      "learning_rate": 0.00011480943428473765,
      "loss": 1.1166,
      "step": 9016
    },
    {
      "epoch": 0.03852858986301136,
      "grad_norm": 3.9602224826812744,
      "learning_rate": 0.00011476670654588959,
      "loss": 1.0039,
      "step": 9017
    },
    {
      "epoch": 0.03853286274643855,
      "grad_norm": 0.8952509164810181,
      "learning_rate": 0.00011472397880704154,
      "loss": 0.616,
      "step": 9018
    },
    {
      "epoch": 0.03853713562986574,
      "grad_norm": 0.42570310831069946,
      "learning_rate": 0.00011468125106819348,
      "loss": 0.1344,
      "step": 9019
    },
    {
      "epoch": 0.03854140851329294,
      "grad_norm": 3.4880242347717285,
      "learning_rate": 0.0001146385233293454,
      "loss": 0.9778,
      "step": 9020
    },
    {
      "epoch": 0.038545681396720134,
      "grad_norm": 0.7843077778816223,
      "learning_rate": 0.00011459579559049734,
      "loss": 0.4916,
      "step": 9021
    },
    {
      "epoch": 0.038549954280147326,
      "grad_norm": 0.41518378257751465,
      "learning_rate": 0.00011455306785164929,
      "loss": 0.1586,
      "step": 9022
    },
    {
      "epoch": 0.038554227163574525,
      "grad_norm": 0.7884443998336792,
      "learning_rate": 0.00011451034011280123,
      "loss": 0.3638,
      "step": 9023
    },
    {
      "epoch": 0.03855850004700172,
      "grad_norm": 0.9528243541717529,
      "learning_rate": 0.00011446761237395317,
      "loss": 0.4185,
      "step": 9024
    },
    {
      "epoch": 0.03856277293042891,
      "grad_norm": 3.8193438053131104,
      "learning_rate": 0.00011442488463510512,
      "loss": 0.9244,
      "step": 9025
    },
    {
      "epoch": 0.03856704581385611,
      "grad_norm": 1.0193030834197998,
      "learning_rate": 0.00011438215689625706,
      "loss": 0.444,
      "step": 9026
    },
    {
      "epoch": 0.0385713186972833,
      "grad_norm": 0.7884047031402588,
      "learning_rate": 0.000114339429157409,
      "loss": 0.3638,
      "step": 9027
    },
    {
      "epoch": 0.03857559158071049,
      "grad_norm": 2.6714413166046143,
      "learning_rate": 0.00011429670141856093,
      "loss": 0.825,
      "step": 9028
    },
    {
      "epoch": 0.03857986446413769,
      "grad_norm": 1.6948661804199219,
      "learning_rate": 0.00011425397367971287,
      "loss": 0.474,
      "step": 9029
    },
    {
      "epoch": 0.038584137347564884,
      "grad_norm": 3.53357195854187,
      "learning_rate": 0.00011421124594086481,
      "loss": 2.5638,
      "step": 9030
    },
    {
      "epoch": 0.038588410230992076,
      "grad_norm": 1.478690505027771,
      "learning_rate": 0.00011416851820201675,
      "loss": 0.3552,
      "step": 9031
    },
    {
      "epoch": 0.038592683114419275,
      "grad_norm": 2.873236656188965,
      "learning_rate": 0.00011412579046316868,
      "loss": 0.8191,
      "step": 9032
    },
    {
      "epoch": 0.03859695599784647,
      "grad_norm": 2.1195971965789795,
      "learning_rate": 0.00011408306272432064,
      "loss": 0.5667,
      "step": 9033
    },
    {
      "epoch": 0.03860122888127366,
      "grad_norm": 0.7445874810218811,
      "learning_rate": 0.00011404033498547257,
      "loss": 0.3054,
      "step": 9034
    },
    {
      "epoch": 0.03860550176470086,
      "grad_norm": 2.739290237426758,
      "learning_rate": 0.00011399760724662451,
      "loss": 1.6713,
      "step": 9035
    },
    {
      "epoch": 0.03860977464812805,
      "grad_norm": 1.0748867988586426,
      "learning_rate": 0.00011395487950777646,
      "loss": 0.4754,
      "step": 9036
    },
    {
      "epoch": 0.03861404753155524,
      "grad_norm": 3.918318033218384,
      "learning_rate": 0.00011391215176892839,
      "loss": 1.3408,
      "step": 9037
    },
    {
      "epoch": 0.03861832041498244,
      "grad_norm": 2.6622936725616455,
      "learning_rate": 0.00011386942403008033,
      "loss": 0.7734,
      "step": 9038
    },
    {
      "epoch": 0.038622593298409634,
      "grad_norm": 1.0384362936019897,
      "learning_rate": 0.00011382669629123226,
      "loss": 0.8779,
      "step": 9039
    },
    {
      "epoch": 0.038626866181836826,
      "grad_norm": 0.9036503434181213,
      "learning_rate": 0.00011378396855238421,
      "loss": 0.6279,
      "step": 9040
    },
    {
      "epoch": 0.03863113906526402,
      "grad_norm": 0.7589508891105652,
      "learning_rate": 0.00011374124081353615,
      "loss": 0.3187,
      "step": 9041
    },
    {
      "epoch": 0.03863541194869122,
      "grad_norm": 2.366316318511963,
      "learning_rate": 0.00011369851307468809,
      "loss": 0.6979,
      "step": 9042
    },
    {
      "epoch": 0.03863968483211841,
      "grad_norm": 1.831001877784729,
      "learning_rate": 0.00011365578533584003,
      "loss": 0.5589,
      "step": 9043
    },
    {
      "epoch": 0.0386439577155456,
      "grad_norm": 1.0201773643493652,
      "learning_rate": 0.00011361305759699198,
      "loss": 0.8817,
      "step": 9044
    },
    {
      "epoch": 0.0386482305989728,
      "grad_norm": 0.8331327438354492,
      "learning_rate": 0.0001135703298581439,
      "loss": 0.418,
      "step": 9045
    },
    {
      "epoch": 0.03865250348239999,
      "grad_norm": 0.7629988789558411,
      "learning_rate": 0.00011352760211929584,
      "loss": 0.5068,
      "step": 9046
    },
    {
      "epoch": 0.038656776365827185,
      "grad_norm": 1.10591459274292,
      "learning_rate": 0.00011348487438044778,
      "loss": 0.474,
      "step": 9047
    },
    {
      "epoch": 0.038661049249254384,
      "grad_norm": 0.6421071290969849,
      "learning_rate": 0.00011344214664159973,
      "loss": 0.2511,
      "step": 9048
    },
    {
      "epoch": 0.038665322132681576,
      "grad_norm": 1.6972492933273315,
      "learning_rate": 0.00011339941890275167,
      "loss": 0.4362,
      "step": 9049
    },
    {
      "epoch": 0.03866959501610877,
      "grad_norm": 0.9286589026451111,
      "learning_rate": 0.0001133566911639036,
      "loss": 0.4325,
      "step": 9050
    },
    {
      "epoch": 0.03867386789953597,
      "grad_norm": 3.1522321701049805,
      "learning_rate": 0.00011331396342505556,
      "loss": 1.0011,
      "step": 9051
    },
    {
      "epoch": 0.03867814078296316,
      "grad_norm": 0.9811514019966125,
      "learning_rate": 0.0001132712356862075,
      "loss": 0.8492,
      "step": 9052
    },
    {
      "epoch": 0.03868241366639035,
      "grad_norm": 0.9032588005065918,
      "learning_rate": 0.00011322850794735942,
      "loss": 0.6432,
      "step": 9053
    },
    {
      "epoch": 0.03868668654981755,
      "grad_norm": 0.9824874997138977,
      "learning_rate": 0.00011318578020851136,
      "loss": 0.4516,
      "step": 9054
    },
    {
      "epoch": 0.03869095943324474,
      "grad_norm": 0.6503727436065674,
      "learning_rate": 0.00011314305246966331,
      "loss": 0.2642,
      "step": 9055
    },
    {
      "epoch": 0.038695232316671935,
      "grad_norm": 2.7619550228118896,
      "learning_rate": 0.00011310032473081525,
      "loss": 0.7694,
      "step": 9056
    },
    {
      "epoch": 0.038699505200099134,
      "grad_norm": 2.928438425064087,
      "learning_rate": 0.00011305759699196719,
      "loss": 0.6883,
      "step": 9057
    },
    {
      "epoch": 0.038703778083526326,
      "grad_norm": 4.61509895324707,
      "learning_rate": 0.00011301486925311912,
      "loss": 1.2228,
      "step": 9058
    },
    {
      "epoch": 0.03870805096695352,
      "grad_norm": 4.586665153503418,
      "learning_rate": 0.00011297214151427107,
      "loss": 1.1715,
      "step": 9059
    },
    {
      "epoch": 0.03871232385038072,
      "grad_norm": 0.46194103360176086,
      "learning_rate": 0.00011292941377542301,
      "loss": 0.1297,
      "step": 9060
    },
    {
      "epoch": 0.03871659673380791,
      "grad_norm": 1.2745572328567505,
      "learning_rate": 0.00011288668603657495,
      "loss": 0.6136,
      "step": 9061
    },
    {
      "epoch": 0.0387208696172351,
      "grad_norm": 0.902678370475769,
      "learning_rate": 0.00011284395829772689,
      "loss": 0.642,
      "step": 9062
    },
    {
      "epoch": 0.03872514250066229,
      "grad_norm": 0.7720006704330444,
      "learning_rate": 0.00011280123055887883,
      "loss": 0.5171,
      "step": 9063
    },
    {
      "epoch": 0.03872941538408949,
      "grad_norm": 1.4302053451538086,
      "learning_rate": 0.00011275850282003076,
      "loss": 0.473,
      "step": 9064
    },
    {
      "epoch": 0.038733688267516685,
      "grad_norm": 2.880021572113037,
      "learning_rate": 0.0001127157750811827,
      "loss": 0.8838,
      "step": 9065
    },
    {
      "epoch": 0.03873796115094388,
      "grad_norm": 0.8518106937408447,
      "learning_rate": 0.00011267304734233465,
      "loss": 0.3993,
      "step": 9066
    },
    {
      "epoch": 0.038742234034371076,
      "grad_norm": 1.6904548406600952,
      "learning_rate": 0.00011263031960348659,
      "loss": 0.4033,
      "step": 9067
    },
    {
      "epoch": 0.03874650691779827,
      "grad_norm": 3.9226934909820557,
      "learning_rate": 0.00011258759186463853,
      "loss": 1.284,
      "step": 9068
    },
    {
      "epoch": 0.03875077980122546,
      "grad_norm": 3.89740252494812,
      "learning_rate": 0.00011254486412579047,
      "loss": 0.8621,
      "step": 9069
    },
    {
      "epoch": 0.03875505268465266,
      "grad_norm": 1.7421222925186157,
      "learning_rate": 0.0001125021363869424,
      "loss": 0.6211,
      "step": 9070
    },
    {
      "epoch": 0.03875932556807985,
      "grad_norm": 0.8342744708061218,
      "learning_rate": 0.00011245940864809434,
      "loss": 0.3328,
      "step": 9071
    },
    {
      "epoch": 0.03876359845150704,
      "grad_norm": 4.004552841186523,
      "learning_rate": 0.00011241668090924628,
      "loss": 1.1155,
      "step": 9072
    },
    {
      "epoch": 0.03876787133493424,
      "grad_norm": 3.1366071701049805,
      "learning_rate": 0.00011237395317039822,
      "loss": 0.9323,
      "step": 9073
    },
    {
      "epoch": 0.038772144218361435,
      "grad_norm": 1.4153602123260498,
      "learning_rate": 0.00011233122543155017,
      "loss": 0.448,
      "step": 9074
    },
    {
      "epoch": 0.03877641710178863,
      "grad_norm": 2.206014394760132,
      "learning_rate": 0.00011228849769270211,
      "loss": 0.5701,
      "step": 9075
    },
    {
      "epoch": 0.038780689985215826,
      "grad_norm": 0.8024987578392029,
      "learning_rate": 0.00011224576995385404,
      "loss": 0.3187,
      "step": 9076
    },
    {
      "epoch": 0.03878496286864302,
      "grad_norm": 0.8001022338867188,
      "learning_rate": 0.000112203042215006,
      "loss": 0.5248,
      "step": 9077
    },
    {
      "epoch": 0.03878923575207021,
      "grad_norm": 0.9901544451713562,
      "learning_rate": 0.00011216031447615792,
      "loss": 0.4883,
      "step": 9078
    },
    {
      "epoch": 0.03879350863549741,
      "grad_norm": 1.730217456817627,
      "learning_rate": 0.00011211758673730986,
      "loss": 0.5907,
      "step": 9079
    },
    {
      "epoch": 0.0387977815189246,
      "grad_norm": 0.7814009189605713,
      "learning_rate": 0.0001120748589984618,
      "loss": 0.4922,
      "step": 9080
    },
    {
      "epoch": 0.03880205440235179,
      "grad_norm": 0.8721358776092529,
      "learning_rate": 0.00011203213125961375,
      "loss": 0.4179,
      "step": 9081
    },
    {
      "epoch": 0.03880632728577899,
      "grad_norm": 1.2158578634262085,
      "learning_rate": 0.00011198940352076569,
      "loss": 0.5696,
      "step": 9082
    },
    {
      "epoch": 0.038810600169206184,
      "grad_norm": 0.7703874111175537,
      "learning_rate": 0.00011194667578191762,
      "loss": 0.3518,
      "step": 9083
    },
    {
      "epoch": 0.03881487305263338,
      "grad_norm": 3.8985581398010254,
      "learning_rate": 0.00011190394804306956,
      "loss": 0.7907,
      "step": 9084
    },
    {
      "epoch": 0.038819145936060576,
      "grad_norm": 0.5655809640884399,
      "learning_rate": 0.00011186122030422151,
      "loss": 0.176,
      "step": 9085
    },
    {
      "epoch": 0.03882341881948777,
      "grad_norm": 2.8527133464813232,
      "learning_rate": 0.00011181849256537344,
      "loss": 1.1297,
      "step": 9086
    },
    {
      "epoch": 0.03882769170291496,
      "grad_norm": 4.558755397796631,
      "learning_rate": 0.00011177576482652537,
      "loss": 3.2339,
      "step": 9087
    },
    {
      "epoch": 0.03883196458634215,
      "grad_norm": 0.7362925410270691,
      "learning_rate": 0.00011173303708767733,
      "loss": 0.3186,
      "step": 9088
    },
    {
      "epoch": 0.03883623746976935,
      "grad_norm": 0.7201606631278992,
      "learning_rate": 0.00011169030934882926,
      "loss": 0.3264,
      "step": 9089
    },
    {
      "epoch": 0.03884051035319654,
      "grad_norm": 2.8343658447265625,
      "learning_rate": 0.0001116475816099812,
      "loss": 1.1256,
      "step": 9090
    },
    {
      "epoch": 0.038844783236623735,
      "grad_norm": 0.8972586393356323,
      "learning_rate": 0.00011160485387113314,
      "loss": 0.61,
      "step": 9091
    },
    {
      "epoch": 0.038849056120050934,
      "grad_norm": 2.804506540298462,
      "learning_rate": 0.00011156212613228509,
      "loss": 1.735,
      "step": 9092
    },
    {
      "epoch": 0.038853329003478126,
      "grad_norm": 2.6677000522613525,
      "learning_rate": 0.00011151939839343703,
      "loss": 0.7198,
      "step": 9093
    },
    {
      "epoch": 0.03885760188690532,
      "grad_norm": 5.074601650238037,
      "learning_rate": 0.00011147667065458895,
      "loss": 1.3066,
      "step": 9094
    },
    {
      "epoch": 0.03886187477033252,
      "grad_norm": 2.3942062854766846,
      "learning_rate": 0.00011143394291574089,
      "loss": 0.6706,
      "step": 9095
    },
    {
      "epoch": 0.03886614765375971,
      "grad_norm": 2.670044183731079,
      "learning_rate": 0.00011139121517689284,
      "loss": 0.7234,
      "step": 9096
    },
    {
      "epoch": 0.0388704205371869,
      "grad_norm": 0.9000831842422485,
      "learning_rate": 0.00011134848743804478,
      "loss": 0.6131,
      "step": 9097
    },
    {
      "epoch": 0.0388746934206141,
      "grad_norm": 0.7575138807296753,
      "learning_rate": 0.00011130575969919672,
      "loss": 0.3345,
      "step": 9098
    },
    {
      "epoch": 0.03887896630404129,
      "grad_norm": 0.8975242972373962,
      "learning_rate": 0.00011126303196034866,
      "loss": 0.5997,
      "step": 9099
    },
    {
      "epoch": 0.038883239187468485,
      "grad_norm": 2.3033030033111572,
      "learning_rate": 0.00011122030422150061,
      "loss": 0.6147,
      "step": 9100
    },
    {
      "epoch": 0.038887512070895684,
      "grad_norm": 2.3527772426605225,
      "learning_rate": 0.00011117757648265254,
      "loss": 0.6342,
      "step": 9101
    },
    {
      "epoch": 0.038891784954322876,
      "grad_norm": 0.8988502621650696,
      "learning_rate": 0.00011113484874380448,
      "loss": 0.3662,
      "step": 9102
    },
    {
      "epoch": 0.03889605783775007,
      "grad_norm": 0.9179089069366455,
      "learning_rate": 0.00011109212100495642,
      "loss": 0.6111,
      "step": 9103
    },
    {
      "epoch": 0.03890033072117727,
      "grad_norm": 0.8039153814315796,
      "learning_rate": 0.00011104939326610836,
      "loss": 0.5141,
      "step": 9104
    },
    {
      "epoch": 0.03890460360460446,
      "grad_norm": 1.017277479171753,
      "learning_rate": 0.0001110066655272603,
      "loss": 0.4186,
      "step": 9105
    },
    {
      "epoch": 0.03890887648803165,
      "grad_norm": 0.7341757416725159,
      "learning_rate": 0.00011096393778841223,
      "loss": 0.3036,
      "step": 9106
    },
    {
      "epoch": 0.03891314937145885,
      "grad_norm": 0.8202748894691467,
      "learning_rate": 0.00011092121004956419,
      "loss": 0.333,
      "step": 9107
    },
    {
      "epoch": 0.03891742225488604,
      "grad_norm": 1.0304046869277954,
      "learning_rate": 0.00011087848231071612,
      "loss": 0.8602,
      "step": 9108
    },
    {
      "epoch": 0.038921695138313235,
      "grad_norm": 2.468410015106201,
      "learning_rate": 0.00011083575457186806,
      "loss": 0.7636,
      "step": 9109
    },
    {
      "epoch": 0.038925968021740434,
      "grad_norm": 0.9136167764663696,
      "learning_rate": 0.00011079302683302,
      "loss": 0.5671,
      "step": 9110
    },
    {
      "epoch": 0.038930240905167626,
      "grad_norm": 0.6831479668617249,
      "learning_rate": 0.00011075029909417194,
      "loss": 0.309,
      "step": 9111
    },
    {
      "epoch": 0.03893451378859482,
      "grad_norm": 0.6364213228225708,
      "learning_rate": 0.00011070757135532387,
      "loss": 0.2637,
      "step": 9112
    },
    {
      "epoch": 0.03893878667202201,
      "grad_norm": 2.212966203689575,
      "learning_rate": 0.00011066484361647581,
      "loss": 0.5315,
      "step": 9113
    },
    {
      "epoch": 0.03894305955544921,
      "grad_norm": 3.13822603225708,
      "learning_rate": 0.00011062211587762776,
      "loss": 0.9063,
      "step": 9114
    },
    {
      "epoch": 0.0389473324388764,
      "grad_norm": 2.1134982109069824,
      "learning_rate": 0.0001105793881387797,
      "loss": 0.7666,
      "step": 9115
    },
    {
      "epoch": 0.038951605322303594,
      "grad_norm": 2.024050712585449,
      "learning_rate": 0.00011053666039993164,
      "loss": 0.7819,
      "step": 9116
    },
    {
      "epoch": 0.03895587820573079,
      "grad_norm": 0.7051680684089661,
      "learning_rate": 0.00011049393266108358,
      "loss": 0.3441,
      "step": 9117
    },
    {
      "epoch": 0.038960151089157985,
      "grad_norm": 2.900050401687622,
      "learning_rate": 0.00011045120492223553,
      "loss": 1.7545,
      "step": 9118
    },
    {
      "epoch": 0.03896442397258518,
      "grad_norm": 0.6235665082931519,
      "learning_rate": 0.00011040847718338745,
      "loss": 0.2714,
      "step": 9119
    },
    {
      "epoch": 0.038968696856012376,
      "grad_norm": 1.5550512075424194,
      "learning_rate": 0.00011036574944453939,
      "loss": 0.3684,
      "step": 9120
    },
    {
      "epoch": 0.03897296973943957,
      "grad_norm": 4.838601589202881,
      "learning_rate": 0.00011032302170569133,
      "loss": 1.1503,
      "step": 9121
    },
    {
      "epoch": 0.03897724262286676,
      "grad_norm": 2.099055051803589,
      "learning_rate": 0.00011028029396684328,
      "loss": 0.7441,
      "step": 9122
    },
    {
      "epoch": 0.03898151550629396,
      "grad_norm": 1.013755440711975,
      "learning_rate": 0.00011023756622799522,
      "loss": 0.3631,
      "step": 9123
    },
    {
      "epoch": 0.03898578838972115,
      "grad_norm": 4.531522274017334,
      "learning_rate": 0.00011019483848914716,
      "loss": 1.0478,
      "step": 9124
    },
    {
      "epoch": 0.038990061273148344,
      "grad_norm": 0.7485364675521851,
      "learning_rate": 0.0001101521107502991,
      "loss": 0.3611,
      "step": 9125
    },
    {
      "epoch": 0.03899433415657554,
      "grad_norm": 1.4225928783416748,
      "learning_rate": 0.00011010938301145105,
      "loss": 0.4447,
      "step": 9126
    },
    {
      "epoch": 0.038998607040002735,
      "grad_norm": 2.755121946334839,
      "learning_rate": 0.00011006665527260297,
      "loss": 0.7293,
      "step": 9127
    },
    {
      "epoch": 0.03900287992342993,
      "grad_norm": 1.8424259424209595,
      "learning_rate": 0.00011002392753375491,
      "loss": 0.5854,
      "step": 9128
    },
    {
      "epoch": 0.039007152806857126,
      "grad_norm": 2.109876871109009,
      "learning_rate": 0.00010998119979490686,
      "loss": 0.4894,
      "step": 9129
    },
    {
      "epoch": 0.03901142569028432,
      "grad_norm": 0.5637875199317932,
      "learning_rate": 0.0001099384720560588,
      "loss": 0.2287,
      "step": 9130
    },
    {
      "epoch": 0.03901569857371151,
      "grad_norm": 0.6742882132530212,
      "learning_rate": 0.00010989574431721073,
      "loss": 0.1898,
      "step": 9131
    },
    {
      "epoch": 0.03901997145713871,
      "grad_norm": 4.0467634201049805,
      "learning_rate": 0.00010985301657836267,
      "loss": 1.1956,
      "step": 9132
    },
    {
      "epoch": 0.0390242443405659,
      "grad_norm": 1.007352352142334,
      "learning_rate": 0.00010981028883951462,
      "loss": 0.4339,
      "step": 9133
    },
    {
      "epoch": 0.039028517223993094,
      "grad_norm": 1.058454990386963,
      "learning_rate": 0.00010976756110066656,
      "loss": 0.4434,
      "step": 9134
    },
    {
      "epoch": 0.03903279010742029,
      "grad_norm": 3.6075022220611572,
      "learning_rate": 0.00010972483336181849,
      "loss": 1.0193,
      "step": 9135
    },
    {
      "epoch": 0.039037062990847485,
      "grad_norm": 0.9087428450584412,
      "learning_rate": 0.00010968210562297042,
      "loss": 0.3518,
      "step": 9136
    },
    {
      "epoch": 0.03904133587427468,
      "grad_norm": 3.605966567993164,
      "learning_rate": 0.00010963937788412238,
      "loss": 2.5395,
      "step": 9137
    },
    {
      "epoch": 0.03904560875770187,
      "grad_norm": 0.9948569536209106,
      "learning_rate": 0.00010959665014527431,
      "loss": 0.8611,
      "step": 9138
    },
    {
      "epoch": 0.03904988164112907,
      "grad_norm": 3.5878000259399414,
      "learning_rate": 0.00010955392240642625,
      "loss": 1.0144,
      "step": 9139
    },
    {
      "epoch": 0.03905415452455626,
      "grad_norm": 3.1459341049194336,
      "learning_rate": 0.0001095111946675782,
      "loss": 0.8845,
      "step": 9140
    },
    {
      "epoch": 0.03905842740798345,
      "grad_norm": 0.8150293231010437,
      "learning_rate": 0.00010946846692873014,
      "loss": 0.5487,
      "step": 9141
    },
    {
      "epoch": 0.03906270029141065,
      "grad_norm": 3.527329206466675,
      "learning_rate": 0.00010942573918988208,
      "loss": 1.2879,
      "step": 9142
    },
    {
      "epoch": 0.039066973174837843,
      "grad_norm": 0.8931846022605896,
      "learning_rate": 0.00010938301145103402,
      "loss": 0.3521,
      "step": 9143
    },
    {
      "epoch": 0.039071246058265036,
      "grad_norm": 3.5107173919677734,
      "learning_rate": 0.00010934028371218595,
      "loss": 1.2488,
      "step": 9144
    },
    {
      "epoch": 0.039075518941692235,
      "grad_norm": 1.9757943153381348,
      "learning_rate": 0.00010929755597333789,
      "loss": 0.4121,
      "step": 9145
    },
    {
      "epoch": 0.03907979182511943,
      "grad_norm": 0.9393054246902466,
      "learning_rate": 0.00010925482823448983,
      "loss": 0.4028,
      "step": 9146
    },
    {
      "epoch": 0.03908406470854662,
      "grad_norm": 4.694141864776611,
      "learning_rate": 0.00010921210049564177,
      "loss": 1.222,
      "step": 9147
    },
    {
      "epoch": 0.03908833759197382,
      "grad_norm": 0.9838913083076477,
      "learning_rate": 0.00010916937275679372,
      "loss": 0.8626,
      "step": 9148
    },
    {
      "epoch": 0.03909261047540101,
      "grad_norm": 2.738903284072876,
      "learning_rate": 0.00010912664501794566,
      "loss": 0.8616,
      "step": 9149
    },
    {
      "epoch": 0.0390968833588282,
      "grad_norm": 2.210402011871338,
      "learning_rate": 0.0001090839172790976,
      "loss": 0.5996,
      "step": 9150
    },
    {
      "epoch": 0.0391011562422554,
      "grad_norm": 0.9718993306159973,
      "learning_rate": 0.00010904118954024953,
      "loss": 0.4326,
      "step": 9151
    },
    {
      "epoch": 0.03910542912568259,
      "grad_norm": 2.720628023147583,
      "learning_rate": 0.00010899846180140147,
      "loss": 0.8467,
      "step": 9152
    },
    {
      "epoch": 0.039109702009109786,
      "grad_norm": 2.163701295852661,
      "learning_rate": 0.00010895573406255341,
      "loss": 0.8098,
      "step": 9153
    },
    {
      "epoch": 0.039113974892536985,
      "grad_norm": 0.9995536804199219,
      "learning_rate": 0.00010891300632370535,
      "loss": 0.4038,
      "step": 9154
    },
    {
      "epoch": 0.03911824777596418,
      "grad_norm": 2.7962887287139893,
      "learning_rate": 0.0001088702785848573,
      "loss": 0.8009,
      "step": 9155
    },
    {
      "epoch": 0.03912252065939137,
      "grad_norm": 0.5571799874305725,
      "learning_rate": 0.00010882755084600923,
      "loss": 0.1539,
      "step": 9156
    },
    {
      "epoch": 0.03912679354281857,
      "grad_norm": 4.124521255493164,
      "learning_rate": 0.00010878482310716117,
      "loss": 1.0036,
      "step": 9157
    },
    {
      "epoch": 0.03913106642624576,
      "grad_norm": 0.9932954907417297,
      "learning_rate": 0.00010874209536831311,
      "loss": 0.8118,
      "step": 9158
    },
    {
      "epoch": 0.03913533930967295,
      "grad_norm": 3.7111716270446777,
      "learning_rate": 0.00010869936762946506,
      "loss": 1.3155,
      "step": 9159
    },
    {
      "epoch": 0.03913961219310015,
      "grad_norm": 3.3394787311553955,
      "learning_rate": 0.00010865663989061699,
      "loss": 1.4904,
      "step": 9160
    },
    {
      "epoch": 0.03914388507652734,
      "grad_norm": 2.7119576930999756,
      "learning_rate": 0.00010861391215176892,
      "loss": 1.6552,
      "step": 9161
    },
    {
      "epoch": 0.039148157959954535,
      "grad_norm": 0.7026798129081726,
      "learning_rate": 0.00010857118441292086,
      "loss": 0.2637,
      "step": 9162
    },
    {
      "epoch": 0.03915243084338173,
      "grad_norm": 0.5632153749465942,
      "learning_rate": 0.00010852845667407281,
      "loss": 0.2284,
      "step": 9163
    },
    {
      "epoch": 0.03915670372680893,
      "grad_norm": 0.8464820384979248,
      "learning_rate": 0.00010848572893522475,
      "loss": 0.3043,
      "step": 9164
    },
    {
      "epoch": 0.03916097661023612,
      "grad_norm": 2.041029930114746,
      "learning_rate": 0.00010844300119637669,
      "loss": 0.6428,
      "step": 9165
    },
    {
      "epoch": 0.03916524949366331,
      "grad_norm": 4.462942600250244,
      "learning_rate": 0.00010840027345752864,
      "loss": 1.1174,
      "step": 9166
    },
    {
      "epoch": 0.03916952237709051,
      "grad_norm": 1.437846302986145,
      "learning_rate": 0.00010835754571868058,
      "loss": 1.1344,
      "step": 9167
    },
    {
      "epoch": 0.0391737952605177,
      "grad_norm": 2.037663698196411,
      "learning_rate": 0.0001083148179798325,
      "loss": 0.6202,
      "step": 9168
    },
    {
      "epoch": 0.039178068143944894,
      "grad_norm": 1.4273393154144287,
      "learning_rate": 0.00010827209024098444,
      "loss": 1.1553,
      "step": 9169
    },
    {
      "epoch": 0.03918234102737209,
      "grad_norm": 4.748167991638184,
      "learning_rate": 0.00010822936250213639,
      "loss": 1.0569,
      "step": 9170
    },
    {
      "epoch": 0.039186613910799285,
      "grad_norm": 2.8131606578826904,
      "learning_rate": 0.00010818663476328833,
      "loss": 0.6923,
      "step": 9171
    },
    {
      "epoch": 0.03919088679422648,
      "grad_norm": 4.41489315032959,
      "learning_rate": 0.00010814390702444027,
      "loss": 1.0676,
      "step": 9172
    },
    {
      "epoch": 0.039195159677653676,
      "grad_norm": 3.0028035640716553,
      "learning_rate": 0.0001081011792855922,
      "loss": 0.8685,
      "step": 9173
    },
    {
      "epoch": 0.03919943256108087,
      "grad_norm": 1.9933754205703735,
      "learning_rate": 0.00010805845154674416,
      "loss": 0.5257,
      "step": 9174
    },
    {
      "epoch": 0.03920370544450806,
      "grad_norm": 0.5797736048698425,
      "learning_rate": 0.0001080157238078961,
      "loss": 0.2052,
      "step": 9175
    },
    {
      "epoch": 0.03920797832793526,
      "grad_norm": 1.8890522718429565,
      "learning_rate": 0.00010797299606904803,
      "loss": 0.6456,
      "step": 9176
    },
    {
      "epoch": 0.03921225121136245,
      "grad_norm": 0.8971018195152283,
      "learning_rate": 0.00010793026833019996,
      "loss": 0.5891,
      "step": 9177
    },
    {
      "epoch": 0.039216524094789644,
      "grad_norm": 3.7295100688934326,
      "learning_rate": 0.00010788754059135191,
      "loss": 0.7709,
      "step": 9178
    },
    {
      "epoch": 0.03922079697821684,
      "grad_norm": 3.090559482574463,
      "learning_rate": 0.00010784481285250385,
      "loss": 0.8006,
      "step": 9179
    },
    {
      "epoch": 0.039225069861644035,
      "grad_norm": 3.011251926422119,
      "learning_rate": 0.00010780208511365578,
      "loss": 1.0561,
      "step": 9180
    },
    {
      "epoch": 0.03922934274507123,
      "grad_norm": 2.082973003387451,
      "learning_rate": 0.00010775935737480773,
      "loss": 0.6693,
      "step": 9181
    },
    {
      "epoch": 0.039233615628498426,
      "grad_norm": 3.001708745956421,
      "learning_rate": 0.00010771662963595967,
      "loss": 1.0562,
      "step": 9182
    },
    {
      "epoch": 0.03923788851192562,
      "grad_norm": 2.746638774871826,
      "learning_rate": 0.00010767390189711161,
      "loss": 0.6572,
      "step": 9183
    },
    {
      "epoch": 0.03924216139535281,
      "grad_norm": 3.058481216430664,
      "learning_rate": 0.00010763117415826355,
      "loss": 0.781,
      "step": 9184
    },
    {
      "epoch": 0.03924643427878001,
      "grad_norm": 1.8221455812454224,
      "learning_rate": 0.00010758844641941549,
      "loss": 0.4641,
      "step": 9185
    },
    {
      "epoch": 0.0392507071622072,
      "grad_norm": 4.092948913574219,
      "learning_rate": 0.00010754571868056742,
      "loss": 0.996,
      "step": 9186
    },
    {
      "epoch": 0.039254980045634394,
      "grad_norm": 2.1013832092285156,
      "learning_rate": 0.00010750299094171936,
      "loss": 0.5368,
      "step": 9187
    },
    {
      "epoch": 0.039259252929061586,
      "grad_norm": 0.9710296392440796,
      "learning_rate": 0.0001074602632028713,
      "loss": 0.438,
      "step": 9188
    },
    {
      "epoch": 0.039263525812488785,
      "grad_norm": 5.101810932159424,
      "learning_rate": 0.00010741753546402325,
      "loss": 1.2503,
      "step": 9189
    },
    {
      "epoch": 0.03926779869591598,
      "grad_norm": 0.8494520783424377,
      "learning_rate": 0.00010737480772517519,
      "loss": 0.2817,
      "step": 9190
    },
    {
      "epoch": 0.03927207157934317,
      "grad_norm": 1.9106673002243042,
      "learning_rate": 0.00010733207998632713,
      "loss": 0.5017,
      "step": 9191
    },
    {
      "epoch": 0.03927634446277037,
      "grad_norm": 1.2952077388763428,
      "learning_rate": 0.00010728935224747908,
      "loss": 1.0646,
      "step": 9192
    },
    {
      "epoch": 0.03928061734619756,
      "grad_norm": 0.913895845413208,
      "learning_rate": 0.000107246624508631,
      "loss": 0.6189,
      "step": 9193
    },
    {
      "epoch": 0.03928489022962475,
      "grad_norm": 0.7830058336257935,
      "learning_rate": 0.00010720389676978294,
      "loss": 0.4036,
      "step": 9194
    },
    {
      "epoch": 0.03928916311305195,
      "grad_norm": 3.441075325012207,
      "learning_rate": 0.00010716116903093488,
      "loss": 1.0247,
      "step": 9195
    },
    {
      "epoch": 0.039293435996479144,
      "grad_norm": 0.6570421457290649,
      "learning_rate": 0.00010711844129208683,
      "loss": 0.3226,
      "step": 9196
    },
    {
      "epoch": 0.039297708879906336,
      "grad_norm": 1.322242259979248,
      "learning_rate": 0.00010707571355323877,
      "loss": 1.1157,
      "step": 9197
    },
    {
      "epoch": 0.039301981763333535,
      "grad_norm": 0.8959097862243652,
      "learning_rate": 0.0001070329858143907,
      "loss": 0.6157,
      "step": 9198
    },
    {
      "epoch": 0.03930625464676073,
      "grad_norm": 0.710092306137085,
      "learning_rate": 0.00010699025807554264,
      "loss": 0.3612,
      "step": 9199
    },
    {
      "epoch": 0.03931052753018792,
      "grad_norm": 0.6459515690803528,
      "learning_rate": 0.0001069475303366946,
      "loss": 0.238,
      "step": 9200
    },
    {
      "epoch": 0.03931480041361512,
      "grad_norm": 0.7301015853881836,
      "learning_rate": 0.00010690480259784652,
      "loss": 0.4075,
      "step": 9201
    },
    {
      "epoch": 0.03931907329704231,
      "grad_norm": 4.874853610992432,
      "learning_rate": 0.00010686207485899846,
      "loss": 1.1413,
      "step": 9202
    },
    {
      "epoch": 0.0393233461804695,
      "grad_norm": 3.0251455307006836,
      "learning_rate": 0.0001068193471201504,
      "loss": 0.7424,
      "step": 9203
    },
    {
      "epoch": 0.0393276190638967,
      "grad_norm": 0.8478827476501465,
      "learning_rate": 0.00010677661938130235,
      "loss": 0.5604,
      "step": 9204
    },
    {
      "epoch": 0.039331891947323894,
      "grad_norm": 4.7686944007873535,
      "learning_rate": 0.00010673389164245428,
      "loss": 1.0931,
      "step": 9205
    },
    {
      "epoch": 0.039336164830751086,
      "grad_norm": 2.1368157863616943,
      "learning_rate": 0.00010669116390360622,
      "loss": 0.5872,
      "step": 9206
    },
    {
      "epoch": 0.039340437714178285,
      "grad_norm": 0.9069575071334839,
      "learning_rate": 0.00010664843616475817,
      "loss": 0.6228,
      "step": 9207
    },
    {
      "epoch": 0.03934471059760548,
      "grad_norm": 4.069703578948975,
      "learning_rate": 0.00010660570842591011,
      "loss": 0.9768,
      "step": 9208
    },
    {
      "epoch": 0.03934898348103267,
      "grad_norm": 0.5875579714775085,
      "learning_rate": 0.00010656298068706204,
      "loss": 0.2049,
      "step": 9209
    },
    {
      "epoch": 0.03935325636445987,
      "grad_norm": 2.7778635025024414,
      "learning_rate": 0.00010652025294821397,
      "loss": 0.8839,
      "step": 9210
    },
    {
      "epoch": 0.03935752924788706,
      "grad_norm": 2.4948880672454834,
      "learning_rate": 0.00010647752520936592,
      "loss": 0.7914,
      "step": 9211
    },
    {
      "epoch": 0.03936180213131425,
      "grad_norm": 0.855739414691925,
      "learning_rate": 0.00010643479747051786,
      "loss": 0.3698,
      "step": 9212
    },
    {
      "epoch": 0.039366075014741445,
      "grad_norm": 3.9891443252563477,
      "learning_rate": 0.0001063920697316698,
      "loss": 0.922,
      "step": 9213
    },
    {
      "epoch": 0.039370347898168644,
      "grad_norm": 0.5110495090484619,
      "learning_rate": 0.00010634934199282174,
      "loss": 0.2575,
      "step": 9214
    },
    {
      "epoch": 0.039374620781595836,
      "grad_norm": 3.8897995948791504,
      "learning_rate": 0.00010630661425397369,
      "loss": 1.1403,
      "step": 9215
    },
    {
      "epoch": 0.03937889366502303,
      "grad_norm": 3.882956027984619,
      "learning_rate": 0.00010626388651512563,
      "loss": 1.1291,
      "step": 9216
    },
    {
      "epoch": 0.03938316654845023,
      "grad_norm": 1.706279993057251,
      "learning_rate": 0.00010622115877627756,
      "loss": 0.4437,
      "step": 9217
    },
    {
      "epoch": 0.03938743943187742,
      "grad_norm": 0.9004988670349121,
      "learning_rate": 0.00010617843103742949,
      "loss": 0.5811,
      "step": 9218
    },
    {
      "epoch": 0.03939171231530461,
      "grad_norm": 0.8991485238075256,
      "learning_rate": 0.00010613570329858144,
      "loss": 0.5787,
      "step": 9219
    },
    {
      "epoch": 0.03939598519873181,
      "grad_norm": 0.884962797164917,
      "learning_rate": 0.00010609297555973338,
      "loss": 0.5702,
      "step": 9220
    },
    {
      "epoch": 0.039400258082159,
      "grad_norm": 2.6288087368011475,
      "learning_rate": 0.00010605024782088532,
      "loss": 0.6294,
      "step": 9221
    },
    {
      "epoch": 0.039404530965586194,
      "grad_norm": 3.2711691856384277,
      "learning_rate": 0.00010600752008203727,
      "loss": 1.3988,
      "step": 9222
    },
    {
      "epoch": 0.039408803849013394,
      "grad_norm": 1.6934953927993774,
      "learning_rate": 0.0001059647923431892,
      "loss": 0.4269,
      "step": 9223
    },
    {
      "epoch": 0.039413076732440586,
      "grad_norm": 1.4023362398147583,
      "learning_rate": 0.00010592206460434114,
      "loss": 0.5356,
      "step": 9224
    },
    {
      "epoch": 0.03941734961586778,
      "grad_norm": 2.160205841064453,
      "learning_rate": 0.00010587933686549308,
      "loss": 0.5342,
      "step": 9225
    },
    {
      "epoch": 0.03942162249929498,
      "grad_norm": 1.7523810863494873,
      "learning_rate": 0.00010583660912664502,
      "loss": 0.5292,
      "step": 9226
    },
    {
      "epoch": 0.03942589538272217,
      "grad_norm": 0.47938400506973267,
      "learning_rate": 0.00010579388138779696,
      "loss": 0.2405,
      "step": 9227
    },
    {
      "epoch": 0.03943016826614936,
      "grad_norm": 0.609981894493103,
      "learning_rate": 0.0001057511536489489,
      "loss": 0.2687,
      "step": 9228
    },
    {
      "epoch": 0.03943444114957656,
      "grad_norm": 1.7545169591903687,
      "learning_rate": 0.00010570842591010083,
      "loss": 0.4481,
      "step": 9229
    },
    {
      "epoch": 0.03943871403300375,
      "grad_norm": 1.0251353979110718,
      "learning_rate": 0.00010566569817125278,
      "loss": 0.8375,
      "step": 9230
    },
    {
      "epoch": 0.039442986916430944,
      "grad_norm": 1.490480661392212,
      "learning_rate": 0.00010562297043240472,
      "loss": 0.4855,
      "step": 9231
    },
    {
      "epoch": 0.03944725979985814,
      "grad_norm": 2.1283981800079346,
      "learning_rate": 0.00010558024269355666,
      "loss": 0.7905,
      "step": 9232
    },
    {
      "epoch": 0.039451532683285336,
      "grad_norm": 1.4236249923706055,
      "learning_rate": 0.00010553751495470861,
      "loss": 0.4833,
      "step": 9233
    },
    {
      "epoch": 0.03945580556671253,
      "grad_norm": 1.4147117137908936,
      "learning_rate": 0.00010549478721586054,
      "loss": 1.1097,
      "step": 9234
    },
    {
      "epoch": 0.03946007845013973,
      "grad_norm": 0.9250427484512329,
      "learning_rate": 0.00010545205947701247,
      "loss": 0.5973,
      "step": 9235
    },
    {
      "epoch": 0.03946435133356692,
      "grad_norm": 1.4305351972579956,
      "learning_rate": 0.00010540933173816441,
      "loss": 1.15,
      "step": 9236
    },
    {
      "epoch": 0.03946862421699411,
      "grad_norm": 2.5224480628967285,
      "learning_rate": 0.00010536660399931636,
      "loss": 0.6482,
      "step": 9237
    },
    {
      "epoch": 0.0394728971004213,
      "grad_norm": 2.3825337886810303,
      "learning_rate": 0.0001053238762604683,
      "loss": 0.6496,
      "step": 9238
    },
    {
      "epoch": 0.0394771699838485,
      "grad_norm": 1.0083355903625488,
      "learning_rate": 0.00010528114852162024,
      "loss": 0.7972,
      "step": 9239
    },
    {
      "epoch": 0.039481442867275694,
      "grad_norm": 0.906024158000946,
      "learning_rate": 0.00010523842078277218,
      "loss": 0.5346,
      "step": 9240
    },
    {
      "epoch": 0.039485715750702886,
      "grad_norm": 0.8944650292396545,
      "learning_rate": 0.00010519569304392413,
      "loss": 0.5735,
      "step": 9241
    },
    {
      "epoch": 0.039489988634130085,
      "grad_norm": 1.2697962522506714,
      "learning_rate": 0.00010515296530507605,
      "loss": 0.4432,
      "step": 9242
    },
    {
      "epoch": 0.03949426151755728,
      "grad_norm": 0.9088237881660461,
      "learning_rate": 0.00010511023756622799,
      "loss": 0.597,
      "step": 9243
    },
    {
      "epoch": 0.03949853440098447,
      "grad_norm": 0.8948153853416443,
      "learning_rate": 0.00010506750982737993,
      "loss": 0.5344,
      "step": 9244
    },
    {
      "epoch": 0.03950280728441167,
      "grad_norm": 0.455565482378006,
      "learning_rate": 0.00010502478208853188,
      "loss": 0.2093,
      "step": 9245
    },
    {
      "epoch": 0.03950708016783886,
      "grad_norm": 0.4927329421043396,
      "learning_rate": 0.00010498205434968382,
      "loss": 0.2409,
      "step": 9246
    },
    {
      "epoch": 0.03951135305126605,
      "grad_norm": 3.2298786640167236,
      "learning_rate": 0.00010493932661083575,
      "loss": 1.0558,
      "step": 9247
    },
    {
      "epoch": 0.03951562593469325,
      "grad_norm": 4.271141052246094,
      "learning_rate": 0.0001048965988719877,
      "loss": 0.8869,
      "step": 9248
    },
    {
      "epoch": 0.039519898818120444,
      "grad_norm": 0.8922575116157532,
      "learning_rate": 0.00010485387113313964,
      "loss": 0.5963,
      "step": 9249
    },
    {
      "epoch": 0.039524171701547636,
      "grad_norm": 2.0395069122314453,
      "learning_rate": 0.00010481114339429158,
      "loss": 0.6187,
      "step": 9250
    },
    {
      "epoch": 0.039528444584974835,
      "grad_norm": 0.6103247404098511,
      "learning_rate": 0.0001047684156554435,
      "loss": 0.2858,
      "step": 9251
    },
    {
      "epoch": 0.03953271746840203,
      "grad_norm": 1.4736933708190918,
      "learning_rate": 0.00010472568791659546,
      "loss": 0.6327,
      "step": 9252
    },
    {
      "epoch": 0.03953699035182922,
      "grad_norm": 3.5412137508392334,
      "learning_rate": 0.0001046829601777474,
      "loss": 1.0636,
      "step": 9253
    },
    {
      "epoch": 0.03954126323525642,
      "grad_norm": 0.8342220783233643,
      "learning_rate": 0.00010464023243889933,
      "loss": 0.289,
      "step": 9254
    },
    {
      "epoch": 0.03954553611868361,
      "grad_norm": 0.8970941305160522,
      "learning_rate": 0.00010459750470005127,
      "loss": 0.5509,
      "step": 9255
    },
    {
      "epoch": 0.0395498090021108,
      "grad_norm": 1.4070228338241577,
      "learning_rate": 0.00010455477696120322,
      "loss": 0.5997,
      "step": 9256
    },
    {
      "epoch": 0.039554081885538,
      "grad_norm": 3.1929454803466797,
      "learning_rate": 0.00010451204922235516,
      "loss": 0.9567,
      "step": 9257
    },
    {
      "epoch": 0.039558354768965194,
      "grad_norm": 2.2210159301757812,
      "learning_rate": 0.0001044693214835071,
      "loss": 0.5727,
      "step": 9258
    },
    {
      "epoch": 0.039562627652392386,
      "grad_norm": 1.3715205192565918,
      "learning_rate": 0.00010442659374465904,
      "loss": 0.5333,
      "step": 9259
    },
    {
      "epoch": 0.039566900535819585,
      "grad_norm": 2.245227098464966,
      "learning_rate": 0.00010438386600581097,
      "loss": 0.5745,
      "step": 9260
    },
    {
      "epoch": 0.03957117341924678,
      "grad_norm": 0.8620098829269409,
      "learning_rate": 0.00010434113826696291,
      "loss": 0.5726,
      "step": 9261
    },
    {
      "epoch": 0.03957544630267397,
      "grad_norm": 1.3624653816223145,
      "learning_rate": 0.00010429841052811485,
      "loss": 0.5843,
      "step": 9262
    },
    {
      "epoch": 0.03957971918610116,
      "grad_norm": 0.6864301562309265,
      "learning_rate": 0.0001042556827892668,
      "loss": 0.2283,
      "step": 9263
    },
    {
      "epoch": 0.03958399206952836,
      "grad_norm": 2.2229485511779785,
      "learning_rate": 0.00010421295505041874,
      "loss": 0.5511,
      "step": 9264
    },
    {
      "epoch": 0.03958826495295555,
      "grad_norm": 3.864196300506592,
      "learning_rate": 0.00010417022731157068,
      "loss": 0.8387,
      "step": 9265
    },
    {
      "epoch": 0.039592537836382745,
      "grad_norm": 3.1993298530578613,
      "learning_rate": 0.00010412749957272261,
      "loss": 1.0022,
      "step": 9266
    },
    {
      "epoch": 0.039596810719809944,
      "grad_norm": 0.5524293780326843,
      "learning_rate": 0.00010408477183387455,
      "loss": 0.2687,
      "step": 9267
    },
    {
      "epoch": 0.039601083603237136,
      "grad_norm": 0.6396353244781494,
      "learning_rate": 0.00010404204409502649,
      "loss": 0.2874,
      "step": 9268
    },
    {
      "epoch": 0.03960535648666433,
      "grad_norm": 0.8802765011787415,
      "learning_rate": 0.00010399931635617843,
      "loss": 0.5302,
      "step": 9269
    },
    {
      "epoch": 0.03960962937009153,
      "grad_norm": 1.857792615890503,
      "learning_rate": 0.00010395658861733037,
      "loss": 0.6644,
      "step": 9270
    },
    {
      "epoch": 0.03961390225351872,
      "grad_norm": 0.5920900702476501,
      "learning_rate": 0.00010391386087848232,
      "loss": 0.256,
      "step": 9271
    },
    {
      "epoch": 0.03961817513694591,
      "grad_norm": 1.5235366821289062,
      "learning_rate": 0.00010387113313963425,
      "loss": 0.3626,
      "step": 9272
    },
    {
      "epoch": 0.03962244802037311,
      "grad_norm": 3.51253604888916,
      "learning_rate": 0.00010382840540078619,
      "loss": 1.0022,
      "step": 9273
    },
    {
      "epoch": 0.0396267209038003,
      "grad_norm": 2.624675750732422,
      "learning_rate": 0.00010378567766193814,
      "loss": 0.8508,
      "step": 9274
    },
    {
      "epoch": 0.039630993787227495,
      "grad_norm": 1.27955961227417,
      "learning_rate": 0.00010374294992309007,
      "loss": 0.5827,
      "step": 9275
    },
    {
      "epoch": 0.039635266670654694,
      "grad_norm": 2.1353812217712402,
      "learning_rate": 0.000103700222184242,
      "loss": 0.4633,
      "step": 9276
    },
    {
      "epoch": 0.039639539554081886,
      "grad_norm": 0.8238068222999573,
      "learning_rate": 0.00010365749444539394,
      "loss": 0.5214,
      "step": 9277
    },
    {
      "epoch": 0.03964381243750908,
      "grad_norm": 1.4911985397338867,
      "learning_rate": 0.0001036147667065459,
      "loss": 0.3466,
      "step": 9278
    },
    {
      "epoch": 0.03964808532093628,
      "grad_norm": 0.7118383646011353,
      "learning_rate": 0.00010357203896769783,
      "loss": 0.3263,
      "step": 9279
    },
    {
      "epoch": 0.03965235820436347,
      "grad_norm": 0.8289732933044434,
      "learning_rate": 0.00010352931122884977,
      "loss": 0.5214,
      "step": 9280
    },
    {
      "epoch": 0.03965663108779066,
      "grad_norm": 0.7843459248542786,
      "learning_rate": 0.00010348658349000171,
      "loss": 0.3033,
      "step": 9281
    },
    {
      "epoch": 0.03966090397121786,
      "grad_norm": 0.6094232201576233,
      "learning_rate": 0.00010344385575115366,
      "loss": 0.2714,
      "step": 9282
    },
    {
      "epoch": 0.03966517685464505,
      "grad_norm": 0.8890537619590759,
      "learning_rate": 0.00010340112801230558,
      "loss": 0.5213,
      "step": 9283
    },
    {
      "epoch": 0.039669449738072245,
      "grad_norm": 2.8554813861846924,
      "learning_rate": 0.00010335840027345752,
      "loss": 0.8566,
      "step": 9284
    },
    {
      "epoch": 0.03967372262149944,
      "grad_norm": 0.5998302102088928,
      "learning_rate": 0.00010331567253460947,
      "loss": 0.2711,
      "step": 9285
    },
    {
      "epoch": 0.039677995504926636,
      "grad_norm": 4.371954441070557,
      "learning_rate": 0.00010327294479576141,
      "loss": 2.5213,
      "step": 9286
    },
    {
      "epoch": 0.03968226838835383,
      "grad_norm": 3.236415147781372,
      "learning_rate": 0.00010323021705691335,
      "loss": 0.8597,
      "step": 9287
    },
    {
      "epoch": 0.03968654127178102,
      "grad_norm": 0.9018665552139282,
      "learning_rate": 0.00010318748931806529,
      "loss": 0.5212,
      "step": 9288
    },
    {
      "epoch": 0.03969081415520822,
      "grad_norm": 1.123664140701294,
      "learning_rate": 0.00010314476157921724,
      "loss": 0.4917,
      "step": 9289
    },
    {
      "epoch": 0.03969508703863541,
      "grad_norm": 1.28042471408844,
      "learning_rate": 0.00010310203384036918,
      "loss": 0.5657,
      "step": 9290
    },
    {
      "epoch": 0.0396993599220626,
      "grad_norm": 0.7167222499847412,
      "learning_rate": 0.00010305930610152111,
      "loss": 0.309,
      "step": 9291
    },
    {
      "epoch": 0.0397036328054898,
      "grad_norm": 1.1157677173614502,
      "learning_rate": 0.00010301657836267304,
      "loss": 0.4584,
      "step": 9292
    },
    {
      "epoch": 0.039707905688916995,
      "grad_norm": 3.220756769180298,
      "learning_rate": 0.00010297385062382499,
      "loss": 0.8067,
      "step": 9293
    },
    {
      "epoch": 0.03971217857234419,
      "grad_norm": 3.032987117767334,
      "learning_rate": 0.00010293112288497693,
      "loss": 0.9359,
      "step": 9294
    },
    {
      "epoch": 0.039716451455771386,
      "grad_norm": 1.0906411409378052,
      "learning_rate": 0.00010288839514612887,
      "loss": 0.3818,
      "step": 9295
    },
    {
      "epoch": 0.03972072433919858,
      "grad_norm": 1.001333475112915,
      "learning_rate": 0.0001028456674072808,
      "loss": 0.397,
      "step": 9296
    },
    {
      "epoch": 0.03972499722262577,
      "grad_norm": 1.2661349773406982,
      "learning_rate": 0.00010280293966843275,
      "loss": 0.4293,
      "step": 9297
    },
    {
      "epoch": 0.03972927010605297,
      "grad_norm": 0.6064685583114624,
      "learning_rate": 0.00010276021192958469,
      "loss": 0.2711,
      "step": 9298
    },
    {
      "epoch": 0.03973354298948016,
      "grad_norm": 0.8833940625190735,
      "learning_rate": 0.00010271748419073663,
      "loss": 0.3185,
      "step": 9299
    },
    {
      "epoch": 0.03973781587290735,
      "grad_norm": 2.016587257385254,
      "learning_rate": 0.00010267475645188857,
      "loss": 0.7173,
      "step": 9300
    },
    {
      "epoch": 0.03974208875633455,
      "grad_norm": 0.5893720388412476,
      "learning_rate": 0.0001026320287130405,
      "loss": 0.2712,
      "step": 9301
    },
    {
      "epoch": 0.039746361639761744,
      "grad_norm": 3.199620485305786,
      "learning_rate": 0.00010258930097419244,
      "loss": 0.9105,
      "step": 9302
    },
    {
      "epoch": 0.03975063452318894,
      "grad_norm": 0.9039222002029419,
      "learning_rate": 0.00010254657323534438,
      "loss": 0.5873,
      "step": 9303
    },
    {
      "epoch": 0.039754907406616136,
      "grad_norm": 0.8960427641868591,
      "learning_rate": 0.00010250384549649633,
      "loss": 0.3478,
      "step": 9304
    },
    {
      "epoch": 0.03975918029004333,
      "grad_norm": 4.258119106292725,
      "learning_rate": 0.00010246111775764827,
      "loss": 1.1521,
      "step": 9305
    },
    {
      "epoch": 0.03976345317347052,
      "grad_norm": 1.191569209098816,
      "learning_rate": 0.00010241839001880021,
      "loss": 0.5025,
      "step": 9306
    },
    {
      "epoch": 0.03976772605689772,
      "grad_norm": 0.5977717041969299,
      "learning_rate": 0.00010237566227995215,
      "loss": 0.2711,
      "step": 9307
    },
    {
      "epoch": 0.03977199894032491,
      "grad_norm": 3.843266010284424,
      "learning_rate": 0.00010233293454110408,
      "loss": 1.0996,
      "step": 9308
    },
    {
      "epoch": 0.0397762718237521,
      "grad_norm": 3.2673416137695312,
      "learning_rate": 0.00010229020680225602,
      "loss": 0.9449,
      "step": 9309
    },
    {
      "epoch": 0.039780544707179295,
      "grad_norm": 2.0647168159484863,
      "learning_rate": 0.00010224747906340796,
      "loss": 0.631,
      "step": 9310
    },
    {
      "epoch": 0.039784817590606494,
      "grad_norm": 1.1488580703735352,
      "learning_rate": 0.00010220475132455991,
      "loss": 0.4588,
      "step": 9311
    },
    {
      "epoch": 0.039789090474033686,
      "grad_norm": 1.125308632850647,
      "learning_rate": 0.00010216202358571185,
      "loss": 0.508,
      "step": 9312
    },
    {
      "epoch": 0.03979336335746088,
      "grad_norm": 2.281160831451416,
      "learning_rate": 0.00010211929584686379,
      "loss": 0.5703,
      "step": 9313
    },
    {
      "epoch": 0.03979763624088808,
      "grad_norm": 0.8701207041740417,
      "learning_rate": 0.00010207656810801573,
      "loss": 0.5098,
      "step": 9314
    },
    {
      "epoch": 0.03980190912431527,
      "grad_norm": 1.9363703727722168,
      "learning_rate": 0.00010203384036916768,
      "loss": 0.6415,
      "step": 9315
    },
    {
      "epoch": 0.03980618200774246,
      "grad_norm": 1.8036612272262573,
      "learning_rate": 0.0001019911126303196,
      "loss": 0.5363,
      "step": 9316
    },
    {
      "epoch": 0.03981045489116966,
      "grad_norm": 1.545383334159851,
      "learning_rate": 0.00010194838489147154,
      "loss": 0.4275,
      "step": 9317
    },
    {
      "epoch": 0.03981472777459685,
      "grad_norm": 0.7846108675003052,
      "learning_rate": 0.00010190565715262348,
      "loss": 0.3034,
      "step": 9318
    },
    {
      "epoch": 0.039819000658024045,
      "grad_norm": 0.705348789691925,
      "learning_rate": 0.00010186292941377543,
      "loss": 0.3228,
      "step": 9319
    },
    {
      "epoch": 0.039823273541451244,
      "grad_norm": 2.0559487342834473,
      "learning_rate": 0.00010182020167492737,
      "loss": 0.6067,
      "step": 9320
    },
    {
      "epoch": 0.039827546424878436,
      "grad_norm": 0.8368893265724182,
      "learning_rate": 0.0001017774739360793,
      "loss": 0.4501,
      "step": 9321
    },
    {
      "epoch": 0.03983181930830563,
      "grad_norm": 0.7005280256271362,
      "learning_rate": 0.00010173474619723124,
      "loss": 0.278,
      "step": 9322
    },
    {
      "epoch": 0.03983609219173283,
      "grad_norm": 0.9046958088874817,
      "learning_rate": 0.00010169201845838319,
      "loss": 0.5037,
      "step": 9323
    },
    {
      "epoch": 0.03984036507516002,
      "grad_norm": 3.241776704788208,
      "learning_rate": 0.00010164929071953512,
      "loss": 0.8526,
      "step": 9324
    },
    {
      "epoch": 0.03984463795858721,
      "grad_norm": 3.228991985321045,
      "learning_rate": 0.00010160656298068706,
      "loss": 0.8426,
      "step": 9325
    },
    {
      "epoch": 0.03984891084201441,
      "grad_norm": 3.953287124633789,
      "learning_rate": 0.000101563835241839,
      "loss": 0.8585,
      "step": 9326
    },
    {
      "epoch": 0.0398531837254416,
      "grad_norm": 0.8715536594390869,
      "learning_rate": 0.00010152110750299094,
      "loss": 0.4439,
      "step": 9327
    },
    {
      "epoch": 0.039857456608868795,
      "grad_norm": 2.5703704357147217,
      "learning_rate": 0.00010147837976414288,
      "loss": 0.7947,
      "step": 9328
    },
    {
      "epoch": 0.039861729492295994,
      "grad_norm": 3.231978178024292,
      "learning_rate": 0.00010143565202529482,
      "loss": 1.0561,
      "step": 9329
    },
    {
      "epoch": 0.039866002375723186,
      "grad_norm": 1.1889756917953491,
      "learning_rate": 0.00010139292428644677,
      "loss": 0.4918,
      "step": 9330
    },
    {
      "epoch": 0.03987027525915038,
      "grad_norm": 2.8044238090515137,
      "learning_rate": 0.00010135019654759871,
      "loss": 0.8553,
      "step": 9331
    },
    {
      "epoch": 0.03987454814257758,
      "grad_norm": 3.139650344848633,
      "learning_rate": 0.00010130746880875065,
      "loss": 0.9935,
      "step": 9332
    },
    {
      "epoch": 0.03987882102600477,
      "grad_norm": 1.3796648979187012,
      "learning_rate": 0.00010126474106990257,
      "loss": 0.9955,
      "step": 9333
    },
    {
      "epoch": 0.03988309390943196,
      "grad_norm": 2.8529319763183594,
      "learning_rate": 0.00010122201333105452,
      "loss": 0.8383,
      "step": 9334
    },
    {
      "epoch": 0.039887366792859154,
      "grad_norm": 0.6704809665679932,
      "learning_rate": 0.00010117928559220646,
      "loss": 0.2685,
      "step": 9335
    },
    {
      "epoch": 0.03989163967628635,
      "grad_norm": 2.2422847747802734,
      "learning_rate": 0.0001011365578533584,
      "loss": 0.5964,
      "step": 9336
    },
    {
      "epoch": 0.039895912559713545,
      "grad_norm": 3.7033770084381104,
      "learning_rate": 0.00010109383011451035,
      "loss": 1.2875,
      "step": 9337
    },
    {
      "epoch": 0.03990018544314074,
      "grad_norm": 1.4224672317504883,
      "learning_rate": 0.00010105110237566229,
      "loss": 0.5186,
      "step": 9338
    },
    {
      "epoch": 0.039904458326567936,
      "grad_norm": 2.2997848987579346,
      "learning_rate": 0.00010100837463681423,
      "loss": 0.4786,
      "step": 9339
    },
    {
      "epoch": 0.03990873120999513,
      "grad_norm": 0.9819027781486511,
      "learning_rate": 0.00010096564689796616,
      "loss": 0.5668,
      "step": 9340
    },
    {
      "epoch": 0.03991300409342232,
      "grad_norm": 1.2041347026824951,
      "learning_rate": 0.0001009229191591181,
      "loss": 0.4747,
      "step": 9341
    },
    {
      "epoch": 0.03991727697684952,
      "grad_norm": 1.7924972772598267,
      "learning_rate": 0.00010088019142027004,
      "loss": 1.2544,
      "step": 9342
    },
    {
      "epoch": 0.03992154986027671,
      "grad_norm": 0.9023175835609436,
      "learning_rate": 0.00010083746368142198,
      "loss": 0.442,
      "step": 9343
    },
    {
      "epoch": 0.039925822743703904,
      "grad_norm": 1.9593122005462646,
      "learning_rate": 0.00010079473594257391,
      "loss": 0.6276,
      "step": 9344
    },
    {
      "epoch": 0.0399300956271311,
      "grad_norm": 1.5212504863739014,
      "learning_rate": 0.00010075200820372587,
      "loss": 0.3792,
      "step": 9345
    },
    {
      "epoch": 0.039934368510558295,
      "grad_norm": 3.2790956497192383,
      "learning_rate": 0.0001007092804648778,
      "loss": 1.9418,
      "step": 9346
    },
    {
      "epoch": 0.03993864139398549,
      "grad_norm": 1.1434394121170044,
      "learning_rate": 0.00010066655272602974,
      "loss": 0.3885,
      "step": 9347
    },
    {
      "epoch": 0.039942914277412686,
      "grad_norm": 0.9058200716972351,
      "learning_rate": 0.00010062382498718168,
      "loss": 0.4419,
      "step": 9348
    },
    {
      "epoch": 0.03994718716083988,
      "grad_norm": 2.000621795654297,
      "learning_rate": 0.00010058109724833362,
      "loss": 0.539,
      "step": 9349
    },
    {
      "epoch": 0.03995146004426707,
      "grad_norm": 4.073060989379883,
      "learning_rate": 0.00010053836950948556,
      "loss": 1.0174,
      "step": 9350
    },
    {
      "epoch": 0.03995573292769427,
      "grad_norm": 3.270030975341797,
      "learning_rate": 0.0001004956417706375,
      "loss": 0.9793,
      "step": 9351
    },
    {
      "epoch": 0.03996000581112146,
      "grad_norm": 0.9196124076843262,
      "learning_rate": 0.00010045291403178944,
      "loss": 0.3183,
      "step": 9352
    },
    {
      "epoch": 0.039964278694548654,
      "grad_norm": 4.0744428634643555,
      "learning_rate": 0.00010041018629294138,
      "loss": 0.8492,
      "step": 9353
    },
    {
      "epoch": 0.03996855157797585,
      "grad_norm": 0.9586397409439087,
      "learning_rate": 0.00010036745855409332,
      "loss": 0.3808,
      "step": 9354
    },
    {
      "epoch": 0.039972824461403045,
      "grad_norm": 1.203027367591858,
      "learning_rate": 0.00010032473081524526,
      "loss": 0.3888,
      "step": 9355
    },
    {
      "epoch": 0.03997709734483024,
      "grad_norm": 4.606515407562256,
      "learning_rate": 0.00010028200307639721,
      "loss": 1.0427,
      "step": 9356
    },
    {
      "epoch": 0.039981370228257436,
      "grad_norm": 1.1605443954467773,
      "learning_rate": 0.00010023927533754913,
      "loss": 0.3971,
      "step": 9357
    },
    {
      "epoch": 0.03998564311168463,
      "grad_norm": 3.006187677383423,
      "learning_rate": 0.00010019654759870107,
      "loss": 0.8344,
      "step": 9358
    },
    {
      "epoch": 0.03998991599511182,
      "grad_norm": 2.3057074546813965,
      "learning_rate": 0.00010015381985985301,
      "loss": 0.6834,
      "step": 9359
    },
    {
      "epoch": 0.03999418887853901,
      "grad_norm": 4.45429801940918,
      "learning_rate": 0.00010011109212100496,
      "loss": 2.6154,
      "step": 9360
    },
    {
      "epoch": 0.03999846176196621,
      "grad_norm": 1.9756120443344116,
      "learning_rate": 0.0001000683643821569,
      "loss": 0.4885,
      "step": 9361
    },
    {
      "epoch": 0.040002734645393404,
      "grad_norm": 1.0247726440429688,
      "learning_rate": 0.00010002563664330884,
      "loss": 0.3988,
      "step": 9362
    },
    {
      "epoch": 0.040007007528820596,
      "grad_norm": 4.281307220458984,
      "learning_rate": 9.998290890446079e-05,
      "loss": 1.4392,
      "step": 9363
    },
    {
      "epoch": 0.040011280412247795,
      "grad_norm": 1.8181984424591064,
      "learning_rate": 9.994018116561273e-05,
      "loss": 0.477,
      "step": 9364
    },
    {
      "epoch": 0.04001555329567499,
      "grad_norm": 3.222818374633789,
      "learning_rate": 9.989745342676466e-05,
      "loss": 1.3552,
      "step": 9365
    },
    {
      "epoch": 0.04001982617910218,
      "grad_norm": 0.9096494913101196,
      "learning_rate": 9.985472568791659e-05,
      "loss": 0.5022,
      "step": 9366
    },
    {
      "epoch": 0.04002409906252938,
      "grad_norm": 3.6489810943603516,
      "learning_rate": 9.981199794906854e-05,
      "loss": 1.0239,
      "step": 9367
    },
    {
      "epoch": 0.04002837194595657,
      "grad_norm": 1.9568328857421875,
      "learning_rate": 9.976927021022048e-05,
      "loss": 0.5985,
      "step": 9368
    },
    {
      "epoch": 0.04003264482938376,
      "grad_norm": 2.1475071907043457,
      "learning_rate": 9.972654247137242e-05,
      "loss": 0.626,
      "step": 9369
    },
    {
      "epoch": 0.04003691771281096,
      "grad_norm": 1.3631114959716797,
      "learning_rate": 9.968381473252435e-05,
      "loss": 0.9931,
      "step": 9370
    },
    {
      "epoch": 0.04004119059623815,
      "grad_norm": 0.9710054993629456,
      "learning_rate": 9.96410869936763e-05,
      "loss": 0.5694,
      "step": 9371
    },
    {
      "epoch": 0.040045463479665346,
      "grad_norm": 0.9562997221946716,
      "learning_rate": 9.959835925482824e-05,
      "loss": 0.3033,
      "step": 9372
    },
    {
      "epoch": 0.040049736363092545,
      "grad_norm": 0.9837934970855713,
      "learning_rate": 9.955563151598018e-05,
      "loss": 0.5751,
      "step": 9373
    },
    {
      "epoch": 0.04005400924651974,
      "grad_norm": 1.2761412858963013,
      "learning_rate": 9.95129037771321e-05,
      "loss": 0.4139,
      "step": 9374
    },
    {
      "epoch": 0.04005828212994693,
      "grad_norm": 1.330727458000183,
      "learning_rate": 9.947017603828406e-05,
      "loss": 0.9447,
      "step": 9375
    },
    {
      "epoch": 0.04006255501337413,
      "grad_norm": 2.835026979446411,
      "learning_rate": 9.9427448299436e-05,
      "loss": 0.8344,
      "step": 9376
    },
    {
      "epoch": 0.04006682789680132,
      "grad_norm": 0.9143680334091187,
      "learning_rate": 9.938472056058793e-05,
      "loss": 0.4931,
      "step": 9377
    },
    {
      "epoch": 0.04007110078022851,
      "grad_norm": 2.945511817932129,
      "learning_rate": 9.934199282173988e-05,
      "loss": 0.8064,
      "step": 9378
    },
    {
      "epoch": 0.04007537366365571,
      "grad_norm": 1.8100619316101074,
      "learning_rate": 9.929926508289182e-05,
      "loss": 0.5845,
      "step": 9379
    },
    {
      "epoch": 0.0400796465470829,
      "grad_norm": 2.2570979595184326,
      "learning_rate": 9.925653734404376e-05,
      "loss": 0.5086,
      "step": 9380
    },
    {
      "epoch": 0.040083919430510095,
      "grad_norm": 2.1114325523376465,
      "learning_rate": 9.92138096051957e-05,
      "loss": 0.7531,
      "step": 9381
    },
    {
      "epoch": 0.040088192313937294,
      "grad_norm": 3.0002968311309814,
      "learning_rate": 9.917108186634763e-05,
      "loss": 0.7635,
      "step": 9382
    },
    {
      "epoch": 0.04009246519736449,
      "grad_norm": 4.893362045288086,
      "learning_rate": 9.912835412749957e-05,
      "loss": 1.2987,
      "step": 9383
    },
    {
      "epoch": 0.04009673808079168,
      "grad_norm": 2.1826677322387695,
      "learning_rate": 9.908562638865151e-05,
      "loss": 0.668,
      "step": 9384
    },
    {
      "epoch": 0.04010101096421887,
      "grad_norm": 1.5583827495574951,
      "learning_rate": 9.904289864980345e-05,
      "loss": 0.5592,
      "step": 9385
    },
    {
      "epoch": 0.04010528384764607,
      "grad_norm": 1.2787927389144897,
      "learning_rate": 9.90001709109554e-05,
      "loss": 0.9348,
      "step": 9386
    },
    {
      "epoch": 0.04010955673107326,
      "grad_norm": 0.8913390040397644,
      "learning_rate": 9.895744317210734e-05,
      "loss": 0.4974,
      "step": 9387
    },
    {
      "epoch": 0.040113829614500454,
      "grad_norm": 0.8973124623298645,
      "learning_rate": 9.891471543325927e-05,
      "loss": 0.5022,
      "step": 9388
    },
    {
      "epoch": 0.04011810249792765,
      "grad_norm": 0.7446731925010681,
      "learning_rate": 9.887198769441123e-05,
      "loss": 0.2711,
      "step": 9389
    },
    {
      "epoch": 0.040122375381354845,
      "grad_norm": 1.0882649421691895,
      "learning_rate": 9.882925995556315e-05,
      "loss": 0.3476,
      "step": 9390
    },
    {
      "epoch": 0.04012664826478204,
      "grad_norm": 0.8437886834144592,
      "learning_rate": 9.878653221671509e-05,
      "loss": 0.2928,
      "step": 9391
    },
    {
      "epoch": 0.040130921148209237,
      "grad_norm": 0.9149835109710693,
      "learning_rate": 9.874380447786703e-05,
      "loss": 0.5138,
      "step": 9392
    },
    {
      "epoch": 0.04013519403163643,
      "grad_norm": 0.6664026379585266,
      "learning_rate": 9.870107673901898e-05,
      "loss": 0.2689,
      "step": 9393
    },
    {
      "epoch": 0.04013946691506362,
      "grad_norm": 1.8797401189804077,
      "learning_rate": 9.865834900017092e-05,
      "loss": 0.5797,
      "step": 9394
    },
    {
      "epoch": 0.04014373979849082,
      "grad_norm": 1.272873044013977,
      "learning_rate": 9.861562126132285e-05,
      "loss": 0.381,
      "step": 9395
    },
    {
      "epoch": 0.04014801268191801,
      "grad_norm": 1.3863472938537598,
      "learning_rate": 9.857289352247479e-05,
      "loss": 0.418,
      "step": 9396
    },
    {
      "epoch": 0.040152285565345204,
      "grad_norm": 3.2595443725585938,
      "learning_rate": 9.853016578362674e-05,
      "loss": 0.9776,
      "step": 9397
    },
    {
      "epoch": 0.0401565584487724,
      "grad_norm": 3.266648292541504,
      "learning_rate": 9.848743804477867e-05,
      "loss": 0.9741,
      "step": 9398
    },
    {
      "epoch": 0.040160831332199595,
      "grad_norm": 3.0714786052703857,
      "learning_rate": 9.84447103059306e-05,
      "loss": 0.6653,
      "step": 9399
    },
    {
      "epoch": 0.04016510421562679,
      "grad_norm": 4.089704990386963,
      "learning_rate": 9.840198256708254e-05,
      "loss": 0.838,
      "step": 9400
    },
    {
      "epoch": 0.040169377099053986,
      "grad_norm": 1.4975733757019043,
      "learning_rate": 9.83592548282345e-05,
      "loss": 0.4436,
      "step": 9401
    },
    {
      "epoch": 0.04017364998248118,
      "grad_norm": 1.3615093231201172,
      "learning_rate": 9.831652708938643e-05,
      "loss": 0.4179,
      "step": 9402
    },
    {
      "epoch": 0.04017792286590837,
      "grad_norm": 0.9691939949989319,
      "learning_rate": 9.827379935053837e-05,
      "loss": 0.562,
      "step": 9403
    },
    {
      "epoch": 0.04018219574933557,
      "grad_norm": 1.850967526435852,
      "learning_rate": 9.823107161169032e-05,
      "loss": 0.6542,
      "step": 9404
    },
    {
      "epoch": 0.04018646863276276,
      "grad_norm": 1.8626590967178345,
      "learning_rate": 9.818834387284226e-05,
      "loss": 1.3122,
      "step": 9405
    },
    {
      "epoch": 0.040190741516189954,
      "grad_norm": 1.3074313402175903,
      "learning_rate": 9.81456161339942e-05,
      "loss": 0.418,
      "step": 9406
    },
    {
      "epoch": 0.04019501439961715,
      "grad_norm": 3.2774500846862793,
      "learning_rate": 9.810288839514612e-05,
      "loss": 0.7406,
      "step": 9407
    },
    {
      "epoch": 0.040199287283044345,
      "grad_norm": 0.935701847076416,
      "learning_rate": 9.806016065629807e-05,
      "loss": 0.5123,
      "step": 9408
    },
    {
      "epoch": 0.04020356016647154,
      "grad_norm": 3.246204137802124,
      "learning_rate": 9.801743291745001e-05,
      "loss": 0.8836,
      "step": 9409
    },
    {
      "epoch": 0.04020783304989873,
      "grad_norm": 0.8510314226150513,
      "learning_rate": 9.797470517860195e-05,
      "loss": 0.4358,
      "step": 9410
    },
    {
      "epoch": 0.04021210593332593,
      "grad_norm": 0.8561452031135559,
      "learning_rate": 9.793197743975389e-05,
      "loss": 0.4358,
      "step": 9411
    },
    {
      "epoch": 0.04021637881675312,
      "grad_norm": 4.727797985076904,
      "learning_rate": 9.788924970090584e-05,
      "loss": 1.139,
      "step": 9412
    },
    {
      "epoch": 0.04022065170018031,
      "grad_norm": 0.8116602897644043,
      "learning_rate": 9.784652196205778e-05,
      "loss": 0.3092,
      "step": 9413
    },
    {
      "epoch": 0.04022492458360751,
      "grad_norm": 2.305234909057617,
      "learning_rate": 9.780379422320971e-05,
      "loss": 0.6109,
      "step": 9414
    },
    {
      "epoch": 0.040229197467034704,
      "grad_norm": 0.8716628551483154,
      "learning_rate": 9.776106648436164e-05,
      "loss": 0.289,
      "step": 9415
    },
    {
      "epoch": 0.040233470350461896,
      "grad_norm": 4.600853443145752,
      "learning_rate": 9.771833874551359e-05,
      "loss": 2.6731,
      "step": 9416
    },
    {
      "epoch": 0.040237743233889095,
      "grad_norm": 1.4805997610092163,
      "learning_rate": 9.767561100666553e-05,
      "loss": 0.5079,
      "step": 9417
    },
    {
      "epoch": 0.04024201611731629,
      "grad_norm": 4.645406723022461,
      "learning_rate": 9.763288326781746e-05,
      "loss": 1.4461,
      "step": 9418
    },
    {
      "epoch": 0.04024628900074348,
      "grad_norm": 3.20151686668396,
      "learning_rate": 9.759015552896942e-05,
      "loss": 0.8266,
      "step": 9419
    },
    {
      "epoch": 0.04025056188417068,
      "grad_norm": 4.4481201171875,
      "learning_rate": 9.754742779012135e-05,
      "loss": 0.9851,
      "step": 9420
    },
    {
      "epoch": 0.04025483476759787,
      "grad_norm": 3.224339723587036,
      "learning_rate": 9.750470005127329e-05,
      "loss": 0.8098,
      "step": 9421
    },
    {
      "epoch": 0.04025910765102506,
      "grad_norm": 1.001783013343811,
      "learning_rate": 9.746197231242523e-05,
      "loss": 0.3808,
      "step": 9422
    },
    {
      "epoch": 0.04026338053445226,
      "grad_norm": 2.074714183807373,
      "learning_rate": 9.741924457357717e-05,
      "loss": 0.5556,
      "step": 9423
    },
    {
      "epoch": 0.040267653417879454,
      "grad_norm": 0.8998199701309204,
      "learning_rate": 9.73765168347291e-05,
      "loss": 0.4267,
      "step": 9424
    },
    {
      "epoch": 0.040271926301306646,
      "grad_norm": 0.7159659266471863,
      "learning_rate": 9.733378909588104e-05,
      "loss": 0.2562,
      "step": 9425
    },
    {
      "epoch": 0.040276199184733845,
      "grad_norm": 1.0097455978393555,
      "learning_rate": 9.729106135703298e-05,
      "loss": 0.3327,
      "step": 9426
    },
    {
      "epoch": 0.04028047206816104,
      "grad_norm": 3.399588108062744,
      "learning_rate": 9.724833361818493e-05,
      "loss": 1.9892,
      "step": 9427
    },
    {
      "epoch": 0.04028474495158823,
      "grad_norm": 0.9611944556236267,
      "learning_rate": 9.720560587933687e-05,
      "loss": 0.3187,
      "step": 9428
    },
    {
      "epoch": 0.04028901783501543,
      "grad_norm": 0.9345106482505798,
      "learning_rate": 9.716287814048881e-05,
      "loss": 0.4268,
      "step": 9429
    },
    {
      "epoch": 0.04029329071844262,
      "grad_norm": 0.9569743275642395,
      "learning_rate": 9.712015040164076e-05,
      "loss": 0.5624,
      "step": 9430
    },
    {
      "epoch": 0.04029756360186981,
      "grad_norm": 1.392095923423767,
      "learning_rate": 9.707742266279268e-05,
      "loss": 0.4511,
      "step": 9431
    },
    {
      "epoch": 0.04030183648529701,
      "grad_norm": 2.758540630340576,
      "learning_rate": 9.703469492394462e-05,
      "loss": 0.9545,
      "step": 9432
    },
    {
      "epoch": 0.040306109368724204,
      "grad_norm": 1.4619817733764648,
      "learning_rate": 9.699196718509656e-05,
      "loss": 0.4698,
      "step": 9433
    },
    {
      "epoch": 0.040310382252151396,
      "grad_norm": 2.1645407676696777,
      "learning_rate": 9.694923944624851e-05,
      "loss": 0.6527,
      "step": 9434
    },
    {
      "epoch": 0.04031465513557859,
      "grad_norm": 1.5441250801086426,
      "learning_rate": 9.690651170740045e-05,
      "loss": 0.3761,
      "step": 9435
    },
    {
      "epoch": 0.04031892801900579,
      "grad_norm": 0.9099892973899841,
      "learning_rate": 9.686378396855239e-05,
      "loss": 0.309,
      "step": 9436
    },
    {
      "epoch": 0.04032320090243298,
      "grad_norm": 3.760850429534912,
      "learning_rate": 9.682105622970432e-05,
      "loss": 1.29,
      "step": 9437
    },
    {
      "epoch": 0.04032747378586017,
      "grad_norm": 1.8101474046707153,
      "learning_rate": 9.677832849085628e-05,
      "loss": 0.4568,
      "step": 9438
    },
    {
      "epoch": 0.04033174666928737,
      "grad_norm": 4.701475620269775,
      "learning_rate": 9.673560075200821e-05,
      "loss": 1.3006,
      "step": 9439
    },
    {
      "epoch": 0.04033601955271456,
      "grad_norm": 4.79560661315918,
      "learning_rate": 9.669287301316014e-05,
      "loss": 1.1495,
      "step": 9440
    },
    {
      "epoch": 0.040340292436141754,
      "grad_norm": 1.5212476253509521,
      "learning_rate": 9.665014527431208e-05,
      "loss": 0.3566,
      "step": 9441
    },
    {
      "epoch": 0.040344565319568954,
      "grad_norm": 0.9538877010345459,
      "learning_rate": 9.660741753546403e-05,
      "loss": 0.4139,
      "step": 9442
    },
    {
      "epoch": 0.040348838202996146,
      "grad_norm": 2.894981861114502,
      "learning_rate": 9.656468979661596e-05,
      "loss": 0.5661,
      "step": 9443
    },
    {
      "epoch": 0.04035311108642334,
      "grad_norm": 1.5015037059783936,
      "learning_rate": 9.65219620577679e-05,
      "loss": 0.4339,
      "step": 9444
    },
    {
      "epoch": 0.04035738396985054,
      "grad_norm": 3.4645814895629883,
      "learning_rate": 9.647923431891985e-05,
      "loss": 1.9852,
      "step": 9445
    },
    {
      "epoch": 0.04036165685327773,
      "grad_norm": 1.029955267906189,
      "learning_rate": 9.643650658007179e-05,
      "loss": 0.6102,
      "step": 9446
    },
    {
      "epoch": 0.04036592973670492,
      "grad_norm": 4.530069828033447,
      "learning_rate": 9.639377884122373e-05,
      "loss": 1.0374,
      "step": 9447
    },
    {
      "epoch": 0.04037020262013212,
      "grad_norm": 2.7854809761047363,
      "learning_rate": 9.635105110237565e-05,
      "loss": 0.7989,
      "step": 9448
    },
    {
      "epoch": 0.04037447550355931,
      "grad_norm": 1.5919269323349,
      "learning_rate": 9.63083233635276e-05,
      "loss": 0.4585,
      "step": 9449
    },
    {
      "epoch": 0.040378748386986504,
      "grad_norm": 3.219186305999756,
      "learning_rate": 9.626559562467954e-05,
      "loss": 0.6691,
      "step": 9450
    },
    {
      "epoch": 0.0403830212704137,
      "grad_norm": 0.951930046081543,
      "learning_rate": 9.622286788583148e-05,
      "loss": 0.4138,
      "step": 9451
    },
    {
      "epoch": 0.040387294153840896,
      "grad_norm": 0.9567375183105469,
      "learning_rate": 9.618014014698342e-05,
      "loss": 0.4137,
      "step": 9452
    },
    {
      "epoch": 0.04039156703726809,
      "grad_norm": 4.34043025970459,
      "learning_rate": 9.613741240813537e-05,
      "loss": 1.3688,
      "step": 9453
    },
    {
      "epoch": 0.04039583992069529,
      "grad_norm": 0.4601775109767914,
      "learning_rate": 9.609468466928731e-05,
      "loss": 0.1717,
      "step": 9454
    },
    {
      "epoch": 0.04040011280412248,
      "grad_norm": 1.5673528909683228,
      "learning_rate": 9.605195693043925e-05,
      "loss": 1.0387,
      "step": 9455
    },
    {
      "epoch": 0.04040438568754967,
      "grad_norm": 0.9567534923553467,
      "learning_rate": 9.600922919159118e-05,
      "loss": 0.5387,
      "step": 9456
    },
    {
      "epoch": 0.04040865857097687,
      "grad_norm": 0.9529500007629395,
      "learning_rate": 9.596650145274312e-05,
      "loss": 0.5387,
      "step": 9457
    },
    {
      "epoch": 0.04041293145440406,
      "grad_norm": 0.954723060131073,
      "learning_rate": 9.592377371389506e-05,
      "loss": 0.5387,
      "step": 9458
    },
    {
      "epoch": 0.040417204337831254,
      "grad_norm": 2.9908487796783447,
      "learning_rate": 9.5881045975047e-05,
      "loss": 0.7657,
      "step": 9459
    },
    {
      "epoch": 0.040421477221258446,
      "grad_norm": 0.8798447847366333,
      "learning_rate": 9.583831823619895e-05,
      "loss": 0.3597,
      "step": 9460
    },
    {
      "epoch": 0.040425750104685645,
      "grad_norm": 0.9024508595466614,
      "learning_rate": 9.579559049735089e-05,
      "loss": 0.2932,
      "step": 9461
    },
    {
      "epoch": 0.04043002298811284,
      "grad_norm": 1.2873936891555786,
      "learning_rate": 9.575286275850282e-05,
      "loss": 0.3808,
      "step": 9462
    },
    {
      "epoch": 0.04043429587154003,
      "grad_norm": 4.266222953796387,
      "learning_rate": 9.571013501965476e-05,
      "loss": 0.8976,
      "step": 9463
    },
    {
      "epoch": 0.04043856875496723,
      "grad_norm": 3.4263710975646973,
      "learning_rate": 9.56674072808067e-05,
      "loss": 1.9756,
      "step": 9464
    },
    {
      "epoch": 0.04044284163839442,
      "grad_norm": 1.698484182357788,
      "learning_rate": 9.562467954195864e-05,
      "loss": 0.5501,
      "step": 9465
    },
    {
      "epoch": 0.04044711452182161,
      "grad_norm": 4.286668300628662,
      "learning_rate": 9.558195180311058e-05,
      "loss": 0.8924,
      "step": 9466
    },
    {
      "epoch": 0.04045138740524881,
      "grad_norm": 3.1757349967956543,
      "learning_rate": 9.553922406426251e-05,
      "loss": 1.2402,
      "step": 9467
    },
    {
      "epoch": 0.040455660288676004,
      "grad_norm": 1.3487112522125244,
      "learning_rate": 9.549649632541446e-05,
      "loss": 0.3887,
      "step": 9468
    },
    {
      "epoch": 0.040459933172103196,
      "grad_norm": 0.7661188244819641,
      "learning_rate": 9.54537685865664e-05,
      "loss": 0.271,
      "step": 9469
    },
    {
      "epoch": 0.040464206055530395,
      "grad_norm": 1.2152483463287354,
      "learning_rate": 9.541104084771834e-05,
      "loss": 0.3971,
      "step": 9470
    },
    {
      "epoch": 0.04046847893895759,
      "grad_norm": 1.870608925819397,
      "learning_rate": 9.536831310887029e-05,
      "loss": 0.6549,
      "step": 9471
    },
    {
      "epoch": 0.04047275182238478,
      "grad_norm": 1.2841501235961914,
      "learning_rate": 9.532558537002222e-05,
      "loss": 0.3756,
      "step": 9472
    },
    {
      "epoch": 0.04047702470581198,
      "grad_norm": 2.05188250541687,
      "learning_rate": 9.528285763117415e-05,
      "loss": 1.3506,
      "step": 9473
    },
    {
      "epoch": 0.04048129758923917,
      "grad_norm": 4.291983604431152,
      "learning_rate": 9.524012989232609e-05,
      "loss": 1.1297,
      "step": 9474
    },
    {
      "epoch": 0.04048557047266636,
      "grad_norm": 4.574924468994141,
      "learning_rate": 9.519740215347804e-05,
      "loss": 2.6591,
      "step": 9475
    },
    {
      "epoch": 0.04048984335609356,
      "grad_norm": 1.329508662223816,
      "learning_rate": 9.515467441462998e-05,
      "loss": 0.4162,
      "step": 9476
    },
    {
      "epoch": 0.040494116239520754,
      "grad_norm": 0.6966677904129028,
      "learning_rate": 9.511194667578192e-05,
      "loss": 0.256,
      "step": 9477
    },
    {
      "epoch": 0.040498389122947946,
      "grad_norm": 2.7438912391662598,
      "learning_rate": 9.506921893693386e-05,
      "loss": 0.9037,
      "step": 9478
    },
    {
      "epoch": 0.040502662006375145,
      "grad_norm": 0.8384474515914917,
      "learning_rate": 9.502649119808581e-05,
      "loss": 0.4745,
      "step": 9479
    },
    {
      "epoch": 0.04050693488980234,
      "grad_norm": 0.9649888277053833,
      "learning_rate": 9.498376345923775e-05,
      "loss": 0.3325,
      "step": 9480
    },
    {
      "epoch": 0.04051120777322953,
      "grad_norm": 0.9552226066589355,
      "learning_rate": 9.494103572038967e-05,
      "loss": 0.3053,
      "step": 9481
    },
    {
      "epoch": 0.04051548065665673,
      "grad_norm": 0.8664026856422424,
      "learning_rate": 9.489830798154162e-05,
      "loss": 0.4393,
      "step": 9482
    },
    {
      "epoch": 0.04051975354008392,
      "grad_norm": 0.9594091176986694,
      "learning_rate": 9.485558024269356e-05,
      "loss": 0.3639,
      "step": 9483
    },
    {
      "epoch": 0.04052402642351111,
      "grad_norm": 4.166401386260986,
      "learning_rate": 9.48128525038455e-05,
      "loss": 0.7405,
      "step": 9484
    },
    {
      "epoch": 0.040528299306938305,
      "grad_norm": 0.879716694355011,
      "learning_rate": 9.477012476499744e-05,
      "loss": 0.5407,
      "step": 9485
    },
    {
      "epoch": 0.040532572190365504,
      "grad_norm": 1.2009376287460327,
      "learning_rate": 9.472739702614939e-05,
      "loss": 0.4883,
      "step": 9486
    },
    {
      "epoch": 0.040536845073792696,
      "grad_norm": 2.5942225456237793,
      "learning_rate": 9.468466928730132e-05,
      "loss": 0.8902,
      "step": 9487
    },
    {
      "epoch": 0.04054111795721989,
      "grad_norm": 1.9140878915786743,
      "learning_rate": 9.464194154845326e-05,
      "loss": 0.4925,
      "step": 9488
    },
    {
      "epoch": 0.04054539084064709,
      "grad_norm": 0.8882341980934143,
      "learning_rate": 9.459921380960519e-05,
      "loss": 0.4882,
      "step": 9489
    },
    {
      "epoch": 0.04054966372407428,
      "grad_norm": 3.673616886138916,
      "learning_rate": 9.455648607075714e-05,
      "loss": 1.1795,
      "step": 9490
    },
    {
      "epoch": 0.04055393660750147,
      "grad_norm": 0.7452501058578491,
      "learning_rate": 9.451375833190908e-05,
      "loss": 0.3092,
      "step": 9491
    },
    {
      "epoch": 0.04055820949092867,
      "grad_norm": 0.5406480431556702,
      "learning_rate": 9.447103059306101e-05,
      "loss": 0.1575,
      "step": 9492
    },
    {
      "epoch": 0.04056248237435586,
      "grad_norm": 2.790536403656006,
      "learning_rate": 9.442830285421295e-05,
      "loss": 0.8072,
      "step": 9493
    },
    {
      "epoch": 0.040566755257783055,
      "grad_norm": 0.81373530626297,
      "learning_rate": 9.43855751153649e-05,
      "loss": 0.4157,
      "step": 9494
    },
    {
      "epoch": 0.040571028141210254,
      "grad_norm": 0.7859556674957275,
      "learning_rate": 9.434284737651684e-05,
      "loss": 0.3033,
      "step": 9495
    },
    {
      "epoch": 0.040575301024637446,
      "grad_norm": 4.006486892700195,
      "learning_rate": 9.430011963766878e-05,
      "loss": 0.6322,
      "step": 9496
    },
    {
      "epoch": 0.04057957390806464,
      "grad_norm": 4.842434883117676,
      "learning_rate": 9.425739189882072e-05,
      "loss": 1.0288,
      "step": 9497
    },
    {
      "epoch": 0.04058384679149184,
      "grad_norm": 4.848668098449707,
      "learning_rate": 9.421466415997265e-05,
      "loss": 1.0332,
      "step": 9498
    },
    {
      "epoch": 0.04058811967491903,
      "grad_norm": 0.8976588845252991,
      "learning_rate": 9.417193642112459e-05,
      "loss": 0.3638,
      "step": 9499
    },
    {
      "epoch": 0.04059239255834622,
      "grad_norm": 0.7732341289520264,
      "learning_rate": 9.412920868227653e-05,
      "loss": 0.2891,
      "step": 9500
    },
    {
      "epoch": 0.04059666544177342,
      "grad_norm": 0.5646615624427795,
      "learning_rate": 9.408648094342848e-05,
      "loss": 0.1979,
      "step": 9501
    },
    {
      "epoch": 0.04060093832520061,
      "grad_norm": 0.9782446026802063,
      "learning_rate": 9.404375320458042e-05,
      "loss": 0.3973,
      "step": 9502
    },
    {
      "epoch": 0.040605211208627805,
      "grad_norm": 4.701196193695068,
      "learning_rate": 9.400102546573236e-05,
      "loss": 1.1792,
      "step": 9503
    },
    {
      "epoch": 0.040609484092055004,
      "grad_norm": 1.8310226202011108,
      "learning_rate": 9.39582977268843e-05,
      "loss": 0.6096,
      "step": 9504
    },
    {
      "epoch": 0.040613756975482196,
      "grad_norm": 0.9578368663787842,
      "learning_rate": 9.391556998803623e-05,
      "loss": 0.4586,
      "step": 9505
    },
    {
      "epoch": 0.04061802985890939,
      "grad_norm": 0.7894791960716248,
      "learning_rate": 9.387284224918817e-05,
      "loss": 0.3184,
      "step": 9506
    },
    {
      "epoch": 0.04062230274233658,
      "grad_norm": 1.9095287322998047,
      "learning_rate": 9.383011451034011e-05,
      "loss": 0.616,
      "step": 9507
    },
    {
      "epoch": 0.04062657562576378,
      "grad_norm": 0.7054249048233032,
      "learning_rate": 9.378738677149206e-05,
      "loss": 0.3094,
      "step": 9508
    },
    {
      "epoch": 0.04063084850919097,
      "grad_norm": 3.794832468032837,
      "learning_rate": 9.3744659032644e-05,
      "loss": 0.5509,
      "step": 9509
    },
    {
      "epoch": 0.04063512139261816,
      "grad_norm": 0.6916396021842957,
      "learning_rate": 9.370193129379594e-05,
      "loss": 0.2507,
      "step": 9510
    },
    {
      "epoch": 0.04063939427604536,
      "grad_norm": 2.7506093978881836,
      "learning_rate": 9.365920355494787e-05,
      "loss": 0.7548,
      "step": 9511
    },
    {
      "epoch": 0.040643667159472555,
      "grad_norm": 1.5467315912246704,
      "learning_rate": 9.361647581609982e-05,
      "loss": 1.0401,
      "step": 9512
    },
    {
      "epoch": 0.04064794004289975,
      "grad_norm": 0.8461154699325562,
      "learning_rate": 9.357374807725175e-05,
      "loss": 0.4153,
      "step": 9513
    },
    {
      "epoch": 0.040652212926326946,
      "grad_norm": 4.670416831970215,
      "learning_rate": 9.353102033840369e-05,
      "loss": 0.8739,
      "step": 9514
    },
    {
      "epoch": 0.04065648580975414,
      "grad_norm": 0.8509761095046997,
      "learning_rate": 9.348829259955562e-05,
      "loss": 0.4138,
      "step": 9515
    },
    {
      "epoch": 0.04066075869318133,
      "grad_norm": 3.4026641845703125,
      "learning_rate": 9.344556486070758e-05,
      "loss": 1.9156,
      "step": 9516
    },
    {
      "epoch": 0.04066503157660853,
      "grad_norm": 0.6093454957008362,
      "learning_rate": 9.340283712185951e-05,
      "loss": 0.1679,
      "step": 9517
    },
    {
      "epoch": 0.04066930446003572,
      "grad_norm": 4.731045722961426,
      "learning_rate": 9.336010938301145e-05,
      "loss": 1.33,
      "step": 9518
    },
    {
      "epoch": 0.04067357734346291,
      "grad_norm": 2.050861358642578,
      "learning_rate": 9.331738164416339e-05,
      "loss": 0.7288,
      "step": 9519
    },
    {
      "epoch": 0.04067785022689011,
      "grad_norm": 3.522820234298706,
      "learning_rate": 9.327465390531534e-05,
      "loss": 0.7225,
      "step": 9520
    },
    {
      "epoch": 0.040682123110317304,
      "grad_norm": 0.762481153011322,
      "learning_rate": 9.323192616646728e-05,
      "loss": 0.2761,
      "step": 9521
    },
    {
      "epoch": 0.0406863959937445,
      "grad_norm": 0.8775579333305359,
      "learning_rate": 9.31891984276192e-05,
      "loss": 0.364,
      "step": 9522
    },
    {
      "epoch": 0.040690668877171696,
      "grad_norm": 2.509247064590454,
      "learning_rate": 9.314647068877115e-05,
      "loss": 0.5921,
      "step": 9523
    },
    {
      "epoch": 0.04069494176059889,
      "grad_norm": 2.146164655685425,
      "learning_rate": 9.310374294992309e-05,
      "loss": 0.6569,
      "step": 9524
    },
    {
      "epoch": 0.04069921464402608,
      "grad_norm": 0.799384355545044,
      "learning_rate": 9.306101521107503e-05,
      "loss": 0.3186,
      "step": 9525
    },
    {
      "epoch": 0.04070348752745328,
      "grad_norm": 3.0841870307922363,
      "learning_rate": 9.301828747222697e-05,
      "loss": 1.1594,
      "step": 9526
    },
    {
      "epoch": 0.04070776041088047,
      "grad_norm": 1.012187123298645,
      "learning_rate": 9.297555973337892e-05,
      "loss": 0.4512,
      "step": 9527
    },
    {
      "epoch": 0.04071203329430766,
      "grad_norm": 5.010523319244385,
      "learning_rate": 9.293283199453086e-05,
      "loss": 1.2652,
      "step": 9528
    },
    {
      "epoch": 0.04071630617773486,
      "grad_norm": 2.7109835147857666,
      "learning_rate": 9.28901042556828e-05,
      "loss": 0.862,
      "step": 9529
    },
    {
      "epoch": 0.040720579061162054,
      "grad_norm": 0.6994758248329163,
      "learning_rate": 9.284737651683472e-05,
      "loss": 0.3093,
      "step": 9530
    },
    {
      "epoch": 0.040724851944589247,
      "grad_norm": 1.6473907232284546,
      "learning_rate": 9.280464877798667e-05,
      "loss": 0.3633,
      "step": 9531
    },
    {
      "epoch": 0.04072912482801644,
      "grad_norm": 0.695156991481781,
      "learning_rate": 9.276192103913861e-05,
      "loss": 0.3094,
      "step": 9532
    },
    {
      "epoch": 0.04073339771144364,
      "grad_norm": 3.4317784309387207,
      "learning_rate": 9.271919330029055e-05,
      "loss": 0.659,
      "step": 9533
    },
    {
      "epoch": 0.04073767059487083,
      "grad_norm": 4.584609508514404,
      "learning_rate": 9.26764655614425e-05,
      "loss": 1.2437,
      "step": 9534
    },
    {
      "epoch": 0.04074194347829802,
      "grad_norm": 0.9234201312065125,
      "learning_rate": 9.263373782259444e-05,
      "loss": 0.3149,
      "step": 9535
    },
    {
      "epoch": 0.04074621636172522,
      "grad_norm": 0.5173605680465698,
      "learning_rate": 9.259101008374637e-05,
      "loss": 0.1494,
      "step": 9536
    },
    {
      "epoch": 0.04075048924515241,
      "grad_norm": 3.723461866378784,
      "learning_rate": 9.254828234489831e-05,
      "loss": 1.1578,
      "step": 9537
    },
    {
      "epoch": 0.040754762128579605,
      "grad_norm": 0.6875149607658386,
      "learning_rate": 9.250555460605025e-05,
      "loss": 0.2778,
      "step": 9538
    },
    {
      "epoch": 0.040759035012006804,
      "grad_norm": 0.8742179870605469,
      "learning_rate": 9.246282686720219e-05,
      "loss": 0.5406,
      "step": 9539
    },
    {
      "epoch": 0.040763307895433996,
      "grad_norm": 0.9751672744750977,
      "learning_rate": 9.242009912835412e-05,
      "loss": 0.4143,
      "step": 9540
    },
    {
      "epoch": 0.04076758077886119,
      "grad_norm": 1.0730812549591064,
      "learning_rate": 9.237737138950606e-05,
      "loss": 0.434,
      "step": 9541
    },
    {
      "epoch": 0.04077185366228839,
      "grad_norm": 0.6820107698440552,
      "learning_rate": 9.233464365065801e-05,
      "loss": 0.2779,
      "step": 9542
    },
    {
      "epoch": 0.04077612654571558,
      "grad_norm": 2.7040657997131348,
      "learning_rate": 9.229191591180995e-05,
      "loss": 0.8264,
      "step": 9543
    },
    {
      "epoch": 0.04078039942914277,
      "grad_norm": 3.840454339981079,
      "learning_rate": 9.224918817296189e-05,
      "loss": 0.5852,
      "step": 9544
    },
    {
      "epoch": 0.04078467231256997,
      "grad_norm": 1.1364827156066895,
      "learning_rate": 9.220646043411383e-05,
      "loss": 0.459,
      "step": 9545
    },
    {
      "epoch": 0.04078894519599716,
      "grad_norm": 0.8786808252334595,
      "learning_rate": 9.216373269526577e-05,
      "loss": 0.4153,
      "step": 9546
    },
    {
      "epoch": 0.040793218079424355,
      "grad_norm": 4.121912956237793,
      "learning_rate": 9.21210049564177e-05,
      "loss": 0.9175,
      "step": 9547
    },
    {
      "epoch": 0.040797490962851554,
      "grad_norm": 0.9443125128746033,
      "learning_rate": 9.207827721756964e-05,
      "loss": 0.3384,
      "step": 9548
    },
    {
      "epoch": 0.040801763846278746,
      "grad_norm": 1.8335206508636475,
      "learning_rate": 9.203554947872159e-05,
      "loss": 0.4619,
      "step": 9549
    },
    {
      "epoch": 0.04080603672970594,
      "grad_norm": 1.5586051940917969,
      "learning_rate": 9.199282173987353e-05,
      "loss": 1.0406,
      "step": 9550
    },
    {
      "epoch": 0.04081030961313314,
      "grad_norm": 0.6846828460693359,
      "learning_rate": 9.195009400102547e-05,
      "loss": 0.293,
      "step": 9551
    },
    {
      "epoch": 0.04081458249656033,
      "grad_norm": 2.6273787021636963,
      "learning_rate": 9.19073662621774e-05,
      "loss": 0.8526,
      "step": 9552
    },
    {
      "epoch": 0.04081885537998752,
      "grad_norm": 4.549892902374268,
      "learning_rate": 9.186463852332936e-05,
      "loss": 2.5489,
      "step": 9553
    },
    {
      "epoch": 0.04082312826341472,
      "grad_norm": 0.9040145874023438,
      "learning_rate": 9.18219107844813e-05,
      "loss": 0.3989,
      "step": 9554
    },
    {
      "epoch": 0.04082740114684191,
      "grad_norm": 0.8911852240562439,
      "learning_rate": 9.177918304563322e-05,
      "loss": 0.399,
      "step": 9555
    },
    {
      "epoch": 0.040831674030269105,
      "grad_norm": 2.592771530151367,
      "learning_rate": 9.173645530678516e-05,
      "loss": 0.8404,
      "step": 9556
    },
    {
      "epoch": 0.0408359469136963,
      "grad_norm": 1.0433437824249268,
      "learning_rate": 9.169372756793711e-05,
      "loss": 0.4178,
      "step": 9557
    },
    {
      "epoch": 0.040840219797123496,
      "grad_norm": 3.3026585578918457,
      "learning_rate": 9.165099982908905e-05,
      "loss": 1.8895,
      "step": 9558
    },
    {
      "epoch": 0.04084449268055069,
      "grad_norm": 1.4490526914596558,
      "learning_rate": 9.160827209024098e-05,
      "loss": 1.0145,
      "step": 9559
    },
    {
      "epoch": 0.04084876556397788,
      "grad_norm": 0.8673095703125,
      "learning_rate": 9.156554435139294e-05,
      "loss": 0.5556,
      "step": 9560
    },
    {
      "epoch": 0.04085303844740508,
      "grad_norm": 0.8775584101676941,
      "learning_rate": 9.152281661254487e-05,
      "loss": 0.4306,
      "step": 9561
    },
    {
      "epoch": 0.04085731133083227,
      "grad_norm": 0.8466467261314392,
      "learning_rate": 9.148008887369681e-05,
      "loss": 0.5512,
      "step": 9562
    },
    {
      "epoch": 0.040861584214259464,
      "grad_norm": 2.990598440170288,
      "learning_rate": 9.143736113484874e-05,
      "loss": 1.0448,
      "step": 9563
    },
    {
      "epoch": 0.04086585709768666,
      "grad_norm": 3.332144021987915,
      "learning_rate": 9.139463339600069e-05,
      "loss": 0.8283,
      "step": 9564
    },
    {
      "epoch": 0.040870129981113855,
      "grad_norm": 2.622565746307373,
      "learning_rate": 9.135190565715263e-05,
      "loss": 0.753,
      "step": 9565
    },
    {
      "epoch": 0.04087440286454105,
      "grad_norm": 1.423722743988037,
      "learning_rate": 9.130917791830456e-05,
      "loss": 1.043,
      "step": 9566
    },
    {
      "epoch": 0.040878675747968246,
      "grad_norm": 0.7440975904464722,
      "learning_rate": 9.12664501794565e-05,
      "loss": 0.2636,
      "step": 9567
    },
    {
      "epoch": 0.04088294863139544,
      "grad_norm": 2.12205171585083,
      "learning_rate": 9.122372244060845e-05,
      "loss": 0.6454,
      "step": 9568
    },
    {
      "epoch": 0.04088722151482263,
      "grad_norm": 0.862634539604187,
      "learning_rate": 9.118099470176039e-05,
      "loss": 0.4543,
      "step": 9569
    },
    {
      "epoch": 0.04089149439824983,
      "grad_norm": 0.9597843289375305,
      "learning_rate": 9.113826696291233e-05,
      "loss": 0.3659,
      "step": 9570
    },
    {
      "epoch": 0.04089576728167702,
      "grad_norm": 1.881223201751709,
      "learning_rate": 9.109553922406425e-05,
      "loss": 0.484,
      "step": 9571
    },
    {
      "epoch": 0.040900040165104214,
      "grad_norm": 0.8848187327384949,
      "learning_rate": 9.10528114852162e-05,
      "loss": 0.3642,
      "step": 9572
    },
    {
      "epoch": 0.04090431304853141,
      "grad_norm": 2.0224266052246094,
      "learning_rate": 9.101008374636814e-05,
      "loss": 0.7154,
      "step": 9573
    },
    {
      "epoch": 0.040908585931958605,
      "grad_norm": 1.1765763759613037,
      "learning_rate": 9.096735600752008e-05,
      "loss": 0.4294,
      "step": 9574
    },
    {
      "epoch": 0.0409128588153858,
      "grad_norm": 2.447819709777832,
      "learning_rate": 9.092462826867203e-05,
      "loss": 0.554,
      "step": 9575
    },
    {
      "epoch": 0.040917131698812996,
      "grad_norm": 3.1037604808807373,
      "learning_rate": 9.088190052982397e-05,
      "loss": 0.7856,
      "step": 9576
    },
    {
      "epoch": 0.04092140458224019,
      "grad_norm": 3.644061326980591,
      "learning_rate": 9.08391727909759e-05,
      "loss": 1.1573,
      "step": 9577
    },
    {
      "epoch": 0.04092567746566738,
      "grad_norm": 2.8158013820648193,
      "learning_rate": 9.079644505212784e-05,
      "loss": 0.7894,
      "step": 9578
    },
    {
      "epoch": 0.04092995034909458,
      "grad_norm": 1.4944511651992798,
      "learning_rate": 9.075371731327978e-05,
      "loss": 0.5223,
      "step": 9579
    },
    {
      "epoch": 0.04093422323252177,
      "grad_norm": 0.6401717066764832,
      "learning_rate": 9.071098957443172e-05,
      "loss": 0.2875,
      "step": 9580
    },
    {
      "epoch": 0.040938496115948964,
      "grad_norm": 3.6521549224853516,
      "learning_rate": 9.066826183558366e-05,
      "loss": 1.0742,
      "step": 9581
    },
    {
      "epoch": 0.040942768999376156,
      "grad_norm": 1.3652443885803223,
      "learning_rate": 9.06255340967356e-05,
      "loss": 0.9893,
      "step": 9582
    },
    {
      "epoch": 0.040947041882803355,
      "grad_norm": 0.852377712726593,
      "learning_rate": 9.058280635788755e-05,
      "loss": 0.4331,
      "step": 9583
    },
    {
      "epoch": 0.04095131476623055,
      "grad_norm": 0.8422797322273254,
      "learning_rate": 9.054007861903948e-05,
      "loss": 0.552,
      "step": 9584
    },
    {
      "epoch": 0.04095558764965774,
      "grad_norm": 1.3626567125320435,
      "learning_rate": 9.049735088019142e-05,
      "loss": 0.9893,
      "step": 9585
    },
    {
      "epoch": 0.04095986053308494,
      "grad_norm": 0.8559356331825256,
      "learning_rate": 9.045462314134337e-05,
      "loss": 0.3698,
      "step": 9586
    },
    {
      "epoch": 0.04096413341651213,
      "grad_norm": 3.9575693607330322,
      "learning_rate": 9.04118954024953e-05,
      "loss": 2.7755,
      "step": 9587
    },
    {
      "epoch": 0.04096840629993932,
      "grad_norm": 3.608757495880127,
      "learning_rate": 9.036916766364724e-05,
      "loss": 1.2959,
      "step": 9588
    },
    {
      "epoch": 0.04097267918336652,
      "grad_norm": 0.8737585544586182,
      "learning_rate": 9.032643992479917e-05,
      "loss": 0.4634,
      "step": 9589
    },
    {
      "epoch": 0.04097695206679371,
      "grad_norm": 0.5515375733375549,
      "learning_rate": 9.028371218595113e-05,
      "loss": 0.2843,
      "step": 9590
    },
    {
      "epoch": 0.040981224950220906,
      "grad_norm": 0.9075039029121399,
      "learning_rate": 9.024098444710306e-05,
      "loss": 0.3642,
      "step": 9591
    },
    {
      "epoch": 0.040985497833648105,
      "grad_norm": 0.6340673565864563,
      "learning_rate": 9.0198256708255e-05,
      "loss": 0.2561,
      "step": 9592
    },
    {
      "epoch": 0.0409897707170753,
      "grad_norm": 3.5476057529449463,
      "learning_rate": 9.015552896940694e-05,
      "loss": 0.9662,
      "step": 9593
    },
    {
      "epoch": 0.04099404360050249,
      "grad_norm": 3.7219412326812744,
      "learning_rate": 9.011280123055889e-05,
      "loss": 0.8524,
      "step": 9594
    },
    {
      "epoch": 0.04099831648392969,
      "grad_norm": 2.8998446464538574,
      "learning_rate": 9.007007349171083e-05,
      "loss": 0.9442,
      "step": 9595
    },
    {
      "epoch": 0.04100258936735688,
      "grad_norm": 0.919730544090271,
      "learning_rate": 9.002734575286275e-05,
      "loss": 0.3639,
      "step": 9596
    },
    {
      "epoch": 0.04100686225078407,
      "grad_norm": 2.3518247604370117,
      "learning_rate": 8.998461801401469e-05,
      "loss": 0.6508,
      "step": 9597
    },
    {
      "epoch": 0.04101113513421127,
      "grad_norm": 0.6571009159088135,
      "learning_rate": 8.994189027516664e-05,
      "loss": 0.2712,
      "step": 9598
    },
    {
      "epoch": 0.04101540801763846,
      "grad_norm": 0.8034785389900208,
      "learning_rate": 8.989916253631858e-05,
      "loss": 0.2759,
      "step": 9599
    },
    {
      "epoch": 0.041019680901065655,
      "grad_norm": 1.1171939373016357,
      "learning_rate": 8.985643479747052e-05,
      "loss": 0.4182,
      "step": 9600
    },
    {
      "epoch": 0.041023953784492855,
      "grad_norm": 0.8271248936653137,
      "learning_rate": 8.981370705862247e-05,
      "loss": 0.3187,
      "step": 9601
    },
    {
      "epoch": 0.04102822666792005,
      "grad_norm": 2.0863168239593506,
      "learning_rate": 8.97709793197744e-05,
      "loss": 0.6075,
      "step": 9602
    },
    {
      "epoch": 0.04103249955134724,
      "grad_norm": 1.2628520727157593,
      "learning_rate": 8.972825158092634e-05,
      "loss": 0.943,
      "step": 9603
    },
    {
      "epoch": 0.04103677243477444,
      "grad_norm": 0.8498552441596985,
      "learning_rate": 8.968552384207827e-05,
      "loss": 0.5675,
      "step": 9604
    },
    {
      "epoch": 0.04104104531820163,
      "grad_norm": 2.6416003704071045,
      "learning_rate": 8.964279610323022e-05,
      "loss": 0.7711,
      "step": 9605
    },
    {
      "epoch": 0.04104531820162882,
      "grad_norm": 1.343430519104004,
      "learning_rate": 8.960006836438216e-05,
      "loss": 0.4876,
      "step": 9606
    },
    {
      "epoch": 0.041049591085056014,
      "grad_norm": 1.0227932929992676,
      "learning_rate": 8.95573406255341e-05,
      "loss": 0.352,
      "step": 9607
    },
    {
      "epoch": 0.04105386396848321,
      "grad_norm": 1.8523876667022705,
      "learning_rate": 8.951461288668603e-05,
      "loss": 0.4665,
      "step": 9608
    },
    {
      "epoch": 0.041058136851910405,
      "grad_norm": 1.8926221132278442,
      "learning_rate": 8.947188514783799e-05,
      "loss": 0.6733,
      "step": 9609
    },
    {
      "epoch": 0.0410624097353376,
      "grad_norm": 3.1692731380462646,
      "learning_rate": 8.942915740898992e-05,
      "loss": 1.818,
      "step": 9610
    },
    {
      "epoch": 0.0410666826187648,
      "grad_norm": 2.6220192909240723,
      "learning_rate": 8.938642967014186e-05,
      "loss": 0.7298,
      "step": 9611
    },
    {
      "epoch": 0.04107095550219199,
      "grad_norm": 0.9921538233757019,
      "learning_rate": 8.934370193129379e-05,
      "loss": 0.3659,
      "step": 9612
    },
    {
      "epoch": 0.04107522838561918,
      "grad_norm": 2.6195192337036133,
      "learning_rate": 8.930097419244574e-05,
      "loss": 0.7294,
      "step": 9613
    },
    {
      "epoch": 0.04107950126904638,
      "grad_norm": 3.177795648574829,
      "learning_rate": 8.925824645359767e-05,
      "loss": 0.575,
      "step": 9614
    },
    {
      "epoch": 0.04108377415247357,
      "grad_norm": 1.967101812362671,
      "learning_rate": 8.921551871474961e-05,
      "loss": 0.6911,
      "step": 9615
    },
    {
      "epoch": 0.041088047035900764,
      "grad_norm": 0.8625495433807373,
      "learning_rate": 8.917279097590156e-05,
      "loss": 0.4808,
      "step": 9616
    },
    {
      "epoch": 0.04109231991932796,
      "grad_norm": 4.830260276794434,
      "learning_rate": 8.91300632370535e-05,
      "loss": 1.1955,
      "step": 9617
    },
    {
      "epoch": 0.041096592802755155,
      "grad_norm": 0.8387097716331482,
      "learning_rate": 8.908733549820544e-05,
      "loss": 0.5685,
      "step": 9618
    },
    {
      "epoch": 0.04110086568618235,
      "grad_norm": 0.7693512439727783,
      "learning_rate": 8.904460775935738e-05,
      "loss": 0.2892,
      "step": 9619
    },
    {
      "epoch": 0.041105138569609546,
      "grad_norm": 2.0941002368927,
      "learning_rate": 8.900188002050931e-05,
      "loss": 0.6024,
      "step": 9620
    },
    {
      "epoch": 0.04110941145303674,
      "grad_norm": 0.8199226260185242,
      "learning_rate": 8.895915228166125e-05,
      "loss": 0.5388,
      "step": 9621
    },
    {
      "epoch": 0.04111368433646393,
      "grad_norm": 4.813344478607178,
      "learning_rate": 8.891642454281319e-05,
      "loss": 1.1509,
      "step": 9622
    },
    {
      "epoch": 0.04111795721989113,
      "grad_norm": 0.8588918447494507,
      "learning_rate": 8.887369680396513e-05,
      "loss": 0.4808,
      "step": 9623
    },
    {
      "epoch": 0.04112223010331832,
      "grad_norm": 2.4784882068634033,
      "learning_rate": 8.883096906511708e-05,
      "loss": 0.6947,
      "step": 9624
    },
    {
      "epoch": 0.041126502986745514,
      "grad_norm": 0.8353944420814514,
      "learning_rate": 8.878824132626902e-05,
      "loss": 0.5381,
      "step": 9625
    },
    {
      "epoch": 0.04113077587017271,
      "grad_norm": 3.5823779106140137,
      "learning_rate": 8.874551358742096e-05,
      "loss": 0.9225,
      "step": 9626
    },
    {
      "epoch": 0.041135048753599905,
      "grad_norm": 0.7299416661262512,
      "learning_rate": 8.870278584857291e-05,
      "loss": 0.344,
      "step": 9627
    },
    {
      "epoch": 0.0411393216370271,
      "grad_norm": 0.8433244228363037,
      "learning_rate": 8.866005810972484e-05,
      "loss": 0.4639,
      "step": 9628
    },
    {
      "epoch": 0.041143594520454296,
      "grad_norm": 1.8899017572402954,
      "learning_rate": 8.861733037087677e-05,
      "loss": 1.3384,
      "step": 9629
    },
    {
      "epoch": 0.04114786740388149,
      "grad_norm": 2.2705135345458984,
      "learning_rate": 8.857460263202871e-05,
      "loss": 0.6205,
      "step": 9630
    },
    {
      "epoch": 0.04115214028730868,
      "grad_norm": 2.3540055751800537,
      "learning_rate": 8.853187489318066e-05,
      "loss": 0.6596,
      "step": 9631
    },
    {
      "epoch": 0.04115641317073587,
      "grad_norm": 0.9554423689842224,
      "learning_rate": 8.84891471543326e-05,
      "loss": 0.3658,
      "step": 9632
    },
    {
      "epoch": 0.04116068605416307,
      "grad_norm": 1.0349714756011963,
      "learning_rate": 8.844641941548453e-05,
      "loss": 0.4179,
      "step": 9633
    },
    {
      "epoch": 0.041164958937590264,
      "grad_norm": 4.482548236846924,
      "learning_rate": 8.840369167663647e-05,
      "loss": 1.3502,
      "step": 9634
    },
    {
      "epoch": 0.041169231821017456,
      "grad_norm": 0.8527405261993408,
      "learning_rate": 8.836096393778842e-05,
      "loss": 0.4639,
      "step": 9635
    },
    {
      "epoch": 0.041173504704444655,
      "grad_norm": 1.7922422885894775,
      "learning_rate": 8.831823619894036e-05,
      "loss": 0.497,
      "step": 9636
    },
    {
      "epoch": 0.04117777758787185,
      "grad_norm": 2.1018457412719727,
      "learning_rate": 8.827550846009229e-05,
      "loss": 0.6418,
      "step": 9637
    },
    {
      "epoch": 0.04118205047129904,
      "grad_norm": 2.2261273860931396,
      "learning_rate": 8.823278072124422e-05,
      "loss": 0.6134,
      "step": 9638
    },
    {
      "epoch": 0.04118632335472624,
      "grad_norm": 0.7874870300292969,
      "learning_rate": 8.819005298239617e-05,
      "loss": 0.3032,
      "step": 9639
    },
    {
      "epoch": 0.04119059623815343,
      "grad_norm": 3.06685733795166,
      "learning_rate": 8.814732524354811e-05,
      "loss": 1.0682,
      "step": 9640
    },
    {
      "epoch": 0.04119486912158062,
      "grad_norm": 2.0898263454437256,
      "learning_rate": 8.810459750470005e-05,
      "loss": 0.5536,
      "step": 9641
    },
    {
      "epoch": 0.04119914200500782,
      "grad_norm": 3.7206571102142334,
      "learning_rate": 8.8061869765852e-05,
      "loss": 1.1649,
      "step": 9642
    },
    {
      "epoch": 0.041203414888435014,
      "grad_norm": 0.9725672602653503,
      "learning_rate": 8.801914202700394e-05,
      "loss": 0.3656,
      "step": 9643
    },
    {
      "epoch": 0.041207687771862206,
      "grad_norm": 4.792642116546631,
      "learning_rate": 8.797641428815588e-05,
      "loss": 1.2467,
      "step": 9644
    },
    {
      "epoch": 0.041211960655289405,
      "grad_norm": 2.0368471145629883,
      "learning_rate": 8.79336865493078e-05,
      "loss": 0.6037,
      "step": 9645
    },
    {
      "epoch": 0.0412162335387166,
      "grad_norm": 3.631148338317871,
      "learning_rate": 8.789095881045975e-05,
      "loss": 1.1838,
      "step": 9646
    },
    {
      "epoch": 0.04122050642214379,
      "grad_norm": 2.2148993015289307,
      "learning_rate": 8.784823107161169e-05,
      "loss": 0.575,
      "step": 9647
    },
    {
      "epoch": 0.04122477930557099,
      "grad_norm": 3.6620893478393555,
      "learning_rate": 8.780550333276363e-05,
      "loss": 1.186,
      "step": 9648
    },
    {
      "epoch": 0.04122905218899818,
      "grad_norm": 4.797219276428223,
      "learning_rate": 8.776277559391557e-05,
      "loss": 1.1936,
      "step": 9649
    },
    {
      "epoch": 0.04123332507242537,
      "grad_norm": 1.078940987586975,
      "learning_rate": 8.772004785506752e-05,
      "loss": 0.3886,
      "step": 9650
    },
    {
      "epoch": 0.04123759795585257,
      "grad_norm": 0.8365285396575928,
      "learning_rate": 8.767732011621946e-05,
      "loss": 0.4262,
      "step": 9651
    },
    {
      "epoch": 0.041241870839279764,
      "grad_norm": 1.1362112760543823,
      "learning_rate": 8.76345923773714e-05,
      "loss": 0.4692,
      "step": 9652
    },
    {
      "epoch": 0.041246143722706956,
      "grad_norm": 2.921003818511963,
      "learning_rate": 8.759186463852333e-05,
      "loss": 0.7299,
      "step": 9653
    },
    {
      "epoch": 0.041250416606134155,
      "grad_norm": 0.8822146058082581,
      "learning_rate": 8.754913689967527e-05,
      "loss": 0.3184,
      "step": 9654
    },
    {
      "epoch": 0.04125468948956135,
      "grad_norm": 3.377324104309082,
      "learning_rate": 8.750640916082721e-05,
      "loss": 0.9549,
      "step": 9655
    },
    {
      "epoch": 0.04125896237298854,
      "grad_norm": 0.7400100231170654,
      "learning_rate": 8.746368142197914e-05,
      "loss": 0.326,
      "step": 9656
    },
    {
      "epoch": 0.04126323525641573,
      "grad_norm": 3.234647750854492,
      "learning_rate": 8.74209536831311e-05,
      "loss": 1.8637,
      "step": 9657
    },
    {
      "epoch": 0.04126750813984293,
      "grad_norm": 2.172071933746338,
      "learning_rate": 8.737822594428303e-05,
      "loss": 0.5619,
      "step": 9658
    },
    {
      "epoch": 0.04127178102327012,
      "grad_norm": 0.506202220916748,
      "learning_rate": 8.733549820543497e-05,
      "loss": 0.2092,
      "step": 9659
    },
    {
      "epoch": 0.041276053906697314,
      "grad_norm": 0.41687294840812683,
      "learning_rate": 8.729277046658691e-05,
      "loss": 0.1293,
      "step": 9660
    },
    {
      "epoch": 0.041280326790124514,
      "grad_norm": 0.8585433959960938,
      "learning_rate": 8.725004272773885e-05,
      "loss": 0.3052,
      "step": 9661
    },
    {
      "epoch": 0.041284599673551706,
      "grad_norm": 1.6157783269882202,
      "learning_rate": 8.720731498889079e-05,
      "loss": 0.5478,
      "step": 9662
    },
    {
      "epoch": 0.0412888725569789,
      "grad_norm": 0.7554078102111816,
      "learning_rate": 8.716458725004272e-05,
      "loss": 0.2758,
      "step": 9663
    },
    {
      "epoch": 0.0412931454404061,
      "grad_norm": 1.411049246788025,
      "learning_rate": 8.712185951119466e-05,
      "loss": 0.5345,
      "step": 9664
    },
    {
      "epoch": 0.04129741832383329,
      "grad_norm": 0.8766710162162781,
      "learning_rate": 8.707913177234661e-05,
      "loss": 0.3808,
      "step": 9665
    },
    {
      "epoch": 0.04130169120726048,
      "grad_norm": 1.1574901342391968,
      "learning_rate": 8.703640403349855e-05,
      "loss": 0.4919,
      "step": 9666
    },
    {
      "epoch": 0.04130596409068768,
      "grad_norm": 0.8452368974685669,
      "learning_rate": 8.699367629465049e-05,
      "loss": 0.4262,
      "step": 9667
    },
    {
      "epoch": 0.04131023697411487,
      "grad_norm": 0.8607791066169739,
      "learning_rate": 8.695094855580244e-05,
      "loss": 0.3476,
      "step": 9668
    },
    {
      "epoch": 0.041314509857542064,
      "grad_norm": 0.801934003829956,
      "learning_rate": 8.690822081695438e-05,
      "loss": 0.392,
      "step": 9669
    },
    {
      "epoch": 0.04131878274096926,
      "grad_norm": 3.74812912940979,
      "learning_rate": 8.68654930781063e-05,
      "loss": 0.813,
      "step": 9670
    },
    {
      "epoch": 0.041323055624396456,
      "grad_norm": 0.9241141080856323,
      "learning_rate": 8.682276533925824e-05,
      "loss": 0.3514,
      "step": 9671
    },
    {
      "epoch": 0.04132732850782365,
      "grad_norm": 2.0927772521972656,
      "learning_rate": 8.678003760041019e-05,
      "loss": 0.7147,
      "step": 9672
    },
    {
      "epoch": 0.04133160139125085,
      "grad_norm": 0.6685401201248169,
      "learning_rate": 8.673730986156213e-05,
      "loss": 0.2636,
      "step": 9673
    },
    {
      "epoch": 0.04133587427467804,
      "grad_norm": 4.801462173461914,
      "learning_rate": 8.669458212271407e-05,
      "loss": 1.0541,
      "step": 9674
    },
    {
      "epoch": 0.04134014715810523,
      "grad_norm": 0.7013226747512817,
      "learning_rate": 8.6651854383866e-05,
      "loss": 0.326,
      "step": 9675
    },
    {
      "epoch": 0.04134442004153243,
      "grad_norm": 0.8534255623817444,
      "learning_rate": 8.660912664501796e-05,
      "loss": 0.4121,
      "step": 9676
    },
    {
      "epoch": 0.04134869292495962,
      "grad_norm": 0.9425478577613831,
      "learning_rate": 8.65663989061699e-05,
      "loss": 0.4139,
      "step": 9677
    },
    {
      "epoch": 0.041352965808386814,
      "grad_norm": 2.537273406982422,
      "learning_rate": 8.652367116732182e-05,
      "loss": 0.5967,
      "step": 9678
    },
    {
      "epoch": 0.04135723869181401,
      "grad_norm": 1.7710375785827637,
      "learning_rate": 8.648094342847377e-05,
      "loss": 0.4538,
      "step": 9679
    },
    {
      "epoch": 0.041361511575241205,
      "grad_norm": 0.7063900232315063,
      "learning_rate": 8.643821568962571e-05,
      "loss": 0.3091,
      "step": 9680
    },
    {
      "epoch": 0.0413657844586684,
      "grad_norm": 0.9211174249649048,
      "learning_rate": 8.639548795077765e-05,
      "loss": 0.3385,
      "step": 9681
    },
    {
      "epoch": 0.04137005734209559,
      "grad_norm": 3.3355257511138916,
      "learning_rate": 8.635276021192958e-05,
      "loss": 0.9197,
      "step": 9682
    },
    {
      "epoch": 0.04137433022552279,
      "grad_norm": 0.7424198985099792,
      "learning_rate": 8.631003247308153e-05,
      "loss": 0.2888,
      "step": 9683
    },
    {
      "epoch": 0.04137860310894998,
      "grad_norm": 0.9231227040290833,
      "learning_rate": 8.626730473423347e-05,
      "loss": 0.424,
      "step": 9684
    },
    {
      "epoch": 0.04138287599237717,
      "grad_norm": 0.9316551685333252,
      "learning_rate": 8.622457699538541e-05,
      "loss": 0.3514,
      "step": 9685
    },
    {
      "epoch": 0.04138714887580437,
      "grad_norm": 0.9175732135772705,
      "learning_rate": 8.618184925653733e-05,
      "loss": 0.4117,
      "step": 9686
    },
    {
      "epoch": 0.041391421759231564,
      "grad_norm": 0.8585202097892761,
      "learning_rate": 8.613912151768929e-05,
      "loss": 0.358,
      "step": 9687
    },
    {
      "epoch": 0.041395694642658756,
      "grad_norm": 0.8743483424186707,
      "learning_rate": 8.609639377884122e-05,
      "loss": 0.3806,
      "step": 9688
    },
    {
      "epoch": 0.041399967526085955,
      "grad_norm": 1.5852676630020142,
      "learning_rate": 8.605366603999316e-05,
      "loss": 1.0403,
      "step": 9689
    },
    {
      "epoch": 0.04140424040951315,
      "grad_norm": 3.7625153064727783,
      "learning_rate": 8.60109383011451e-05,
      "loss": 1.0886,
      "step": 9690
    },
    {
      "epoch": 0.04140851329294034,
      "grad_norm": 0.9558815956115723,
      "learning_rate": 8.596821056229705e-05,
      "loss": 0.5948,
      "step": 9691
    },
    {
      "epoch": 0.04141278617636754,
      "grad_norm": 0.8633614182472229,
      "learning_rate": 8.592548282344899e-05,
      "loss": 0.3184,
      "step": 9692
    },
    {
      "epoch": 0.04141705905979473,
      "grad_norm": 2.9069948196411133,
      "learning_rate": 8.588275508460093e-05,
      "loss": 0.7063,
      "step": 9693
    },
    {
      "epoch": 0.04142133194322192,
      "grad_norm": 2.092905044555664,
      "learning_rate": 8.584002734575286e-05,
      "loss": 0.5252,
      "step": 9694
    },
    {
      "epoch": 0.04142560482664912,
      "grad_norm": 1.4788063764572144,
      "learning_rate": 8.57972996069048e-05,
      "loss": 0.3499,
      "step": 9695
    },
    {
      "epoch": 0.041429877710076314,
      "grad_norm": 3.758664846420288,
      "learning_rate": 8.575457186805674e-05,
      "loss": 1.0389,
      "step": 9696
    },
    {
      "epoch": 0.041434150593503506,
      "grad_norm": 1.6357483863830566,
      "learning_rate": 8.571184412920868e-05,
      "loss": 1.0396,
      "step": 9697
    },
    {
      "epoch": 0.041438423476930705,
      "grad_norm": 1.1844656467437744,
      "learning_rate": 8.566911639036063e-05,
      "loss": 0.4433,
      "step": 9698
    },
    {
      "epoch": 0.0414426963603579,
      "grad_norm": 0.9657945036888123,
      "learning_rate": 8.562638865151257e-05,
      "loss": 0.6157,
      "step": 9699
    },
    {
      "epoch": 0.04144696924378509,
      "grad_norm": 2.2293860912323,
      "learning_rate": 8.55836609126645e-05,
      "loss": 1.4686,
      "step": 9700
    },
    {
      "epoch": 0.04145124212721229,
      "grad_norm": 0.9568603038787842,
      "learning_rate": 8.554093317381644e-05,
      "loss": 0.5936,
      "step": 9701
    },
    {
      "epoch": 0.04145551501063948,
      "grad_norm": 0.8777249455451965,
      "learning_rate": 8.549820543496838e-05,
      "loss": 0.531,
      "step": 9702
    },
    {
      "epoch": 0.04145978789406667,
      "grad_norm": 3.151258945465088,
      "learning_rate": 8.545547769612032e-05,
      "loss": 0.6139,
      "step": 9703
    },
    {
      "epoch": 0.04146406077749387,
      "grad_norm": 1.460439682006836,
      "learning_rate": 8.541274995727226e-05,
      "loss": 0.3245,
      "step": 9704
    },
    {
      "epoch": 0.041468333660921064,
      "grad_norm": 2.865229845046997,
      "learning_rate": 8.537002221842421e-05,
      "loss": 0.644,
      "step": 9705
    },
    {
      "epoch": 0.041472606544348256,
      "grad_norm": 1.443062424659729,
      "learning_rate": 8.532729447957615e-05,
      "loss": 0.6008,
      "step": 9706
    },
    {
      "epoch": 0.04147687942777545,
      "grad_norm": 0.9009121060371399,
      "learning_rate": 8.528456674072808e-05,
      "loss": 0.4097,
      "step": 9707
    },
    {
      "epoch": 0.04148115231120265,
      "grad_norm": 0.8437166213989258,
      "learning_rate": 8.524183900188002e-05,
      "loss": 0.3052,
      "step": 9708
    },
    {
      "epoch": 0.04148542519462984,
      "grad_norm": 4.238221168518066,
      "learning_rate": 8.519911126303197e-05,
      "loss": 1.1755,
      "step": 9709
    },
    {
      "epoch": 0.04148969807805703,
      "grad_norm": 3.115734338760376,
      "learning_rate": 8.515638352418391e-05,
      "loss": 0.9366,
      "step": 9710
    },
    {
      "epoch": 0.04149397096148423,
      "grad_norm": 1.8820093870162964,
      "learning_rate": 8.511365578533583e-05,
      "loss": 0.4579,
      "step": 9711
    },
    {
      "epoch": 0.04149824384491142,
      "grad_norm": 0.8512955904006958,
      "learning_rate": 8.507092804648777e-05,
      "loss": 0.3324,
      "step": 9712
    },
    {
      "epoch": 0.041502516728338615,
      "grad_norm": 2.584820032119751,
      "learning_rate": 8.502820030763972e-05,
      "loss": 0.5798,
      "step": 9713
    },
    {
      "epoch": 0.041506789611765814,
      "grad_norm": 0.9312339425086975,
      "learning_rate": 8.498547256879166e-05,
      "loss": 0.6062,
      "step": 9714
    },
    {
      "epoch": 0.041511062495193006,
      "grad_norm": 0.6850504279136658,
      "learning_rate": 8.49427448299436e-05,
      "loss": 0.2636,
      "step": 9715
    },
    {
      "epoch": 0.0415153353786202,
      "grad_norm": 0.6537512540817261,
      "learning_rate": 8.490001709109554e-05,
      "loss": 0.2503,
      "step": 9716
    },
    {
      "epoch": 0.0415196082620474,
      "grad_norm": 0.9346968531608582,
      "learning_rate": 8.485728935224749e-05,
      "loss": 0.6058,
      "step": 9717
    },
    {
      "epoch": 0.04152388114547459,
      "grad_norm": 0.8651061058044434,
      "learning_rate": 8.481456161339943e-05,
      "loss": 0.5218,
      "step": 9718
    },
    {
      "epoch": 0.04152815402890178,
      "grad_norm": 0.9398869872093201,
      "learning_rate": 8.477183387455135e-05,
      "loss": 0.6056,
      "step": 9719
    },
    {
      "epoch": 0.04153242691232898,
      "grad_norm": 2.2226297855377197,
      "learning_rate": 8.47291061357033e-05,
      "loss": 0.5473,
      "step": 9720
    },
    {
      "epoch": 0.04153669979575617,
      "grad_norm": 2.0895321369171143,
      "learning_rate": 8.468637839685524e-05,
      "loss": 0.6808,
      "step": 9721
    },
    {
      "epoch": 0.041540972679183365,
      "grad_norm": 0.47487884759902954,
      "learning_rate": 8.464365065800718e-05,
      "loss": 0.1818,
      "step": 9722
    },
    {
      "epoch": 0.041545245562610564,
      "grad_norm": 2.7368574142456055,
      "learning_rate": 8.460092291915912e-05,
      "loss": 0.7264,
      "step": 9723
    },
    {
      "epoch": 0.041549518446037756,
      "grad_norm": 3.634481430053711,
      "learning_rate": 8.455819518031107e-05,
      "loss": 1.012,
      "step": 9724
    },
    {
      "epoch": 0.04155379132946495,
      "grad_norm": 0.672817587852478,
      "learning_rate": 8.4515467441463e-05,
      "loss": 0.2635,
      "step": 9725
    },
    {
      "epoch": 0.04155806421289215,
      "grad_norm": 2.21197247505188,
      "learning_rate": 8.447273970261494e-05,
      "loss": 0.7972,
      "step": 9726
    },
    {
      "epoch": 0.04156233709631934,
      "grad_norm": 1.5683125257492065,
      "learning_rate": 8.443001196376687e-05,
      "loss": 1.0806,
      "step": 9727
    },
    {
      "epoch": 0.04156660997974653,
      "grad_norm": 0.8419178128242493,
      "learning_rate": 8.438728422491882e-05,
      "loss": 0.305,
      "step": 9728
    },
    {
      "epoch": 0.04157088286317372,
      "grad_norm": 3.470918893814087,
      "learning_rate": 8.434455648607076e-05,
      "loss": 0.8676,
      "step": 9729
    },
    {
      "epoch": 0.04157515574660092,
      "grad_norm": 2.735058307647705,
      "learning_rate": 8.43018287472227e-05,
      "loss": 0.724,
      "step": 9730
    },
    {
      "epoch": 0.041579428630028115,
      "grad_norm": 0.8047870397567749,
      "learning_rate": 8.425910100837465e-05,
      "loss": 0.4039,
      "step": 9731
    },
    {
      "epoch": 0.04158370151345531,
      "grad_norm": 2.0597381591796875,
      "learning_rate": 8.421637326952658e-05,
      "loss": 0.7182,
      "step": 9732
    },
    {
      "epoch": 0.041587974396882506,
      "grad_norm": 1.896285057067871,
      "learning_rate": 8.417364553067852e-05,
      "loss": 0.4195,
      "step": 9733
    },
    {
      "epoch": 0.0415922472803097,
      "grad_norm": 0.8620461225509644,
      "learning_rate": 8.413091779183046e-05,
      "loss": 0.4632,
      "step": 9734
    },
    {
      "epoch": 0.04159652016373689,
      "grad_norm": 0.7747617959976196,
      "learning_rate": 8.40881900529824e-05,
      "loss": 0.2889,
      "step": 9735
    },
    {
      "epoch": 0.04160079304716409,
      "grad_norm": 0.8679904937744141,
      "learning_rate": 8.404546231413433e-05,
      "loss": 0.4633,
      "step": 9736
    },
    {
      "epoch": 0.04160506593059128,
      "grad_norm": 0.8385992646217346,
      "learning_rate": 8.400273457528627e-05,
      "loss": 0.3694,
      "step": 9737
    },
    {
      "epoch": 0.04160933881401847,
      "grad_norm": 0.8129074573516846,
      "learning_rate": 8.396000683643821e-05,
      "loss": 0.3693,
      "step": 9738
    },
    {
      "epoch": 0.04161361169744567,
      "grad_norm": 1.8911272287368774,
      "learning_rate": 8.391727909759016e-05,
      "loss": 0.4747,
      "step": 9739
    },
    {
      "epoch": 0.041617884580872865,
      "grad_norm": 0.9778504371643066,
      "learning_rate": 8.38745513587421e-05,
      "loss": 0.4318,
      "step": 9740
    },
    {
      "epoch": 0.04162215746430006,
      "grad_norm": 0.8557427525520325,
      "learning_rate": 8.383182361989404e-05,
      "loss": 0.4509,
      "step": 9741
    },
    {
      "epoch": 0.041626430347727256,
      "grad_norm": 4.98292875289917,
      "learning_rate": 8.378909588104598e-05,
      "loss": 1.1114,
      "step": 9742
    },
    {
      "epoch": 0.04163070323115445,
      "grad_norm": 0.8915827870368958,
      "learning_rate": 8.374636814219793e-05,
      "loss": 0.4632,
      "step": 9743
    },
    {
      "epoch": 0.04163497611458164,
      "grad_norm": 0.94346684217453,
      "learning_rate": 8.370364040334985e-05,
      "loss": 0.5165,
      "step": 9744
    },
    {
      "epoch": 0.04163924899800884,
      "grad_norm": 4.254133701324463,
      "learning_rate": 8.366091266450179e-05,
      "loss": 1.2238,
      "step": 9745
    },
    {
      "epoch": 0.04164352188143603,
      "grad_norm": 1.0840665102005005,
      "learning_rate": 8.361818492565374e-05,
      "loss": 0.4507,
      "step": 9746
    },
    {
      "epoch": 0.04164779476486322,
      "grad_norm": 0.8363012671470642,
      "learning_rate": 8.357545718680568e-05,
      "loss": 0.3883,
      "step": 9747
    },
    {
      "epoch": 0.04165206764829042,
      "grad_norm": 4.257519245147705,
      "learning_rate": 8.353272944795762e-05,
      "loss": 1.1697,
      "step": 9748
    },
    {
      "epoch": 0.041656340531717614,
      "grad_norm": 0.8143078684806824,
      "learning_rate": 8.349000170910955e-05,
      "loss": 0.3513,
      "step": 9749
    },
    {
      "epoch": 0.04166061341514481,
      "grad_norm": 1.8421536684036255,
      "learning_rate": 8.34472739702615e-05,
      "loss": 0.3862,
      "step": 9750
    },
    {
      "epoch": 0.041664886298572006,
      "grad_norm": 0.9745435118675232,
      "learning_rate": 8.340454623141344e-05,
      "loss": 0.3516,
      "step": 9751
    },
    {
      "epoch": 0.0416691591819992,
      "grad_norm": 0.6805887222290039,
      "learning_rate": 8.336181849256537e-05,
      "loss": 0.2501,
      "step": 9752
    },
    {
      "epoch": 0.04167343206542639,
      "grad_norm": 4.6193108558654785,
      "learning_rate": 8.33190907537173e-05,
      "loss": 1.2895,
      "step": 9753
    },
    {
      "epoch": 0.04167770494885358,
      "grad_norm": 1.005513310432434,
      "learning_rate": 8.327636301486926e-05,
      "loss": 0.3656,
      "step": 9754
    },
    {
      "epoch": 0.04168197783228078,
      "grad_norm": 1.3076980113983154,
      "learning_rate": 8.32336352760212e-05,
      "loss": 0.4731,
      "step": 9755
    },
    {
      "epoch": 0.04168625071570797,
      "grad_norm": 1.0939091444015503,
      "learning_rate": 8.319090753717313e-05,
      "loss": 0.4175,
      "step": 9756
    },
    {
      "epoch": 0.041690523599135165,
      "grad_norm": 4.266212463378906,
      "learning_rate": 8.314817979832508e-05,
      "loss": 0.9344,
      "step": 9757
    },
    {
      "epoch": 0.041694796482562364,
      "grad_norm": 0.46332821249961853,
      "learning_rate": 8.310545205947702e-05,
      "loss": 0.1265,
      "step": 9758
    },
    {
      "epoch": 0.041699069365989556,
      "grad_norm": 0.8790472745895386,
      "learning_rate": 8.306272432062896e-05,
      "loss": 0.2817,
      "step": 9759
    },
    {
      "epoch": 0.04170334224941675,
      "grad_norm": 0.8924012780189514,
      "learning_rate": 8.301999658178088e-05,
      "loss": 0.3182,
      "step": 9760
    },
    {
      "epoch": 0.04170761513284395,
      "grad_norm": 3.423346519470215,
      "learning_rate": 8.297726884293284e-05,
      "loss": 0.7882,
      "step": 9761
    },
    {
      "epoch": 0.04171188801627114,
      "grad_norm": 2.489598035812378,
      "learning_rate": 8.293454110408477e-05,
      "loss": 1.5642,
      "step": 9762
    },
    {
      "epoch": 0.04171616089969833,
      "grad_norm": 0.9571613669395447,
      "learning_rate": 8.289181336523671e-05,
      "loss": 0.4392,
      "step": 9763
    },
    {
      "epoch": 0.04172043378312553,
      "grad_norm": 2.193232536315918,
      "learning_rate": 8.284908562638865e-05,
      "loss": 0.7388,
      "step": 9764
    },
    {
      "epoch": 0.04172470666655272,
      "grad_norm": 0.963493287563324,
      "learning_rate": 8.28063578875406e-05,
      "loss": 0.4392,
      "step": 9765
    },
    {
      "epoch": 0.041728979549979915,
      "grad_norm": 1.596616268157959,
      "learning_rate": 8.276363014869254e-05,
      "loss": 0.5452,
      "step": 9766
    },
    {
      "epoch": 0.041733252433407114,
      "grad_norm": 2.26505708694458,
      "learning_rate": 8.272090240984448e-05,
      "loss": 0.635,
      "step": 9767
    },
    {
      "epoch": 0.041737525316834306,
      "grad_norm": 1.1038957834243774,
      "learning_rate": 8.26781746709964e-05,
      "loss": 0.4026,
      "step": 9768
    },
    {
      "epoch": 0.0417417982002615,
      "grad_norm": 3.6983590126037598,
      "learning_rate": 8.263544693214835e-05,
      "loss": 2.0785,
      "step": 9769
    },
    {
      "epoch": 0.0417460710836887,
      "grad_norm": 0.9397609233856201,
      "learning_rate": 8.259271919330029e-05,
      "loss": 0.3987,
      "step": 9770
    },
    {
      "epoch": 0.04175034396711589,
      "grad_norm": 1.5687834024429321,
      "learning_rate": 8.254999145445223e-05,
      "loss": 0.5214,
      "step": 9771
    },
    {
      "epoch": 0.04175461685054308,
      "grad_norm": 1.2341018915176392,
      "learning_rate": 8.250726371560418e-05,
      "loss": 0.4289,
      "step": 9772
    },
    {
      "epoch": 0.04175888973397028,
      "grad_norm": 0.6803931593894958,
      "learning_rate": 8.246453597675612e-05,
      "loss": 0.2501,
      "step": 9773
    },
    {
      "epoch": 0.04176316261739747,
      "grad_norm": 0.9480066299438477,
      "learning_rate": 8.242180823790805e-05,
      "loss": 0.4378,
      "step": 9774
    },
    {
      "epoch": 0.041767435500824665,
      "grad_norm": 1.0453596115112305,
      "learning_rate": 8.237908049905999e-05,
      "loss": 0.4138,
      "step": 9775
    },
    {
      "epoch": 0.041771708384251864,
      "grad_norm": 0.7887693643569946,
      "learning_rate": 8.233635276021193e-05,
      "loss": 0.3029,
      "step": 9776
    },
    {
      "epoch": 0.041775981267679056,
      "grad_norm": 2.2931549549102783,
      "learning_rate": 8.229362502136387e-05,
      "loss": 0.5497,
      "step": 9777
    },
    {
      "epoch": 0.04178025415110625,
      "grad_norm": 1.1758099794387817,
      "learning_rate": 8.22508972825158e-05,
      "loss": 0.4744,
      "step": 9778
    },
    {
      "epoch": 0.04178452703453344,
      "grad_norm": 4.249275207519531,
      "learning_rate": 8.220816954366774e-05,
      "loss": 0.8841,
      "step": 9779
    },
    {
      "epoch": 0.04178879991796064,
      "grad_norm": 0.9614260196685791,
      "learning_rate": 8.21654418048197e-05,
      "loss": 0.5161,
      "step": 9780
    },
    {
      "epoch": 0.04179307280138783,
      "grad_norm": 0.7018673419952393,
      "learning_rate": 8.212271406597163e-05,
      "loss": 0.2774,
      "step": 9781
    },
    {
      "epoch": 0.041797345684815024,
      "grad_norm": 4.415409564971924,
      "learning_rate": 8.207998632712357e-05,
      "loss": 3.141,
      "step": 9782
    },
    {
      "epoch": 0.04180161856824222,
      "grad_norm": 2.2931017875671387,
      "learning_rate": 8.203725858827552e-05,
      "loss": 0.5813,
      "step": 9783
    },
    {
      "epoch": 0.041805891451669415,
      "grad_norm": 1.0682430267333984,
      "learning_rate": 8.199453084942746e-05,
      "loss": 0.5444,
      "step": 9784
    },
    {
      "epoch": 0.04181016433509661,
      "grad_norm": 2.4161202907562256,
      "learning_rate": 8.195180311057938e-05,
      "loss": 1.5092,
      "step": 9785
    },
    {
      "epoch": 0.041814437218523806,
      "grad_norm": 0.5225964784622192,
      "learning_rate": 8.190907537173132e-05,
      "loss": 0.2098,
      "step": 9786
    },
    {
      "epoch": 0.041818710101951,
      "grad_norm": 2.5070207118988037,
      "learning_rate": 8.186634763288327e-05,
      "loss": 0.5668,
      "step": 9787
    },
    {
      "epoch": 0.04182298298537819,
      "grad_norm": 0.9240472316741943,
      "learning_rate": 8.182361989403521e-05,
      "loss": 0.5166,
      "step": 9788
    },
    {
      "epoch": 0.04182725586880539,
      "grad_norm": 2.981339931488037,
      "learning_rate": 8.178089215518715e-05,
      "loss": 0.9043,
      "step": 9789
    },
    {
      "epoch": 0.04183152875223258,
      "grad_norm": 1.2981886863708496,
      "learning_rate": 8.173816441633909e-05,
      "loss": 0.4582,
      "step": 9790
    },
    {
      "epoch": 0.041835801635659774,
      "grad_norm": 2.816847324371338,
      "learning_rate": 8.169543667749104e-05,
      "loss": 0.9103,
      "step": 9791
    },
    {
      "epoch": 0.04184007451908697,
      "grad_norm": 1.481166958808899,
      "learning_rate": 8.165270893864298e-05,
      "loss": 0.4503,
      "step": 9792
    },
    {
      "epoch": 0.041844347402514165,
      "grad_norm": 1.9512124061584473,
      "learning_rate": 8.16099811997949e-05,
      "loss": 0.6573,
      "step": 9793
    },
    {
      "epoch": 0.04184862028594136,
      "grad_norm": 4.674378395080566,
      "learning_rate": 8.156725346094684e-05,
      "loss": 2.7024,
      "step": 9794
    },
    {
      "epoch": 0.041852893169368556,
      "grad_norm": 1.97641921043396,
      "learning_rate": 8.152452572209879e-05,
      "loss": 0.6265,
      "step": 9795
    },
    {
      "epoch": 0.04185716605279575,
      "grad_norm": 2.9422295093536377,
      "learning_rate": 8.148179798325073e-05,
      "loss": 0.8538,
      "step": 9796
    },
    {
      "epoch": 0.04186143893622294,
      "grad_norm": 1.1506314277648926,
      "learning_rate": 8.143907024440267e-05,
      "loss": 0.4036,
      "step": 9797
    },
    {
      "epoch": 0.04186571181965014,
      "grad_norm": 0.8024843335151672,
      "learning_rate": 8.139634250555462e-05,
      "loss": 0.2757,
      "step": 9798
    },
    {
      "epoch": 0.04186998470307733,
      "grad_norm": 0.5364871025085449,
      "learning_rate": 8.135361476670655e-05,
      "loss": 0.2099,
      "step": 9799
    },
    {
      "epoch": 0.041874257586504524,
      "grad_norm": 1.902581810951233,
      "learning_rate": 8.131088702785849e-05,
      "loss": 0.6436,
      "step": 9800
    },
    {
      "epoch": 0.04187853046993172,
      "grad_norm": 4.045346260070801,
      "learning_rate": 8.126815928901042e-05,
      "loss": 1.0364,
      "step": 9801
    },
    {
      "epoch": 0.041882803353358915,
      "grad_norm": 4.426723003387451,
      "learning_rate": 8.122543155016237e-05,
      "loss": 1.425,
      "step": 9802
    },
    {
      "epoch": 0.04188707623678611,
      "grad_norm": 1.4366155862808228,
      "learning_rate": 8.11827038113143e-05,
      "loss": 0.4289,
      "step": 9803
    },
    {
      "epoch": 0.0418913491202133,
      "grad_norm": 2.4721288681030273,
      "learning_rate": 8.113997607246624e-05,
      "loss": 0.5513,
      "step": 9804
    },
    {
      "epoch": 0.0418956220036405,
      "grad_norm": 0.80609130859375,
      "learning_rate": 8.109724833361818e-05,
      "loss": 0.4577,
      "step": 9805
    },
    {
      "epoch": 0.04189989488706769,
      "grad_norm": 0.532314658164978,
      "learning_rate": 8.105452059477013e-05,
      "loss": 0.1863,
      "step": 9806
    },
    {
      "epoch": 0.04190416777049488,
      "grad_norm": 3.7516214847564697,
      "learning_rate": 8.101179285592207e-05,
      "loss": 1.0977,
      "step": 9807
    },
    {
      "epoch": 0.04190844065392208,
      "grad_norm": 2.064204216003418,
      "learning_rate": 8.096906511707401e-05,
      "loss": 0.7029,
      "step": 9808
    },
    {
      "epoch": 0.04191271353734927,
      "grad_norm": 0.5294111967086792,
      "learning_rate": 8.092633737822593e-05,
      "loss": 0.1862,
      "step": 9809
    },
    {
      "epoch": 0.041916986420776466,
      "grad_norm": 4.653476715087891,
      "learning_rate": 8.088360963937788e-05,
      "loss": 1.0821,
      "step": 9810
    },
    {
      "epoch": 0.041921259304203665,
      "grad_norm": 0.9335005879402161,
      "learning_rate": 8.084088190052982e-05,
      "loss": 0.3027,
      "step": 9811
    },
    {
      "epoch": 0.04192553218763086,
      "grad_norm": 3.723839044570923,
      "learning_rate": 8.079815416168176e-05,
      "loss": 1.0738,
      "step": 9812
    },
    {
      "epoch": 0.04192980507105805,
      "grad_norm": 2.2008004188537598,
      "learning_rate": 8.075542642283371e-05,
      "loss": 1.4565,
      "step": 9813
    },
    {
      "epoch": 0.04193407795448525,
      "grad_norm": 0.7839758396148682,
      "learning_rate": 8.071269868398565e-05,
      "loss": 0.3257,
      "step": 9814
    },
    {
      "epoch": 0.04193835083791244,
      "grad_norm": 0.8176755905151367,
      "learning_rate": 8.066997094513759e-05,
      "loss": 0.4755,
      "step": 9815
    },
    {
      "epoch": 0.04194262372133963,
      "grad_norm": 2.209587812423706,
      "learning_rate": 8.062724320628952e-05,
      "loss": 0.5353,
      "step": 9816
    },
    {
      "epoch": 0.04194689660476683,
      "grad_norm": 3.6835408210754395,
      "learning_rate": 8.058451546744148e-05,
      "loss": 1.0042,
      "step": 9817
    },
    {
      "epoch": 0.04195116948819402,
      "grad_norm": 0.789553701877594,
      "learning_rate": 8.05417877285934e-05,
      "loss": 0.4566,
      "step": 9818
    },
    {
      "epoch": 0.041955442371621215,
      "grad_norm": 0.7973672151565552,
      "learning_rate": 8.049905998974534e-05,
      "loss": 0.4582,
      "step": 9819
    },
    {
      "epoch": 0.041959715255048415,
      "grad_norm": 0.7546736001968384,
      "learning_rate": 8.045633225089728e-05,
      "loss": 0.4101,
      "step": 9820
    },
    {
      "epoch": 0.04196398813847561,
      "grad_norm": 1.3273385763168335,
      "learning_rate": 8.041360451204923e-05,
      "loss": 0.5068,
      "step": 9821
    },
    {
      "epoch": 0.0419682610219028,
      "grad_norm": 1.3076509237289429,
      "learning_rate": 8.037087677320117e-05,
      "loss": 0.5183,
      "step": 9822
    },
    {
      "epoch": 0.04197253390533,
      "grad_norm": 4.180056095123291,
      "learning_rate": 8.03281490343531e-05,
      "loss": 0.966,
      "step": 9823
    },
    {
      "epoch": 0.04197680678875719,
      "grad_norm": 0.4762076139450073,
      "learning_rate": 8.028542129550505e-05,
      "loss": 0.1821,
      "step": 9824
    },
    {
      "epoch": 0.04198107967218438,
      "grad_norm": 4.643554210662842,
      "learning_rate": 8.024269355665699e-05,
      "loss": 2.5862,
      "step": 9825
    },
    {
      "epoch": 0.04198535255561158,
      "grad_norm": 0.8725302219390869,
      "learning_rate": 8.019996581780892e-05,
      "loss": 0.4699,
      "step": 9826
    },
    {
      "epoch": 0.04198962543903877,
      "grad_norm": 0.537041425704956,
      "learning_rate": 8.015723807896085e-05,
      "loss": 0.2575,
      "step": 9827
    },
    {
      "epoch": 0.041993898322465965,
      "grad_norm": 2.751145839691162,
      "learning_rate": 8.01145103401128e-05,
      "loss": 0.8501,
      "step": 9828
    },
    {
      "epoch": 0.04199817120589316,
      "grad_norm": 4.0071611404418945,
      "learning_rate": 8.007178260126474e-05,
      "loss": 0.9956,
      "step": 9829
    },
    {
      "epoch": 0.04200244408932036,
      "grad_norm": 2.144915819168091,
      "learning_rate": 8.002905486241668e-05,
      "loss": 0.5946,
      "step": 9830
    },
    {
      "epoch": 0.04200671697274755,
      "grad_norm": 3.462228298187256,
      "learning_rate": 7.998632712356862e-05,
      "loss": 1.973,
      "step": 9831
    },
    {
      "epoch": 0.04201098985617474,
      "grad_norm": 1.2393513917922974,
      "learning_rate": 7.994359938472057e-05,
      "loss": 0.4023,
      "step": 9832
    },
    {
      "epoch": 0.04201526273960194,
      "grad_norm": 1.3148870468139648,
      "learning_rate": 7.990087164587251e-05,
      "loss": 0.5924,
      "step": 9833
    },
    {
      "epoch": 0.04201953562302913,
      "grad_norm": 0.6055231094360352,
      "learning_rate": 7.985814390702443e-05,
      "loss": 0.327,
      "step": 9834
    },
    {
      "epoch": 0.042023808506456324,
      "grad_norm": 1.7962467670440674,
      "learning_rate": 7.981541616817637e-05,
      "loss": 0.4567,
      "step": 9835
    },
    {
      "epoch": 0.04202808138988352,
      "grad_norm": 0.8337855935096741,
      "learning_rate": 7.977268842932832e-05,
      "loss": 0.4716,
      "step": 9836
    },
    {
      "epoch": 0.042032354273310715,
      "grad_norm": 2.186776638031006,
      "learning_rate": 7.972996069048026e-05,
      "loss": 0.4878,
      "step": 9837
    },
    {
      "epoch": 0.04203662715673791,
      "grad_norm": 2.1612942218780518,
      "learning_rate": 7.96872329516322e-05,
      "loss": 0.5398,
      "step": 9838
    },
    {
      "epoch": 0.042040900040165106,
      "grad_norm": 1.0576155185699463,
      "learning_rate": 7.964450521278415e-05,
      "loss": 0.3883,
      "step": 9839
    },
    {
      "epoch": 0.0420451729235923,
      "grad_norm": 2.6578009128570557,
      "learning_rate": 7.960177747393609e-05,
      "loss": 0.9324,
      "step": 9840
    },
    {
      "epoch": 0.04204944580701949,
      "grad_norm": 4.335902214050293,
      "learning_rate": 7.955904973508803e-05,
      "loss": 1.3279,
      "step": 9841
    },
    {
      "epoch": 0.04205371869044669,
      "grad_norm": 1.2437193393707275,
      "learning_rate": 7.951632199623995e-05,
      "loss": 0.4871,
      "step": 9842
    },
    {
      "epoch": 0.04205799157387388,
      "grad_norm": 2.453660249710083,
      "learning_rate": 7.94735942573919e-05,
      "loss": 0.5285,
      "step": 9843
    },
    {
      "epoch": 0.042062264457301074,
      "grad_norm": 2.1311566829681396,
      "learning_rate": 7.943086651854384e-05,
      "loss": 0.6402,
      "step": 9844
    },
    {
      "epoch": 0.04206653734072827,
      "grad_norm": 0.8323041200637817,
      "learning_rate": 7.938813877969578e-05,
      "loss": 0.4641,
      "step": 9845
    },
    {
      "epoch": 0.042070810224155465,
      "grad_norm": 1.224602222442627,
      "learning_rate": 7.934541104084771e-05,
      "loss": 0.5354,
      "step": 9846
    },
    {
      "epoch": 0.04207508310758266,
      "grad_norm": 2.7247474193573,
      "learning_rate": 7.930268330199967e-05,
      "loss": 0.8222,
      "step": 9847
    },
    {
      "epoch": 0.042079355991009856,
      "grad_norm": 1.8270297050476074,
      "learning_rate": 7.92599555631516e-05,
      "loss": 0.6041,
      "step": 9848
    },
    {
      "epoch": 0.04208362887443705,
      "grad_norm": 3.661754846572876,
      "learning_rate": 7.921722782430354e-05,
      "loss": 0.9869,
      "step": 9849
    },
    {
      "epoch": 0.04208790175786424,
      "grad_norm": 2.7236008644104004,
      "learning_rate": 7.917450008545548e-05,
      "loss": 0.7995,
      "step": 9850
    },
    {
      "epoch": 0.04209217464129144,
      "grad_norm": 4.196870803833008,
      "learning_rate": 7.913177234660742e-05,
      "loss": 1.272,
      "step": 9851
    },
    {
      "epoch": 0.04209644752471863,
      "grad_norm": 1.0742839574813843,
      "learning_rate": 7.908904460775935e-05,
      "loss": 0.4289,
      "step": 9852
    },
    {
      "epoch": 0.042100720408145824,
      "grad_norm": 0.7461065053939819,
      "learning_rate": 7.904631686891129e-05,
      "loss": 0.3409,
      "step": 9853
    },
    {
      "epoch": 0.042104993291573016,
      "grad_norm": 4.704245567321777,
      "learning_rate": 7.900358913006324e-05,
      "loss": 1.2625,
      "step": 9854
    },
    {
      "epoch": 0.042109266175000215,
      "grad_norm": 0.8720920085906982,
      "learning_rate": 7.896086139121518e-05,
      "loss": 0.2611,
      "step": 9855
    },
    {
      "epoch": 0.04211353905842741,
      "grad_norm": 3.324223041534424,
      "learning_rate": 7.891813365236712e-05,
      "loss": 0.9735,
      "step": 9856
    },
    {
      "epoch": 0.0421178119418546,
      "grad_norm": 0.6452032327651978,
      "learning_rate": 7.887540591351906e-05,
      "loss": 0.322,
      "step": 9857
    },
    {
      "epoch": 0.0421220848252818,
      "grad_norm": 0.8503551483154297,
      "learning_rate": 7.883267817467101e-05,
      "loss": 0.5172,
      "step": 9858
    },
    {
      "epoch": 0.04212635770870899,
      "grad_norm": 1.0349375009536743,
      "learning_rate": 7.878995043582293e-05,
      "loss": 0.4688,
      "step": 9859
    },
    {
      "epoch": 0.04213063059213618,
      "grad_norm": 0.9084563255310059,
      "learning_rate": 7.874722269697487e-05,
      "loss": 0.3808,
      "step": 9860
    },
    {
      "epoch": 0.04213490347556338,
      "grad_norm": 0.852623462677002,
      "learning_rate": 7.870449495812681e-05,
      "loss": 0.5172,
      "step": 9861
    },
    {
      "epoch": 0.042139176358990574,
      "grad_norm": 0.8605053424835205,
      "learning_rate": 7.866176721927876e-05,
      "loss": 0.5172,
      "step": 9862
    },
    {
      "epoch": 0.042143449242417766,
      "grad_norm": 0.6436852216720581,
      "learning_rate": 7.86190394804307e-05,
      "loss": 0.2382,
      "step": 9863
    },
    {
      "epoch": 0.042147722125844965,
      "grad_norm": 4.726212978363037,
      "learning_rate": 7.857631174158264e-05,
      "loss": 1.0352,
      "step": 9864
    },
    {
      "epoch": 0.04215199500927216,
      "grad_norm": 0.8688684701919556,
      "learning_rate": 7.853358400273459e-05,
      "loss": 0.517,
      "step": 9865
    },
    {
      "epoch": 0.04215626789269935,
      "grad_norm": 3.6540651321411133,
      "learning_rate": 7.849085626388653e-05,
      "loss": 0.9306,
      "step": 9866
    },
    {
      "epoch": 0.04216054077612655,
      "grad_norm": 1.741700291633606,
      "learning_rate": 7.844812852503845e-05,
      "loss": 1.1407,
      "step": 9867
    },
    {
      "epoch": 0.04216481365955374,
      "grad_norm": 4.200742721557617,
      "learning_rate": 7.840540078619039e-05,
      "loss": 2.892,
      "step": 9868
    },
    {
      "epoch": 0.04216908654298093,
      "grad_norm": 2.2369143962860107,
      "learning_rate": 7.836267304734234e-05,
      "loss": 1.4584,
      "step": 9869
    },
    {
      "epoch": 0.04217335942640813,
      "grad_norm": 0.944739818572998,
      "learning_rate": 7.831994530849428e-05,
      "loss": 0.4142,
      "step": 9870
    },
    {
      "epoch": 0.042177632309835324,
      "grad_norm": 0.7536138296127319,
      "learning_rate": 7.827721756964621e-05,
      "loss": 0.4034,
      "step": 9871
    },
    {
      "epoch": 0.042181905193262516,
      "grad_norm": 1.8450844287872314,
      "learning_rate": 7.823448983079815e-05,
      "loss": 0.5841,
      "step": 9872
    },
    {
      "epoch": 0.042186178076689715,
      "grad_norm": 1.1267904043197632,
      "learning_rate": 7.81917620919501e-05,
      "loss": 0.416,
      "step": 9873
    },
    {
      "epoch": 0.04219045096011691,
      "grad_norm": 3.186502695083618,
      "learning_rate": 7.814903435310204e-05,
      "loss": 0.7083,
      "step": 9874
    },
    {
      "epoch": 0.0421947238435441,
      "grad_norm": 0.9759031534194946,
      "learning_rate": 7.810630661425397e-05,
      "loss": 0.4029,
      "step": 9875
    },
    {
      "epoch": 0.0421989967269713,
      "grad_norm": 0.647249698638916,
      "learning_rate": 7.806357887540592e-05,
      "loss": 0.2504,
      "step": 9876
    },
    {
      "epoch": 0.04220326961039849,
      "grad_norm": 2.073246955871582,
      "learning_rate": 7.802085113655786e-05,
      "loss": 0.6935,
      "step": 9877
    },
    {
      "epoch": 0.04220754249382568,
      "grad_norm": 2.6488311290740967,
      "learning_rate": 7.797812339770979e-05,
      "loss": 0.6781,
      "step": 9878
    },
    {
      "epoch": 0.042211815377252875,
      "grad_norm": 1.8019338846206665,
      "learning_rate": 7.793539565886173e-05,
      "loss": 0.5161,
      "step": 9879
    },
    {
      "epoch": 0.042216088260680074,
      "grad_norm": 0.8522753715515137,
      "learning_rate": 7.789266792001368e-05,
      "loss": 0.3991,
      "step": 9880
    },
    {
      "epoch": 0.042220361144107266,
      "grad_norm": 3.598965644836426,
      "learning_rate": 7.784994018116562e-05,
      "loss": 0.843,
      "step": 9881
    },
    {
      "epoch": 0.04222463402753446,
      "grad_norm": 3.330702781677246,
      "learning_rate": 7.780721244231756e-05,
      "loss": 0.9001,
      "step": 9882
    },
    {
      "epoch": 0.04222890691096166,
      "grad_norm": 0.8107332587242126,
      "learning_rate": 7.776448470346948e-05,
      "loss": 0.4766,
      "step": 9883
    },
    {
      "epoch": 0.04223317979438885,
      "grad_norm": 2.0228474140167236,
      "learning_rate": 7.772175696462143e-05,
      "loss": 0.6226,
      "step": 9884
    },
    {
      "epoch": 0.04223745267781604,
      "grad_norm": 0.6447559595108032,
      "learning_rate": 7.767902922577337e-05,
      "loss": 0.2504,
      "step": 9885
    },
    {
      "epoch": 0.04224172556124324,
      "grad_norm": 0.8451716303825378,
      "learning_rate": 7.763630148692531e-05,
      "loss": 0.465,
      "step": 9886
    },
    {
      "epoch": 0.04224599844467043,
      "grad_norm": 2.998070478439331,
      "learning_rate": 7.759357374807725e-05,
      "loss": 0.9533,
      "step": 9887
    },
    {
      "epoch": 0.042250271328097624,
      "grad_norm": 2.1265931129455566,
      "learning_rate": 7.75508460092292e-05,
      "loss": 0.6127,
      "step": 9888
    },
    {
      "epoch": 0.042254544211524823,
      "grad_norm": 3.5826222896575928,
      "learning_rate": 7.750811827038114e-05,
      "loss": 0.8143,
      "step": 9889
    },
    {
      "epoch": 0.042258817094952016,
      "grad_norm": 0.7744492888450623,
      "learning_rate": 7.746539053153307e-05,
      "loss": 0.4524,
      "step": 9890
    },
    {
      "epoch": 0.04226308997837921,
      "grad_norm": 2.9499199390411377,
      "learning_rate": 7.742266279268501e-05,
      "loss": 0.7596,
      "step": 9891
    },
    {
      "epoch": 0.04226736286180641,
      "grad_norm": 0.7532700300216675,
      "learning_rate": 7.737993505383695e-05,
      "loss": 0.4035,
      "step": 9892
    },
    {
      "epoch": 0.0422716357452336,
      "grad_norm": 1.04173743724823,
      "learning_rate": 7.733720731498889e-05,
      "loss": 0.363,
      "step": 9893
    },
    {
      "epoch": 0.04227590862866079,
      "grad_norm": 2.0159547328948975,
      "learning_rate": 7.729447957614083e-05,
      "loss": 0.675,
      "step": 9894
    },
    {
      "epoch": 0.04228018151208799,
      "grad_norm": 0.506337583065033,
      "learning_rate": 7.725175183729278e-05,
      "loss": 0.142,
      "step": 9895
    },
    {
      "epoch": 0.04228445439551518,
      "grad_norm": 0.8053281903266907,
      "learning_rate": 7.720902409844471e-05,
      "loss": 0.4522,
      "step": 9896
    },
    {
      "epoch": 0.042288727278942374,
      "grad_norm": 2.2164039611816406,
      "learning_rate": 7.716629635959665e-05,
      "loss": 1.4047,
      "step": 9897
    },
    {
      "epoch": 0.04229300016236957,
      "grad_norm": 2.1265532970428467,
      "learning_rate": 7.712356862074859e-05,
      "loss": 0.5926,
      "step": 9898
    },
    {
      "epoch": 0.042297273045796766,
      "grad_norm": 3.3862545490264893,
      "learning_rate": 7.708084088190054e-05,
      "loss": 0.8921,
      "step": 9899
    },
    {
      "epoch": 0.04230154592922396,
      "grad_norm": 2.9496829509735107,
      "learning_rate": 7.703811314305247e-05,
      "loss": 0.8316,
      "step": 9900
    },
    {
      "epoch": 0.04230581881265116,
      "grad_norm": 4.980344295501709,
      "learning_rate": 7.69953854042044e-05,
      "loss": 1.1706,
      "step": 9901
    },
    {
      "epoch": 0.04231009169607835,
      "grad_norm": 0.851465106010437,
      "learning_rate": 7.695265766535636e-05,
      "loss": 0.4627,
      "step": 9902
    },
    {
      "epoch": 0.04231436457950554,
      "grad_norm": 2.0005674362182617,
      "learning_rate": 7.69099299265083e-05,
      "loss": 0.6573,
      "step": 9903
    },
    {
      "epoch": 0.04231863746293273,
      "grad_norm": 1.0813168287277222,
      "learning_rate": 7.686720218766023e-05,
      "loss": 0.4584,
      "step": 9904
    },
    {
      "epoch": 0.04232291034635993,
      "grad_norm": 3.3194456100463867,
      "learning_rate": 7.682447444881217e-05,
      "loss": 0.8343,
      "step": 9905
    },
    {
      "epoch": 0.042327183229787124,
      "grad_norm": 0.8405631184577942,
      "learning_rate": 7.678174670996412e-05,
      "loss": 0.4517,
      "step": 9906
    },
    {
      "epoch": 0.042331456113214316,
      "grad_norm": 0.7873633503913879,
      "learning_rate": 7.673901897111606e-05,
      "loss": 0.3909,
      "step": 9907
    },
    {
      "epoch": 0.042335728996641515,
      "grad_norm": 0.7957515120506287,
      "learning_rate": 7.669629123226798e-05,
      "loss": 0.3909,
      "step": 9908
    },
    {
      "epoch": 0.04234000188006871,
      "grad_norm": 2.677612543106079,
      "learning_rate": 7.665356349341992e-05,
      "loss": 0.6474,
      "step": 9909
    },
    {
      "epoch": 0.0423442747634959,
      "grad_norm": 3.223862648010254,
      "learning_rate": 7.661083575457187e-05,
      "loss": 0.665,
      "step": 9910
    },
    {
      "epoch": 0.0423485476469231,
      "grad_norm": 2.934027671813965,
      "learning_rate": 7.656810801572381e-05,
      "loss": 0.8065,
      "step": 9911
    },
    {
      "epoch": 0.04235282053035029,
      "grad_norm": 4.552663803100586,
      "learning_rate": 7.652538027687575e-05,
      "loss": 1.2804,
      "step": 9912
    },
    {
      "epoch": 0.04235709341377748,
      "grad_norm": 1.085172414779663,
      "learning_rate": 7.648265253802769e-05,
      "loss": 0.4433,
      "step": 9913
    },
    {
      "epoch": 0.04236136629720468,
      "grad_norm": 3.451671600341797,
      "learning_rate": 7.643992479917964e-05,
      "loss": 1.2211,
      "step": 9914
    },
    {
      "epoch": 0.042365639180631874,
      "grad_norm": 0.8098343014717102,
      "learning_rate": 7.639719706033157e-05,
      "loss": 0.4088,
      "step": 9915
    },
    {
      "epoch": 0.042369912064059066,
      "grad_norm": 2.910911798477173,
      "learning_rate": 7.63544693214835e-05,
      "loss": 0.7826,
      "step": 9916
    },
    {
      "epoch": 0.042374184947486265,
      "grad_norm": 0.736414909362793,
      "learning_rate": 7.631174158263545e-05,
      "loss": 0.2525,
      "step": 9917
    },
    {
      "epoch": 0.04237845783091346,
      "grad_norm": 0.570011556148529,
      "learning_rate": 7.626901384378739e-05,
      "loss": 0.1862,
      "step": 9918
    },
    {
      "epoch": 0.04238273071434065,
      "grad_norm": 0.8407291173934937,
      "learning_rate": 7.622628610493933e-05,
      "loss": 0.3794,
      "step": 9919
    },
    {
      "epoch": 0.04238700359776785,
      "grad_norm": 4.0336198806762695,
      "learning_rate": 7.618355836609126e-05,
      "loss": 0.8753,
      "step": 9920
    },
    {
      "epoch": 0.04239127648119504,
      "grad_norm": 0.922694742679596,
      "learning_rate": 7.614083062724322e-05,
      "loss": 0.4377,
      "step": 9921
    },
    {
      "epoch": 0.04239554936462223,
      "grad_norm": 0.7534500360488892,
      "learning_rate": 7.609810288839515e-05,
      "loss": 0.2759,
      "step": 9922
    },
    {
      "epoch": 0.04239982224804943,
      "grad_norm": 0.867233395576477,
      "learning_rate": 7.605537514954709e-05,
      "loss": 0.3797,
      "step": 9923
    },
    {
      "epoch": 0.042404095131476624,
      "grad_norm": 3.221099615097046,
      "learning_rate": 7.601264741069902e-05,
      "loss": 0.6379,
      "step": 9924
    },
    {
      "epoch": 0.042408368014903816,
      "grad_norm": 2.0592238903045654,
      "learning_rate": 7.596991967185097e-05,
      "loss": 1.2015,
      "step": 9925
    },
    {
      "epoch": 0.042412640898331015,
      "grad_norm": 1.0480669736862183,
      "learning_rate": 7.59271919330029e-05,
      "loss": 0.4177,
      "step": 9926
    },
    {
      "epoch": 0.04241691378175821,
      "grad_norm": 0.9204859137535095,
      "learning_rate": 7.588446419415484e-05,
      "loss": 0.3514,
      "step": 9927
    },
    {
      "epoch": 0.0424211866651854,
      "grad_norm": 2.454519748687744,
      "learning_rate": 7.58417364553068e-05,
      "loss": 1.5047,
      "step": 9928
    },
    {
      "epoch": 0.04242545954861259,
      "grad_norm": 0.5104964375495911,
      "learning_rate": 7.579900871645873e-05,
      "loss": 0.1353,
      "step": 9929
    },
    {
      "epoch": 0.04242973243203979,
      "grad_norm": 0.8850991129875183,
      "learning_rate": 7.575628097761067e-05,
      "loss": 0.3476,
      "step": 9930
    },
    {
      "epoch": 0.04243400531546698,
      "grad_norm": 0.9603895545005798,
      "learning_rate": 7.571355323876261e-05,
      "loss": 0.3808,
      "step": 9931
    },
    {
      "epoch": 0.042438278198894175,
      "grad_norm": 2.0081913471221924,
      "learning_rate": 7.567082549991456e-05,
      "loss": 1.1922,
      "step": 9932
    },
    {
      "epoch": 0.042442551082321374,
      "grad_norm": 0.8999771475791931,
      "learning_rate": 7.562809776106648e-05,
      "loss": 0.3797,
      "step": 9933
    },
    {
      "epoch": 0.042446823965748566,
      "grad_norm": 2.047870397567749,
      "learning_rate": 7.558537002221842e-05,
      "loss": 0.6681,
      "step": 9934
    },
    {
      "epoch": 0.04245109684917576,
      "grad_norm": 2.2103078365325928,
      "learning_rate": 7.554264228337036e-05,
      "loss": 0.5532,
      "step": 9935
    },
    {
      "epoch": 0.04245536973260296,
      "grad_norm": 0.9608115553855896,
      "learning_rate": 7.549991454452231e-05,
      "loss": 0.3969,
      "step": 9936
    },
    {
      "epoch": 0.04245964261603015,
      "grad_norm": 2.9737675189971924,
      "learning_rate": 7.545718680567425e-05,
      "loss": 0.9293,
      "step": 9937
    },
    {
      "epoch": 0.04246391549945734,
      "grad_norm": 1.9238953590393066,
      "learning_rate": 7.541445906682619e-05,
      "loss": 0.5804,
      "step": 9938
    },
    {
      "epoch": 0.04246818838288454,
      "grad_norm": 1.9890872240066528,
      "learning_rate": 7.537173132797812e-05,
      "loss": 0.5764,
      "step": 9939
    },
    {
      "epoch": 0.04247246126631173,
      "grad_norm": 3.3187222480773926,
      "learning_rate": 7.532900358913007e-05,
      "loss": 0.8034,
      "step": 9940
    },
    {
      "epoch": 0.042476734149738925,
      "grad_norm": 0.9846140146255493,
      "learning_rate": 7.5286275850282e-05,
      "loss": 0.5863,
      "step": 9941
    },
    {
      "epoch": 0.042481007033166124,
      "grad_norm": 0.6117808818817139,
      "learning_rate": 7.524354811143394e-05,
      "loss": 0.2561,
      "step": 9942
    },
    {
      "epoch": 0.042485279916593316,
      "grad_norm": 2.0786828994750977,
      "learning_rate": 7.520082037258589e-05,
      "loss": 0.5678,
      "step": 9943
    },
    {
      "epoch": 0.04248955280002051,
      "grad_norm": 4.760581016540527,
      "learning_rate": 7.515809263373783e-05,
      "loss": 2.6498,
      "step": 9944
    },
    {
      "epoch": 0.04249382568344771,
      "grad_norm": 1.4979896545410156,
      "learning_rate": 7.511536489488976e-05,
      "loss": 0.485,
      "step": 9945
    },
    {
      "epoch": 0.0424980985668749,
      "grad_norm": 1.9997864961624146,
      "learning_rate": 7.50726371560417e-05,
      "loss": 0.6431,
      "step": 9946
    },
    {
      "epoch": 0.04250237145030209,
      "grad_norm": 1.8088736534118652,
      "learning_rate": 7.502990941719365e-05,
      "loss": 0.4693,
      "step": 9947
    },
    {
      "epoch": 0.04250664433372929,
      "grad_norm": 1.1885343790054321,
      "learning_rate": 7.498718167834559e-05,
      "loss": 0.3922,
      "step": 9948
    },
    {
      "epoch": 0.04251091721715648,
      "grad_norm": 2.3384811878204346,
      "learning_rate": 7.494445393949752e-05,
      "loss": 1.4536,
      "step": 9949
    },
    {
      "epoch": 0.042515190100583675,
      "grad_norm": 0.9601842761039734,
      "learning_rate": 7.490172620064945e-05,
      "loss": 0.3148,
      "step": 9950
    },
    {
      "epoch": 0.04251946298401087,
      "grad_norm": 4.812452793121338,
      "learning_rate": 7.48589984618014e-05,
      "loss": 1.0567,
      "step": 9951
    },
    {
      "epoch": 0.042523735867438066,
      "grad_norm": 1.8621692657470703,
      "learning_rate": 7.481627072295334e-05,
      "loss": 0.5576,
      "step": 9952
    },
    {
      "epoch": 0.04252800875086526,
      "grad_norm": 0.9633786082267761,
      "learning_rate": 7.477354298410528e-05,
      "loss": 0.4731,
      "step": 9953
    },
    {
      "epoch": 0.04253228163429245,
      "grad_norm": 0.8514307737350464,
      "learning_rate": 7.473081524525722e-05,
      "loss": 0.3417,
      "step": 9954
    },
    {
      "epoch": 0.04253655451771965,
      "grad_norm": 0.9262189865112305,
      "learning_rate": 7.468808750640917e-05,
      "loss": 0.3988,
      "step": 9955
    },
    {
      "epoch": 0.04254082740114684,
      "grad_norm": 0.9243151545524597,
      "learning_rate": 7.464535976756111e-05,
      "loss": 0.3912,
      "step": 9956
    },
    {
      "epoch": 0.04254510028457403,
      "grad_norm": 1.9005310535430908,
      "learning_rate": 7.460263202871303e-05,
      "loss": 1.1412,
      "step": 9957
    },
    {
      "epoch": 0.04254937316800123,
      "grad_norm": 3.640683174133301,
      "learning_rate": 7.455990428986498e-05,
      "loss": 1.0566,
      "step": 9958
    },
    {
      "epoch": 0.042553646051428425,
      "grad_norm": 2.0299770832061768,
      "learning_rate": 7.451717655101692e-05,
      "loss": 0.534,
      "step": 9959
    },
    {
      "epoch": 0.04255791893485562,
      "grad_norm": 0.9819269180297852,
      "learning_rate": 7.447444881216886e-05,
      "loss": 0.3516,
      "step": 9960
    },
    {
      "epoch": 0.042562191818282816,
      "grad_norm": 0.6823054552078247,
      "learning_rate": 7.44317210733208e-05,
      "loss": 0.2638,
      "step": 9961
    },
    {
      "epoch": 0.04256646470171001,
      "grad_norm": 2.186203956604004,
      "learning_rate": 7.438899333447275e-05,
      "loss": 0.5299,
      "step": 9962
    },
    {
      "epoch": 0.0425707375851372,
      "grad_norm": 3.5592241287231445,
      "learning_rate": 7.434626559562469e-05,
      "loss": 1.9751,
      "step": 9963
    },
    {
      "epoch": 0.0425750104685644,
      "grad_norm": 0.9069828391075134,
      "learning_rate": 7.430353785677662e-05,
      "loss": 0.3502,
      "step": 9964
    },
    {
      "epoch": 0.04257928335199159,
      "grad_norm": 1.8812763690948486,
      "learning_rate": 7.426081011792855e-05,
      "loss": 1.1002,
      "step": 9965
    },
    {
      "epoch": 0.04258355623541878,
      "grad_norm": 0.8769376873970032,
      "learning_rate": 7.42180823790805e-05,
      "loss": 0.3181,
      "step": 9966
    },
    {
      "epoch": 0.04258782911884598,
      "grad_norm": 0.9744240045547485,
      "learning_rate": 7.417535464023244e-05,
      "loss": 0.6002,
      "step": 9967
    },
    {
      "epoch": 0.042592102002273174,
      "grad_norm": 2.5353152751922607,
      "learning_rate": 7.413262690138437e-05,
      "loss": 0.5968,
      "step": 9968
    },
    {
      "epoch": 0.04259637488570037,
      "grad_norm": 0.7817639708518982,
      "learning_rate": 7.408989916253633e-05,
      "loss": 0.3442,
      "step": 9969
    },
    {
      "epoch": 0.042600647769127566,
      "grad_norm": 1.8699688911437988,
      "learning_rate": 7.404717142368826e-05,
      "loss": 0.4745,
      "step": 9970
    },
    {
      "epoch": 0.04260492065255476,
      "grad_norm": 1.1472946405410767,
      "learning_rate": 7.40044436848402e-05,
      "loss": 0.4039,
      "step": 9971
    },
    {
      "epoch": 0.04260919353598195,
      "grad_norm": 1.1926249265670776,
      "learning_rate": 7.396171594599214e-05,
      "loss": 0.4036,
      "step": 9972
    },
    {
      "epoch": 0.04261346641940915,
      "grad_norm": 0.6252454519271851,
      "learning_rate": 7.391898820714409e-05,
      "loss": 0.1942,
      "step": 9973
    },
    {
      "epoch": 0.04261773930283634,
      "grad_norm": 4.338332176208496,
      "learning_rate": 7.387626046829602e-05,
      "loss": 1.3589,
      "step": 9974
    },
    {
      "epoch": 0.04262201218626353,
      "grad_norm": 3.5216264724731445,
      "learning_rate": 7.383353272944795e-05,
      "loss": 1.9752,
      "step": 9975
    },
    {
      "epoch": 0.042626285069690725,
      "grad_norm": 1.0776225328445435,
      "learning_rate": 7.379080499059989e-05,
      "loss": 0.3631,
      "step": 9976
    },
    {
      "epoch": 0.042630557953117924,
      "grad_norm": 4.597257614135742,
      "learning_rate": 7.374807725175184e-05,
      "loss": 0.9602,
      "step": 9977
    },
    {
      "epoch": 0.042634830836545116,
      "grad_norm": 4.603604793548584,
      "learning_rate": 7.370534951290378e-05,
      "loss": 0.9694,
      "step": 9978
    },
    {
      "epoch": 0.04263910371997231,
      "grad_norm": 1.8104097843170166,
      "learning_rate": 7.366262177405572e-05,
      "loss": 1.1022,
      "step": 9979
    },
    {
      "epoch": 0.04264337660339951,
      "grad_norm": 4.031210899353027,
      "learning_rate": 7.361989403520766e-05,
      "loss": 1.0431,
      "step": 9980
    },
    {
      "epoch": 0.0426476494868267,
      "grad_norm": 0.5924955010414124,
      "learning_rate": 7.357716629635961e-05,
      "loss": 0.2286,
      "step": 9981
    },
    {
      "epoch": 0.04265192237025389,
      "grad_norm": 1.9494882822036743,
      "learning_rate": 7.353443855751153e-05,
      "loss": 0.6118,
      "step": 9982
    },
    {
      "epoch": 0.04265619525368109,
      "grad_norm": 0.8584179282188416,
      "learning_rate": 7.349171081866347e-05,
      "loss": 0.3502,
      "step": 9983
    },
    {
      "epoch": 0.04266046813710828,
      "grad_norm": 0.6003499031066895,
      "learning_rate": 7.344898307981542e-05,
      "loss": 0.2755,
      "step": 9984
    },
    {
      "epoch": 0.042664741020535475,
      "grad_norm": 0.6011273264884949,
      "learning_rate": 7.340625534096736e-05,
      "loss": 0.2755,
      "step": 9985
    },
    {
      "epoch": 0.042669013903962674,
      "grad_norm": 3.3519411087036133,
      "learning_rate": 7.33635276021193e-05,
      "loss": 1.1822,
      "step": 9986
    },
    {
      "epoch": 0.042673286787389866,
      "grad_norm": 0.8417143821716309,
      "learning_rate": 7.332079986327123e-05,
      "loss": 0.3436,
      "step": 9987
    },
    {
      "epoch": 0.04267755967081706,
      "grad_norm": 1.7591888904571533,
      "learning_rate": 7.327807212442319e-05,
      "loss": 1.1023,
      "step": 9988
    },
    {
      "epoch": 0.04268183255424426,
      "grad_norm": 1.7905945777893066,
      "learning_rate": 7.323534438557512e-05,
      "loss": 0.6004,
      "step": 9989
    },
    {
      "epoch": 0.04268610543767145,
      "grad_norm": 0.9244017601013184,
      "learning_rate": 7.319261664672705e-05,
      "loss": 0.3146,
      "step": 9990
    },
    {
      "epoch": 0.04269037832109864,
      "grad_norm": 3.458385467529297,
      "learning_rate": 7.314988890787899e-05,
      "loss": 1.9759,
      "step": 9991
    },
    {
      "epoch": 0.04269465120452584,
      "grad_norm": 1.070515751838684,
      "learning_rate": 7.310716116903094e-05,
      "loss": 0.5134,
      "step": 9992
    },
    {
      "epoch": 0.04269892408795303,
      "grad_norm": 1.763373851776123,
      "learning_rate": 7.306443343018288e-05,
      "loss": 0.5863,
      "step": 9993
    },
    {
      "epoch": 0.042703196971380225,
      "grad_norm": 2.2162070274353027,
      "learning_rate": 7.302170569133481e-05,
      "loss": 0.5343,
      "step": 9994
    },
    {
      "epoch": 0.042707469854807424,
      "grad_norm": 4.626712322235107,
      "learning_rate": 7.297897795248676e-05,
      "loss": 1.3402,
      "step": 9995
    },
    {
      "epoch": 0.042711742738234616,
      "grad_norm": 0.8439010977745056,
      "learning_rate": 7.29362502136387e-05,
      "loss": 0.3184,
      "step": 9996
    },
    {
      "epoch": 0.04271601562166181,
      "grad_norm": 1.9242289066314697,
      "learning_rate": 7.289352247479064e-05,
      "loss": 0.5934,
      "step": 9997
    },
    {
      "epoch": 0.04272028850508901,
      "grad_norm": 3.270648717880249,
      "learning_rate": 7.285079473594256e-05,
      "loss": 1.0546,
      "step": 9998
    },
    {
      "epoch": 0.0427245613885162,
      "grad_norm": 3.3166444301605225,
      "learning_rate": 7.280806699709452e-05,
      "loss": 1.1306,
      "step": 9999
    },
    {
      "epoch": 0.04272883427194339,
      "grad_norm": 3.380807638168335,
      "learning_rate": 7.276533925824645e-05,
      "loss": 1.9057,
      "step": 10000
    },
    {
      "epoch": 0.042733107155370584,
      "grad_norm": 1.9147579669952393,
      "learning_rate": 7.272261151939839e-05,
      "loss": 0.5705,
      "step": 10001
    },
    {
      "epoch": 0.04273738003879778,
      "grad_norm": 1.8943594694137573,
      "learning_rate": 7.267988378055033e-05,
      "loss": 0.5567,
      "step": 10002
    },
    {
      "epoch": 0.042741652922224975,
      "grad_norm": 1.0056568384170532,
      "learning_rate": 7.263715604170228e-05,
      "loss": 0.3414,
      "step": 10003
    },
    {
      "epoch": 0.04274592580565217,
      "grad_norm": 3.614243745803833,
      "learning_rate": 7.259442830285422e-05,
      "loss": 1.0787,
      "step": 10004
    },
    {
      "epoch": 0.042750198689079366,
      "grad_norm": 1.8917310237884521,
      "learning_rate": 7.255170056400616e-05,
      "loss": 0.5437,
      "step": 10005
    },
    {
      "epoch": 0.04275447157250656,
      "grad_norm": 1.0852391719818115,
      "learning_rate": 7.25089728251581e-05,
      "loss": 0.3752,
      "step": 10006
    },
    {
      "epoch": 0.04275874445593375,
      "grad_norm": 2.014240264892578,
      "learning_rate": 7.246624508631003e-05,
      "loss": 0.5538,
      "step": 10007
    },
    {
      "epoch": 0.04276301733936095,
      "grad_norm": 0.6140490770339966,
      "learning_rate": 7.242351734746197e-05,
      "loss": 0.2419,
      "step": 10008
    },
    {
      "epoch": 0.04276729022278814,
      "grad_norm": 0.6863877177238464,
      "learning_rate": 7.238078960861391e-05,
      "loss": 0.2638,
      "step": 10009
    },
    {
      "epoch": 0.042771563106215334,
      "grad_norm": 0.6496423482894897,
      "learning_rate": 7.233806186976586e-05,
      "loss": 0.2269,
      "step": 10010
    },
    {
      "epoch": 0.04277583598964253,
      "grad_norm": 3.324347496032715,
      "learning_rate": 7.22953341309178e-05,
      "loss": 1.0872,
      "step": 10011
    },
    {
      "epoch": 0.042780108873069725,
      "grad_norm": 3.749128580093384,
      "learning_rate": 7.225260639206973e-05,
      "loss": 0.8521,
      "step": 10012
    },
    {
      "epoch": 0.04278438175649692,
      "grad_norm": 1.7925654649734497,
      "learning_rate": 7.220987865322167e-05,
      "loss": 0.4589,
      "step": 10013
    },
    {
      "epoch": 0.042788654639924116,
      "grad_norm": 2.6815571784973145,
      "learning_rate": 7.216715091437362e-05,
      "loss": 0.9604,
      "step": 10014
    },
    {
      "epoch": 0.04279292752335131,
      "grad_norm": 4.567227840423584,
      "learning_rate": 7.212442317552555e-05,
      "loss": 2.4172,
      "step": 10015
    },
    {
      "epoch": 0.0427972004067785,
      "grad_norm": 0.8696200251579285,
      "learning_rate": 7.208169543667749e-05,
      "loss": 0.3887,
      "step": 10016
    },
    {
      "epoch": 0.0428014732902057,
      "grad_norm": 3.21762752532959,
      "learning_rate": 7.203896769782942e-05,
      "loss": 0.7386,
      "step": 10017
    },
    {
      "epoch": 0.04280574617363289,
      "grad_norm": 0.8677918910980225,
      "learning_rate": 7.199623995898138e-05,
      "loss": 0.3548,
      "step": 10018
    },
    {
      "epoch": 0.042810019057060084,
      "grad_norm": 0.6312841176986694,
      "learning_rate": 7.195351222013331e-05,
      "loss": 0.2531,
      "step": 10019
    },
    {
      "epoch": 0.04281429194048728,
      "grad_norm": 0.6000009179115295,
      "learning_rate": 7.191078448128525e-05,
      "loss": 0.2286,
      "step": 10020
    },
    {
      "epoch": 0.042818564823914475,
      "grad_norm": 0.5834975838661194,
      "learning_rate": 7.18680567424372e-05,
      "loss": 0.2163,
      "step": 10021
    },
    {
      "epoch": 0.04282283770734167,
      "grad_norm": 2.877833366394043,
      "learning_rate": 7.182532900358914e-05,
      "loss": 0.7235,
      "step": 10022
    },
    {
      "epoch": 0.042827110590768866,
      "grad_norm": 2.7991158962249756,
      "learning_rate": 7.178260126474106e-05,
      "loss": 1.0102,
      "step": 10023
    },
    {
      "epoch": 0.04283138347419606,
      "grad_norm": 1.4303830862045288,
      "learning_rate": 7.1739873525893e-05,
      "loss": 0.5656,
      "step": 10024
    },
    {
      "epoch": 0.04283565635762325,
      "grad_norm": 2.9498653411865234,
      "learning_rate": 7.169714578704495e-05,
      "loss": 0.7747,
      "step": 10025
    },
    {
      "epoch": 0.04283992924105044,
      "grad_norm": 3.2115988731384277,
      "learning_rate": 7.165441804819689e-05,
      "loss": 0.7105,
      "step": 10026
    },
    {
      "epoch": 0.04284420212447764,
      "grad_norm": 1.6235778331756592,
      "learning_rate": 7.161169030934883e-05,
      "loss": 1.0561,
      "step": 10027
    },
    {
      "epoch": 0.042848475007904833,
      "grad_norm": 1.0084329843521118,
      "learning_rate": 7.156896257050077e-05,
      "loss": 0.526,
      "step": 10028
    },
    {
      "epoch": 0.042852747891332026,
      "grad_norm": 2.1762545108795166,
      "learning_rate": 7.152623483165272e-05,
      "loss": 0.4785,
      "step": 10029
    },
    {
      "epoch": 0.042857020774759225,
      "grad_norm": 0.58885258436203,
      "learning_rate": 7.148350709280466e-05,
      "loss": 0.1861,
      "step": 10030
    },
    {
      "epoch": 0.04286129365818642,
      "grad_norm": 3.986585855484009,
      "learning_rate": 7.144077935395658e-05,
      "loss": 2.6186,
      "step": 10031
    },
    {
      "epoch": 0.04286556654161361,
      "grad_norm": 0.97404944896698,
      "learning_rate": 7.139805161510852e-05,
      "loss": 0.3631,
      "step": 10032
    },
    {
      "epoch": 0.04286983942504081,
      "grad_norm": 0.9442275166511536,
      "learning_rate": 7.135532387626047e-05,
      "loss": 0.6621,
      "step": 10033
    },
    {
      "epoch": 0.042874112308468,
      "grad_norm": 0.8749528527259827,
      "learning_rate": 7.131259613741241e-05,
      "loss": 0.4085,
      "step": 10034
    },
    {
      "epoch": 0.04287838519189519,
      "grad_norm": 3.656346082687378,
      "learning_rate": 7.126986839856435e-05,
      "loss": 1.0184,
      "step": 10035
    },
    {
      "epoch": 0.04288265807532239,
      "grad_norm": 0.992371678352356,
      "learning_rate": 7.12271406597163e-05,
      "loss": 0.4164,
      "step": 10036
    },
    {
      "epoch": 0.04288693095874958,
      "grad_norm": 3.6840715408325195,
      "learning_rate": 7.118441292086824e-05,
      "loss": 1.2916,
      "step": 10037
    },
    {
      "epoch": 0.042891203842176776,
      "grad_norm": 1.8210022449493408,
      "learning_rate": 7.114168518202017e-05,
      "loss": 0.4548,
      "step": 10038
    },
    {
      "epoch": 0.042895476725603975,
      "grad_norm": 3.29742169380188,
      "learning_rate": 7.10989574431721e-05,
      "loss": 0.9883,
      "step": 10039
    },
    {
      "epoch": 0.04289974960903117,
      "grad_norm": 0.9895476698875427,
      "learning_rate": 7.105622970432405e-05,
      "loss": 0.4291,
      "step": 10040
    },
    {
      "epoch": 0.04290402249245836,
      "grad_norm": 0.8610032796859741,
      "learning_rate": 7.101350196547599e-05,
      "loss": 0.4291,
      "step": 10041
    },
    {
      "epoch": 0.04290829537588556,
      "grad_norm": 0.9108633995056152,
      "learning_rate": 7.097077422662792e-05,
      "loss": 0.6175,
      "step": 10042
    },
    {
      "epoch": 0.04291256825931275,
      "grad_norm": 0.9713570475578308,
      "learning_rate": 7.092804648777986e-05,
      "loss": 0.3754,
      "step": 10043
    },
    {
      "epoch": 0.04291684114273994,
      "grad_norm": 1.8935644626617432,
      "learning_rate": 7.088531874893181e-05,
      "loss": 0.4883,
      "step": 10044
    },
    {
      "epoch": 0.04292111402616714,
      "grad_norm": 0.5060122013092041,
      "learning_rate": 7.084259101008375e-05,
      "loss": 0.1603,
      "step": 10045
    },
    {
      "epoch": 0.04292538690959433,
      "grad_norm": 1.8160239458084106,
      "learning_rate": 7.079986327123569e-05,
      "loss": 0.4411,
      "step": 10046
    },
    {
      "epoch": 0.042929659793021525,
      "grad_norm": 1.9720076322555542,
      "learning_rate": 7.075713553238764e-05,
      "loss": 0.6077,
      "step": 10047
    },
    {
      "epoch": 0.042933932676448724,
      "grad_norm": 4.330641746520996,
      "learning_rate": 7.071440779353956e-05,
      "loss": 1.0633,
      "step": 10048
    },
    {
      "epoch": 0.04293820555987592,
      "grad_norm": 4.721652030944824,
      "learning_rate": 7.06716800546915e-05,
      "loss": 1.2836,
      "step": 10049
    },
    {
      "epoch": 0.04294247844330311,
      "grad_norm": 2.7685980796813965,
      "learning_rate": 7.062895231584344e-05,
      "loss": 0.9526,
      "step": 10050
    },
    {
      "epoch": 0.0429467513267303,
      "grad_norm": 3.259653091430664,
      "learning_rate": 7.058622457699539e-05,
      "loss": 1.8675,
      "step": 10051
    },
    {
      "epoch": 0.0429510242101575,
      "grad_norm": 2.1985666751861572,
      "learning_rate": 7.054349683814733e-05,
      "loss": 0.4623,
      "step": 10052
    },
    {
      "epoch": 0.04295529709358469,
      "grad_norm": 0.5615098476409912,
      "learning_rate": 7.050076909929927e-05,
      "loss": 0.1758,
      "step": 10053
    },
    {
      "epoch": 0.042959569977011884,
      "grad_norm": 2.0147745609283447,
      "learning_rate": 7.04580413604512e-05,
      "loss": 0.5349,
      "step": 10054
    },
    {
      "epoch": 0.04296384286043908,
      "grad_norm": 0.850583553314209,
      "learning_rate": 7.041531362160316e-05,
      "loss": 0.4292,
      "step": 10055
    },
    {
      "epoch": 0.042968115743866275,
      "grad_norm": 0.9505091309547424,
      "learning_rate": 7.037258588275508e-05,
      "loss": 0.451,
      "step": 10056
    },
    {
      "epoch": 0.04297238862729347,
      "grad_norm": 2.760554313659668,
      "learning_rate": 7.032985814390702e-05,
      "loss": 0.7347,
      "step": 10057
    },
    {
      "epoch": 0.042976661510720666,
      "grad_norm": 0.6799666285514832,
      "learning_rate": 7.028713040505896e-05,
      "loss": 0.2637,
      "step": 10058
    },
    {
      "epoch": 0.04298093439414786,
      "grad_norm": 2.8974533081054688,
      "learning_rate": 7.024440266621091e-05,
      "loss": 0.964,
      "step": 10059
    },
    {
      "epoch": 0.04298520727757505,
      "grad_norm": 2.5288610458374023,
      "learning_rate": 7.020167492736285e-05,
      "loss": 0.5937,
      "step": 10060
    },
    {
      "epoch": 0.04298948016100225,
      "grad_norm": 0.8057008385658264,
      "learning_rate": 7.015894718851478e-05,
      "loss": 0.3052,
      "step": 10061
    },
    {
      "epoch": 0.04299375304442944,
      "grad_norm": 0.8966989517211914,
      "learning_rate": 7.011621944966674e-05,
      "loss": 0.3385,
      "step": 10062
    },
    {
      "epoch": 0.042998025927856634,
      "grad_norm": 3.5992040634155273,
      "learning_rate": 7.007349171081867e-05,
      "loss": 0.9483,
      "step": 10063
    },
    {
      "epoch": 0.04300229881128383,
      "grad_norm": 1.4950377941131592,
      "learning_rate": 7.00307639719706e-05,
      "loss": 0.3837,
      "step": 10064
    },
    {
      "epoch": 0.043006571694711025,
      "grad_norm": 0.8098012804985046,
      "learning_rate": 6.998803623312254e-05,
      "loss": 0.3885,
      "step": 10065
    },
    {
      "epoch": 0.04301084457813822,
      "grad_norm": 0.40865790843963623,
      "learning_rate": 6.994530849427449e-05,
      "loss": 0.1213,
      "step": 10066
    },
    {
      "epoch": 0.043015117461565416,
      "grad_norm": 1.8817559480667114,
      "learning_rate": 6.990258075542642e-05,
      "loss": 0.4958,
      "step": 10067
    },
    {
      "epoch": 0.04301939034499261,
      "grad_norm": 0.8158745765686035,
      "learning_rate": 6.985985301657836e-05,
      "loss": 0.2927,
      "step": 10068
    },
    {
      "epoch": 0.0430236632284198,
      "grad_norm": 1.1004281044006348,
      "learning_rate": 6.98171252777303e-05,
      "loss": 0.508,
      "step": 10069
    },
    {
      "epoch": 0.043027936111847,
      "grad_norm": 0.9400720000267029,
      "learning_rate": 6.977439753888225e-05,
      "loss": 0.4036,
      "step": 10070
    },
    {
      "epoch": 0.04303220899527419,
      "grad_norm": 0.6109552383422852,
      "learning_rate": 6.973166980003419e-05,
      "loss": 0.1943,
      "step": 10071
    },
    {
      "epoch": 0.043036481878701384,
      "grad_norm": 0.9026423096656799,
      "learning_rate": 6.968894206118611e-05,
      "loss": 0.5981,
      "step": 10072
    },
    {
      "epoch": 0.04304075476212858,
      "grad_norm": 0.8099992275238037,
      "learning_rate": 6.964621432233805e-05,
      "loss": 0.3695,
      "step": 10073
    },
    {
      "epoch": 0.043045027645555775,
      "grad_norm": 1.9562900066375732,
      "learning_rate": 6.960348658349e-05,
      "loss": 0.6284,
      "step": 10074
    },
    {
      "epoch": 0.04304930052898297,
      "grad_norm": 4.364404678344727,
      "learning_rate": 6.956075884464194e-05,
      "loss": 1.3091,
      "step": 10075
    },
    {
      "epoch": 0.04305357341241016,
      "grad_norm": 3.1268110275268555,
      "learning_rate": 6.951803110579388e-05,
      "loss": 1.3003,
      "step": 10076
    },
    {
      "epoch": 0.04305784629583736,
      "grad_norm": 2.9995267391204834,
      "learning_rate": 6.947530336694583e-05,
      "loss": 0.7654,
      "step": 10077
    },
    {
      "epoch": 0.04306211917926455,
      "grad_norm": 4.618067741394043,
      "learning_rate": 6.943257562809777e-05,
      "loss": 1.2929,
      "step": 10078
    },
    {
      "epoch": 0.04306639206269174,
      "grad_norm": 1.0011241436004639,
      "learning_rate": 6.93898478892497e-05,
      "loss": 0.432,
      "step": 10079
    },
    {
      "epoch": 0.04307066494611894,
      "grad_norm": 2.535693645477295,
      "learning_rate": 6.934712015040164e-05,
      "loss": 0.7363,
      "step": 10080
    },
    {
      "epoch": 0.043074937829546134,
      "grad_norm": 0.5441197156906128,
      "learning_rate": 6.930439241155358e-05,
      "loss": 0.1662,
      "step": 10081
    },
    {
      "epoch": 0.043079210712973326,
      "grad_norm": 4.361984729766846,
      "learning_rate": 6.926166467270552e-05,
      "loss": 1.0271,
      "step": 10082
    },
    {
      "epoch": 0.043083483596400525,
      "grad_norm": 3.1244256496429443,
      "learning_rate": 6.921893693385746e-05,
      "loss": 1.2723,
      "step": 10083
    },
    {
      "epoch": 0.04308775647982772,
      "grad_norm": 0.7775571942329407,
      "learning_rate": 6.91762091950094e-05,
      "loss": 0.3032,
      "step": 10084
    },
    {
      "epoch": 0.04309202936325491,
      "grad_norm": 3.6015641689300537,
      "learning_rate": 6.913348145616135e-05,
      "loss": 0.8889,
      "step": 10085
    },
    {
      "epoch": 0.04309630224668211,
      "grad_norm": 0.7707257866859436,
      "learning_rate": 6.909075371731328e-05,
      "loss": 0.289,
      "step": 10086
    },
    {
      "epoch": 0.0431005751301093,
      "grad_norm": 0.7982280850410461,
      "learning_rate": 6.904802597846522e-05,
      "loss": 0.303,
      "step": 10087
    },
    {
      "epoch": 0.04310484801353649,
      "grad_norm": 0.9023132920265198,
      "learning_rate": 6.900529823961717e-05,
      "loss": 0.3691,
      "step": 10088
    },
    {
      "epoch": 0.04310912089696369,
      "grad_norm": 4.71481990814209,
      "learning_rate": 6.89625705007691e-05,
      "loss": 1.2061,
      "step": 10089
    },
    {
      "epoch": 0.043113393780390884,
      "grad_norm": 1.1125288009643555,
      "learning_rate": 6.891984276192104e-05,
      "loss": 0.4509,
      "step": 10090
    },
    {
      "epoch": 0.043117666663818076,
      "grad_norm": 0.9130474328994751,
      "learning_rate": 6.887711502307297e-05,
      "loss": 0.3692,
      "step": 10091
    },
    {
      "epoch": 0.043121939547245275,
      "grad_norm": 3.3334386348724365,
      "learning_rate": 6.883438728422492e-05,
      "loss": 0.938,
      "step": 10092
    },
    {
      "epoch": 0.04312621243067247,
      "grad_norm": 1.6387567520141602,
      "learning_rate": 6.879165954537686e-05,
      "loss": 0.732,
      "step": 10093
    },
    {
      "epoch": 0.04313048531409966,
      "grad_norm": 1.9175838232040405,
      "learning_rate": 6.87489318065288e-05,
      "loss": 0.4704,
      "step": 10094
    },
    {
      "epoch": 0.04313475819752686,
      "grad_norm": 2.9251868724823,
      "learning_rate": 6.870620406768074e-05,
      "loss": 0.9393,
      "step": 10095
    },
    {
      "epoch": 0.04313903108095405,
      "grad_norm": 1.162837266921997,
      "learning_rate": 6.866347632883269e-05,
      "loss": 0.5289,
      "step": 10096
    },
    {
      "epoch": 0.04314330396438124,
      "grad_norm": 1.9620558023452759,
      "learning_rate": 6.862074858998461e-05,
      "loss": 0.616,
      "step": 10097
    },
    {
      "epoch": 0.04314757684780844,
      "grad_norm": 3.33080792427063,
      "learning_rate": 6.857802085113655e-05,
      "loss": 1.8552,
      "step": 10098
    },
    {
      "epoch": 0.043151849731235634,
      "grad_norm": 0.5816738605499268,
      "learning_rate": 6.853529311228849e-05,
      "loss": 0.2374,
      "step": 10099
    },
    {
      "epoch": 0.043156122614662826,
      "grad_norm": 0.9622981548309326,
      "learning_rate": 6.849256537344044e-05,
      "loss": 0.6495,
      "step": 10100
    },
    {
      "epoch": 0.04316039549809002,
      "grad_norm": 0.6432183384895325,
      "learning_rate": 6.844983763459238e-05,
      "loss": 0.3039,
      "step": 10101
    },
    {
      "epoch": 0.04316466838151722,
      "grad_norm": 0.7684962153434753,
      "learning_rate": 6.840710989574432e-05,
      "loss": 0.3259,
      "step": 10102
    },
    {
      "epoch": 0.04316894126494441,
      "grad_norm": 1.515555500984192,
      "learning_rate": 6.836438215689627e-05,
      "loss": 0.3698,
      "step": 10103
    },
    {
      "epoch": 0.0431732141483716,
      "grad_norm": 1.9383074045181274,
      "learning_rate": 6.83216544180482e-05,
      "loss": 0.5829,
      "step": 10104
    },
    {
      "epoch": 0.0431774870317988,
      "grad_norm": 1.0899702310562134,
      "learning_rate": 6.827892667920013e-05,
      "loss": 0.3631,
      "step": 10105
    },
    {
      "epoch": 0.04318175991522599,
      "grad_norm": 1.8274627923965454,
      "learning_rate": 6.823619894035207e-05,
      "loss": 0.438,
      "step": 10106
    },
    {
      "epoch": 0.043186032798653184,
      "grad_norm": 3.6141695976257324,
      "learning_rate": 6.819347120150402e-05,
      "loss": 0.8577,
      "step": 10107
    },
    {
      "epoch": 0.043190305682080384,
      "grad_norm": 0.9215772747993469,
      "learning_rate": 6.815074346265596e-05,
      "loss": 0.3686,
      "step": 10108
    },
    {
      "epoch": 0.043194578565507576,
      "grad_norm": 1.1062452793121338,
      "learning_rate": 6.81080157238079e-05,
      "loss": 0.3885,
      "step": 10109
    },
    {
      "epoch": 0.04319885144893477,
      "grad_norm": 0.9460381269454956,
      "learning_rate": 6.806528798495983e-05,
      "loss": 0.6185,
      "step": 10110
    },
    {
      "epoch": 0.04320312433236197,
      "grad_norm": 3.4089267253875732,
      "learning_rate": 6.802256024611178e-05,
      "loss": 1.067,
      "step": 10111
    },
    {
      "epoch": 0.04320739721578916,
      "grad_norm": 0.47217419743537903,
      "learning_rate": 6.797983250726372e-05,
      "loss": 0.142,
      "step": 10112
    },
    {
      "epoch": 0.04321167009921635,
      "grad_norm": 1.69376540184021,
      "learning_rate": 6.793710476841565e-05,
      "loss": 1.0564,
      "step": 10113
    },
    {
      "epoch": 0.04321594298264355,
      "grad_norm": 0.9203577041625977,
      "learning_rate": 6.78943770295676e-05,
      "loss": 0.3326,
      "step": 10114
    },
    {
      "epoch": 0.04322021586607074,
      "grad_norm": 1.903613805770874,
      "learning_rate": 6.785164929071954e-05,
      "loss": 0.5458,
      "step": 10115
    },
    {
      "epoch": 0.043224488749497934,
      "grad_norm": 2.0864107608795166,
      "learning_rate": 6.780892155187147e-05,
      "loss": 0.5822,
      "step": 10116
    },
    {
      "epoch": 0.04322876163292513,
      "grad_norm": 0.6874157786369324,
      "learning_rate": 6.776619381302341e-05,
      "loss": 0.2633,
      "step": 10117
    },
    {
      "epoch": 0.043233034516352326,
      "grad_norm": 0.9141969680786133,
      "learning_rate": 6.772346607417536e-05,
      "loss": 0.3685,
      "step": 10118
    },
    {
      "epoch": 0.04323730739977952,
      "grad_norm": 3.73498797416687,
      "learning_rate": 6.76807383353273e-05,
      "loss": 1.0642,
      "step": 10119
    },
    {
      "epoch": 0.04324158028320672,
      "grad_norm": 0.9560322165489197,
      "learning_rate": 6.763801059647924e-05,
      "loss": 0.315,
      "step": 10120
    },
    {
      "epoch": 0.04324585316663391,
      "grad_norm": 1.51487135887146,
      "learning_rate": 6.759528285763118e-05,
      "loss": 0.3429,
      "step": 10121
    },
    {
      "epoch": 0.0432501260500611,
      "grad_norm": 2.2470510005950928,
      "learning_rate": 6.755255511878311e-05,
      "loss": 0.5605,
      "step": 10122
    },
    {
      "epoch": 0.0432543989334883,
      "grad_norm": 3.4610447883605957,
      "learning_rate": 6.750982737993505e-05,
      "loss": 0.8422,
      "step": 10123
    },
    {
      "epoch": 0.04325867181691549,
      "grad_norm": 2.1617612838745117,
      "learning_rate": 6.746709964108699e-05,
      "loss": 1.417,
      "step": 10124
    },
    {
      "epoch": 0.043262944700342684,
      "grad_norm": 3.5507757663726807,
      "learning_rate": 6.742437190223893e-05,
      "loss": 0.8776,
      "step": 10125
    },
    {
      "epoch": 0.043267217583769876,
      "grad_norm": 1.8731898069381714,
      "learning_rate": 6.738164416339088e-05,
      "loss": 0.515,
      "step": 10126
    },
    {
      "epoch": 0.043271490467197075,
      "grad_norm": 0.9382097125053406,
      "learning_rate": 6.733891642454282e-05,
      "loss": 0.6185,
      "step": 10127
    },
    {
      "epoch": 0.04327576335062427,
      "grad_norm": 1.9577250480651855,
      "learning_rate": 6.729618868569475e-05,
      "loss": 0.5402,
      "step": 10128
    },
    {
      "epoch": 0.04328003623405146,
      "grad_norm": 0.9862469434738159,
      "learning_rate": 6.72534609468467e-05,
      "loss": 0.451,
      "step": 10129
    },
    {
      "epoch": 0.04328430911747866,
      "grad_norm": 1.2203521728515625,
      "learning_rate": 6.721073320799863e-05,
      "loss": 0.4601,
      "step": 10130
    },
    {
      "epoch": 0.04328858200090585,
      "grad_norm": 1.9097816944122314,
      "learning_rate": 6.716800546915057e-05,
      "loss": 0.4431,
      "step": 10131
    },
    {
      "epoch": 0.04329285488433304,
      "grad_norm": 0.7478472590446472,
      "learning_rate": 6.71252777303025e-05,
      "loss": 0.3032,
      "step": 10132
    },
    {
      "epoch": 0.04329712776776024,
      "grad_norm": 0.9001737833023071,
      "learning_rate": 6.708254999145446e-05,
      "loss": 0.3684,
      "step": 10133
    },
    {
      "epoch": 0.043301400651187434,
      "grad_norm": 5.1651530265808105,
      "learning_rate": 6.70398222526064e-05,
      "loss": 1.2932,
      "step": 10134
    },
    {
      "epoch": 0.043305673534614626,
      "grad_norm": 0.7101117372512817,
      "learning_rate": 6.699709451375833e-05,
      "loss": 0.3089,
      "step": 10135
    },
    {
      "epoch": 0.043309946418041825,
      "grad_norm": 1.9108240604400635,
      "learning_rate": 6.695436677491027e-05,
      "loss": 0.5151,
      "step": 10136
    },
    {
      "epoch": 0.04331421930146902,
      "grad_norm": 0.4222678244113922,
      "learning_rate": 6.691163903606222e-05,
      "loss": 0.1631,
      "step": 10137
    },
    {
      "epoch": 0.04331849218489621,
      "grad_norm": 2.1146106719970703,
      "learning_rate": 6.686891129721415e-05,
      "loss": 1.3685,
      "step": 10138
    },
    {
      "epoch": 0.04332276506832341,
      "grad_norm": 0.7088205218315125,
      "learning_rate": 6.682618355836608e-05,
      "loss": 0.3089,
      "step": 10139
    },
    {
      "epoch": 0.0433270379517506,
      "grad_norm": 0.8611395359039307,
      "learning_rate": 6.678345581951804e-05,
      "loss": 0.5331,
      "step": 10140
    },
    {
      "epoch": 0.04333131083517779,
      "grad_norm": 3.268847703933716,
      "learning_rate": 6.674072808066997e-05,
      "loss": 0.7017,
      "step": 10141
    },
    {
      "epoch": 0.04333558371860499,
      "grad_norm": 1.6782400608062744,
      "learning_rate": 6.669800034182191e-05,
      "loss": 1.056,
      "step": 10142
    },
    {
      "epoch": 0.043339856602032184,
      "grad_norm": 1.0853006839752197,
      "learning_rate": 6.665527260297385e-05,
      "loss": 0.3925,
      "step": 10143
    },
    {
      "epoch": 0.043344129485459376,
      "grad_norm": 0.8152319192886353,
      "learning_rate": 6.66125448641258e-05,
      "loss": 0.305,
      "step": 10144
    },
    {
      "epoch": 0.043348402368886575,
      "grad_norm": 1.6718703508377075,
      "learning_rate": 6.656981712527774e-05,
      "loss": 1.0956,
      "step": 10145
    },
    {
      "epoch": 0.04335267525231377,
      "grad_norm": 0.9206416606903076,
      "learning_rate": 6.652708938642966e-05,
      "loss": 0.6185,
      "step": 10146
    },
    {
      "epoch": 0.04335694813574096,
      "grad_norm": 0.6972475051879883,
      "learning_rate": 6.64843616475816e-05,
      "loss": 0.2927,
      "step": 10147
    },
    {
      "epoch": 0.04336122101916816,
      "grad_norm": 0.9118156433105469,
      "learning_rate": 6.644163390873355e-05,
      "loss": 0.6184,
      "step": 10148
    },
    {
      "epoch": 0.04336549390259535,
      "grad_norm": 0.5684711337089539,
      "learning_rate": 6.639890616988549e-05,
      "loss": 0.2164,
      "step": 10149
    },
    {
      "epoch": 0.04336976678602254,
      "grad_norm": 4.574594020843506,
      "learning_rate": 6.635617843103743e-05,
      "loss": 1.2179,
      "step": 10150
    },
    {
      "epoch": 0.043374039669449735,
      "grad_norm": 3.4151885509490967,
      "learning_rate": 6.631345069218937e-05,
      "loss": 0.9064,
      "step": 10151
    },
    {
      "epoch": 0.043378312552876934,
      "grad_norm": 0.7148678302764893,
      "learning_rate": 6.627072295334132e-05,
      "loss": 0.3088,
      "step": 10152
    },
    {
      "epoch": 0.043382585436304126,
      "grad_norm": 4.763753890991211,
      "learning_rate": 6.622799521449326e-05,
      "loss": 1.1958,
      "step": 10153
    },
    {
      "epoch": 0.04338685831973132,
      "grad_norm": 2.1909148693084717,
      "learning_rate": 6.618526747564518e-05,
      "loss": 0.5249,
      "step": 10154
    },
    {
      "epoch": 0.04339113120315852,
      "grad_norm": 0.8401559591293335,
      "learning_rate": 6.614253973679713e-05,
      "loss": 0.3659,
      "step": 10155
    },
    {
      "epoch": 0.04339540408658571,
      "grad_norm": 1.9129194021224976,
      "learning_rate": 6.609981199794907e-05,
      "loss": 0.4925,
      "step": 10156
    },
    {
      "epoch": 0.0433996769700129,
      "grad_norm": 2.3292999267578125,
      "learning_rate": 6.6057084259101e-05,
      "loss": 0.6764,
      "step": 10157
    },
    {
      "epoch": 0.0434039498534401,
      "grad_norm": 4.575367450714111,
      "learning_rate": 6.601435652025294e-05,
      "loss": 1.1746,
      "step": 10158
    },
    {
      "epoch": 0.04340822273686729,
      "grad_norm": 0.8752410411834717,
      "learning_rate": 6.59716287814049e-05,
      "loss": 0.4081,
      "step": 10159
    },
    {
      "epoch": 0.043412495620294485,
      "grad_norm": 1.5862202644348145,
      "learning_rate": 6.592890104255683e-05,
      "loss": 1.0444,
      "step": 10160
    },
    {
      "epoch": 0.043416768503721684,
      "grad_norm": 1.914284348487854,
      "learning_rate": 6.588617330370877e-05,
      "loss": 0.4798,
      "step": 10161
    },
    {
      "epoch": 0.043421041387148876,
      "grad_norm": 0.8544960021972656,
      "learning_rate": 6.584344556486071e-05,
      "loss": 0.5444,
      "step": 10162
    },
    {
      "epoch": 0.04342531427057607,
      "grad_norm": 1.025757908821106,
      "learning_rate": 6.580071782601265e-05,
      "loss": 0.4707,
      "step": 10163
    },
    {
      "epoch": 0.04342958715400327,
      "grad_norm": 1.1864988803863525,
      "learning_rate": 6.575799008716459e-05,
      "loss": 0.4745,
      "step": 10164
    },
    {
      "epoch": 0.04343386003743046,
      "grad_norm": 2.8587379455566406,
      "learning_rate": 6.571526234831652e-05,
      "loss": 0.9656,
      "step": 10165
    },
    {
      "epoch": 0.04343813292085765,
      "grad_norm": 5.438144207000732,
      "learning_rate": 6.567253460946847e-05,
      "loss": 3.2279,
      "step": 10166
    },
    {
      "epoch": 0.04344240580428485,
      "grad_norm": 1.5792590379714966,
      "learning_rate": 6.562980687062041e-05,
      "loss": 1.0443,
      "step": 10167
    },
    {
      "epoch": 0.04344667868771204,
      "grad_norm": 0.8878995180130005,
      "learning_rate": 6.558707913177235e-05,
      "loss": 0.4195,
      "step": 10168
    },
    {
      "epoch": 0.043450951571139235,
      "grad_norm": 0.8239067792892456,
      "learning_rate": 6.554435139292429e-05,
      "loss": 0.3837,
      "step": 10169
    },
    {
      "epoch": 0.043455224454566434,
      "grad_norm": 0.8370009660720825,
      "learning_rate": 6.550162365407624e-05,
      "loss": 0.2814,
      "step": 10170
    },
    {
      "epoch": 0.043459497337993626,
      "grad_norm": 2.9606873989105225,
      "learning_rate": 6.545889591522816e-05,
      "loss": 0.7744,
      "step": 10171
    },
    {
      "epoch": 0.04346377022142082,
      "grad_norm": 4.778204917907715,
      "learning_rate": 6.54161681763801e-05,
      "loss": 1.0245,
      "step": 10172
    },
    {
      "epoch": 0.04346804310484801,
      "grad_norm": 3.315805435180664,
      "learning_rate": 6.537344043753204e-05,
      "loss": 0.7377,
      "step": 10173
    },
    {
      "epoch": 0.04347231598827521,
      "grad_norm": 0.8740108609199524,
      "learning_rate": 6.533071269868399e-05,
      "loss": 0.4155,
      "step": 10174
    },
    {
      "epoch": 0.0434765888717024,
      "grad_norm": 0.38285914063453674,
      "learning_rate": 6.528798495983593e-05,
      "loss": 0.1188,
      "step": 10175
    },
    {
      "epoch": 0.04348086175512959,
      "grad_norm": 0.9221031069755554,
      "learning_rate": 6.524525722098787e-05,
      "loss": 0.3041,
      "step": 10176
    },
    {
      "epoch": 0.04348513463855679,
      "grad_norm": 0.7638295888900757,
      "learning_rate": 6.52025294821398e-05,
      "loss": 0.3261,
      "step": 10177
    },
    {
      "epoch": 0.043489407521983985,
      "grad_norm": 0.8069310784339905,
      "learning_rate": 6.515980174329176e-05,
      "loss": 0.3693,
      "step": 10178
    },
    {
      "epoch": 0.04349368040541118,
      "grad_norm": 3.7522542476654053,
      "learning_rate": 6.511707400444368e-05,
      "loss": 0.944,
      "step": 10179
    },
    {
      "epoch": 0.043497953288838376,
      "grad_norm": 0.8700656294822693,
      "learning_rate": 6.507434626559562e-05,
      "loss": 0.4081,
      "step": 10180
    },
    {
      "epoch": 0.04350222617226557,
      "grad_norm": 3.29062819480896,
      "learning_rate": 6.503161852674757e-05,
      "loss": 1.8529,
      "step": 10181
    },
    {
      "epoch": 0.04350649905569276,
      "grad_norm": 0.5018982291221619,
      "learning_rate": 6.498889078789951e-05,
      "loss": 0.1979,
      "step": 10182
    },
    {
      "epoch": 0.04351077193911996,
      "grad_norm": 4.82484769821167,
      "learning_rate": 6.494616304905144e-05,
      "loss": 1.194,
      "step": 10183
    },
    {
      "epoch": 0.04351504482254715,
      "grad_norm": 3.996508836746216,
      "learning_rate": 6.490343531020338e-05,
      "loss": 0.8938,
      "step": 10184
    },
    {
      "epoch": 0.04351931770597434,
      "grad_norm": 1.8171404600143433,
      "learning_rate": 6.486070757135533e-05,
      "loss": 0.417,
      "step": 10185
    },
    {
      "epoch": 0.04352359058940154,
      "grad_norm": 3.6879584789276123,
      "learning_rate": 6.481797983250727e-05,
      "loss": 0.8829,
      "step": 10186
    },
    {
      "epoch": 0.043527863472828734,
      "grad_norm": 1.9053994417190552,
      "learning_rate": 6.47752520936592e-05,
      "loss": 0.6598,
      "step": 10187
    },
    {
      "epoch": 0.04353213635625593,
      "grad_norm": 1.8559449911117554,
      "learning_rate": 6.473252435481113e-05,
      "loss": 0.4241,
      "step": 10188
    },
    {
      "epoch": 0.043536409239683126,
      "grad_norm": 0.8107801079750061,
      "learning_rate": 6.468979661596309e-05,
      "loss": 0.3883,
      "step": 10189
    },
    {
      "epoch": 0.04354068212311032,
      "grad_norm": 0.7250379920005798,
      "learning_rate": 6.464706887711502e-05,
      "loss": 0.242,
      "step": 10190
    },
    {
      "epoch": 0.04354495500653751,
      "grad_norm": 0.8854061365127563,
      "learning_rate": 6.460434113826696e-05,
      "loss": 0.4082,
      "step": 10191
    },
    {
      "epoch": 0.04354922788996471,
      "grad_norm": 2.8858251571655273,
      "learning_rate": 6.456161339941891e-05,
      "loss": 0.9638,
      "step": 10192
    },
    {
      "epoch": 0.0435535007733919,
      "grad_norm": 0.6482868194580078,
      "learning_rate": 6.451888566057085e-05,
      "loss": 0.2378,
      "step": 10193
    },
    {
      "epoch": 0.04355777365681909,
      "grad_norm": 0.639092206954956,
      "learning_rate": 6.447615792172279e-05,
      "loss": 0.3037,
      "step": 10194
    },
    {
      "epoch": 0.04356204654024629,
      "grad_norm": 3.6452622413635254,
      "learning_rate": 6.443343018287473e-05,
      "loss": 0.8216,
      "step": 10195
    },
    {
      "epoch": 0.043566319423673484,
      "grad_norm": 0.8019835352897644,
      "learning_rate": 6.439070244402666e-05,
      "loss": 0.4039,
      "step": 10196
    },
    {
      "epoch": 0.043570592307100676,
      "grad_norm": 4.1857476234436035,
      "learning_rate": 6.43479747051786e-05,
      "loss": 1.1055,
      "step": 10197
    },
    {
      "epoch": 0.04357486519052787,
      "grad_norm": 2.8896546363830566,
      "learning_rate": 6.430524696633054e-05,
      "loss": 0.9447,
      "step": 10198
    },
    {
      "epoch": 0.04357913807395507,
      "grad_norm": 1.9051927328109741,
      "learning_rate": 6.426251922748248e-05,
      "loss": 0.4342,
      "step": 10199
    },
    {
      "epoch": 0.04358341095738226,
      "grad_norm": 0.6137397885322571,
      "learning_rate": 6.421979148863443e-05,
      "loss": 0.2415,
      "step": 10200
    },
    {
      "epoch": 0.04358768384080945,
      "grad_norm": 0.4721500873565674,
      "learning_rate": 6.417706374978637e-05,
      "loss": 0.1946,
      "step": 10201
    },
    {
      "epoch": 0.04359195672423665,
      "grad_norm": 1.3522260189056396,
      "learning_rate": 6.41343360109383e-05,
      "loss": 0.6198,
      "step": 10202
    },
    {
      "epoch": 0.04359622960766384,
      "grad_norm": 4.1735124588012695,
      "learning_rate": 6.409160827209024e-05,
      "loss": 1.074,
      "step": 10203
    },
    {
      "epoch": 0.043600502491091035,
      "grad_norm": 1.05262291431427,
      "learning_rate": 6.404888053324218e-05,
      "loss": 0.3414,
      "step": 10204
    },
    {
      "epoch": 0.043604775374518234,
      "grad_norm": 0.6402730941772461,
      "learning_rate": 6.400615279439412e-05,
      "loss": 0.2556,
      "step": 10205
    },
    {
      "epoch": 0.043609048257945426,
      "grad_norm": 1.5205382108688354,
      "learning_rate": 6.396342505554606e-05,
      "loss": 0.6695,
      "step": 10206
    },
    {
      "epoch": 0.04361332114137262,
      "grad_norm": 0.770132303237915,
      "learning_rate": 6.392069731669801e-05,
      "loss": 0.2632,
      "step": 10207
    },
    {
      "epoch": 0.04361759402479982,
      "grad_norm": 1.1558629274368286,
      "learning_rate": 6.387796957784994e-05,
      "loss": 0.5079,
      "step": 10208
    },
    {
      "epoch": 0.04362186690822701,
      "grad_norm": 0.8350293040275574,
      "learning_rate": 6.383524183900188e-05,
      "loss": 0.3524,
      "step": 10209
    },
    {
      "epoch": 0.0436261397916542,
      "grad_norm": 3.6859452724456787,
      "learning_rate": 6.379251410015382e-05,
      "loss": 0.7874,
      "step": 10210
    },
    {
      "epoch": 0.0436304126750814,
      "grad_norm": 1.3661742210388184,
      "learning_rate": 6.374978636130577e-05,
      "loss": 0.6398,
      "step": 10211
    },
    {
      "epoch": 0.04363468555850859,
      "grad_norm": 0.4247017204761505,
      "learning_rate": 6.37070586224577e-05,
      "loss": 0.1695,
      "step": 10212
    },
    {
      "epoch": 0.043638958441935785,
      "grad_norm": 1.7074352502822876,
      "learning_rate": 6.366433088360963e-05,
      "loss": 1.0989,
      "step": 10213
    },
    {
      "epoch": 0.043643231325362984,
      "grad_norm": 1.1307635307312012,
      "learning_rate": 6.362160314476157e-05,
      "loss": 0.5078,
      "step": 10214
    },
    {
      "epoch": 0.043647504208790176,
      "grad_norm": 1.2138888835906982,
      "learning_rate": 6.357887540591352e-05,
      "loss": 0.4363,
      "step": 10215
    },
    {
      "epoch": 0.04365177709221737,
      "grad_norm": 2.2238118648529053,
      "learning_rate": 6.353614766706546e-05,
      "loss": 0.6464,
      "step": 10216
    },
    {
      "epoch": 0.04365604997564457,
      "grad_norm": 2.885308265686035,
      "learning_rate": 6.34934199282174e-05,
      "loss": 0.8717,
      "step": 10217
    },
    {
      "epoch": 0.04366032285907176,
      "grad_norm": 2.883395195007324,
      "learning_rate": 6.345069218936935e-05,
      "loss": 0.8734,
      "step": 10218
    },
    {
      "epoch": 0.04366459574249895,
      "grad_norm": 3.2477121353149414,
      "learning_rate": 6.340796445052129e-05,
      "loss": 0.6862,
      "step": 10219
    },
    {
      "epoch": 0.04366886862592615,
      "grad_norm": 0.6776132583618164,
      "learning_rate": 6.336523671167321e-05,
      "loss": 0.2633,
      "step": 10220
    },
    {
      "epoch": 0.04367314150935334,
      "grad_norm": 3.656407117843628,
      "learning_rate": 6.332250897282515e-05,
      "loss": 0.7569,
      "step": 10221
    },
    {
      "epoch": 0.043677414392780535,
      "grad_norm": 0.8774267435073853,
      "learning_rate": 6.32797812339771e-05,
      "loss": 0.6023,
      "step": 10222
    },
    {
      "epoch": 0.04368168727620773,
      "grad_norm": 1.9608242511749268,
      "learning_rate": 6.323705349512904e-05,
      "loss": 0.6623,
      "step": 10223
    },
    {
      "epoch": 0.043685960159634926,
      "grad_norm": 1.8862911462783813,
      "learning_rate": 6.319432575628098e-05,
      "loss": 0.4593,
      "step": 10224
    },
    {
      "epoch": 0.04369023304306212,
      "grad_norm": 2.872156858444214,
      "learning_rate": 6.315159801743292e-05,
      "loss": 0.8409,
      "step": 10225
    },
    {
      "epoch": 0.04369450592648931,
      "grad_norm": 0.8858925700187683,
      "learning_rate": 6.310887027858487e-05,
      "loss": 0.6113,
      "step": 10226
    },
    {
      "epoch": 0.04369877880991651,
      "grad_norm": 3.84356951713562,
      "learning_rate": 6.30661425397368e-05,
      "loss": 1.0982,
      "step": 10227
    },
    {
      "epoch": 0.0437030516933437,
      "grad_norm": 4.878756999969482,
      "learning_rate": 6.302341480088873e-05,
      "loss": 1.126,
      "step": 10228
    },
    {
      "epoch": 0.043707324576770894,
      "grad_norm": 2.1116044521331787,
      "learning_rate": 6.298068706204067e-05,
      "loss": 1.4154,
      "step": 10229
    },
    {
      "epoch": 0.04371159746019809,
      "grad_norm": 4.51013708114624,
      "learning_rate": 6.293795932319262e-05,
      "loss": 1.0897,
      "step": 10230
    },
    {
      "epoch": 0.043715870343625285,
      "grad_norm": 2.2407848834991455,
      "learning_rate": 6.289523158434456e-05,
      "loss": 0.6439,
      "step": 10231
    },
    {
      "epoch": 0.04372014322705248,
      "grad_norm": 0.807960033416748,
      "learning_rate": 6.28525038454965e-05,
      "loss": 0.3882,
      "step": 10232
    },
    {
      "epoch": 0.043724416110479676,
      "grad_norm": 0.5128021836280823,
      "learning_rate": 6.280977610664845e-05,
      "loss": 0.2387,
      "step": 10233
    },
    {
      "epoch": 0.04372868899390687,
      "grad_norm": 1.5818209648132324,
      "learning_rate": 6.276704836780038e-05,
      "loss": 1.0404,
      "step": 10234
    },
    {
      "epoch": 0.04373296187733406,
      "grad_norm": 0.8274283409118652,
      "learning_rate": 6.272432062895232e-05,
      "loss": 0.5404,
      "step": 10235
    },
    {
      "epoch": 0.04373723476076126,
      "grad_norm": 3.034799337387085,
      "learning_rate": 6.268159289010426e-05,
      "loss": 0.7807,
      "step": 10236
    },
    {
      "epoch": 0.04374150764418845,
      "grad_norm": 2.211271047592163,
      "learning_rate": 6.26388651512562e-05,
      "loss": 0.5995,
      "step": 10237
    },
    {
      "epoch": 0.043745780527615644,
      "grad_norm": 2.1994760036468506,
      "learning_rate": 6.259613741240813e-05,
      "loss": 0.593,
      "step": 10238
    },
    {
      "epoch": 0.04375005341104284,
      "grad_norm": 0.9065219759941101,
      "learning_rate": 6.255340967356007e-05,
      "loss": 0.4176,
      "step": 10239
    },
    {
      "epoch": 0.043754326294470035,
      "grad_norm": 1.5540611743927002,
      "learning_rate": 6.251068193471201e-05,
      "loss": 1.0404,
      "step": 10240
    },
    {
      "epoch": 0.04375859917789723,
      "grad_norm": 0.6699899435043335,
      "learning_rate": 6.246795419586396e-05,
      "loss": 0.304,
      "step": 10241
    },
    {
      "epoch": 0.043762872061324426,
      "grad_norm": 2.6129977703094482,
      "learning_rate": 6.24252264570159e-05,
      "loss": 0.6287,
      "step": 10242
    },
    {
      "epoch": 0.04376714494475162,
      "grad_norm": 0.872866153717041,
      "learning_rate": 6.238249871816784e-05,
      "loss": 0.6159,
      "step": 10243
    },
    {
      "epoch": 0.04377141782817881,
      "grad_norm": 0.8317824602127075,
      "learning_rate": 6.233977097931977e-05,
      "loss": 0.3048,
      "step": 10244
    },
    {
      "epoch": 0.04377569071160601,
      "grad_norm": 2.7853283882141113,
      "learning_rate": 6.229704324047171e-05,
      "loss": 0.7659,
      "step": 10245
    },
    {
      "epoch": 0.0437799635950332,
      "grad_norm": 0.8219906091690063,
      "learning_rate": 6.225431550162366e-05,
      "loss": 0.2926,
      "step": 10246
    },
    {
      "epoch": 0.043784236478460394,
      "grad_norm": 0.6616618633270264,
      "learning_rate": 6.221158776277559e-05,
      "loss": 0.3222,
      "step": 10247
    },
    {
      "epoch": 0.043788509361887586,
      "grad_norm": 0.7324515581130981,
      "learning_rate": 6.216886002392754e-05,
      "loss": 0.2756,
      "step": 10248
    },
    {
      "epoch": 0.043792782245314785,
      "grad_norm": 4.320841312408447,
      "learning_rate": 6.212613228507948e-05,
      "loss": 1.0115,
      "step": 10249
    },
    {
      "epoch": 0.04379705512874198,
      "grad_norm": 0.6606230139732361,
      "learning_rate": 6.208340454623142e-05,
      "loss": 0.3222,
      "step": 10250
    },
    {
      "epoch": 0.04380132801216917,
      "grad_norm": 1.105840802192688,
      "learning_rate": 6.204067680738335e-05,
      "loss": 0.4159,
      "step": 10251
    },
    {
      "epoch": 0.04380560089559637,
      "grad_norm": 1.007543683052063,
      "learning_rate": 6.199794906853529e-05,
      "loss": 0.471,
      "step": 10252
    },
    {
      "epoch": 0.04380987377902356,
      "grad_norm": 0.8366427421569824,
      "learning_rate": 6.195522132968723e-05,
      "loss": 0.415,
      "step": 10253
    },
    {
      "epoch": 0.04381414666245075,
      "grad_norm": 0.9094709157943726,
      "learning_rate": 6.191249359083918e-05,
      "loss": 0.4179,
      "step": 10254
    },
    {
      "epoch": 0.04381841954587795,
      "grad_norm": 0.43531420826911926,
      "learning_rate": 6.18697658519911e-05,
      "loss": 0.1762,
      "step": 10255
    },
    {
      "epoch": 0.04382269242930514,
      "grad_norm": 0.7878081798553467,
      "learning_rate": 6.182703811314306e-05,
      "loss": 0.3514,
      "step": 10256
    },
    {
      "epoch": 0.043826965312732336,
      "grad_norm": 0.8378546833992004,
      "learning_rate": 6.1784310374295e-05,
      "loss": 0.4149,
      "step": 10257
    },
    {
      "epoch": 0.043831238196159535,
      "grad_norm": 2.2154698371887207,
      "learning_rate": 6.174158263544693e-05,
      "loss": 0.4972,
      "step": 10258
    },
    {
      "epoch": 0.04383551107958673,
      "grad_norm": 4.47560977935791,
      "learning_rate": 6.169885489659887e-05,
      "loss": 2.4364,
      "step": 10259
    },
    {
      "epoch": 0.04383978396301392,
      "grad_norm": 0.6669163703918457,
      "learning_rate": 6.165612715775081e-05,
      "loss": 0.2777,
      "step": 10260
    },
    {
      "epoch": 0.04384405684644112,
      "grad_norm": 1.5095467567443848,
      "learning_rate": 6.161339941890276e-05,
      "loss": 0.5853,
      "step": 10261
    },
    {
      "epoch": 0.04384832972986831,
      "grad_norm": 2.1100432872772217,
      "learning_rate": 6.15706716800547e-05,
      "loss": 0.4965,
      "step": 10262
    },
    {
      "epoch": 0.0438526026132955,
      "grad_norm": 4.490753650665283,
      "learning_rate": 6.152794394120663e-05,
      "loss": 2.4245,
      "step": 10263
    },
    {
      "epoch": 0.0438568754967227,
      "grad_norm": 0.811996579170227,
      "learning_rate": 6.148521620235857e-05,
      "loss": 0.2927,
      "step": 10264
    },
    {
      "epoch": 0.04386114838014989,
      "grad_norm": 3.4437875747680664,
      "learning_rate": 6.144248846351051e-05,
      "loss": 0.8995,
      "step": 10265
    },
    {
      "epoch": 0.043865421263577085,
      "grad_norm": 2.0912952423095703,
      "learning_rate": 6.139976072466245e-05,
      "loss": 0.4756,
      "step": 10266
    },
    {
      "epoch": 0.043869694147004284,
      "grad_norm": 0.9188489317893982,
      "learning_rate": 6.135703298581439e-05,
      "loss": 0.3383,
      "step": 10267
    },
    {
      "epoch": 0.04387396703043148,
      "grad_norm": 4.123741149902344,
      "learning_rate": 6.131430524696632e-05,
      "loss": 0.9803,
      "step": 10268
    },
    {
      "epoch": 0.04387823991385867,
      "grad_norm": 1.1319762468338013,
      "learning_rate": 6.127157750811828e-05,
      "loss": 0.4037,
      "step": 10269
    },
    {
      "epoch": 0.04388251279728587,
      "grad_norm": 0.8265631198883057,
      "learning_rate": 6.122884976927021e-05,
      "loss": 0.5553,
      "step": 10270
    },
    {
      "epoch": 0.04388678568071306,
      "grad_norm": 0.8482739925384521,
      "learning_rate": 6.118612203042215e-05,
      "loss": 0.3476,
      "step": 10271
    },
    {
      "epoch": 0.04389105856414025,
      "grad_norm": 2.8411779403686523,
      "learning_rate": 6.114339429157409e-05,
      "loss": 1.0091,
      "step": 10272
    },
    {
      "epoch": 0.043895331447567444,
      "grad_norm": 2.8410398960113525,
      "learning_rate": 6.110066655272603e-05,
      "loss": 1.0033,
      "step": 10273
    },
    {
      "epoch": 0.04389960433099464,
      "grad_norm": 1.1520981788635254,
      "learning_rate": 6.105793881387798e-05,
      "loss": 0.4747,
      "step": 10274
    },
    {
      "epoch": 0.043903877214421835,
      "grad_norm": 0.879068911075592,
      "learning_rate": 6.1015211075029916e-05,
      "loss": 0.3989,
      "step": 10275
    },
    {
      "epoch": 0.04390815009784903,
      "grad_norm": 1.0322314500808716,
      "learning_rate": 6.097248333618185e-05,
      "loss": 0.3887,
      "step": 10276
    },
    {
      "epoch": 0.043912422981276227,
      "grad_norm": 3.830444574356079,
      "learning_rate": 6.092975559733379e-05,
      "loss": 1.2766,
      "step": 10277
    },
    {
      "epoch": 0.04391669586470342,
      "grad_norm": 3.317821741104126,
      "learning_rate": 6.088702785848573e-05,
      "loss": 0.7411,
      "step": 10278
    },
    {
      "epoch": 0.04392096874813061,
      "grad_norm": 3.415173053741455,
      "learning_rate": 6.0844300119637674e-05,
      "loss": 0.858,
      "step": 10279
    },
    {
      "epoch": 0.04392524163155781,
      "grad_norm": 3.29580020904541,
      "learning_rate": 6.0801572380789605e-05,
      "loss": 1.1074,
      "step": 10280
    },
    {
      "epoch": 0.043929514514985,
      "grad_norm": 0.7067914605140686,
      "learning_rate": 6.075884464194155e-05,
      "loss": 0.2928,
      "step": 10281
    },
    {
      "epoch": 0.043933787398412194,
      "grad_norm": 0.8397190570831299,
      "learning_rate": 6.071611690309349e-05,
      "loss": 0.3475,
      "step": 10282
    },
    {
      "epoch": 0.04393806028183939,
      "grad_norm": 0.8442474007606506,
      "learning_rate": 6.067338916424543e-05,
      "loss": 0.3637,
      "step": 10283
    },
    {
      "epoch": 0.043942333165266585,
      "grad_norm": 2.790276288986206,
      "learning_rate": 6.063066142539737e-05,
      "loss": 0.9011,
      "step": 10284
    },
    {
      "epoch": 0.04394660604869378,
      "grad_norm": 3.3025875091552734,
      "learning_rate": 6.058793368654931e-05,
      "loss": 0.7203,
      "step": 10285
    },
    {
      "epoch": 0.043950878932120976,
      "grad_norm": 4.494582653045654,
      "learning_rate": 6.054520594770125e-05,
      "loss": 1.3422,
      "step": 10286
    },
    {
      "epoch": 0.04395515181554817,
      "grad_norm": 2.774522066116333,
      "learning_rate": 6.050247820885319e-05,
      "loss": 0.8912,
      "step": 10287
    },
    {
      "epoch": 0.04395942469897536,
      "grad_norm": 0.8569648265838623,
      "learning_rate": 6.045975047000513e-05,
      "loss": 0.4222,
      "step": 10288
    },
    {
      "epoch": 0.04396369758240256,
      "grad_norm": 2.0171408653259277,
      "learning_rate": 6.0417022731157066e-05,
      "loss": 0.4424,
      "step": 10289
    },
    {
      "epoch": 0.04396797046582975,
      "grad_norm": 2.9112188816070557,
      "learning_rate": 6.037429499230901e-05,
      "loss": 0.8265,
      "step": 10290
    },
    {
      "epoch": 0.043972243349256944,
      "grad_norm": 1.806871771812439,
      "learning_rate": 6.033156725346095e-05,
      "loss": 0.4115,
      "step": 10291
    },
    {
      "epoch": 0.04397651623268414,
      "grad_norm": 0.8443010449409485,
      "learning_rate": 6.0288839514612886e-05,
      "loss": 0.3476,
      "step": 10292
    },
    {
      "epoch": 0.043980789116111335,
      "grad_norm": 4.751787185668945,
      "learning_rate": 6.0246111775764824e-05,
      "loss": 1.0218,
      "step": 10293
    },
    {
      "epoch": 0.04398506199953853,
      "grad_norm": 1.4493117332458496,
      "learning_rate": 6.020338403691677e-05,
      "loss": 0.9928,
      "step": 10294
    },
    {
      "epoch": 0.043989334882965726,
      "grad_norm": 2.10323429107666,
      "learning_rate": 6.0160656298068707e-05,
      "loss": 0.5497,
      "step": 10295
    },
    {
      "epoch": 0.04399360776639292,
      "grad_norm": 0.7174072861671448,
      "learning_rate": 6.0117928559220644e-05,
      "loss": 0.3091,
      "step": 10296
    },
    {
      "epoch": 0.04399788064982011,
      "grad_norm": 0.8082579374313354,
      "learning_rate": 6.007520082037259e-05,
      "loss": 0.5472,
      "step": 10297
    },
    {
      "epoch": 0.0440021535332473,
      "grad_norm": 1.5949426889419556,
      "learning_rate": 6.003247308152453e-05,
      "loss": 0.6506,
      "step": 10298
    },
    {
      "epoch": 0.0440064264166745,
      "grad_norm": 4.479794502258301,
      "learning_rate": 5.998974534267647e-05,
      "loss": 2.3338,
      "step": 10299
    },
    {
      "epoch": 0.044010699300101694,
      "grad_norm": 2.2406816482543945,
      "learning_rate": 5.99470176038284e-05,
      "loss": 0.4834,
      "step": 10300
    },
    {
      "epoch": 0.044014972183528886,
      "grad_norm": 1.5193520784378052,
      "learning_rate": 5.990428986498035e-05,
      "loss": 0.5568,
      "step": 10301
    },
    {
      "epoch": 0.044019245066956085,
      "grad_norm": 2.23882794380188,
      "learning_rate": 5.9861562126132285e-05,
      "loss": 0.4825,
      "step": 10302
    },
    {
      "epoch": 0.04402351795038328,
      "grad_norm": 0.8410745859146118,
      "learning_rate": 5.981883438728423e-05,
      "loss": 0.3981,
      "step": 10303
    },
    {
      "epoch": 0.04402779083381047,
      "grad_norm": 2.1159257888793945,
      "learning_rate": 5.977610664843616e-05,
      "loss": 0.6593,
      "step": 10304
    },
    {
      "epoch": 0.04403206371723767,
      "grad_norm": 0.9314820170402527,
      "learning_rate": 5.9733378909588105e-05,
      "loss": 0.3655,
      "step": 10305
    },
    {
      "epoch": 0.04403633660066486,
      "grad_norm": 3.6150898933410645,
      "learning_rate": 5.969065117074004e-05,
      "loss": 0.8353,
      "step": 10306
    },
    {
      "epoch": 0.04404060948409205,
      "grad_norm": 1.9702587127685547,
      "learning_rate": 5.964792343189199e-05,
      "loss": 0.4189,
      "step": 10307
    },
    {
      "epoch": 0.04404488236751925,
      "grad_norm": 0.8609082102775574,
      "learning_rate": 5.9605195693043926e-05,
      "loss": 0.3328,
      "step": 10308
    },
    {
      "epoch": 0.044049155250946444,
      "grad_norm": 0.834811806678772,
      "learning_rate": 5.9562467954195863e-05,
      "loss": 0.3323,
      "step": 10309
    },
    {
      "epoch": 0.044053428134373636,
      "grad_norm": 3.202915906906128,
      "learning_rate": 5.951974021534781e-05,
      "loss": 0.6258,
      "step": 10310
    },
    {
      "epoch": 0.044057701017800835,
      "grad_norm": 0.8331812024116516,
      "learning_rate": 5.9477012476499746e-05,
      "loss": 0.3182,
      "step": 10311
    },
    {
      "epoch": 0.04406197390122803,
      "grad_norm": 2.0371479988098145,
      "learning_rate": 5.943428473765169e-05,
      "loss": 0.6991,
      "step": 10312
    },
    {
      "epoch": 0.04406624678465522,
      "grad_norm": 0.8680122494697571,
      "learning_rate": 5.939155699880362e-05,
      "loss": 0.3477,
      "step": 10313
    },
    {
      "epoch": 0.04407051966808242,
      "grad_norm": 0.7981742024421692,
      "learning_rate": 5.9348829259955566e-05,
      "loss": 0.3343,
      "step": 10314
    },
    {
      "epoch": 0.04407479255150961,
      "grad_norm": 0.7818166017532349,
      "learning_rate": 5.9306101521107504e-05,
      "loss": 0.3184,
      "step": 10315
    },
    {
      "epoch": 0.0440790654349368,
      "grad_norm": 1.910736083984375,
      "learning_rate": 5.926337378225945e-05,
      "loss": 0.4694,
      "step": 10316
    },
    {
      "epoch": 0.044083338318364,
      "grad_norm": 0.7703221440315247,
      "learning_rate": 5.922064604341138e-05,
      "loss": 0.3033,
      "step": 10317
    },
    {
      "epoch": 0.044087611201791194,
      "grad_norm": 3.6035759449005127,
      "learning_rate": 5.9177918304563324e-05,
      "loss": 1.2063,
      "step": 10318
    },
    {
      "epoch": 0.044091884085218386,
      "grad_norm": 1.8580578565597534,
      "learning_rate": 5.913519056571526e-05,
      "loss": 0.4791,
      "step": 10319
    },
    {
      "epoch": 0.044096156968645585,
      "grad_norm": 4.3317790031433105,
      "learning_rate": 5.909246282686721e-05,
      "loss": 1.0115,
      "step": 10320
    },
    {
      "epoch": 0.04410042985207278,
      "grad_norm": 2.1635830402374268,
      "learning_rate": 5.904973508801914e-05,
      "loss": 0.6794,
      "step": 10321
    },
    {
      "epoch": 0.04410470273549997,
      "grad_norm": 3.7458736896514893,
      "learning_rate": 5.900700734917108e-05,
      "loss": 1.1973,
      "step": 10322
    },
    {
      "epoch": 0.04410897561892716,
      "grad_norm": 0.8040320873260498,
      "learning_rate": 5.896427961032303e-05,
      "loss": 0.3184,
      "step": 10323
    },
    {
      "epoch": 0.04411324850235436,
      "grad_norm": 0.953981339931488,
      "learning_rate": 5.8921551871474965e-05,
      "loss": 0.3806,
      "step": 10324
    },
    {
      "epoch": 0.04411752138578155,
      "grad_norm": 2.5812923908233643,
      "learning_rate": 5.88788241326269e-05,
      "loss": 0.6109,
      "step": 10325
    },
    {
      "epoch": 0.044121794269208744,
      "grad_norm": 4.682155132293701,
      "learning_rate": 5.883609639377884e-05,
      "loss": 0.9481,
      "step": 10326
    },
    {
      "epoch": 0.044126067152635944,
      "grad_norm": 2.8400328159332275,
      "learning_rate": 5.8793368654930785e-05,
      "loss": 0.7534,
      "step": 10327
    },
    {
      "epoch": 0.044130340036063136,
      "grad_norm": 2.0658507347106934,
      "learning_rate": 5.875064091608272e-05,
      "loss": 0.5345,
      "step": 10328
    },
    {
      "epoch": 0.04413461291949033,
      "grad_norm": 2.8790054321289062,
      "learning_rate": 5.870791317723466e-05,
      "loss": 0.8347,
      "step": 10329
    },
    {
      "epoch": 0.04413888580291753,
      "grad_norm": 0.9620745778083801,
      "learning_rate": 5.86651854383866e-05,
      "loss": 0.3968,
      "step": 10330
    },
    {
      "epoch": 0.04414315868634472,
      "grad_norm": 2.3134560585021973,
      "learning_rate": 5.8622457699538543e-05,
      "loss": 0.5914,
      "step": 10331
    },
    {
      "epoch": 0.04414743156977191,
      "grad_norm": 4.427822113037109,
      "learning_rate": 5.857972996069048e-05,
      "loss": 1.2527,
      "step": 10332
    },
    {
      "epoch": 0.04415170445319911,
      "grad_norm": 1.8587417602539062,
      "learning_rate": 5.853700222184242e-05,
      "loss": 0.4505,
      "step": 10333
    },
    {
      "epoch": 0.0441559773366263,
      "grad_norm": 4.278400421142578,
      "learning_rate": 5.849427448299436e-05,
      "loss": 1.0418,
      "step": 10334
    },
    {
      "epoch": 0.044160250220053494,
      "grad_norm": 0.8739808201789856,
      "learning_rate": 5.84515467441463e-05,
      "loss": 0.3637,
      "step": 10335
    },
    {
      "epoch": 0.04416452310348069,
      "grad_norm": 2.0681209564208984,
      "learning_rate": 5.8408819005298246e-05,
      "loss": 0.6353,
      "step": 10336
    },
    {
      "epoch": 0.044168795986907886,
      "grad_norm": 0.7846241593360901,
      "learning_rate": 5.836609126645018e-05,
      "loss": 0.3033,
      "step": 10337
    },
    {
      "epoch": 0.04417306887033508,
      "grad_norm": 2.0287890434265137,
      "learning_rate": 5.832336352760212e-05,
      "loss": 1.3963,
      "step": 10338
    },
    {
      "epoch": 0.04417734175376228,
      "grad_norm": 0.7951467633247375,
      "learning_rate": 5.828063578875406e-05,
      "loss": 0.5472,
      "step": 10339
    },
    {
      "epoch": 0.04418161463718947,
      "grad_norm": 4.396263122558594,
      "learning_rate": 5.8237908049906004e-05,
      "loss": 1.2174,
      "step": 10340
    },
    {
      "epoch": 0.04418588752061666,
      "grad_norm": 2.805943250656128,
      "learning_rate": 5.8195180311057935e-05,
      "loss": 0.7994,
      "step": 10341
    },
    {
      "epoch": 0.04419016040404386,
      "grad_norm": 1.5412147045135498,
      "learning_rate": 5.815245257220988e-05,
      "loss": 0.3824,
      "step": 10342
    },
    {
      "epoch": 0.04419443328747105,
      "grad_norm": 2.0078461170196533,
      "learning_rate": 5.810972483336182e-05,
      "loss": 1.3963,
      "step": 10343
    },
    {
      "epoch": 0.044198706170898244,
      "grad_norm": 0.854653537273407,
      "learning_rate": 5.806699709451376e-05,
      "loss": 0.3638,
      "step": 10344
    },
    {
      "epoch": 0.04420297905432544,
      "grad_norm": 0.8571857213973999,
      "learning_rate": 5.80242693556657e-05,
      "loss": 0.3637,
      "step": 10345
    },
    {
      "epoch": 0.044207251937752635,
      "grad_norm": 0.9569310545921326,
      "learning_rate": 5.798154161681764e-05,
      "loss": 0.3971,
      "step": 10346
    },
    {
      "epoch": 0.04421152482117983,
      "grad_norm": 1.441805124282837,
      "learning_rate": 5.7938813877969576e-05,
      "loss": 1.0213,
      "step": 10347
    },
    {
      "epoch": 0.04421579770460702,
      "grad_norm": 1.5176000595092773,
      "learning_rate": 5.789608613912152e-05,
      "loss": 0.366,
      "step": 10348
    },
    {
      "epoch": 0.04422007058803422,
      "grad_norm": 0.8084315657615662,
      "learning_rate": 5.7853358400273465e-05,
      "loss": 0.3964,
      "step": 10349
    },
    {
      "epoch": 0.04422434347146141,
      "grad_norm": 1.9432977437973022,
      "learning_rate": 5.7810630661425396e-05,
      "loss": 1.3351,
      "step": 10350
    },
    {
      "epoch": 0.0442286163548886,
      "grad_norm": 0.8134041428565979,
      "learning_rate": 5.776790292257734e-05,
      "loss": 0.564,
      "step": 10351
    },
    {
      "epoch": 0.0442328892383158,
      "grad_norm": 0.7480859756469727,
      "learning_rate": 5.772517518372928e-05,
      "loss": 0.3031,
      "step": 10352
    },
    {
      "epoch": 0.044237162121742994,
      "grad_norm": 0.9333767890930176,
      "learning_rate": 5.768244744488122e-05,
      "loss": 0.3968,
      "step": 10353
    },
    {
      "epoch": 0.044241435005170186,
      "grad_norm": 0.8139700889587402,
      "learning_rate": 5.7639719706033154e-05,
      "loss": 0.5855,
      "step": 10354
    },
    {
      "epoch": 0.044245707888597385,
      "grad_norm": 1.99039626121521,
      "learning_rate": 5.75969919671851e-05,
      "loss": 0.7026,
      "step": 10355
    },
    {
      "epoch": 0.04424998077202458,
      "grad_norm": 3.4333035945892334,
      "learning_rate": 5.755426422833704e-05,
      "loss": 0.8284,
      "step": 10356
    },
    {
      "epoch": 0.04425425365545177,
      "grad_norm": 0.8163821697235107,
      "learning_rate": 5.751153648948898e-05,
      "loss": 0.5639,
      "step": 10357
    },
    {
      "epoch": 0.04425852653887897,
      "grad_norm": 0.7677080035209656,
      "learning_rate": 5.746880875064091e-05,
      "loss": 0.3032,
      "step": 10358
    },
    {
      "epoch": 0.04426279942230616,
      "grad_norm": 1.6561543941497803,
      "learning_rate": 5.742608101179286e-05,
      "loss": 0.6296,
      "step": 10359
    },
    {
      "epoch": 0.04426707230573335,
      "grad_norm": 3.6707112789154053,
      "learning_rate": 5.7383353272944795e-05,
      "loss": 1.1181,
      "step": 10360
    },
    {
      "epoch": 0.04427134518916055,
      "grad_norm": 3.5331695079803467,
      "learning_rate": 5.734062553409674e-05,
      "loss": 1.1493,
      "step": 10361
    },
    {
      "epoch": 0.044275618072587744,
      "grad_norm": 1.511749267578125,
      "learning_rate": 5.729789779524867e-05,
      "loss": 0.3396,
      "step": 10362
    },
    {
      "epoch": 0.044279890956014936,
      "grad_norm": 0.8300880193710327,
      "learning_rate": 5.7255170056400615e-05,
      "loss": 0.3182,
      "step": 10363
    },
    {
      "epoch": 0.044284163839442135,
      "grad_norm": 4.806128025054932,
      "learning_rate": 5.721244231755256e-05,
      "loss": 0.9687,
      "step": 10364
    },
    {
      "epoch": 0.04428843672286933,
      "grad_norm": 2.806781530380249,
      "learning_rate": 5.71697145787045e-05,
      "loss": 0.7276,
      "step": 10365
    },
    {
      "epoch": 0.04429270960629652,
      "grad_norm": 0.8134787082672119,
      "learning_rate": 5.7126986839856436e-05,
      "loss": 0.3517,
      "step": 10366
    },
    {
      "epoch": 0.04429698248972372,
      "grad_norm": 0.8210583329200745,
      "learning_rate": 5.7084259101008373e-05,
      "loss": 0.3696,
      "step": 10367
    },
    {
      "epoch": 0.04430125537315091,
      "grad_norm": 3.7590668201446533,
      "learning_rate": 5.704153136216032e-05,
      "loss": 1.0824,
      "step": 10368
    },
    {
      "epoch": 0.0443055282565781,
      "grad_norm": 0.8467720746994019,
      "learning_rate": 5.6998803623312256e-05,
      "loss": 0.3477,
      "step": 10369
    },
    {
      "epoch": 0.044309801140005295,
      "grad_norm": 0.942520260810852,
      "learning_rate": 5.6956075884464194e-05,
      "loss": 0.3658,
      "step": 10370
    },
    {
      "epoch": 0.044314074023432494,
      "grad_norm": 0.9501464366912842,
      "learning_rate": 5.691334814561613e-05,
      "loss": 0.3973,
      "step": 10371
    },
    {
      "epoch": 0.044318346906859686,
      "grad_norm": 0.8233606219291687,
      "learning_rate": 5.6870620406768076e-05,
      "loss": 0.3183,
      "step": 10372
    },
    {
      "epoch": 0.04432261979028688,
      "grad_norm": 0.8014675974845886,
      "learning_rate": 5.6827892667920014e-05,
      "loss": 0.531,
      "step": 10373
    },
    {
      "epoch": 0.04432689267371408,
      "grad_norm": 4.311717510223389,
      "learning_rate": 5.678516492907195e-05,
      "loss": 1.1444,
      "step": 10374
    },
    {
      "epoch": 0.04433116555714127,
      "grad_norm": 1.8990164995193481,
      "learning_rate": 5.674243719022389e-05,
      "loss": 0.4432,
      "step": 10375
    },
    {
      "epoch": 0.04433543844056846,
      "grad_norm": 1.844699740409851,
      "learning_rate": 5.6699709451375834e-05,
      "loss": 0.4305,
      "step": 10376
    },
    {
      "epoch": 0.04433971132399566,
      "grad_norm": 4.269100666046143,
      "learning_rate": 5.665698171252778e-05,
      "loss": 1.0833,
      "step": 10377
    },
    {
      "epoch": 0.04434398420742285,
      "grad_norm": 0.8355150818824768,
      "learning_rate": 5.661425397367971e-05,
      "loss": 0.5626,
      "step": 10378
    },
    {
      "epoch": 0.044348257090850045,
      "grad_norm": 0.7508237361907959,
      "learning_rate": 5.6571526234831655e-05,
      "loss": 0.2758,
      "step": 10379
    },
    {
      "epoch": 0.044352529974277244,
      "grad_norm": 1.13206946849823,
      "learning_rate": 5.652879849598359e-05,
      "loss": 0.5357,
      "step": 10380
    },
    {
      "epoch": 0.044356802857704436,
      "grad_norm": 0.8727267384529114,
      "learning_rate": 5.648607075713554e-05,
      "loss": 0.4686,
      "step": 10381
    },
    {
      "epoch": 0.04436107574113163,
      "grad_norm": 1.482448697090149,
      "learning_rate": 5.6443343018287475e-05,
      "loss": 0.308,
      "step": 10382
    },
    {
      "epoch": 0.04436534862455883,
      "grad_norm": 1.9138810634613037,
      "learning_rate": 5.640061527943941e-05,
      "loss": 1.3435,
      "step": 10383
    },
    {
      "epoch": 0.04436962150798602,
      "grad_norm": 1.9197863340377808,
      "learning_rate": 5.635788754059135e-05,
      "loss": 0.453,
      "step": 10384
    },
    {
      "epoch": 0.04437389439141321,
      "grad_norm": 1.031307339668274,
      "learning_rate": 5.6315159801743295e-05,
      "loss": 0.3315,
      "step": 10385
    },
    {
      "epoch": 0.04437816727484041,
      "grad_norm": 2.02555251121521,
      "learning_rate": 5.627243206289523e-05,
      "loss": 0.6216,
      "step": 10386
    },
    {
      "epoch": 0.0443824401582676,
      "grad_norm": 0.7732607126235962,
      "learning_rate": 5.622970432404717e-05,
      "loss": 0.3344,
      "step": 10387
    },
    {
      "epoch": 0.044386713041694795,
      "grad_norm": 1.2340372800827026,
      "learning_rate": 5.618697658519911e-05,
      "loss": 0.5539,
      "step": 10388
    },
    {
      "epoch": 0.044390985925121994,
      "grad_norm": 0.6943309307098389,
      "learning_rate": 5.6144248846351053e-05,
      "loss": 0.2928,
      "step": 10389
    },
    {
      "epoch": 0.044395258808549186,
      "grad_norm": 0.9692094326019287,
      "learning_rate": 5.6101521107503e-05,
      "loss": 0.4139,
      "step": 10390
    },
    {
      "epoch": 0.04439953169197638,
      "grad_norm": 0.8669420480728149,
      "learning_rate": 5.605879336865493e-05,
      "loss": 0.4673,
      "step": 10391
    },
    {
      "epoch": 0.04440380457540358,
      "grad_norm": 0.9227030873298645,
      "learning_rate": 5.6016065629806874e-05,
      "loss": 0.3807,
      "step": 10392
    },
    {
      "epoch": 0.04440807745883077,
      "grad_norm": 1.9244437217712402,
      "learning_rate": 5.597333789095881e-05,
      "loss": 1.3074,
      "step": 10393
    },
    {
      "epoch": 0.04441235034225796,
      "grad_norm": 3.116846799850464,
      "learning_rate": 5.5930610152110756e-05,
      "loss": 0.8243,
      "step": 10394
    },
    {
      "epoch": 0.04441662322568515,
      "grad_norm": 0.8216582536697388,
      "learning_rate": 5.588788241326269e-05,
      "loss": 0.5299,
      "step": 10395
    },
    {
      "epoch": 0.04442089610911235,
      "grad_norm": 4.435351371765137,
      "learning_rate": 5.584515467441463e-05,
      "loss": 0.9982,
      "step": 10396
    },
    {
      "epoch": 0.044425168992539545,
      "grad_norm": 3.37593936920166,
      "learning_rate": 5.580242693556657e-05,
      "loss": 0.9484,
      "step": 10397
    },
    {
      "epoch": 0.04442944187596674,
      "grad_norm": 0.8874542713165283,
      "learning_rate": 5.5759699196718514e-05,
      "loss": 0.495,
      "step": 10398
    },
    {
      "epoch": 0.044433714759393936,
      "grad_norm": 1.8736532926559448,
      "learning_rate": 5.5716971457870445e-05,
      "loss": 0.4165,
      "step": 10399
    },
    {
      "epoch": 0.04443798764282113,
      "grad_norm": 3.7283613681793213,
      "learning_rate": 5.567424371902239e-05,
      "loss": 1.0014,
      "step": 10400
    },
    {
      "epoch": 0.04444226052624832,
      "grad_norm": 3.0501601696014404,
      "learning_rate": 5.563151598017433e-05,
      "loss": 0.8149,
      "step": 10401
    },
    {
      "epoch": 0.04444653340967552,
      "grad_norm": 1.9882259368896484,
      "learning_rate": 5.558878824132627e-05,
      "loss": 0.574,
      "step": 10402
    },
    {
      "epoch": 0.04445080629310271,
      "grad_norm": 4.753427028656006,
      "learning_rate": 5.554606050247821e-05,
      "loss": 1.1118,
      "step": 10403
    },
    {
      "epoch": 0.0444550791765299,
      "grad_norm": 3.6709530353546143,
      "learning_rate": 5.550333276363015e-05,
      "loss": 0.9299,
      "step": 10404
    },
    {
      "epoch": 0.0444593520599571,
      "grad_norm": 4.23192024230957,
      "learning_rate": 5.546060502478209e-05,
      "loss": 0.9964,
      "step": 10405
    },
    {
      "epoch": 0.044463624943384294,
      "grad_norm": 0.8510489463806152,
      "learning_rate": 5.541787728593403e-05,
      "loss": 0.4571,
      "step": 10406
    },
    {
      "epoch": 0.04446789782681149,
      "grad_norm": 1.981771469116211,
      "learning_rate": 5.537514954708597e-05,
      "loss": 0.5607,
      "step": 10407
    },
    {
      "epoch": 0.044472170710238686,
      "grad_norm": 1.449362874031067,
      "learning_rate": 5.5332421808237906e-05,
      "loss": 1.0455,
      "step": 10408
    },
    {
      "epoch": 0.04447644359366588,
      "grad_norm": 3.172989845275879,
      "learning_rate": 5.528969406938985e-05,
      "loss": 0.8083,
      "step": 10409
    },
    {
      "epoch": 0.04448071647709307,
      "grad_norm": 0.9277350902557373,
      "learning_rate": 5.524696633054179e-05,
      "loss": 0.3969,
      "step": 10410
    },
    {
      "epoch": 0.04448498936052027,
      "grad_norm": 4.377030849456787,
      "learning_rate": 5.5204238591693727e-05,
      "loss": 0.9729,
      "step": 10411
    },
    {
      "epoch": 0.04448926224394746,
      "grad_norm": 1.9772729873657227,
      "learning_rate": 5.5161510852845664e-05,
      "loss": 0.6689,
      "step": 10412
    },
    {
      "epoch": 0.04449353512737465,
      "grad_norm": 0.8679254651069641,
      "learning_rate": 5.511878311399761e-05,
      "loss": 0.3639,
      "step": 10413
    },
    {
      "epoch": 0.04449780801080185,
      "grad_norm": 0.8393471837043762,
      "learning_rate": 5.507605537514955e-05,
      "loss": 0.5553,
      "step": 10414
    },
    {
      "epoch": 0.044502080894229044,
      "grad_norm": 3.664560079574585,
      "learning_rate": 5.5033327636301485e-05,
      "loss": 0.9113,
      "step": 10415
    },
    {
      "epoch": 0.044506353777656237,
      "grad_norm": 1.4537353515625,
      "learning_rate": 5.499059989745343e-05,
      "loss": 1.0445,
      "step": 10416
    },
    {
      "epoch": 0.044510626661083436,
      "grad_norm": 1.9700161218643188,
      "learning_rate": 5.494787215860537e-05,
      "loss": 1.3597,
      "step": 10417
    },
    {
      "epoch": 0.04451489954451063,
      "grad_norm": 1.4477285146713257,
      "learning_rate": 5.490514441975731e-05,
      "loss": 1.0451,
      "step": 10418
    },
    {
      "epoch": 0.04451917242793782,
      "grad_norm": 1.0882850885391235,
      "learning_rate": 5.486241668090924e-05,
      "loss": 0.4292,
      "step": 10419
    },
    {
      "epoch": 0.04452344531136501,
      "grad_norm": 0.8278515934944153,
      "learning_rate": 5.481968894206119e-05,
      "loss": 0.3182,
      "step": 10420
    },
    {
      "epoch": 0.04452771819479221,
      "grad_norm": 2.6421282291412354,
      "learning_rate": 5.4776961203213125e-05,
      "loss": 0.6186,
      "step": 10421
    },
    {
      "epoch": 0.0445319910782194,
      "grad_norm": 3.296149253845215,
      "learning_rate": 5.473423346436507e-05,
      "loss": 0.7292,
      "step": 10422
    },
    {
      "epoch": 0.044536263961646595,
      "grad_norm": 2.6280486583709717,
      "learning_rate": 5.469150572551701e-05,
      "loss": 0.6146,
      "step": 10423
    },
    {
      "epoch": 0.044540536845073794,
      "grad_norm": 0.7569670081138611,
      "learning_rate": 5.4648777986668946e-05,
      "loss": 0.3031,
      "step": 10424
    },
    {
      "epoch": 0.044544809728500986,
      "grad_norm": 0.9489206671714783,
      "learning_rate": 5.4606050247820883e-05,
      "loss": 0.4974,
      "step": 10425
    },
    {
      "epoch": 0.04454908261192818,
      "grad_norm": 0.8083272576332092,
      "learning_rate": 5.456332250897283e-05,
      "loss": 0.4206,
      "step": 10426
    },
    {
      "epoch": 0.04455335549535538,
      "grad_norm": 0.8142561316490173,
      "learning_rate": 5.4520594770124766e-05,
      "loss": 0.5446,
      "step": 10427
    },
    {
      "epoch": 0.04455762837878257,
      "grad_norm": 2.0808777809143066,
      "learning_rate": 5.4477867031276704e-05,
      "loss": 0.5288,
      "step": 10428
    },
    {
      "epoch": 0.04456190126220976,
      "grad_norm": 0.9291543960571289,
      "learning_rate": 5.443513929242865e-05,
      "loss": 0.3659,
      "step": 10429
    },
    {
      "epoch": 0.04456617414563696,
      "grad_norm": 3.4720263481140137,
      "learning_rate": 5.4392411553580586e-05,
      "loss": 1.1106,
      "step": 10430
    },
    {
      "epoch": 0.04457044702906415,
      "grad_norm": 0.9271789789199829,
      "learning_rate": 5.434968381473253e-05,
      "loss": 0.3968,
      "step": 10431
    },
    {
      "epoch": 0.044574719912491345,
      "grad_norm": 0.8313291668891907,
      "learning_rate": 5.430695607588446e-05,
      "loss": 0.3325,
      "step": 10432
    },
    {
      "epoch": 0.044578992795918544,
      "grad_norm": 4.421243190765381,
      "learning_rate": 5.4264228337036407e-05,
      "loss": 2.3678,
      "step": 10433
    },
    {
      "epoch": 0.044583265679345736,
      "grad_norm": 0.6891509890556335,
      "learning_rate": 5.4221500598188344e-05,
      "loss": 0.3225,
      "step": 10434
    },
    {
      "epoch": 0.04458753856277293,
      "grad_norm": 1.9141297340393066,
      "learning_rate": 5.417877285934029e-05,
      "loss": 1.3096,
      "step": 10435
    },
    {
      "epoch": 0.04459181144620013,
      "grad_norm": 0.8321705460548401,
      "learning_rate": 5.413604512049222e-05,
      "loss": 0.3476,
      "step": 10436
    },
    {
      "epoch": 0.04459608432962732,
      "grad_norm": 0.5645052194595337,
      "learning_rate": 5.4093317381644165e-05,
      "loss": 0.1975,
      "step": 10437
    },
    {
      "epoch": 0.04460035721305451,
      "grad_norm": 3.5816543102264404,
      "learning_rate": 5.40505896427961e-05,
      "loss": 1.0757,
      "step": 10438
    },
    {
      "epoch": 0.04460463009648171,
      "grad_norm": 3.341623544692993,
      "learning_rate": 5.400786190394805e-05,
      "loss": 0.8969,
      "step": 10439
    },
    {
      "epoch": 0.0446089029799089,
      "grad_norm": 1.0339959859848022,
      "learning_rate": 5.396513416509998e-05,
      "loss": 0.4181,
      "step": 10440
    },
    {
      "epoch": 0.044613175863336095,
      "grad_norm": 0.6424950361251831,
      "learning_rate": 5.392240642625192e-05,
      "loss": 0.3073,
      "step": 10441
    },
    {
      "epoch": 0.044617448746763294,
      "grad_norm": 2.2608935832977295,
      "learning_rate": 5.387967868740387e-05,
      "loss": 0.5925,
      "step": 10442
    },
    {
      "epoch": 0.044621721630190486,
      "grad_norm": 0.9489966630935669,
      "learning_rate": 5.3836950948555805e-05,
      "loss": 0.3971,
      "step": 10443
    },
    {
      "epoch": 0.04462599451361768,
      "grad_norm": 2.2640326023101807,
      "learning_rate": 5.379422320970774e-05,
      "loss": 0.5067,
      "step": 10444
    },
    {
      "epoch": 0.04463026739704487,
      "grad_norm": 0.8374854922294617,
      "learning_rate": 5.375149547085968e-05,
      "loss": 0.5375,
      "step": 10445
    },
    {
      "epoch": 0.04463454028047207,
      "grad_norm": 0.7035863399505615,
      "learning_rate": 5.3708767732011626e-05,
      "loss": 0.3089,
      "step": 10446
    },
    {
      "epoch": 0.04463881316389926,
      "grad_norm": 0.9106810688972473,
      "learning_rate": 5.3666039993163563e-05,
      "loss": 0.3657,
      "step": 10447
    },
    {
      "epoch": 0.044643086047326454,
      "grad_norm": 2.0699715614318848,
      "learning_rate": 5.36233122543155e-05,
      "loss": 0.5145,
      "step": 10448
    },
    {
      "epoch": 0.04464735893075365,
      "grad_norm": 2.802860975265503,
      "learning_rate": 5.358058451546744e-05,
      "loss": 0.8153,
      "step": 10449
    },
    {
      "epoch": 0.044651631814180845,
      "grad_norm": 3.6133384704589844,
      "learning_rate": 5.3537856776619384e-05,
      "loss": 0.8555,
      "step": 10450
    },
    {
      "epoch": 0.04465590469760804,
      "grad_norm": 0.8242080807685852,
      "learning_rate": 5.349512903777132e-05,
      "loss": 0.5346,
      "step": 10451
    },
    {
      "epoch": 0.044660177581035236,
      "grad_norm": 4.706602096557617,
      "learning_rate": 5.345240129892326e-05,
      "loss": 1.0632,
      "step": 10452
    },
    {
      "epoch": 0.04466445046446243,
      "grad_norm": 2.5705626010894775,
      "learning_rate": 5.34096735600752e-05,
      "loss": 0.5469,
      "step": 10453
    },
    {
      "epoch": 0.04466872334788962,
      "grad_norm": 0.8434250354766846,
      "learning_rate": 5.336694582122714e-05,
      "loss": 0.3479,
      "step": 10454
    },
    {
      "epoch": 0.04467299623131682,
      "grad_norm": 3.2483432292938232,
      "learning_rate": 5.3324218082379087e-05,
      "loss": 0.7948,
      "step": 10455
    },
    {
      "epoch": 0.04467726911474401,
      "grad_norm": 1.9708967208862305,
      "learning_rate": 5.328149034353102e-05,
      "loss": 0.6606,
      "step": 10456
    },
    {
      "epoch": 0.044681541998171204,
      "grad_norm": 3.350743532180786,
      "learning_rate": 5.323876260468296e-05,
      "loss": 0.7357,
      "step": 10457
    },
    {
      "epoch": 0.0446858148815984,
      "grad_norm": 4.180253028869629,
      "learning_rate": 5.31960348658349e-05,
      "loss": 1.3024,
      "step": 10458
    },
    {
      "epoch": 0.044690087765025595,
      "grad_norm": 1.8630130290985107,
      "learning_rate": 5.3153307126986845e-05,
      "loss": 1.2856,
      "step": 10459
    },
    {
      "epoch": 0.04469436064845279,
      "grad_norm": 1.3840066194534302,
      "learning_rate": 5.311057938813878e-05,
      "loss": 0.5202,
      "step": 10460
    },
    {
      "epoch": 0.044698633531879986,
      "grad_norm": 0.8326579928398132,
      "learning_rate": 5.306785164929072e-05,
      "loss": 0.5344,
      "step": 10461
    },
    {
      "epoch": 0.04470290641530718,
      "grad_norm": 0.7681589722633362,
      "learning_rate": 5.302512391044266e-05,
      "loss": 0.3182,
      "step": 10462
    },
    {
      "epoch": 0.04470717929873437,
      "grad_norm": 1.118259072303772,
      "learning_rate": 5.29823961715946e-05,
      "loss": 0.4884,
      "step": 10463
    },
    {
      "epoch": 0.04471145218216157,
      "grad_norm": 3.3090224266052246,
      "learning_rate": 5.293966843274654e-05,
      "loss": 0.7051,
      "step": 10464
    },
    {
      "epoch": 0.04471572506558876,
      "grad_norm": 0.9479299783706665,
      "learning_rate": 5.289694069389848e-05,
      "loss": 0.3968,
      "step": 10465
    },
    {
      "epoch": 0.044719997949015954,
      "grad_norm": 1.0539852380752563,
      "learning_rate": 5.2854212955050416e-05,
      "loss": 0.434,
      "step": 10466
    },
    {
      "epoch": 0.04472427083244315,
      "grad_norm": 4.173034191131592,
      "learning_rate": 5.281148521620236e-05,
      "loss": 0.9695,
      "step": 10467
    },
    {
      "epoch": 0.044728543715870345,
      "grad_norm": 2.8172860145568848,
      "learning_rate": 5.2768757477354306e-05,
      "loss": 0.7296,
      "step": 10468
    },
    {
      "epoch": 0.04473281659929754,
      "grad_norm": 2.8238697052001953,
      "learning_rate": 5.272602973850624e-05,
      "loss": 0.7398,
      "step": 10469
    },
    {
      "epoch": 0.04473708948272473,
      "grad_norm": 1.713407039642334,
      "learning_rate": 5.268330199965818e-05,
      "loss": 0.6456,
      "step": 10470
    },
    {
      "epoch": 0.04474136236615193,
      "grad_norm": 0.8405264616012573,
      "learning_rate": 5.264057426081012e-05,
      "loss": 0.5345,
      "step": 10471
    },
    {
      "epoch": 0.04474563524957912,
      "grad_norm": 5.173847675323486,
      "learning_rate": 5.2597846521962064e-05,
      "loss": 1.3586,
      "step": 10472
    },
    {
      "epoch": 0.04474990813300631,
      "grad_norm": 2.8727855682373047,
      "learning_rate": 5.2555118783113995e-05,
      "loss": 0.7866,
      "step": 10473
    },
    {
      "epoch": 0.04475418101643351,
      "grad_norm": 0.7694493532180786,
      "learning_rate": 5.251239104426594e-05,
      "loss": 0.3344,
      "step": 10474
    },
    {
      "epoch": 0.0447584538998607,
      "grad_norm": 2.0257456302642822,
      "learning_rate": 5.246966330541788e-05,
      "loss": 0.5731,
      "step": 10475
    },
    {
      "epoch": 0.044762726783287896,
      "grad_norm": 0.6991391777992249,
      "learning_rate": 5.242693556656982e-05,
      "loss": 0.293,
      "step": 10476
    },
    {
      "epoch": 0.044766999666715095,
      "grad_norm": 1.044946551322937,
      "learning_rate": 5.238420782772175e-05,
      "loss": 0.4509,
      "step": 10477
    },
    {
      "epoch": 0.04477127255014229,
      "grad_norm": 0.851939857006073,
      "learning_rate": 5.23414800888737e-05,
      "loss": 0.3323,
      "step": 10478
    },
    {
      "epoch": 0.04477554543356948,
      "grad_norm": 1.6924793720245361,
      "learning_rate": 5.2298752350025635e-05,
      "loss": 0.6393,
      "step": 10479
    },
    {
      "epoch": 0.04477981831699668,
      "grad_norm": 1.292580246925354,
      "learning_rate": 5.225602461117758e-05,
      "loss": 0.4362,
      "step": 10480
    },
    {
      "epoch": 0.04478409120042387,
      "grad_norm": 0.7895414233207703,
      "learning_rate": 5.221329687232952e-05,
      "loss": 0.3515,
      "step": 10481
    },
    {
      "epoch": 0.04478836408385106,
      "grad_norm": 1.9854716062545776,
      "learning_rate": 5.2170569133481456e-05,
      "loss": 0.5574,
      "step": 10482
    },
    {
      "epoch": 0.04479263696727826,
      "grad_norm": 0.7734811902046204,
      "learning_rate": 5.21278413946334e-05,
      "loss": 0.3031,
      "step": 10483
    },
    {
      "epoch": 0.04479690985070545,
      "grad_norm": 2.193908929824829,
      "learning_rate": 5.208511365578534e-05,
      "loss": 0.5397,
      "step": 10484
    },
    {
      "epoch": 0.044801182734132645,
      "grad_norm": 0.76102215051651,
      "learning_rate": 5.2042385916937276e-05,
      "loss": 0.3183,
      "step": 10485
    },
    {
      "epoch": 0.044805455617559845,
      "grad_norm": 0.8762304186820984,
      "learning_rate": 5.1999658178089214e-05,
      "loss": 0.3637,
      "step": 10486
    },
    {
      "epoch": 0.04480972850098704,
      "grad_norm": 0.6327873468399048,
      "learning_rate": 5.195693043924116e-05,
      "loss": 0.1845,
      "step": 10487
    },
    {
      "epoch": 0.04481400138441423,
      "grad_norm": 3.5934064388275146,
      "learning_rate": 5.1914202700393096e-05,
      "loss": 0.8191,
      "step": 10488
    },
    {
      "epoch": 0.04481827426784143,
      "grad_norm": 0.8499624729156494,
      "learning_rate": 5.1871474961545034e-05,
      "loss": 0.3474,
      "step": 10489
    },
    {
      "epoch": 0.04482254715126862,
      "grad_norm": 4.35618257522583,
      "learning_rate": 5.182874722269697e-05,
      "loss": 2.3721,
      "step": 10490
    },
    {
      "epoch": 0.04482682003469581,
      "grad_norm": 0.8291895389556885,
      "learning_rate": 5.1786019483848917e-05,
      "loss": 0.3324,
      "step": 10491
    },
    {
      "epoch": 0.04483109291812301,
      "grad_norm": 0.8480017781257629,
      "learning_rate": 5.1743291745000854e-05,
      "loss": 0.3181,
      "step": 10492
    },
    {
      "epoch": 0.0448353658015502,
      "grad_norm": 0.7729768753051758,
      "learning_rate": 5.170056400615279e-05,
      "loss": 0.3032,
      "step": 10493
    },
    {
      "epoch": 0.044839638684977395,
      "grad_norm": 3.258901596069336,
      "learning_rate": 5.165783626730474e-05,
      "loss": 0.8497,
      "step": 10494
    },
    {
      "epoch": 0.04484391156840459,
      "grad_norm": 1.9161750078201294,
      "learning_rate": 5.1615108528456675e-05,
      "loss": 0.621,
      "step": 10495
    },
    {
      "epoch": 0.04484818445183179,
      "grad_norm": 0.8190651535987854,
      "learning_rate": 5.157238078960862e-05,
      "loss": 0.4954,
      "step": 10496
    },
    {
      "epoch": 0.04485245733525898,
      "grad_norm": 4.0268073081970215,
      "learning_rate": 5.152965305076056e-05,
      "loss": 0.9112,
      "step": 10497
    },
    {
      "epoch": 0.04485673021868617,
      "grad_norm": 0.8228386640548706,
      "learning_rate": 5.1486925311912495e-05,
      "loss": 0.4951,
      "step": 10498
    },
    {
      "epoch": 0.04486100310211337,
      "grad_norm": 1.8855979442596436,
      "learning_rate": 5.144419757306443e-05,
      "loss": 1.3079,
      "step": 10499
    },
    {
      "epoch": 0.04486527598554056,
      "grad_norm": 0.8584837913513184,
      "learning_rate": 5.140146983421638e-05,
      "loss": 0.5596,
      "step": 10500
    },
    {
      "epoch": 0.044869548868967754,
      "grad_norm": 2.15480375289917,
      "learning_rate": 5.1358742095368315e-05,
      "loss": 0.4631,
      "step": 10501
    },
    {
      "epoch": 0.04487382175239495,
      "grad_norm": 1.0940271615982056,
      "learning_rate": 5.131601435652025e-05,
      "loss": 0.4915,
      "step": 10502
    },
    {
      "epoch": 0.044878094635822145,
      "grad_norm": 0.5106995701789856,
      "learning_rate": 5.127328661767219e-05,
      "loss": 0.1419,
      "step": 10503
    },
    {
      "epoch": 0.04488236751924934,
      "grad_norm": 0.829314112663269,
      "learning_rate": 5.1230558878824136e-05,
      "loss": 0.4954,
      "step": 10504
    },
    {
      "epoch": 0.044886640402676536,
      "grad_norm": 0.8944838047027588,
      "learning_rate": 5.1187831139976073e-05,
      "loss": 0.3807,
      "step": 10505
    },
    {
      "epoch": 0.04489091328610373,
      "grad_norm": 1.3448023796081543,
      "learning_rate": 5.114510340112801e-05,
      "loss": 0.9954,
      "step": 10506
    },
    {
      "epoch": 0.04489518616953092,
      "grad_norm": 4.264558792114258,
      "learning_rate": 5.1102375662279956e-05,
      "loss": 1.2573,
      "step": 10507
    },
    {
      "epoch": 0.04489945905295812,
      "grad_norm": 3.2289278507232666,
      "learning_rate": 5.1059647923431894e-05,
      "loss": 0.6471,
      "step": 10508
    },
    {
      "epoch": 0.04490373193638531,
      "grad_norm": 3.0391104221343994,
      "learning_rate": 5.101692018458384e-05,
      "loss": 0.7342,
      "step": 10509
    },
    {
      "epoch": 0.044908004819812504,
      "grad_norm": 3.2557177543640137,
      "learning_rate": 5.097419244573577e-05,
      "loss": 1.3769,
      "step": 10510
    },
    {
      "epoch": 0.0449122777032397,
      "grad_norm": 0.874737560749054,
      "learning_rate": 5.0931464706887714e-05,
      "loss": 0.3807,
      "step": 10511
    },
    {
      "epoch": 0.044916550586666895,
      "grad_norm": 3.451815366744995,
      "learning_rate": 5.088873696803965e-05,
      "loss": 1.0704,
      "step": 10512
    },
    {
      "epoch": 0.04492082347009409,
      "grad_norm": 1.8819373846054077,
      "learning_rate": 5.0846009229191597e-05,
      "loss": 1.307,
      "step": 10513
    },
    {
      "epoch": 0.044925096353521286,
      "grad_norm": 3.0880484580993652,
      "learning_rate": 5.080328149034353e-05,
      "loss": 1.8079,
      "step": 10514
    },
    {
      "epoch": 0.04492936923694848,
      "grad_norm": 3.1747756004333496,
      "learning_rate": 5.076055375149547e-05,
      "loss": 1.1455,
      "step": 10515
    },
    {
      "epoch": 0.04493364212037567,
      "grad_norm": 0.8925557136535645,
      "learning_rate": 5.071782601264741e-05,
      "loss": 0.3637,
      "step": 10516
    },
    {
      "epoch": 0.04493791500380287,
      "grad_norm": 3.609020471572876,
      "learning_rate": 5.0675098273799355e-05,
      "loss": 0.9409,
      "step": 10517
    },
    {
      "epoch": 0.04494218788723006,
      "grad_norm": 3.069018602371216,
      "learning_rate": 5.0632370534951286e-05,
      "loss": 1.7847,
      "step": 10518
    },
    {
      "epoch": 0.044946460770657254,
      "grad_norm": 0.791000247001648,
      "learning_rate": 5.058964279610323e-05,
      "loss": 0.3343,
      "step": 10519
    },
    {
      "epoch": 0.044950733654084446,
      "grad_norm": 0.8701072335243225,
      "learning_rate": 5.0546915057255175e-05,
      "loss": 0.5123,
      "step": 10520
    },
    {
      "epoch": 0.044955006537511645,
      "grad_norm": 0.860626757144928,
      "learning_rate": 5.050418731840711e-05,
      "loss": 0.4973,
      "step": 10521
    },
    {
      "epoch": 0.04495927942093884,
      "grad_norm": 0.45248258113861084,
      "learning_rate": 5.046145957955905e-05,
      "loss": 0.1338,
      "step": 10522
    },
    {
      "epoch": 0.04496355230436603,
      "grad_norm": 0.9541201591491699,
      "learning_rate": 5.041873184071099e-05,
      "loss": 0.3657,
      "step": 10523
    },
    {
      "epoch": 0.04496782518779323,
      "grad_norm": 4.7285895347595215,
      "learning_rate": 5.037600410186293e-05,
      "loss": 1.0732,
      "step": 10524
    },
    {
      "epoch": 0.04497209807122042,
      "grad_norm": 0.7876529097557068,
      "learning_rate": 5.033327636301487e-05,
      "loss": 0.3186,
      "step": 10525
    },
    {
      "epoch": 0.04497637095464761,
      "grad_norm": 0.873458206653595,
      "learning_rate": 5.029054862416681e-05,
      "loss": 0.3475,
      "step": 10526
    },
    {
      "epoch": 0.04498064383807481,
      "grad_norm": 5.10725736618042,
      "learning_rate": 5.024782088531875e-05,
      "loss": 1.1024,
      "step": 10527
    },
    {
      "epoch": 0.044984916721502004,
      "grad_norm": 3.5759270191192627,
      "learning_rate": 5.020509314647069e-05,
      "loss": 1.0818,
      "step": 10528
    },
    {
      "epoch": 0.044989189604929196,
      "grad_norm": 1.8916288614273071,
      "learning_rate": 5.016236540762263e-05,
      "loss": 0.6087,
      "step": 10529
    },
    {
      "epoch": 0.044993462488356395,
      "grad_norm": 3.0442850589752197,
      "learning_rate": 5.011963766877457e-05,
      "loss": 1.753,
      "step": 10530
    },
    {
      "epoch": 0.04499773537178359,
      "grad_norm": 2.1314048767089844,
      "learning_rate": 5.0076909929926505e-05,
      "loss": 0.6502,
      "step": 10531
    },
    {
      "epoch": 0.04500200825521078,
      "grad_norm": 1.8458024263381958,
      "learning_rate": 5.003418219107845e-05,
      "loss": 1.31,
      "step": 10532
    },
    {
      "epoch": 0.04500628113863798,
      "grad_norm": 0.8413489460945129,
      "learning_rate": 4.9991454452230394e-05,
      "loss": 0.4982,
      "step": 10533
    },
    {
      "epoch": 0.04501055402206517,
      "grad_norm": 2.1062562465667725,
      "learning_rate": 4.994872671338233e-05,
      "loss": 0.6233,
      "step": 10534
    },
    {
      "epoch": 0.04501482690549236,
      "grad_norm": 1.1405092477798462,
      "learning_rate": 4.990599897453427e-05,
      "loss": 0.4747,
      "step": 10535
    },
    {
      "epoch": 0.04501909978891956,
      "grad_norm": 0.8693826198577881,
      "learning_rate": 4.986327123568621e-05,
      "loss": 0.3476,
      "step": 10536
    },
    {
      "epoch": 0.045023372672346754,
      "grad_norm": 3.095334768295288,
      "learning_rate": 4.982054349683815e-05,
      "loss": 0.8039,
      "step": 10537
    },
    {
      "epoch": 0.045027645555773946,
      "grad_norm": 3.6047861576080322,
      "learning_rate": 4.977781575799009e-05,
      "loss": 1.077,
      "step": 10538
    },
    {
      "epoch": 0.045031918439201145,
      "grad_norm": 1.8433960676193237,
      "learning_rate": 4.973508801914203e-05,
      "loss": 0.4569,
      "step": 10539
    },
    {
      "epoch": 0.04503619132262834,
      "grad_norm": 0.8359761834144592,
      "learning_rate": 4.9692360280293966e-05,
      "loss": 0.4753,
      "step": 10540
    },
    {
      "epoch": 0.04504046420605553,
      "grad_norm": 0.7968510389328003,
      "learning_rate": 4.964963254144591e-05,
      "loss": 0.3346,
      "step": 10541
    },
    {
      "epoch": 0.04504473708948273,
      "grad_norm": 2.1822586059570312,
      "learning_rate": 4.960690480259785e-05,
      "loss": 0.5363,
      "step": 10542
    },
    {
      "epoch": 0.04504900997290992,
      "grad_norm": 0.44577133655548096,
      "learning_rate": 4.9564177063749786e-05,
      "loss": 0.1338,
      "step": 10543
    },
    {
      "epoch": 0.04505328285633711,
      "grad_norm": 0.8834917545318604,
      "learning_rate": 4.9521449324901724e-05,
      "loss": 0.5054,
      "step": 10544
    },
    {
      "epoch": 0.045057555739764305,
      "grad_norm": 3.0618741512298584,
      "learning_rate": 4.947872158605367e-05,
      "loss": 0.7859,
      "step": 10545
    },
    {
      "epoch": 0.045061828623191504,
      "grad_norm": 5.172426223754883,
      "learning_rate": 4.943599384720561e-05,
      "loss": 3.0034,
      "step": 10546
    },
    {
      "epoch": 0.045066101506618696,
      "grad_norm": 1.842737078666687,
      "learning_rate": 4.9393266108357544e-05,
      "loss": 0.4486,
      "step": 10547
    },
    {
      "epoch": 0.04507037439004589,
      "grad_norm": 0.6627245545387268,
      "learning_rate": 4.935053836950949e-05,
      "loss": 0.2502,
      "step": 10548
    },
    {
      "epoch": 0.04507464727347309,
      "grad_norm": 0.7591460943222046,
      "learning_rate": 4.9307810630661427e-05,
      "loss": 0.3031,
      "step": 10549
    },
    {
      "epoch": 0.04507892015690028,
      "grad_norm": 3.6204872131347656,
      "learning_rate": 4.926508289181337e-05,
      "loss": 0.9459,
      "step": 10550
    },
    {
      "epoch": 0.04508319304032747,
      "grad_norm": 1.8286694288253784,
      "learning_rate": 4.92223551529653e-05,
      "loss": 0.4311,
      "step": 10551
    },
    {
      "epoch": 0.04508746592375467,
      "grad_norm": 2.656461715698242,
      "learning_rate": 4.917962741411725e-05,
      "loss": 0.8696,
      "step": 10552
    },
    {
      "epoch": 0.04509173880718186,
      "grad_norm": 4.26021671295166,
      "learning_rate": 4.9136899675269185e-05,
      "loss": 1.2125,
      "step": 10553
    },
    {
      "epoch": 0.045096011690609054,
      "grad_norm": 1.8285174369812012,
      "learning_rate": 4.909417193642113e-05,
      "loss": 1.2885,
      "step": 10554
    },
    {
      "epoch": 0.04510028457403625,
      "grad_norm": 1.3766591548919678,
      "learning_rate": 4.905144419757306e-05,
      "loss": 0.507,
      "step": 10555
    },
    {
      "epoch": 0.045104557457463446,
      "grad_norm": 0.8759492635726929,
      "learning_rate": 4.9008716458725005e-05,
      "loss": 0.3476,
      "step": 10556
    },
    {
      "epoch": 0.04510883034089064,
      "grad_norm": 3.416210651397705,
      "learning_rate": 4.896598871987694e-05,
      "loss": 1.0264,
      "step": 10557
    },
    {
      "epoch": 0.04511310322431784,
      "grad_norm": 2.8292739391326904,
      "learning_rate": 4.892326098102889e-05,
      "loss": 0.812,
      "step": 10558
    },
    {
      "epoch": 0.04511737610774503,
      "grad_norm": 3.0467288494110107,
      "learning_rate": 4.888053324218082e-05,
      "loss": 0.7823,
      "step": 10559
    },
    {
      "epoch": 0.04512164899117222,
      "grad_norm": 0.8791956305503845,
      "learning_rate": 4.883780550333276e-05,
      "loss": 0.5058,
      "step": 10560
    },
    {
      "epoch": 0.04512592187459942,
      "grad_norm": 1.8701890707015991,
      "learning_rate": 4.879507776448471e-05,
      "loss": 0.6064,
      "step": 10561
    },
    {
      "epoch": 0.04513019475802661,
      "grad_norm": 0.6838915944099426,
      "learning_rate": 4.8752350025636646e-05,
      "loss": 0.327,
      "step": 10562
    },
    {
      "epoch": 0.045134467641453804,
      "grad_norm": 1.8151804208755493,
      "learning_rate": 4.8709622286788583e-05,
      "loss": 1.2557,
      "step": 10563
    },
    {
      "epoch": 0.045138740524881,
      "grad_norm": 0.8695725798606873,
      "learning_rate": 4.866689454794052e-05,
      "loss": 0.5038,
      "step": 10564
    },
    {
      "epoch": 0.045143013408308195,
      "grad_norm": 0.5649014711380005,
      "learning_rate": 4.8624166809092466e-05,
      "loss": 0.1758,
      "step": 10565
    },
    {
      "epoch": 0.04514728629173539,
      "grad_norm": 4.1906914710998535,
      "learning_rate": 4.8581439070244404e-05,
      "loss": 1.0961,
      "step": 10566
    },
    {
      "epoch": 0.04515155917516259,
      "grad_norm": 1.2885721921920776,
      "learning_rate": 4.853871133139634e-05,
      "loss": 1.0065,
      "step": 10567
    },
    {
      "epoch": 0.04515583205858978,
      "grad_norm": 1.9463921785354614,
      "learning_rate": 4.849598359254828e-05,
      "loss": 0.4976,
      "step": 10568
    },
    {
      "epoch": 0.04516010494201697,
      "grad_norm": 1.2701879739761353,
      "learning_rate": 4.8453255853700224e-05,
      "loss": 0.9818,
      "step": 10569
    },
    {
      "epoch": 0.04516437782544416,
      "grad_norm": 0.6337814927101135,
      "learning_rate": 4.841052811485216e-05,
      "loss": 0.256,
      "step": 10570
    },
    {
      "epoch": 0.04516865070887136,
      "grad_norm": 0.6920637488365173,
      "learning_rate": 4.8367800376004107e-05,
      "loss": 0.2633,
      "step": 10571
    },
    {
      "epoch": 0.045172923592298554,
      "grad_norm": 3.3855206966400146,
      "learning_rate": 4.832507263715604e-05,
      "loss": 0.7718,
      "step": 10572
    },
    {
      "epoch": 0.045177196475725746,
      "grad_norm": 4.642848014831543,
      "learning_rate": 4.828234489830798e-05,
      "loss": 1.2285,
      "step": 10573
    },
    {
      "epoch": 0.045181469359152945,
      "grad_norm": 2.068011999130249,
      "learning_rate": 4.823961715945993e-05,
      "loss": 0.5957,
      "step": 10574
    },
    {
      "epoch": 0.04518574224258014,
      "grad_norm": 0.5645653605461121,
      "learning_rate": 4.8196889420611865e-05,
      "loss": 0.2234,
      "step": 10575
    },
    {
      "epoch": 0.04519001512600733,
      "grad_norm": 2.9800703525543213,
      "learning_rate": 4.81541616817638e-05,
      "loss": 1.7089,
      "step": 10576
    },
    {
      "epoch": 0.04519428800943453,
      "grad_norm": 0.7332124710083008,
      "learning_rate": 4.811143394291574e-05,
      "loss": 0.3088,
      "step": 10577
    },
    {
      "epoch": 0.04519856089286172,
      "grad_norm": 0.6737881302833557,
      "learning_rate": 4.8068706204067685e-05,
      "loss": 0.3144,
      "step": 10578
    },
    {
      "epoch": 0.04520283377628891,
      "grad_norm": 0.7558896541595459,
      "learning_rate": 4.802597846521962e-05,
      "loss": 0.3259,
      "step": 10579
    },
    {
      "epoch": 0.04520710665971611,
      "grad_norm": 0.8829325437545776,
      "learning_rate": 4.798325072637156e-05,
      "loss": 0.5214,
      "step": 10580
    },
    {
      "epoch": 0.045211379543143304,
      "grad_norm": 1.067407488822937,
      "learning_rate": 4.79405229875235e-05,
      "loss": 0.4181,
      "step": 10581
    },
    {
      "epoch": 0.045215652426570496,
      "grad_norm": 0.8784328103065491,
      "learning_rate": 4.789779524867544e-05,
      "loss": 0.5236,
      "step": 10582
    },
    {
      "epoch": 0.045219925309997695,
      "grad_norm": 2.6211740970611572,
      "learning_rate": 4.785506750982738e-05,
      "loss": 0.8428,
      "step": 10583
    },
    {
      "epoch": 0.04522419819342489,
      "grad_norm": 2.9720213413238525,
      "learning_rate": 4.781233977097932e-05,
      "loss": 0.7697,
      "step": 10584
    },
    {
      "epoch": 0.04522847107685208,
      "grad_norm": 2.5996148586273193,
      "learning_rate": 4.776961203213126e-05,
      "loss": 0.8165,
      "step": 10585
    },
    {
      "epoch": 0.04523274396027928,
      "grad_norm": 2.1777920722961426,
      "learning_rate": 4.77268842932832e-05,
      "loss": 0.4757,
      "step": 10586
    },
    {
      "epoch": 0.04523701684370647,
      "grad_norm": 0.8195096850395203,
      "learning_rate": 4.7684156554435146e-05,
      "loss": 0.303,
      "step": 10587
    },
    {
      "epoch": 0.04524128972713366,
      "grad_norm": 0.8640103340148926,
      "learning_rate": 4.764142881558708e-05,
      "loss": 0.3695,
      "step": 10588
    },
    {
      "epoch": 0.04524556261056086,
      "grad_norm": 0.8750784397125244,
      "learning_rate": 4.759870107673902e-05,
      "loss": 0.5212,
      "step": 10589
    },
    {
      "epoch": 0.045249835493988054,
      "grad_norm": 2.8529653549194336,
      "learning_rate": 4.755597333789096e-05,
      "loss": 0.7336,
      "step": 10590
    },
    {
      "epoch": 0.045254108377415246,
      "grad_norm": 1.264096736907959,
      "learning_rate": 4.7513245599042904e-05,
      "loss": 1.0072,
      "step": 10591
    },
    {
      "epoch": 0.04525838126084244,
      "grad_norm": 2.77600359916687,
      "learning_rate": 4.7470517860194835e-05,
      "loss": 0.7002,
      "step": 10592
    },
    {
      "epoch": 0.04526265414426964,
      "grad_norm": 3.027745485305786,
      "learning_rate": 4.742779012134678e-05,
      "loss": 0.8348,
      "step": 10593
    },
    {
      "epoch": 0.04526692702769683,
      "grad_norm": 3.276998996734619,
      "learning_rate": 4.738506238249872e-05,
      "loss": 0.8425,
      "step": 10594
    },
    {
      "epoch": 0.04527119991112402,
      "grad_norm": 0.8780458569526672,
      "learning_rate": 4.734233464365066e-05,
      "loss": 0.5212,
      "step": 10595
    },
    {
      "epoch": 0.04527547279455122,
      "grad_norm": 0.7545289397239685,
      "learning_rate": 4.729960690480259e-05,
      "loss": 0.326,
      "step": 10596
    },
    {
      "epoch": 0.04527974567797841,
      "grad_norm": 4.582943916320801,
      "learning_rate": 4.725687916595454e-05,
      "loss": 1.162,
      "step": 10597
    },
    {
      "epoch": 0.045284018561405605,
      "grad_norm": 3.1263444423675537,
      "learning_rate": 4.7214151427106476e-05,
      "loss": 1.2942,
      "step": 10598
    },
    {
      "epoch": 0.045288291444832804,
      "grad_norm": 2.989905595779419,
      "learning_rate": 4.717142368825842e-05,
      "loss": 0.735,
      "step": 10599
    },
    {
      "epoch": 0.045292564328259996,
      "grad_norm": 1.2518091201782227,
      "learning_rate": 4.712869594941036e-05,
      "loss": 0.4259,
      "step": 10600
    },
    {
      "epoch": 0.04529683721168719,
      "grad_norm": 1.7924623489379883,
      "learning_rate": 4.7085968210562296e-05,
      "loss": 1.2653,
      "step": 10601
    },
    {
      "epoch": 0.04530111009511439,
      "grad_norm": 4.551907539367676,
      "learning_rate": 4.704324047171424e-05,
      "loss": 1.1432,
      "step": 10602
    },
    {
      "epoch": 0.04530538297854158,
      "grad_norm": 1.450020432472229,
      "learning_rate": 4.700051273286618e-05,
      "loss": 0.3294,
      "step": 10603
    },
    {
      "epoch": 0.04530965586196877,
      "grad_norm": 0.9165853261947632,
      "learning_rate": 4.6957784994018116e-05,
      "loss": 0.5213,
      "step": 10604
    },
    {
      "epoch": 0.04531392874539597,
      "grad_norm": 3.6795918941497803,
      "learning_rate": 4.6915057255170054e-05,
      "loss": 1.1235,
      "step": 10605
    },
    {
      "epoch": 0.04531820162882316,
      "grad_norm": 0.8803751468658447,
      "learning_rate": 4.6872329516322e-05,
      "loss": 0.5212,
      "step": 10606
    },
    {
      "epoch": 0.045322474512250355,
      "grad_norm": 5.210096836090088,
      "learning_rate": 4.682960177747394e-05,
      "loss": 1.3733,
      "step": 10607
    },
    {
      "epoch": 0.045326747395677554,
      "grad_norm": 1.2697958946228027,
      "learning_rate": 4.6786874038625874e-05,
      "loss": 0.4489,
      "step": 10608
    },
    {
      "epoch": 0.045331020279104746,
      "grad_norm": 0.8487837910652161,
      "learning_rate": 4.674414629977781e-05,
      "loss": 0.4837,
      "step": 10609
    },
    {
      "epoch": 0.04533529316253194,
      "grad_norm": 0.5738417506217957,
      "learning_rate": 4.670141856092976e-05,
      "loss": 0.2536,
      "step": 10610
    },
    {
      "epoch": 0.04533956604595914,
      "grad_norm": 2.750288963317871,
      "learning_rate": 4.6658690822081695e-05,
      "loss": 0.8067,
      "step": 10611
    },
    {
      "epoch": 0.04534383892938633,
      "grad_norm": 2.5341761112213135,
      "learning_rate": 4.661596308323364e-05,
      "loss": 0.7374,
      "step": 10612
    },
    {
      "epoch": 0.04534811181281352,
      "grad_norm": 1.154848575592041,
      "learning_rate": 4.657323534438558e-05,
      "loss": 0.4433,
      "step": 10613
    },
    {
      "epoch": 0.04535238469624072,
      "grad_norm": 4.191542625427246,
      "learning_rate": 4.6530507605537515e-05,
      "loss": 1.1946,
      "step": 10614
    },
    {
      "epoch": 0.04535665757966791,
      "grad_norm": 1.1896898746490479,
      "learning_rate": 4.648777986668946e-05,
      "loss": 0.4746,
      "step": 10615
    },
    {
      "epoch": 0.045360930463095105,
      "grad_norm": 2.1259970664978027,
      "learning_rate": 4.64450521278414e-05,
      "loss": 0.4466,
      "step": 10616
    },
    {
      "epoch": 0.0453652033465223,
      "grad_norm": 2.4636378288269043,
      "learning_rate": 4.6402324388993335e-05,
      "loss": 0.6356,
      "step": 10617
    },
    {
      "epoch": 0.045369476229949496,
      "grad_norm": 2.636160135269165,
      "learning_rate": 4.635959665014527e-05,
      "loss": 0.5995,
      "step": 10618
    },
    {
      "epoch": 0.04537374911337669,
      "grad_norm": 1.1777153015136719,
      "learning_rate": 4.631686891129722e-05,
      "loss": 0.5097,
      "step": 10619
    },
    {
      "epoch": 0.04537802199680388,
      "grad_norm": 3.1931586265563965,
      "learning_rate": 4.6274141172449156e-05,
      "loss": 0.6454,
      "step": 10620
    },
    {
      "epoch": 0.04538229488023108,
      "grad_norm": 1.2584103345870972,
      "learning_rate": 4.6231413433601094e-05,
      "loss": 0.9833,
      "step": 10621
    },
    {
      "epoch": 0.04538656776365827,
      "grad_norm": 1.898093342781067,
      "learning_rate": 4.618868569475303e-05,
      "loss": 0.4869,
      "step": 10622
    },
    {
      "epoch": 0.04539084064708546,
      "grad_norm": 1.4270161390304565,
      "learning_rate": 4.6145957955904976e-05,
      "loss": 0.3139,
      "step": 10623
    },
    {
      "epoch": 0.04539511353051266,
      "grad_norm": 2.741696834564209,
      "learning_rate": 4.6103230217056914e-05,
      "loss": 0.7269,
      "step": 10624
    },
    {
      "epoch": 0.045399386413939855,
      "grad_norm": 0.9856106042861938,
      "learning_rate": 4.606050247820885e-05,
      "loss": 0.397,
      "step": 10625
    },
    {
      "epoch": 0.04540365929736705,
      "grad_norm": 2.6557061672210693,
      "learning_rate": 4.6017774739360796e-05,
      "loss": 0.6012,
      "step": 10626
    },
    {
      "epoch": 0.045407932180794246,
      "grad_norm": 1.0622013807296753,
      "learning_rate": 4.5975047000512734e-05,
      "loss": 0.4341,
      "step": 10627
    },
    {
      "epoch": 0.04541220506422144,
      "grad_norm": 3.0906295776367188,
      "learning_rate": 4.593231926166468e-05,
      "loss": 1.2472,
      "step": 10628
    },
    {
      "epoch": 0.04541647794764863,
      "grad_norm": 0.8419720530509949,
      "learning_rate": 4.588959152281661e-05,
      "loss": 0.481,
      "step": 10629
    },
    {
      "epoch": 0.04542075083107583,
      "grad_norm": 1.9277145862579346,
      "learning_rate": 4.5846863783968554e-05,
      "loss": 0.4739,
      "step": 10630
    },
    {
      "epoch": 0.04542502371450302,
      "grad_norm": 2.639185905456543,
      "learning_rate": 4.580413604512049e-05,
      "loss": 0.5821,
      "step": 10631
    },
    {
      "epoch": 0.04542929659793021,
      "grad_norm": 0.8626388311386108,
      "learning_rate": 4.576140830627244e-05,
      "loss": 0.3476,
      "step": 10632
    },
    {
      "epoch": 0.04543356948135741,
      "grad_norm": 2.9821512699127197,
      "learning_rate": 4.571868056742437e-05,
      "loss": 1.0727,
      "step": 10633
    },
    {
      "epoch": 0.045437842364784604,
      "grad_norm": 1.2608178853988647,
      "learning_rate": 4.567595282857631e-05,
      "loss": 0.981,
      "step": 10634
    },
    {
      "epoch": 0.0454421152482118,
      "grad_norm": 3.345689535140991,
      "learning_rate": 4.563322508972825e-05,
      "loss": 0.7594,
      "step": 10635
    },
    {
      "epoch": 0.045446388131638996,
      "grad_norm": 1.8798130750656128,
      "learning_rate": 4.5590497350880195e-05,
      "loss": 0.5695,
      "step": 10636
    },
    {
      "epoch": 0.04545066101506619,
      "grad_norm": 2.104546308517456,
      "learning_rate": 4.5547769612032126e-05,
      "loss": 0.6316,
      "step": 10637
    },
    {
      "epoch": 0.04545493389849338,
      "grad_norm": 1.9604421854019165,
      "learning_rate": 4.550504187318407e-05,
      "loss": 0.5898,
      "step": 10638
    },
    {
      "epoch": 0.04545920678192058,
      "grad_norm": 1.5854564905166626,
      "learning_rate": 4.5462314134336015e-05,
      "loss": 0.6167,
      "step": 10639
    },
    {
      "epoch": 0.04546347966534777,
      "grad_norm": 2.0755720138549805,
      "learning_rate": 4.541958639548795e-05,
      "loss": 0.6176,
      "step": 10640
    },
    {
      "epoch": 0.04546775254877496,
      "grad_norm": 1.12257719039917,
      "learning_rate": 4.537685865663989e-05,
      "loss": 0.4748,
      "step": 10641
    },
    {
      "epoch": 0.045472025432202155,
      "grad_norm": 0.61270672082901,
      "learning_rate": 4.533413091779183e-05,
      "loss": 0.2883,
      "step": 10642
    },
    {
      "epoch": 0.045476298315629354,
      "grad_norm": 0.9389867782592773,
      "learning_rate": 4.5291403178943773e-05,
      "loss": 0.3974,
      "step": 10643
    },
    {
      "epoch": 0.045480571199056546,
      "grad_norm": 4.103515148162842,
      "learning_rate": 4.524867544009571e-05,
      "loss": 1.1063,
      "step": 10644
    },
    {
      "epoch": 0.04548484408248374,
      "grad_norm": 0.9727219343185425,
      "learning_rate": 4.520594770124765e-05,
      "loss": 0.5555,
      "step": 10645
    },
    {
      "epoch": 0.04548911696591094,
      "grad_norm": 1.8630205392837524,
      "learning_rate": 4.516321996239959e-05,
      "loss": 0.564,
      "step": 10646
    },
    {
      "epoch": 0.04549338984933813,
      "grad_norm": 0.8339328169822693,
      "learning_rate": 4.512049222355153e-05,
      "loss": 0.4753,
      "step": 10647
    },
    {
      "epoch": 0.04549766273276532,
      "grad_norm": 0.6679733395576477,
      "learning_rate": 4.507776448470347e-05,
      "loss": 0.2636,
      "step": 10648
    },
    {
      "epoch": 0.04550193561619252,
      "grad_norm": 3.3594791889190674,
      "learning_rate": 4.5035036745855414e-05,
      "loss": 0.8479,
      "step": 10649
    },
    {
      "epoch": 0.04550620849961971,
      "grad_norm": 1.1359184980392456,
      "learning_rate": 4.4992309007007345e-05,
      "loss": 0.5081,
      "step": 10650
    },
    {
      "epoch": 0.045510481383046905,
      "grad_norm": 0.9359002113342285,
      "learning_rate": 4.494958126815929e-05,
      "loss": 0.3975,
      "step": 10651
    },
    {
      "epoch": 0.045514754266474104,
      "grad_norm": 2.656900405883789,
      "learning_rate": 4.4906853529311234e-05,
      "loss": 0.7976,
      "step": 10652
    },
    {
      "epoch": 0.045519027149901296,
      "grad_norm": 0.9759321212768555,
      "learning_rate": 4.486412579046317e-05,
      "loss": 0.5792,
      "step": 10653
    },
    {
      "epoch": 0.04552330003332849,
      "grad_norm": 1.5608896017074585,
      "learning_rate": 4.482139805161511e-05,
      "loss": 0.5873,
      "step": 10654
    },
    {
      "epoch": 0.04552757291675569,
      "grad_norm": 1.2786608934402466,
      "learning_rate": 4.477867031276705e-05,
      "loss": 0.9751,
      "step": 10655
    },
    {
      "epoch": 0.04553184580018288,
      "grad_norm": 0.9553945064544678,
      "learning_rate": 4.473594257391899e-05,
      "loss": 0.5528,
      "step": 10656
    },
    {
      "epoch": 0.04553611868361007,
      "grad_norm": 2.959921360015869,
      "learning_rate": 4.469321483507093e-05,
      "loss": 0.7399,
      "step": 10657
    },
    {
      "epoch": 0.04554039156703727,
      "grad_norm": 0.9352723956108093,
      "learning_rate": 4.465048709622287e-05,
      "loss": 0.381,
      "step": 10658
    },
    {
      "epoch": 0.04554466445046446,
      "grad_norm": 0.8535937070846558,
      "learning_rate": 4.4607759357374806e-05,
      "loss": 0.3639,
      "step": 10659
    },
    {
      "epoch": 0.045548937333891655,
      "grad_norm": 0.9284709095954895,
      "learning_rate": 4.456503161852675e-05,
      "loss": 0.5372,
      "step": 10660
    },
    {
      "epoch": 0.045553210217318854,
      "grad_norm": 0.7209674715995789,
      "learning_rate": 4.452230387967869e-05,
      "loss": 0.3093,
      "step": 10661
    },
    {
      "epoch": 0.045557483100746046,
      "grad_norm": 1.872753620147705,
      "learning_rate": 4.4479576140830626e-05,
      "loss": 0.4685,
      "step": 10662
    },
    {
      "epoch": 0.04556175598417324,
      "grad_norm": 0.8301070928573608,
      "learning_rate": 4.4436848401982564e-05,
      "loss": 0.3477,
      "step": 10663
    },
    {
      "epoch": 0.04556602886760044,
      "grad_norm": 1.3218194246292114,
      "learning_rate": 4.439412066313451e-05,
      "loss": 0.5358,
      "step": 10664
    },
    {
      "epoch": 0.04557030175102763,
      "grad_norm": 4.143325328826904,
      "learning_rate": 4.4351392924286453e-05,
      "loss": 1.1146,
      "step": 10665
    },
    {
      "epoch": 0.04557457463445482,
      "grad_norm": 0.9157065749168396,
      "learning_rate": 4.4308665185438384e-05,
      "loss": 0.5372,
      "step": 10666
    },
    {
      "epoch": 0.045578847517882014,
      "grad_norm": 3.305637836456299,
      "learning_rate": 4.426593744659033e-05,
      "loss": 0.8183,
      "step": 10667
    },
    {
      "epoch": 0.04558312040130921,
      "grad_norm": 3.0706822872161865,
      "learning_rate": 4.422320970774227e-05,
      "loss": 1.7246,
      "step": 10668
    },
    {
      "epoch": 0.045587393284736405,
      "grad_norm": 3.2505316734313965,
      "learning_rate": 4.418048196889421e-05,
      "loss": 0.7208,
      "step": 10669
    },
    {
      "epoch": 0.0455916661681636,
      "grad_norm": 3.625657320022583,
      "learning_rate": 4.413775423004614e-05,
      "loss": 1.0223,
      "step": 10670
    },
    {
      "epoch": 0.045595939051590796,
      "grad_norm": 0.8869476914405823,
      "learning_rate": 4.409502649119809e-05,
      "loss": 0.5111,
      "step": 10671
    },
    {
      "epoch": 0.04560021193501799,
      "grad_norm": 0.8602466583251953,
      "learning_rate": 4.4052298752350025e-05,
      "loss": 0.3638,
      "step": 10672
    },
    {
      "epoch": 0.04560448481844518,
      "grad_norm": 3.230498790740967,
      "learning_rate": 4.400957101350197e-05,
      "loss": 0.6679,
      "step": 10673
    },
    {
      "epoch": 0.04560875770187238,
      "grad_norm": 3.258976936340332,
      "learning_rate": 4.39668432746539e-05,
      "loss": 0.7099,
      "step": 10674
    },
    {
      "epoch": 0.04561303058529957,
      "grad_norm": 0.765173614025116,
      "learning_rate": 4.3924115535805845e-05,
      "loss": 0.3032,
      "step": 10675
    },
    {
      "epoch": 0.045617303468726764,
      "grad_norm": 0.8143327236175537,
      "learning_rate": 4.388138779695778e-05,
      "loss": 0.4972,
      "step": 10676
    },
    {
      "epoch": 0.04562157635215396,
      "grad_norm": 0.6925850510597229,
      "learning_rate": 4.383866005810973e-05,
      "loss": 0.2932,
      "step": 10677
    },
    {
      "epoch": 0.045625849235581155,
      "grad_norm": 1.5492238998413086,
      "learning_rate": 4.3795932319261666e-05,
      "loss": 0.5777,
      "step": 10678
    },
    {
      "epoch": 0.04563012211900835,
      "grad_norm": 0.8107150793075562,
      "learning_rate": 4.3753204580413604e-05,
      "loss": 0.4967,
      "step": 10679
    },
    {
      "epoch": 0.045634395002435546,
      "grad_norm": 0.845981776714325,
      "learning_rate": 4.371047684156555e-05,
      "loss": 0.4969,
      "step": 10680
    },
    {
      "epoch": 0.04563866788586274,
      "grad_norm": 3.7861130237579346,
      "learning_rate": 4.3667749102717486e-05,
      "loss": 2.5646,
      "step": 10681
    },
    {
      "epoch": 0.04564294076928993,
      "grad_norm": 2.0834689140319824,
      "learning_rate": 4.3625021363869424e-05,
      "loss": 0.5939,
      "step": 10682
    },
    {
      "epoch": 0.04564721365271713,
      "grad_norm": 0.8561863303184509,
      "learning_rate": 4.358229362502136e-05,
      "loss": 0.3478,
      "step": 10683
    },
    {
      "epoch": 0.04565148653614432,
      "grad_norm": 1.2877830266952515,
      "learning_rate": 4.3539565886173306e-05,
      "loss": 0.4874,
      "step": 10684
    },
    {
      "epoch": 0.045655759419571514,
      "grad_norm": 0.8481557965278625,
      "learning_rate": 4.3496838147325244e-05,
      "loss": 0.4973,
      "step": 10685
    },
    {
      "epoch": 0.04566003230299871,
      "grad_norm": 1.8842724561691284,
      "learning_rate": 4.345411040847719e-05,
      "loss": 1.3094,
      "step": 10686
    },
    {
      "epoch": 0.045664305186425905,
      "grad_norm": 3.086143970489502,
      "learning_rate": 4.341138266962912e-05,
      "loss": 1.7472,
      "step": 10687
    },
    {
      "epoch": 0.0456685780698531,
      "grad_norm": 0.793986439704895,
      "learning_rate": 4.3368654930781064e-05,
      "loss": 0.352,
      "step": 10688
    },
    {
      "epoch": 0.045672850953280296,
      "grad_norm": 0.7022770643234253,
      "learning_rate": 4.3325927191933e-05,
      "loss": 0.3091,
      "step": 10689
    },
    {
      "epoch": 0.04567712383670749,
      "grad_norm": 0.9307901263237,
      "learning_rate": 4.328319945308495e-05,
      "loss": 0.3657,
      "step": 10690
    },
    {
      "epoch": 0.04568139672013468,
      "grad_norm": 0.659398078918457,
      "learning_rate": 4.3240471714236885e-05,
      "loss": 0.2415,
      "step": 10691
    },
    {
      "epoch": 0.04568566960356187,
      "grad_norm": 0.7822248339653015,
      "learning_rate": 4.319774397538882e-05,
      "loss": 0.3345,
      "step": 10692
    },
    {
      "epoch": 0.04568994248698907,
      "grad_norm": 2.087674379348755,
      "learning_rate": 4.315501623654077e-05,
      "loss": 0.5926,
      "step": 10693
    },
    {
      "epoch": 0.04569421537041626,
      "grad_norm": 1.8725519180297852,
      "learning_rate": 4.3112288497692705e-05,
      "loss": 1.3093,
      "step": 10694
    },
    {
      "epoch": 0.045698488253843456,
      "grad_norm": 0.9787512421607971,
      "learning_rate": 4.306956075884464e-05,
      "loss": 0.3972,
      "step": 10695
    },
    {
      "epoch": 0.045702761137270655,
      "grad_norm": 2.6516733169555664,
      "learning_rate": 4.302683301999658e-05,
      "loss": 0.7746,
      "step": 10696
    },
    {
      "epoch": 0.04570703402069785,
      "grad_norm": 1.257642388343811,
      "learning_rate": 4.2984105281148525e-05,
      "loss": 0.9518,
      "step": 10697
    },
    {
      "epoch": 0.04571130690412504,
      "grad_norm": 1.8434092998504639,
      "learning_rate": 4.294137754230046e-05,
      "loss": 1.2877,
      "step": 10698
    },
    {
      "epoch": 0.04571557978755224,
      "grad_norm": 3.236828565597534,
      "learning_rate": 4.28986498034524e-05,
      "loss": 0.6416,
      "step": 10699
    },
    {
      "epoch": 0.04571985267097943,
      "grad_norm": 2.6495044231414795,
      "learning_rate": 4.285592206460434e-05,
      "loss": 0.7657,
      "step": 10700
    },
    {
      "epoch": 0.04572412555440662,
      "grad_norm": 0.6258442401885986,
      "learning_rate": 4.2813194325756283e-05,
      "loss": 0.1678,
      "step": 10701
    },
    {
      "epoch": 0.04572839843783382,
      "grad_norm": 1.2825826406478882,
      "learning_rate": 4.277046658690822e-05,
      "loss": 1.003,
      "step": 10702
    },
    {
      "epoch": 0.04573267132126101,
      "grad_norm": 0.7961178421974182,
      "learning_rate": 4.272773884806016e-05,
      "loss": 0.3188,
      "step": 10703
    },
    {
      "epoch": 0.045736944204688205,
      "grad_norm": 0.95131516456604,
      "learning_rate": 4.2685011109212104e-05,
      "loss": 0.366,
      "step": 10704
    },
    {
      "epoch": 0.045741217088115405,
      "grad_norm": 2.0619990825653076,
      "learning_rate": 4.264228337036404e-05,
      "loss": 0.551,
      "step": 10705
    },
    {
      "epoch": 0.0457454899715426,
      "grad_norm": 0.7149901986122131,
      "learning_rate": 4.2599555631515986e-05,
      "loss": 0.309,
      "step": 10706
    },
    {
      "epoch": 0.04574976285496979,
      "grad_norm": 2.76190185546875,
      "learning_rate": 4.255682789266792e-05,
      "loss": 0.7353,
      "step": 10707
    },
    {
      "epoch": 0.04575403573839699,
      "grad_norm": 4.204176902770996,
      "learning_rate": 4.251410015381986e-05,
      "loss": 1.0595,
      "step": 10708
    },
    {
      "epoch": 0.04575830862182418,
      "grad_norm": 1.0274667739868164,
      "learning_rate": 4.24713724149718e-05,
      "loss": 0.4586,
      "step": 10709
    },
    {
      "epoch": 0.04576258150525137,
      "grad_norm": 4.197997570037842,
      "learning_rate": 4.2428644676123744e-05,
      "loss": 1.2447,
      "step": 10710
    },
    {
      "epoch": 0.04576685438867857,
      "grad_norm": 0.8702925443649292,
      "learning_rate": 4.2385916937275675e-05,
      "loss": 0.3478,
      "step": 10711
    },
    {
      "epoch": 0.04577112727210576,
      "grad_norm": 1.2341797351837158,
      "learning_rate": 4.234318919842762e-05,
      "loss": 0.9519,
      "step": 10712
    },
    {
      "epoch": 0.045775400155532955,
      "grad_norm": 3.2217202186584473,
      "learning_rate": 4.230046145957956e-05,
      "loss": 0.6338,
      "step": 10713
    },
    {
      "epoch": 0.045779673038960154,
      "grad_norm": 3.198457956314087,
      "learning_rate": 4.22577337207315e-05,
      "loss": 0.6116,
      "step": 10714
    },
    {
      "epoch": 0.04578394592238735,
      "grad_norm": 0.8232063055038452,
      "learning_rate": 4.2215005981883434e-05,
      "loss": 0.5146,
      "step": 10715
    },
    {
      "epoch": 0.04578821880581454,
      "grad_norm": 2.524118185043335,
      "learning_rate": 4.217227824303538e-05,
      "loss": 0.7577,
      "step": 10716
    },
    {
      "epoch": 0.04579249168924173,
      "grad_norm": 0.6249817609786987,
      "learning_rate": 4.212955050418732e-05,
      "loss": 0.1677,
      "step": 10717
    },
    {
      "epoch": 0.04579676457266893,
      "grad_norm": 0.5573717951774597,
      "learning_rate": 4.208682276533926e-05,
      "loss": 0.1573,
      "step": 10718
    },
    {
      "epoch": 0.04580103745609612,
      "grad_norm": 3.5788495540618896,
      "learning_rate": 4.20440950264912e-05,
      "loss": 0.9177,
      "step": 10719
    },
    {
      "epoch": 0.045805310339523314,
      "grad_norm": 0.849353551864624,
      "learning_rate": 4.2001367287643136e-05,
      "loss": 0.3477,
      "step": 10720
    },
    {
      "epoch": 0.04580958322295051,
      "grad_norm": 0.8939651250839233,
      "learning_rate": 4.195863954879508e-05,
      "loss": 0.5442,
      "step": 10721
    },
    {
      "epoch": 0.045813856106377705,
      "grad_norm": 0.8756990432739258,
      "learning_rate": 4.191591180994702e-05,
      "loss": 0.5146,
      "step": 10722
    },
    {
      "epoch": 0.0458181289898049,
      "grad_norm": 0.8667355179786682,
      "learning_rate": 4.1873184071098963e-05,
      "loss": 0.5144,
      "step": 10723
    },
    {
      "epoch": 0.045822401873232096,
      "grad_norm": 2.0087552070617676,
      "learning_rate": 4.1830456332250895e-05,
      "loss": 0.6037,
      "step": 10724
    },
    {
      "epoch": 0.04582667475665929,
      "grad_norm": 1.290449619293213,
      "learning_rate": 4.178772859340284e-05,
      "loss": 0.4257,
      "step": 10725
    },
    {
      "epoch": 0.04583094764008648,
      "grad_norm": 0.860126793384552,
      "learning_rate": 4.174500085455478e-05,
      "loss": 0.3638,
      "step": 10726
    },
    {
      "epoch": 0.04583522052351368,
      "grad_norm": 0.7854350209236145,
      "learning_rate": 4.170227311570672e-05,
      "loss": 0.3186,
      "step": 10727
    },
    {
      "epoch": 0.04583949340694087,
      "grad_norm": 0.9221609234809875,
      "learning_rate": 4.165954537685865e-05,
      "loss": 0.3809,
      "step": 10728
    },
    {
      "epoch": 0.045843766290368064,
      "grad_norm": 0.8685025572776794,
      "learning_rate": 4.16168176380106e-05,
      "loss": 0.5199,
      "step": 10729
    },
    {
      "epoch": 0.04584803917379526,
      "grad_norm": 2.644355535507202,
      "learning_rate": 4.157408989916254e-05,
      "loss": 0.7136,
      "step": 10730
    },
    {
      "epoch": 0.045852312057222455,
      "grad_norm": 0.7459315657615662,
      "learning_rate": 4.153136216031448e-05,
      "loss": 0.2417,
      "step": 10731
    },
    {
      "epoch": 0.04585658494064965,
      "grad_norm": 2.0037107467651367,
      "learning_rate": 4.148863442146642e-05,
      "loss": 0.6006,
      "step": 10732
    },
    {
      "epoch": 0.045860857824076846,
      "grad_norm": 0.8199135661125183,
      "learning_rate": 4.1445906682618355e-05,
      "loss": 0.3324,
      "step": 10733
    },
    {
      "epoch": 0.04586513070750404,
      "grad_norm": 1.0092213153839111,
      "learning_rate": 4.14031789437703e-05,
      "loss": 0.4178,
      "step": 10734
    },
    {
      "epoch": 0.04586940359093123,
      "grad_norm": 3.330458641052246,
      "learning_rate": 4.136045120492224e-05,
      "loss": 0.816,
      "step": 10735
    },
    {
      "epoch": 0.04587367647435843,
      "grad_norm": 0.7863191962242126,
      "learning_rate": 4.1317723466074176e-05,
      "loss": 0.3347,
      "step": 10736
    },
    {
      "epoch": 0.04587794935778562,
      "grad_norm": 1.9742567539215088,
      "learning_rate": 4.1274995727226114e-05,
      "loss": 0.5675,
      "step": 10737
    },
    {
      "epoch": 0.045882222241212814,
      "grad_norm": 2.5801844596862793,
      "learning_rate": 4.123226798837806e-05,
      "loss": 0.7873,
      "step": 10738
    },
    {
      "epoch": 0.04588649512464001,
      "grad_norm": 1.859386682510376,
      "learning_rate": 4.1189540249529996e-05,
      "loss": 0.631,
      "step": 10739
    },
    {
      "epoch": 0.045890768008067205,
      "grad_norm": 3.7546119689941406,
      "learning_rate": 4.1146812510681934e-05,
      "loss": 2.5377,
      "step": 10740
    },
    {
      "epoch": 0.0458950408914944,
      "grad_norm": 0.8277755975723267,
      "learning_rate": 4.110408477183387e-05,
      "loss": 0.3325,
      "step": 10741
    },
    {
      "epoch": 0.04589931377492159,
      "grad_norm": 4.208890914916992,
      "learning_rate": 4.1061357032985816e-05,
      "loss": 1.2106,
      "step": 10742
    },
    {
      "epoch": 0.04590358665834879,
      "grad_norm": 3.602405309677124,
      "learning_rate": 4.101862929413776e-05,
      "loss": 0.9246,
      "step": 10743
    },
    {
      "epoch": 0.04590785954177598,
      "grad_norm": 0.8447583913803101,
      "learning_rate": 4.097590155528969e-05,
      "loss": 0.3639,
      "step": 10744
    },
    {
      "epoch": 0.04591213242520317,
      "grad_norm": 3.695496082305908,
      "learning_rate": 4.093317381644164e-05,
      "loss": 1.1647,
      "step": 10745
    },
    {
      "epoch": 0.04591640530863037,
      "grad_norm": 1.4611297845840454,
      "learning_rate": 4.0890446077593574e-05,
      "loss": 0.3209,
      "step": 10746
    },
    {
      "epoch": 0.045920678192057564,
      "grad_norm": 0.8494464159011841,
      "learning_rate": 4.084771833874552e-05,
      "loss": 0.5146,
      "step": 10747
    },
    {
      "epoch": 0.045924951075484756,
      "grad_norm": 0.5623854398727417,
      "learning_rate": 4.080499059989745e-05,
      "loss": 0.2702,
      "step": 10748
    },
    {
      "epoch": 0.045929223958911955,
      "grad_norm": 1.0143777132034302,
      "learning_rate": 4.0762262861049395e-05,
      "loss": 0.418,
      "step": 10749
    },
    {
      "epoch": 0.04593349684233915,
      "grad_norm": 4.488645553588867,
      "learning_rate": 4.071953512220133e-05,
      "loss": 1.1206,
      "step": 10750
    },
    {
      "epoch": 0.04593776972576634,
      "grad_norm": 0.840822696685791,
      "learning_rate": 4.067680738335328e-05,
      "loss": 0.5145,
      "step": 10751
    },
    {
      "epoch": 0.04594204260919354,
      "grad_norm": 0.8485919237136841,
      "learning_rate": 4.063407964450521e-05,
      "loss": 0.5146,
      "step": 10752
    },
    {
      "epoch": 0.04594631549262073,
      "grad_norm": 3.4958763122558594,
      "learning_rate": 4.059135190565715e-05,
      "loss": 0.8396,
      "step": 10753
    },
    {
      "epoch": 0.04595058837604792,
      "grad_norm": 1.799964427947998,
      "learning_rate": 4.054862416680909e-05,
      "loss": 1.2878,
      "step": 10754
    },
    {
      "epoch": 0.04595486125947512,
      "grad_norm": 4.600287914276123,
      "learning_rate": 4.0505896427961035e-05,
      "loss": 1.1144,
      "step": 10755
    },
    {
      "epoch": 0.045959134142902314,
      "grad_norm": 4.1963043212890625,
      "learning_rate": 4.0463168689112966e-05,
      "loss": 1.0976,
      "step": 10756
    },
    {
      "epoch": 0.045963407026329506,
      "grad_norm": 0.8392513394355774,
      "learning_rate": 4.042044095026491e-05,
      "loss": 0.3639,
      "step": 10757
    },
    {
      "epoch": 0.045967679909756705,
      "grad_norm": 2.0613746643066406,
      "learning_rate": 4.0377713211416856e-05,
      "loss": 0.5442,
      "step": 10758
    },
    {
      "epoch": 0.0459719527931839,
      "grad_norm": 0.8298138380050659,
      "learning_rate": 4.0334985472568794e-05,
      "loss": 0.3637,
      "step": 10759
    },
    {
      "epoch": 0.04597622567661109,
      "grad_norm": 4.454985618591309,
      "learning_rate": 4.029225773372074e-05,
      "loss": 1.0674,
      "step": 10760
    },
    {
      "epoch": 0.04598049856003829,
      "grad_norm": 3.048480987548828,
      "learning_rate": 4.024952999487267e-05,
      "loss": 1.0997,
      "step": 10761
    },
    {
      "epoch": 0.04598477144346548,
      "grad_norm": 0.689191997051239,
      "learning_rate": 4.0206802256024614e-05,
      "loss": 0.3092,
      "step": 10762
    },
    {
      "epoch": 0.04598904432689267,
      "grad_norm": 1.574845552444458,
      "learning_rate": 4.016407451717655e-05,
      "loss": 0.5552,
      "step": 10763
    },
    {
      "epoch": 0.04599331721031987,
      "grad_norm": 1.7875112295150757,
      "learning_rate": 4.0121346778328496e-05,
      "loss": 1.2304,
      "step": 10764
    },
    {
      "epoch": 0.045997590093747064,
      "grad_norm": 2.349142551422119,
      "learning_rate": 4.007861903948043e-05,
      "loss": 0.7512,
      "step": 10765
    },
    {
      "epoch": 0.046001862977174256,
      "grad_norm": 3.0822269916534424,
      "learning_rate": 4.003589130063237e-05,
      "loss": 0.8655,
      "step": 10766
    },
    {
      "epoch": 0.04600613586060145,
      "grad_norm": 0.8212730884552002,
      "learning_rate": 3.999316356178431e-05,
      "loss": 0.3329,
      "step": 10767
    },
    {
      "epoch": 0.04601040874402865,
      "grad_norm": 4.451883316040039,
      "learning_rate": 3.9950435822936254e-05,
      "loss": 1.0362,
      "step": 10768
    },
    {
      "epoch": 0.04601468162745584,
      "grad_norm": 3.0465545654296875,
      "learning_rate": 3.9907708084088185e-05,
      "loss": 1.7253,
      "step": 10769
    },
    {
      "epoch": 0.04601895451088303,
      "grad_norm": 4.685257434844971,
      "learning_rate": 3.986498034524013e-05,
      "loss": 1.2132,
      "step": 10770
    },
    {
      "epoch": 0.04602322739431023,
      "grad_norm": 3.046470880508423,
      "learning_rate": 3.9822252606392075e-05,
      "loss": 1.7253,
      "step": 10771
    },
    {
      "epoch": 0.04602750027773742,
      "grad_norm": 0.5068594813346863,
      "learning_rate": 3.977952486754401e-05,
      "loss": 0.1493,
      "step": 10772
    },
    {
      "epoch": 0.046031773161164614,
      "grad_norm": 3.341662645339966,
      "learning_rate": 3.973679712869595e-05,
      "loss": 0.8673,
      "step": 10773
    },
    {
      "epoch": 0.046036046044591813,
      "grad_norm": 0.7214281558990479,
      "learning_rate": 3.969406938984789e-05,
      "loss": 0.3033,
      "step": 10774
    },
    {
      "epoch": 0.046040318928019006,
      "grad_norm": 0.6506823301315308,
      "learning_rate": 3.965134165099983e-05,
      "loss": 0.2778,
      "step": 10775
    },
    {
      "epoch": 0.0460445918114462,
      "grad_norm": 3.2466416358947754,
      "learning_rate": 3.960861391215177e-05,
      "loss": 0.5816,
      "step": 10776
    },
    {
      "epoch": 0.0460488646948734,
      "grad_norm": 3.023150682449341,
      "learning_rate": 3.956588617330371e-05,
      "loss": 1.0599,
      "step": 10777
    },
    {
      "epoch": 0.04605313757830059,
      "grad_norm": 0.8125914931297302,
      "learning_rate": 3.9523158434455646e-05,
      "loss": 0.4817,
      "step": 10778
    },
    {
      "epoch": 0.04605741046172778,
      "grad_norm": 2.9955177307128906,
      "learning_rate": 3.948043069560759e-05,
      "loss": 1.7317,
      "step": 10779
    },
    {
      "epoch": 0.04606168334515498,
      "grad_norm": 2.9961438179016113,
      "learning_rate": 3.943770295675953e-05,
      "loss": 1.7315,
      "step": 10780
    },
    {
      "epoch": 0.04606595622858217,
      "grad_norm": 0.7382144927978516,
      "learning_rate": 3.939497521791147e-05,
      "loss": 0.3032,
      "step": 10781
    },
    {
      "epoch": 0.046070229112009364,
      "grad_norm": 1.212519884109497,
      "learning_rate": 3.9352247479063405e-05,
      "loss": 0.9839,
      "step": 10782
    },
    {
      "epoch": 0.04607450199543656,
      "grad_norm": 3.688591957092285,
      "learning_rate": 3.930951974021535e-05,
      "loss": 1.1155,
      "step": 10783
    },
    {
      "epoch": 0.046078774878863756,
      "grad_norm": 2.257657051086426,
      "learning_rate": 3.9266792001367294e-05,
      "loss": 0.5009,
      "step": 10784
    },
    {
      "epoch": 0.04608304776229095,
      "grad_norm": 1.867989182472229,
      "learning_rate": 3.9224064262519225e-05,
      "loss": 0.6455,
      "step": 10785
    },
    {
      "epoch": 0.04608732064571815,
      "grad_norm": 4.166576862335205,
      "learning_rate": 3.918133652367117e-05,
      "loss": 1.0725,
      "step": 10786
    },
    {
      "epoch": 0.04609159352914534,
      "grad_norm": 0.722039520740509,
      "learning_rate": 3.913860878482311e-05,
      "loss": 0.289,
      "step": 10787
    },
    {
      "epoch": 0.04609586641257253,
      "grad_norm": 0.6631450653076172,
      "learning_rate": 3.909588104597505e-05,
      "loss": 0.2781,
      "step": 10788
    },
    {
      "epoch": 0.04610013929599973,
      "grad_norm": 3.0773262977600098,
      "learning_rate": 3.905315330712698e-05,
      "loss": 0.8285,
      "step": 10789
    },
    {
      "epoch": 0.04610441217942692,
      "grad_norm": 4.43743896484375,
      "learning_rate": 3.901042556827893e-05,
      "loss": 0.9798,
      "step": 10790
    },
    {
      "epoch": 0.046108685062854114,
      "grad_norm": 0.7785571217536926,
      "learning_rate": 3.8967697829430865e-05,
      "loss": 0.5479,
      "step": 10791
    },
    {
      "epoch": 0.046112957946281306,
      "grad_norm": 0.8005040287971497,
      "learning_rate": 3.892497009058281e-05,
      "loss": 0.3184,
      "step": 10792
    },
    {
      "epoch": 0.046117230829708505,
      "grad_norm": 0.8006232380867004,
      "learning_rate": 3.888224235173474e-05,
      "loss": 0.364,
      "step": 10793
    },
    {
      "epoch": 0.0461215037131357,
      "grad_norm": 0.8133578300476074,
      "learning_rate": 3.8839514612886686e-05,
      "loss": 0.3639,
      "step": 10794
    },
    {
      "epoch": 0.04612577659656289,
      "grad_norm": 1.8384437561035156,
      "learning_rate": 3.8796786874038624e-05,
      "loss": 0.6219,
      "step": 10795
    },
    {
      "epoch": 0.04613004947999009,
      "grad_norm": 1.5743303298950195,
      "learning_rate": 3.875405913519057e-05,
      "loss": 0.5668,
      "step": 10796
    },
    {
      "epoch": 0.04613432236341728,
      "grad_norm": 1.9302632808685303,
      "learning_rate": 3.8711331396342506e-05,
      "loss": 0.6058,
      "step": 10797
    },
    {
      "epoch": 0.04613859524684447,
      "grad_norm": 2.550912857055664,
      "learning_rate": 3.8668603657494444e-05,
      "loss": 0.7622,
      "step": 10798
    },
    {
      "epoch": 0.04614286813027167,
      "grad_norm": 0.7881860733032227,
      "learning_rate": 3.862587591864639e-05,
      "loss": 0.3325,
      "step": 10799
    },
    {
      "epoch": 0.046147141013698864,
      "grad_norm": 0.44577327370643616,
      "learning_rate": 3.8583148179798326e-05,
      "loss": 0.1339,
      "step": 10800
    },
    {
      "epoch": 0.046151413897126056,
      "grad_norm": 2.268611431121826,
      "learning_rate": 3.854042044095027e-05,
      "loss": 0.5851,
      "step": 10801
    },
    {
      "epoch": 0.046155686780553255,
      "grad_norm": 0.7971221804618835,
      "learning_rate": 3.84976927021022e-05,
      "loss": 0.5595,
      "step": 10802
    },
    {
      "epoch": 0.04615995966398045,
      "grad_norm": 2.537503957748413,
      "learning_rate": 3.845496496325415e-05,
      "loss": 0.6353,
      "step": 10803
    },
    {
      "epoch": 0.04616423254740764,
      "grad_norm": 1.9450206756591797,
      "learning_rate": 3.8412237224406084e-05,
      "loss": 0.5573,
      "step": 10804
    },
    {
      "epoch": 0.04616850543083484,
      "grad_norm": 0.6576109528541565,
      "learning_rate": 3.836950948555803e-05,
      "loss": 0.2929,
      "step": 10805
    },
    {
      "epoch": 0.04617277831426203,
      "grad_norm": 1.1632837057113647,
      "learning_rate": 3.832678174670996e-05,
      "loss": 0.932,
      "step": 10806
    },
    {
      "epoch": 0.04617705119768922,
      "grad_norm": 3.46816086769104,
      "learning_rate": 3.8284054007861905e-05,
      "loss": 0.8069,
      "step": 10807
    },
    {
      "epoch": 0.04618132408111642,
      "grad_norm": 0.8261395692825317,
      "learning_rate": 3.824132626901384e-05,
      "loss": 0.4947,
      "step": 10808
    },
    {
      "epoch": 0.046185596964543614,
      "grad_norm": 0.3795689642429352,
      "learning_rate": 3.819859853016579e-05,
      "loss": 0.1139,
      "step": 10809
    },
    {
      "epoch": 0.046189869847970806,
      "grad_norm": 0.8047814965248108,
      "learning_rate": 3.8155870791317725e-05,
      "loss": 0.4839,
      "step": 10810
    },
    {
      "epoch": 0.046194142731398005,
      "grad_norm": 1.1658391952514648,
      "learning_rate": 3.811314305246966e-05,
      "loss": 0.961,
      "step": 10811
    },
    {
      "epoch": 0.0461984156148252,
      "grad_norm": 0.8811525106430054,
      "learning_rate": 3.807041531362161e-05,
      "loss": 0.397,
      "step": 10812
    },
    {
      "epoch": 0.04620268849825239,
      "grad_norm": 3.6457326412200928,
      "learning_rate": 3.8027687574773545e-05,
      "loss": 1.0815,
      "step": 10813
    },
    {
      "epoch": 0.04620696138167958,
      "grad_norm": 3.5838394165039062,
      "learning_rate": 3.798495983592548e-05,
      "loss": 0.9654,
      "step": 10814
    },
    {
      "epoch": 0.04621123426510678,
      "grad_norm": 2.510765552520752,
      "learning_rate": 3.794223209707742e-05,
      "loss": 0.7367,
      "step": 10815
    },
    {
      "epoch": 0.04621550714853397,
      "grad_norm": 1.161501169204712,
      "learning_rate": 3.7899504358229366e-05,
      "loss": 0.9609,
      "step": 10816
    },
    {
      "epoch": 0.046219780031961165,
      "grad_norm": 2.800442934036255,
      "learning_rate": 3.7856776619381304e-05,
      "loss": 0.7127,
      "step": 10817
    },
    {
      "epoch": 0.046224052915388364,
      "grad_norm": 3.077448606491089,
      "learning_rate": 3.781404888053324e-05,
      "loss": 0.771,
      "step": 10818
    },
    {
      "epoch": 0.046228325798815556,
      "grad_norm": 0.6464110612869263,
      "learning_rate": 3.777132114168518e-05,
      "loss": 0.2639,
      "step": 10819
    },
    {
      "epoch": 0.04623259868224275,
      "grad_norm": 4.988013744354248,
      "learning_rate": 3.7728593402837124e-05,
      "loss": 1.1282,
      "step": 10820
    },
    {
      "epoch": 0.04623687156566995,
      "grad_norm": 4.694192409515381,
      "learning_rate": 3.768586566398906e-05,
      "loss": 1.2267,
      "step": 10821
    },
    {
      "epoch": 0.04624114444909714,
      "grad_norm": 0.8016082048416138,
      "learning_rate": 3.7643137925141e-05,
      "loss": 0.3188,
      "step": 10822
    },
    {
      "epoch": 0.04624541733252433,
      "grad_norm": 0.8688449859619141,
      "learning_rate": 3.7600410186292944e-05,
      "loss": 0.3809,
      "step": 10823
    },
    {
      "epoch": 0.04624969021595153,
      "grad_norm": 2.7741081714630127,
      "learning_rate": 3.755768244744488e-05,
      "loss": 0.6982,
      "step": 10824
    },
    {
      "epoch": 0.04625396309937872,
      "grad_norm": 3.0766854286193848,
      "learning_rate": 3.7514954708596827e-05,
      "loss": 0.7423,
      "step": 10825
    },
    {
      "epoch": 0.046258235982805915,
      "grad_norm": 0.7829949259757996,
      "learning_rate": 3.747222696974876e-05,
      "loss": 0.3478,
      "step": 10826
    },
    {
      "epoch": 0.046262508866233114,
      "grad_norm": 1.6950929164886475,
      "learning_rate": 3.74294992309007e-05,
      "loss": 1.2109,
      "step": 10827
    },
    {
      "epoch": 0.046266781749660306,
      "grad_norm": 1.1866257190704346,
      "learning_rate": 3.738677149205264e-05,
      "loss": 0.9839,
      "step": 10828
    },
    {
      "epoch": 0.0462710546330875,
      "grad_norm": 2.567938804626465,
      "learning_rate": 3.7344043753204585e-05,
      "loss": 0.7463,
      "step": 10829
    },
    {
      "epoch": 0.0462753275165147,
      "grad_norm": 1.9101905822753906,
      "learning_rate": 3.7301316014356516e-05,
      "loss": 0.546,
      "step": 10830
    },
    {
      "epoch": 0.04627960039994189,
      "grad_norm": 0.7907090187072754,
      "learning_rate": 3.725858827550846e-05,
      "loss": 0.557,
      "step": 10831
    },
    {
      "epoch": 0.04628387328336908,
      "grad_norm": 1.0749683380126953,
      "learning_rate": 3.72158605366604e-05,
      "loss": 0.4882,
      "step": 10832
    },
    {
      "epoch": 0.04628814616679628,
      "grad_norm": 3.528748035430908,
      "learning_rate": 3.717313279781234e-05,
      "loss": 0.9083,
      "step": 10833
    },
    {
      "epoch": 0.04629241905022347,
      "grad_norm": 1.938951015472412,
      "learning_rate": 3.7130405058964274e-05,
      "loss": 0.555,
      "step": 10834
    },
    {
      "epoch": 0.046296691933650665,
      "grad_norm": 3.527963399887085,
      "learning_rate": 3.708767732011622e-05,
      "loss": 0.9034,
      "step": 10835
    },
    {
      "epoch": 0.046300964817077864,
      "grad_norm": 2.942833662033081,
      "learning_rate": 3.704494958126816e-05,
      "loss": 1.6811,
      "step": 10836
    },
    {
      "epoch": 0.046305237700505056,
      "grad_norm": 3.2333927154541016,
      "learning_rate": 3.70022218424201e-05,
      "loss": 0.5867,
      "step": 10837
    },
    {
      "epoch": 0.04630951058393225,
      "grad_norm": 0.712724506855011,
      "learning_rate": 3.6959494103572046e-05,
      "loss": 0.2892,
      "step": 10838
    },
    {
      "epoch": 0.04631378346735944,
      "grad_norm": 1.0745458602905273,
      "learning_rate": 3.691676636472398e-05,
      "loss": 0.475,
      "step": 10839
    },
    {
      "epoch": 0.04631805635078664,
      "grad_norm": 0.7019109725952148,
      "learning_rate": 3.687403862587592e-05,
      "loss": 0.2892,
      "step": 10840
    },
    {
      "epoch": 0.04632232923421383,
      "grad_norm": 1.930776596069336,
      "learning_rate": 3.683131088702786e-05,
      "loss": 0.6223,
      "step": 10841
    },
    {
      "epoch": 0.04632660211764102,
      "grad_norm": 2.932664394378662,
      "learning_rate": 3.6788583148179804e-05,
      "loss": 1.6486,
      "step": 10842
    },
    {
      "epoch": 0.04633087500106822,
      "grad_norm": 0.8406738638877869,
      "learning_rate": 3.6745855409331735e-05,
      "loss": 0.5236,
      "step": 10843
    },
    {
      "epoch": 0.046335147884495415,
      "grad_norm": 2.507957696914673,
      "learning_rate": 3.670312767048368e-05,
      "loss": 0.7162,
      "step": 10844
    },
    {
      "epoch": 0.04633942076792261,
      "grad_norm": 2.2551333904266357,
      "learning_rate": 3.666039993163562e-05,
      "loss": 0.7125,
      "step": 10845
    },
    {
      "epoch": 0.046343693651349806,
      "grad_norm": 2.245224714279175,
      "learning_rate": 3.661767219278756e-05,
      "loss": 0.7173,
      "step": 10846
    },
    {
      "epoch": 0.046347966534777,
      "grad_norm": 1.927490472793579,
      "learning_rate": 3.657494445393949e-05,
      "loss": 0.4804,
      "step": 10847
    },
    {
      "epoch": 0.04635223941820419,
      "grad_norm": 0.9878250956535339,
      "learning_rate": 3.653221671509144e-05,
      "loss": 0.4181,
      "step": 10848
    },
    {
      "epoch": 0.04635651230163139,
      "grad_norm": 4.199052333831787,
      "learning_rate": 3.648948897624338e-05,
      "loss": 1.248,
      "step": 10849
    },
    {
      "epoch": 0.04636078518505858,
      "grad_norm": 1.935402750968933,
      "learning_rate": 3.644676123739532e-05,
      "loss": 0.4663,
      "step": 10850
    },
    {
      "epoch": 0.04636505806848577,
      "grad_norm": 0.8551315665245056,
      "learning_rate": 3.640403349854726e-05,
      "loss": 0.3659,
      "step": 10851
    },
    {
      "epoch": 0.04636933095191297,
      "grad_norm": 3.235076665878296,
      "learning_rate": 3.6361305759699196e-05,
      "loss": 0.572,
      "step": 10852
    },
    {
      "epoch": 0.046373603835340164,
      "grad_norm": 0.6435268521308899,
      "learning_rate": 3.631857802085114e-05,
      "loss": 0.278,
      "step": 10853
    },
    {
      "epoch": 0.04637787671876736,
      "grad_norm": 3.4020299911499023,
      "learning_rate": 3.627585028200308e-05,
      "loss": 1.1562,
      "step": 10854
    },
    {
      "epoch": 0.046382149602194556,
      "grad_norm": 1.6646804809570312,
      "learning_rate": 3.6233122543155016e-05,
      "loss": 1.19,
      "step": 10855
    },
    {
      "epoch": 0.04638642248562175,
      "grad_norm": 1.9178475141525269,
      "learning_rate": 3.6190394804306954e-05,
      "loss": 0.5392,
      "step": 10856
    },
    {
      "epoch": 0.04639069536904894,
      "grad_norm": 0.7085150480270386,
      "learning_rate": 3.61476670654589e-05,
      "loss": 0.2892,
      "step": 10857
    },
    {
      "epoch": 0.04639496825247614,
      "grad_norm": 0.8432497382164001,
      "learning_rate": 3.6104939326610836e-05,
      "loss": 0.5026,
      "step": 10858
    },
    {
      "epoch": 0.04639924113590333,
      "grad_norm": 0.7106154561042786,
      "learning_rate": 3.6062211587762774e-05,
      "loss": 0.2893,
      "step": 10859
    },
    {
      "epoch": 0.04640351401933052,
      "grad_norm": 0.8577727675437927,
      "learning_rate": 3.601948384891471e-05,
      "loss": 0.5347,
      "step": 10860
    },
    {
      "epoch": 0.04640778690275772,
      "grad_norm": 1.9067153930664062,
      "learning_rate": 3.597675611006666e-05,
      "loss": 0.4369,
      "step": 10861
    },
    {
      "epoch": 0.046412059786184914,
      "grad_norm": 1.9002095460891724,
      "learning_rate": 3.59340283712186e-05,
      "loss": 0.603,
      "step": 10862
    },
    {
      "epoch": 0.046416332669612106,
      "grad_norm": 0.7829994559288025,
      "learning_rate": 3.589130063237053e-05,
      "loss": 0.5596,
      "step": 10863
    },
    {
      "epoch": 0.0464206055530393,
      "grad_norm": 0.8419756889343262,
      "learning_rate": 3.584857289352248e-05,
      "loss": 0.5026,
      "step": 10864
    },
    {
      "epoch": 0.0464248784364665,
      "grad_norm": 0.7824654579162598,
      "learning_rate": 3.5805845154674415e-05,
      "loss": 0.3055,
      "step": 10865
    },
    {
      "epoch": 0.04642915131989369,
      "grad_norm": 0.7882149815559387,
      "learning_rate": 3.576311741582636e-05,
      "loss": 0.5595,
      "step": 10866
    },
    {
      "epoch": 0.04643342420332088,
      "grad_norm": 3.6344971656799316,
      "learning_rate": 3.572038967697829e-05,
      "loss": 1.0874,
      "step": 10867
    },
    {
      "epoch": 0.04643769708674808,
      "grad_norm": 1.1313788890838623,
      "learning_rate": 3.5677661938130235e-05,
      "loss": 0.9408,
      "step": 10868
    },
    {
      "epoch": 0.04644196997017527,
      "grad_norm": 0.886009931564331,
      "learning_rate": 3.563493419928217e-05,
      "loss": 0.3516,
      "step": 10869
    },
    {
      "epoch": 0.046446242853602465,
      "grad_norm": 0.5645042657852173,
      "learning_rate": 3.559220646043412e-05,
      "loss": 0.2371,
      "step": 10870
    },
    {
      "epoch": 0.046450515737029664,
      "grad_norm": 0.7840250134468079,
      "learning_rate": 3.554947872158605e-05,
      "loss": 0.5571,
      "step": 10871
    },
    {
      "epoch": 0.046454788620456856,
      "grad_norm": 1.88948655128479,
      "learning_rate": 3.550675098273799e-05,
      "loss": 0.5896,
      "step": 10872
    },
    {
      "epoch": 0.04645906150388405,
      "grad_norm": 0.714705228805542,
      "learning_rate": 3.546402324388993e-05,
      "loss": 0.2759,
      "step": 10873
    },
    {
      "epoch": 0.04646333438731125,
      "grad_norm": 4.67479133605957,
      "learning_rate": 3.5421295505041876e-05,
      "loss": 1.1623,
      "step": 10874
    },
    {
      "epoch": 0.04646760727073844,
      "grad_norm": 0.701806366443634,
      "learning_rate": 3.537856776619382e-05,
      "loss": 0.2762,
      "step": 10875
    },
    {
      "epoch": 0.04647188015416563,
      "grad_norm": 1.1570096015930176,
      "learning_rate": 3.533584002734575e-05,
      "loss": 0.961,
      "step": 10876
    },
    {
      "epoch": 0.04647615303759283,
      "grad_norm": 0.5674557089805603,
      "learning_rate": 3.5293112288497696e-05,
      "loss": 0.2288,
      "step": 10877
    },
    {
      "epoch": 0.04648042592102002,
      "grad_norm": 0.7745379209518433,
      "learning_rate": 3.5250384549649634e-05,
      "loss": 0.3698,
      "step": 10878
    },
    {
      "epoch": 0.046484698804447215,
      "grad_norm": 1.0728703737258911,
      "learning_rate": 3.520765681080158e-05,
      "loss": 0.4161,
      "step": 10879
    },
    {
      "epoch": 0.046488971687874414,
      "grad_norm": 4.693428039550781,
      "learning_rate": 3.516492907195351e-05,
      "loss": 1.1753,
      "step": 10880
    },
    {
      "epoch": 0.046493244571301606,
      "grad_norm": 1.1593126058578491,
      "learning_rate": 3.5122201333105454e-05,
      "loss": 0.9306,
      "step": 10881
    },
    {
      "epoch": 0.0464975174547288,
      "grad_norm": 5.070651531219482,
      "learning_rate": 3.507947359425739e-05,
      "loss": 1.1288,
      "step": 10882
    },
    {
      "epoch": 0.046501790338156,
      "grad_norm": 3.5578300952911377,
      "learning_rate": 3.503674585540934e-05,
      "loss": 0.8441,
      "step": 10883
    },
    {
      "epoch": 0.04650606322158319,
      "grad_norm": 2.774226665496826,
      "learning_rate": 3.499401811656127e-05,
      "loss": 0.6556,
      "step": 10884
    },
    {
      "epoch": 0.04651033610501038,
      "grad_norm": 3.415107250213623,
      "learning_rate": 3.495129037771321e-05,
      "loss": 0.8336,
      "step": 10885
    },
    {
      "epoch": 0.04651460898843758,
      "grad_norm": 0.506198525428772,
      "learning_rate": 3.490856263886515e-05,
      "loss": 0.2092,
      "step": 10886
    },
    {
      "epoch": 0.04651888187186477,
      "grad_norm": 0.8075526356697083,
      "learning_rate": 3.4865834900017095e-05,
      "loss": 0.3638,
      "step": 10887
    },
    {
      "epoch": 0.046523154755291965,
      "grad_norm": 0.554347038269043,
      "learning_rate": 3.4823107161169026e-05,
      "loss": 0.2287,
      "step": 10888
    },
    {
      "epoch": 0.04652742763871916,
      "grad_norm": 5.031768321990967,
      "learning_rate": 3.478037942232097e-05,
      "loss": 1.0732,
      "step": 10889
    },
    {
      "epoch": 0.046531700522146356,
      "grad_norm": 2.983128547668457,
      "learning_rate": 3.4737651683472915e-05,
      "loss": 1.0637,
      "step": 10890
    },
    {
      "epoch": 0.04653597340557355,
      "grad_norm": 0.8996300101280212,
      "learning_rate": 3.469492394462485e-05,
      "loss": 0.3658,
      "step": 10891
    },
    {
      "epoch": 0.04654024628900074,
      "grad_norm": 0.757362425327301,
      "learning_rate": 3.465219620577679e-05,
      "loss": 0.3344,
      "step": 10892
    },
    {
      "epoch": 0.04654451917242794,
      "grad_norm": 3.056492328643799,
      "learning_rate": 3.460946846692873e-05,
      "loss": 0.7074,
      "step": 10893
    },
    {
      "epoch": 0.04654879205585513,
      "grad_norm": 3.3925395011901855,
      "learning_rate": 3.456674072808067e-05,
      "loss": 1.1025,
      "step": 10894
    },
    {
      "epoch": 0.046553064939282324,
      "grad_norm": 2.9126100540161133,
      "learning_rate": 3.452401298923261e-05,
      "loss": 1.6484,
      "step": 10895
    },
    {
      "epoch": 0.04655733782270952,
      "grad_norm": 3.0917649269104004,
      "learning_rate": 3.448128525038455e-05,
      "loss": 1.2698,
      "step": 10896
    },
    {
      "epoch": 0.046561610706136715,
      "grad_norm": 1.5634362697601318,
      "learning_rate": 3.443855751153649e-05,
      "loss": 0.5556,
      "step": 10897
    },
    {
      "epoch": 0.04656588358956391,
      "grad_norm": 0.8597874641418457,
      "learning_rate": 3.439582977268843e-05,
      "loss": 0.5332,
      "step": 10898
    },
    {
      "epoch": 0.046570156472991106,
      "grad_norm": 2.0633273124694824,
      "learning_rate": 3.435310203384037e-05,
      "loss": 0.6287,
      "step": 10899
    },
    {
      "epoch": 0.0465744293564183,
      "grad_norm": 0.7834861278533936,
      "learning_rate": 3.431037429499231e-05,
      "loss": 0.5556,
      "step": 10900
    },
    {
      "epoch": 0.04657870223984549,
      "grad_norm": 2.6296582221984863,
      "learning_rate": 3.4267646556144245e-05,
      "loss": 0.5752,
      "step": 10901
    },
    {
      "epoch": 0.04658297512327269,
      "grad_norm": 4.132948875427246,
      "learning_rate": 3.422491881729619e-05,
      "loss": 1.213,
      "step": 10902
    },
    {
      "epoch": 0.04658724800669988,
      "grad_norm": 0.389385849237442,
      "learning_rate": 3.4182191078448134e-05,
      "loss": 0.1139,
      "step": 10903
    },
    {
      "epoch": 0.046591520890127074,
      "grad_norm": 0.8312347531318665,
      "learning_rate": 3.4139463339600065e-05,
      "loss": 0.3809,
      "step": 10904
    },
    {
      "epoch": 0.04659579377355427,
      "grad_norm": 2.908021926879883,
      "learning_rate": 3.409673560075201e-05,
      "loss": 1.6209,
      "step": 10905
    },
    {
      "epoch": 0.046600066656981465,
      "grad_norm": 0.8407208919525146,
      "learning_rate": 3.405400786190395e-05,
      "loss": 0.3988,
      "step": 10906
    },
    {
      "epoch": 0.04660433954040866,
      "grad_norm": 0.8278610706329346,
      "learning_rate": 3.401128012305589e-05,
      "loss": 0.4936,
      "step": 10907
    },
    {
      "epoch": 0.046608612423835856,
      "grad_norm": 4.283786296844482,
      "learning_rate": 3.396855238420782e-05,
      "loss": 1.0714,
      "step": 10908
    },
    {
      "epoch": 0.04661288530726305,
      "grad_norm": 0.8452792763710022,
      "learning_rate": 3.392582464535977e-05,
      "loss": 0.5023,
      "step": 10909
    },
    {
      "epoch": 0.04661715819069024,
      "grad_norm": 2.63588285446167,
      "learning_rate": 3.3883096906511706e-05,
      "loss": 0.5803,
      "step": 10910
    },
    {
      "epoch": 0.04662143107411744,
      "grad_norm": 0.7183207869529724,
      "learning_rate": 3.384036916766365e-05,
      "loss": 0.3032,
      "step": 10911
    },
    {
      "epoch": 0.04662570395754463,
      "grad_norm": 1.6889612674713135,
      "learning_rate": 3.379764142881559e-05,
      "loss": 1.2458,
      "step": 10912
    },
    {
      "epoch": 0.046629976840971823,
      "grad_norm": 5.158997058868408,
      "learning_rate": 3.3754913689967526e-05,
      "loss": 1.3456,
      "step": 10913
    },
    {
      "epoch": 0.046634249724399016,
      "grad_norm": 1.8781583309173584,
      "learning_rate": 3.3712185951119464e-05,
      "loss": 0.5906,
      "step": 10914
    },
    {
      "epoch": 0.046638522607826215,
      "grad_norm": 2.9126758575439453,
      "learning_rate": 3.366945821227141e-05,
      "loss": 1.6484,
      "step": 10915
    },
    {
      "epoch": 0.04664279549125341,
      "grad_norm": 1.68626070022583,
      "learning_rate": 3.362673047342335e-05,
      "loss": 1.2458,
      "step": 10916
    },
    {
      "epoch": 0.0466470683746806,
      "grad_norm": 0.8254624605178833,
      "learning_rate": 3.3584002734575284e-05,
      "loss": 0.4959,
      "step": 10917
    },
    {
      "epoch": 0.0466513412581078,
      "grad_norm": 1.4931365251541138,
      "learning_rate": 3.354127499572723e-05,
      "loss": 0.3449,
      "step": 10918
    },
    {
      "epoch": 0.04665561414153499,
      "grad_norm": 4.105813980102539,
      "learning_rate": 3.349854725687917e-05,
      "loss": 1.1758,
      "step": 10919
    },
    {
      "epoch": 0.04665988702496218,
      "grad_norm": 2.9143424034118652,
      "learning_rate": 3.345581951803111e-05,
      "loss": 1.6209,
      "step": 10920
    },
    {
      "epoch": 0.04666415990838938,
      "grad_norm": 0.3462658226490021,
      "learning_rate": 3.341309177918304e-05,
      "loss": 0.1036,
      "step": 10921
    },
    {
      "epoch": 0.04666843279181657,
      "grad_norm": 0.7796357870101929,
      "learning_rate": 3.337036404033499e-05,
      "loss": 0.5585,
      "step": 10922
    },
    {
      "epoch": 0.046672705675243766,
      "grad_norm": 3.6374995708465576,
      "learning_rate": 3.3327636301486925e-05,
      "loss": 2.4333,
      "step": 10923
    },
    {
      "epoch": 0.046676978558670965,
      "grad_norm": 0.6376500725746155,
      "learning_rate": 3.328490856263887e-05,
      "loss": 0.2163,
      "step": 10924
    },
    {
      "epoch": 0.04668125144209816,
      "grad_norm": 3.5329110622406006,
      "learning_rate": 3.32421808237908e-05,
      "loss": 1.2035,
      "step": 10925
    },
    {
      "epoch": 0.04668552432552535,
      "grad_norm": 0.894271194934845,
      "learning_rate": 3.3199453084942745e-05,
      "loss": 0.3809,
      "step": 10926
    },
    {
      "epoch": 0.04668979720895255,
      "grad_norm": 3.3363685607910156,
      "learning_rate": 3.315672534609468e-05,
      "loss": 0.8652,
      "step": 10927
    },
    {
      "epoch": 0.04669407009237974,
      "grad_norm": 2.756032705307007,
      "learning_rate": 3.311399760724663e-05,
      "loss": 0.6704,
      "step": 10928
    },
    {
      "epoch": 0.04669834297580693,
      "grad_norm": 2.0809271335601807,
      "learning_rate": 3.3071269868398565e-05,
      "loss": 0.6348,
      "step": 10929
    },
    {
      "epoch": 0.04670261585923413,
      "grad_norm": 2.4929137229919434,
      "learning_rate": 3.30285421295505e-05,
      "loss": 0.7176,
      "step": 10930
    },
    {
      "epoch": 0.04670688874266132,
      "grad_norm": 2.6616833209991455,
      "learning_rate": 3.298581439070245e-05,
      "loss": 0.8224,
      "step": 10931
    },
    {
      "epoch": 0.046711161626088515,
      "grad_norm": 1.672771692276001,
      "learning_rate": 3.2943086651854386e-05,
      "loss": 1.2468,
      "step": 10932
    },
    {
      "epoch": 0.046715434509515714,
      "grad_norm": 0.7476001977920532,
      "learning_rate": 3.2900358913006324e-05,
      "loss": 0.3184,
      "step": 10933
    },
    {
      "epoch": 0.04671970739294291,
      "grad_norm": 0.4601706266403198,
      "learning_rate": 3.285763117415826e-05,
      "loss": 0.1847,
      "step": 10934
    },
    {
      "epoch": 0.0467239802763701,
      "grad_norm": 5.091977596282959,
      "learning_rate": 3.2814903435310206e-05,
      "loss": 1.2917,
      "step": 10935
    },
    {
      "epoch": 0.0467282531597973,
      "grad_norm": 1.8607200384140015,
      "learning_rate": 3.2772175696462144e-05,
      "loss": 0.4625,
      "step": 10936
    },
    {
      "epoch": 0.04673252604322449,
      "grad_norm": 3.6451239585876465,
      "learning_rate": 3.272944795761408e-05,
      "loss": 1.0795,
      "step": 10937
    },
    {
      "epoch": 0.04673679892665168,
      "grad_norm": 0.9636419415473938,
      "learning_rate": 3.268672021876602e-05,
      "loss": 0.3756,
      "step": 10938
    },
    {
      "epoch": 0.046741071810078874,
      "grad_norm": 2.9746158123016357,
      "learning_rate": 3.2643992479917964e-05,
      "loss": 1.0689,
      "step": 10939
    },
    {
      "epoch": 0.04674534469350607,
      "grad_norm": 3.245751142501831,
      "learning_rate": 3.26012647410699e-05,
      "loss": 0.5749,
      "step": 10940
    },
    {
      "epoch": 0.046749617576933265,
      "grad_norm": 4.172451496124268,
      "learning_rate": 3.255853700222184e-05,
      "loss": 1.023,
      "step": 10941
    },
    {
      "epoch": 0.04675389046036046,
      "grad_norm": 0.8032431602478027,
      "learning_rate": 3.2515809263373784e-05,
      "loss": 0.5693,
      "step": 10942
    },
    {
      "epoch": 0.046758163343787656,
      "grad_norm": 1.0666797161102295,
      "learning_rate": 3.247308152452572e-05,
      "loss": 0.492,
      "step": 10943
    },
    {
      "epoch": 0.04676243622721485,
      "grad_norm": 3.0452873706817627,
      "learning_rate": 3.243035378567767e-05,
      "loss": 0.8278,
      "step": 10944
    },
    {
      "epoch": 0.04676670911064204,
      "grad_norm": 2.238607406616211,
      "learning_rate": 3.23876260468296e-05,
      "loss": 0.51,
      "step": 10945
    },
    {
      "epoch": 0.04677098199406924,
      "grad_norm": 0.8188653588294983,
      "learning_rate": 3.234489830798154e-05,
      "loss": 0.4968,
      "step": 10946
    },
    {
      "epoch": 0.04677525487749643,
      "grad_norm": 4.149938583374023,
      "learning_rate": 3.230217056913348e-05,
      "loss": 2.1907,
      "step": 10947
    },
    {
      "epoch": 0.046779527760923624,
      "grad_norm": 2.079941511154175,
      "learning_rate": 3.2259442830285425e-05,
      "loss": 0.6232,
      "step": 10948
    },
    {
      "epoch": 0.04678380064435082,
      "grad_norm": 5.047397136688232,
      "learning_rate": 3.221671509143736e-05,
      "loss": 1.2583,
      "step": 10949
    },
    {
      "epoch": 0.046788073527778015,
      "grad_norm": 2.4626803398132324,
      "learning_rate": 3.21739873525893e-05,
      "loss": 0.6848,
      "step": 10950
    },
    {
      "epoch": 0.04679234641120521,
      "grad_norm": 0.8250948786735535,
      "learning_rate": 3.213125961374124e-05,
      "loss": 0.3808,
      "step": 10951
    },
    {
      "epoch": 0.046796619294632406,
      "grad_norm": 3.297877073287964,
      "learning_rate": 3.208853187489318e-05,
      "loss": 0.8442,
      "step": 10952
    },
    {
      "epoch": 0.0468008921780596,
      "grad_norm": 4.125529766082764,
      "learning_rate": 3.204580413604512e-05,
      "loss": 1.1109,
      "step": 10953
    },
    {
      "epoch": 0.04680516506148679,
      "grad_norm": 0.7363532185554504,
      "learning_rate": 3.200307639719706e-05,
      "loss": 0.3185,
      "step": 10954
    },
    {
      "epoch": 0.04680943794491399,
      "grad_norm": 0.8455400466918945,
      "learning_rate": 3.1960348658349004e-05,
      "loss": 0.5066,
      "step": 10955
    },
    {
      "epoch": 0.04681371082834118,
      "grad_norm": 2.0607385635375977,
      "learning_rate": 3.191762091950094e-05,
      "loss": 0.6037,
      "step": 10956
    },
    {
      "epoch": 0.046817983711768374,
      "grad_norm": 2.2647528648376465,
      "learning_rate": 3.1874893180652886e-05,
      "loss": 0.6065,
      "step": 10957
    },
    {
      "epoch": 0.04682225659519557,
      "grad_norm": 1.887036919593811,
      "learning_rate": 3.183216544180482e-05,
      "loss": 0.462,
      "step": 10958
    },
    {
      "epoch": 0.046826529478622765,
      "grad_norm": 1.3808073997497559,
      "learning_rate": 3.178943770295676e-05,
      "loss": 0.5702,
      "step": 10959
    },
    {
      "epoch": 0.04683080236204996,
      "grad_norm": 2.464484930038452,
      "learning_rate": 3.17467099641087e-05,
      "loss": 0.6704,
      "step": 10960
    },
    {
      "epoch": 0.046835075245477156,
      "grad_norm": 0.8034318685531616,
      "learning_rate": 3.1703982225260644e-05,
      "loss": 0.5692,
      "step": 10961
    },
    {
      "epoch": 0.04683934812890435,
      "grad_norm": 0.7326703071594238,
      "learning_rate": 3.1661254486412575e-05,
      "loss": 0.3185,
      "step": 10962
    },
    {
      "epoch": 0.04684362101233154,
      "grad_norm": 0.7073172330856323,
      "learning_rate": 3.161852674756452e-05,
      "loss": 0.3262,
      "step": 10963
    },
    {
      "epoch": 0.04684789389575873,
      "grad_norm": 0.6745736002922058,
      "learning_rate": 3.157579900871646e-05,
      "loss": 0.3092,
      "step": 10964
    },
    {
      "epoch": 0.04685216677918593,
      "grad_norm": 2.5256552696228027,
      "learning_rate": 3.15330712698684e-05,
      "loss": 0.7283,
      "step": 10965
    },
    {
      "epoch": 0.046856439662613124,
      "grad_norm": 0.81175696849823,
      "learning_rate": 3.149034353102033e-05,
      "loss": 0.3809,
      "step": 10966
    },
    {
      "epoch": 0.046860712546040316,
      "grad_norm": 0.6496150493621826,
      "learning_rate": 3.144761579217228e-05,
      "loss": 0.2779,
      "step": 10967
    },
    {
      "epoch": 0.046864985429467515,
      "grad_norm": 1.8735297918319702,
      "learning_rate": 3.140488805332422e-05,
      "loss": 0.451,
      "step": 10968
    },
    {
      "epoch": 0.04686925831289471,
      "grad_norm": 0.7873098254203796,
      "learning_rate": 3.136216031447616e-05,
      "loss": 0.5651,
      "step": 10969
    },
    {
      "epoch": 0.0468735311963219,
      "grad_norm": 3.004772663116455,
      "learning_rate": 3.13194325756281e-05,
      "loss": 0.7915,
      "step": 10970
    },
    {
      "epoch": 0.0468778040797491,
      "grad_norm": 3.183622121810913,
      "learning_rate": 3.1276704836780036e-05,
      "loss": 0.5585,
      "step": 10971
    },
    {
      "epoch": 0.04688207696317629,
      "grad_norm": 1.011297345161438,
      "learning_rate": 3.123397709793198e-05,
      "loss": 0.4029,
      "step": 10972
    },
    {
      "epoch": 0.04688634984660348,
      "grad_norm": 1.1263316869735718,
      "learning_rate": 3.119124935908392e-05,
      "loss": 0.9649,
      "step": 10973
    },
    {
      "epoch": 0.04689062273003068,
      "grad_norm": 0.7878141403198242,
      "learning_rate": 3.1148521620235856e-05,
      "loss": 0.5656,
      "step": 10974
    },
    {
      "epoch": 0.046894895613457874,
      "grad_norm": 2.0736939907073975,
      "learning_rate": 3.1105793881387794e-05,
      "loss": 0.5815,
      "step": 10975
    },
    {
      "epoch": 0.046899168496885066,
      "grad_norm": 0.8309774994850159,
      "learning_rate": 3.106306614253974e-05,
      "loss": 0.5027,
      "step": 10976
    },
    {
      "epoch": 0.046903441380312265,
      "grad_norm": 1.8950600624084473,
      "learning_rate": 3.102033840369168e-05,
      "loss": 0.586,
      "step": 10977
    },
    {
      "epoch": 0.04690771426373946,
      "grad_norm": 0.8942154049873352,
      "learning_rate": 3.0977610664843615e-05,
      "loss": 0.4143,
      "step": 10978
    },
    {
      "epoch": 0.04691198714716665,
      "grad_norm": 1.9695631265640259,
      "learning_rate": 3.093488292599555e-05,
      "loss": 0.5825,
      "step": 10979
    },
    {
      "epoch": 0.04691626003059385,
      "grad_norm": 0.4542246460914612,
      "learning_rate": 3.08921551871475e-05,
      "loss": 0.1696,
      "step": 10980
    },
    {
      "epoch": 0.04692053291402104,
      "grad_norm": 2.2421152591705322,
      "learning_rate": 3.0849427448299435e-05,
      "loss": 0.5885,
      "step": 10981
    },
    {
      "epoch": 0.04692480579744823,
      "grad_norm": 1.6700456142425537,
      "learning_rate": 3.080669970945138e-05,
      "loss": 1.2467,
      "step": 10982
    },
    {
      "epoch": 0.04692907868087543,
      "grad_norm": 1.6721895933151245,
      "learning_rate": 3.076397197060332e-05,
      "loss": 1.2467,
      "step": 10983
    },
    {
      "epoch": 0.046933351564302624,
      "grad_norm": 1.1203991174697876,
      "learning_rate": 3.0721244231755255e-05,
      "loss": 0.9406,
      "step": 10984
    },
    {
      "epoch": 0.046937624447729816,
      "grad_norm": 0.7327522039413452,
      "learning_rate": 3.067851649290719e-05,
      "loss": 0.3344,
      "step": 10985
    },
    {
      "epoch": 0.046941897331157015,
      "grad_norm": 0.6794697642326355,
      "learning_rate": 3.063578875405914e-05,
      "loss": 0.293,
      "step": 10986
    },
    {
      "epoch": 0.04694617021458421,
      "grad_norm": 4.900149822235107,
      "learning_rate": 3.0593061015211075e-05,
      "loss": 1.063,
      "step": 10987
    },
    {
      "epoch": 0.0469504430980114,
      "grad_norm": 0.8117655515670776,
      "learning_rate": 3.055033327636301e-05,
      "loss": 0.3813,
      "step": 10988
    },
    {
      "epoch": 0.04695471598143859,
      "grad_norm": 0.7256414890289307,
      "learning_rate": 3.0507605537514958e-05,
      "loss": 0.3184,
      "step": 10989
    },
    {
      "epoch": 0.04695898886486579,
      "grad_norm": 2.879582405090332,
      "learning_rate": 3.0464877798666896e-05,
      "loss": 1.6281,
      "step": 10990
    },
    {
      "epoch": 0.04696326174829298,
      "grad_norm": 0.7338190078735352,
      "learning_rate": 3.0422150059818837e-05,
      "loss": 0.3033,
      "step": 10991
    },
    {
      "epoch": 0.046967534631720174,
      "grad_norm": 3.181671380996704,
      "learning_rate": 3.0379422320970775e-05,
      "loss": 0.7607,
      "step": 10992
    },
    {
      "epoch": 0.046971807515147374,
      "grad_norm": 3.5346243381500244,
      "learning_rate": 3.0336694582122716e-05,
      "loss": 0.9151,
      "step": 10993
    },
    {
      "epoch": 0.046976080398574566,
      "grad_norm": 0.6613380312919617,
      "learning_rate": 3.0293966843274654e-05,
      "loss": 0.2929,
      "step": 10994
    },
    {
      "epoch": 0.04698035328200176,
      "grad_norm": 1.0132700204849243,
      "learning_rate": 3.0251239104426595e-05,
      "loss": 0.4029,
      "step": 10995
    },
    {
      "epoch": 0.04698462616542896,
      "grad_norm": 0.9660090804100037,
      "learning_rate": 3.0208511365578533e-05,
      "loss": 0.4028,
      "step": 10996
    },
    {
      "epoch": 0.04698889904885615,
      "grad_norm": 0.8156731724739075,
      "learning_rate": 3.0165783626730474e-05,
      "loss": 0.3637,
      "step": 10997
    },
    {
      "epoch": 0.04699317193228334,
      "grad_norm": 0.7009380459785461,
      "learning_rate": 3.0123055887882412e-05,
      "loss": 0.3092,
      "step": 10998
    },
    {
      "epoch": 0.04699744481571054,
      "grad_norm": 0.7887955904006958,
      "learning_rate": 3.0080328149034353e-05,
      "loss": 0.5656,
      "step": 10999
    },
    {
      "epoch": 0.04700171769913773,
      "grad_norm": 3.319749593734741,
      "learning_rate": 3.0037600410186295e-05,
      "loss": 0.7777,
      "step": 11000
    },
    {
      "epoch": 0.047005990582564924,
      "grad_norm": 3.090946912765503,
      "learning_rate": 2.9994872671338236e-05,
      "loss": 1.2484,
      "step": 11001
    },
    {
      "epoch": 0.04701026346599212,
      "grad_norm": 0.8490704298019409,
      "learning_rate": 2.9952144932490174e-05,
      "loss": 0.5039,
      "step": 11002
    },
    {
      "epoch": 0.047014536349419316,
      "grad_norm": 3.0857393741607666,
      "learning_rate": 2.9909417193642115e-05,
      "loss": 1.237,
      "step": 11003
    },
    {
      "epoch": 0.04701880923284651,
      "grad_norm": 3.2893877029418945,
      "learning_rate": 2.9866689454794053e-05,
      "loss": 0.8237,
      "step": 11004
    },
    {
      "epoch": 0.04702308211627371,
      "grad_norm": 4.122146129608154,
      "learning_rate": 2.9823961715945994e-05,
      "loss": 1.0929,
      "step": 11005
    },
    {
      "epoch": 0.0470273549997009,
      "grad_norm": 1.650994896888733,
      "learning_rate": 2.9781233977097932e-05,
      "loss": 1.2245,
      "step": 11006
    },
    {
      "epoch": 0.04703162788312809,
      "grad_norm": 5.0444464683532715,
      "learning_rate": 2.9738506238249873e-05,
      "loss": 1.2312,
      "step": 11007
    },
    {
      "epoch": 0.04703590076655529,
      "grad_norm": 0.7940114736557007,
      "learning_rate": 2.969577849940181e-05,
      "loss": 0.5649,
      "step": 11008
    },
    {
      "epoch": 0.04704017364998248,
      "grad_norm": 1.9002699851989746,
      "learning_rate": 2.9653050760553752e-05,
      "loss": 0.573,
      "step": 11009
    },
    {
      "epoch": 0.047044446533409674,
      "grad_norm": 3.1940534114837646,
      "learning_rate": 2.961032302170569e-05,
      "loss": 0.5578,
      "step": 11010
    },
    {
      "epoch": 0.04704871941683687,
      "grad_norm": 1.8457449674606323,
      "learning_rate": 2.956759528285763e-05,
      "loss": 0.4731,
      "step": 11011
    },
    {
      "epoch": 0.047052992300264065,
      "grad_norm": 0.6943618655204773,
      "learning_rate": 2.952486754400957e-05,
      "loss": 0.3033,
      "step": 11012
    },
    {
      "epoch": 0.04705726518369126,
      "grad_norm": 1.9754092693328857,
      "learning_rate": 2.9482139805161514e-05,
      "loss": 0.5758,
      "step": 11013
    },
    {
      "epoch": 0.04706153806711845,
      "grad_norm": 0.5777201652526855,
      "learning_rate": 2.943941206631345e-05,
      "loss": 0.2562,
      "step": 11014
    },
    {
      "epoch": 0.04706581095054565,
      "grad_norm": 0.8516324758529663,
      "learning_rate": 2.9396684327465393e-05,
      "loss": 0.5065,
      "step": 11015
    },
    {
      "epoch": 0.04707008383397284,
      "grad_norm": 1.8793301582336426,
      "learning_rate": 2.935395658861733e-05,
      "loss": 0.6589,
      "step": 11016
    },
    {
      "epoch": 0.04707435671740003,
      "grad_norm": 1.1041796207427979,
      "learning_rate": 2.9311228849769272e-05,
      "loss": 0.9177,
      "step": 11017
    },
    {
      "epoch": 0.04707862960082723,
      "grad_norm": 1.1311341524124146,
      "learning_rate": 2.926850111092121e-05,
      "loss": 0.9397,
      "step": 11018
    },
    {
      "epoch": 0.047082902484254424,
      "grad_norm": 3.2599165439605713,
      "learning_rate": 2.922577337207315e-05,
      "loss": 0.7945,
      "step": 11019
    },
    {
      "epoch": 0.047087175367681616,
      "grad_norm": 0.8035971522331238,
      "learning_rate": 2.918304563322509e-05,
      "loss": 0.3478,
      "step": 11020
    },
    {
      "epoch": 0.047091448251108815,
      "grad_norm": 0.8396907448768616,
      "learning_rate": 2.914031789437703e-05,
      "loss": 0.5024,
      "step": 11021
    },
    {
      "epoch": 0.04709572113453601,
      "grad_norm": 0.8036355376243591,
      "learning_rate": 2.9097590155528968e-05,
      "loss": 0.3477,
      "step": 11022
    },
    {
      "epoch": 0.0470999940179632,
      "grad_norm": 0.8360941410064697,
      "learning_rate": 2.905486241668091e-05,
      "loss": 0.5024,
      "step": 11023
    },
    {
      "epoch": 0.0471042669013904,
      "grad_norm": 0.6239023804664612,
      "learning_rate": 2.901213467783285e-05,
      "loss": 0.1977,
      "step": 11024
    },
    {
      "epoch": 0.04710853978481759,
      "grad_norm": 1.8985929489135742,
      "learning_rate": 2.8969406938984788e-05,
      "loss": 0.4475,
      "step": 11025
    },
    {
      "epoch": 0.04711281266824478,
      "grad_norm": 0.7809945344924927,
      "learning_rate": 2.8926679200136733e-05,
      "loss": 0.5274,
      "step": 11026
    },
    {
      "epoch": 0.04711708555167198,
      "grad_norm": 0.8229486346244812,
      "learning_rate": 2.888395146128867e-05,
      "loss": 0.3807,
      "step": 11027
    },
    {
      "epoch": 0.047121358435099174,
      "grad_norm": 2.077859401702881,
      "learning_rate": 2.884122372244061e-05,
      "loss": 0.6246,
      "step": 11028
    },
    {
      "epoch": 0.047125631318526366,
      "grad_norm": 0.9177204370498657,
      "learning_rate": 2.879849598359255e-05,
      "loss": 0.381,
      "step": 11029
    },
    {
      "epoch": 0.047129904201953565,
      "grad_norm": 2.6724417209625244,
      "learning_rate": 2.875576824474449e-05,
      "loss": 0.6336,
      "step": 11030
    },
    {
      "epoch": 0.04713417708538076,
      "grad_norm": 0.7840541005134583,
      "learning_rate": 2.871304050589643e-05,
      "loss": 0.3325,
      "step": 11031
    },
    {
      "epoch": 0.04713844996880795,
      "grad_norm": 0.730364203453064,
      "learning_rate": 2.867031276704837e-05,
      "loss": 0.3183,
      "step": 11032
    },
    {
      "epoch": 0.04714272285223515,
      "grad_norm": 0.6148404479026794,
      "learning_rate": 2.8627585028200308e-05,
      "loss": 0.1897,
      "step": 11033
    },
    {
      "epoch": 0.04714699573566234,
      "grad_norm": 0.6139651536941528,
      "learning_rate": 2.858485728935225e-05,
      "loss": 0.1898,
      "step": 11034
    },
    {
      "epoch": 0.04715126861908953,
      "grad_norm": 4.726898670196533,
      "learning_rate": 2.8542129550504187e-05,
      "loss": 1.1719,
      "step": 11035
    },
    {
      "epoch": 0.047155541502516725,
      "grad_norm": 2.703136920928955,
      "learning_rate": 2.8499401811656128e-05,
      "loss": 0.8463,
      "step": 11036
    },
    {
      "epoch": 0.047159814385943924,
      "grad_norm": 0.7324085831642151,
      "learning_rate": 2.8456674072808066e-05,
      "loss": 0.3344,
      "step": 11037
    },
    {
      "epoch": 0.047164087269371116,
      "grad_norm": 0.7835382223129272,
      "learning_rate": 2.8413946333960007e-05,
      "loss": 0.3324,
      "step": 11038
    },
    {
      "epoch": 0.04716836015279831,
      "grad_norm": 2.618374824523926,
      "learning_rate": 2.8371218595111945e-05,
      "loss": 0.5828,
      "step": 11039
    },
    {
      "epoch": 0.04717263303622551,
      "grad_norm": 3.600846290588379,
      "learning_rate": 2.832849085626389e-05,
      "loss": 2.4397,
      "step": 11040
    },
    {
      "epoch": 0.0471769059196527,
      "grad_norm": 1.6572517156600952,
      "learning_rate": 2.8285763117415827e-05,
      "loss": 1.1897,
      "step": 11041
    },
    {
      "epoch": 0.04718117880307989,
      "grad_norm": 0.832078218460083,
      "learning_rate": 2.824303537856777e-05,
      "loss": 0.5022,
      "step": 11042
    },
    {
      "epoch": 0.04718545168650709,
      "grad_norm": 0.7027620673179626,
      "learning_rate": 2.8200307639719706e-05,
      "loss": 0.2758,
      "step": 11043
    },
    {
      "epoch": 0.04718972456993428,
      "grad_norm": 0.4062652587890625,
      "learning_rate": 2.8157579900871648e-05,
      "loss": 0.1478,
      "step": 11044
    },
    {
      "epoch": 0.047193997453361475,
      "grad_norm": 0.7730144262313843,
      "learning_rate": 2.8114852162023585e-05,
      "loss": 0.3325,
      "step": 11045
    },
    {
      "epoch": 0.047198270336788674,
      "grad_norm": 0.6373635530471802,
      "learning_rate": 2.8072124423175527e-05,
      "loss": 0.2636,
      "step": 11046
    },
    {
      "epoch": 0.047202543220215866,
      "grad_norm": 0.6107388138771057,
      "learning_rate": 2.8029396684327465e-05,
      "loss": 0.1824,
      "step": 11047
    },
    {
      "epoch": 0.04720681610364306,
      "grad_norm": 0.8188406229019165,
      "learning_rate": 2.7986668945479406e-05,
      "loss": 0.3808,
      "step": 11048
    },
    {
      "epoch": 0.04721108898707026,
      "grad_norm": 1.9027302265167236,
      "learning_rate": 2.7943941206631344e-05,
      "loss": 0.5868,
      "step": 11049
    },
    {
      "epoch": 0.04721536187049745,
      "grad_norm": 4.127089023590088,
      "learning_rate": 2.7901213467783285e-05,
      "loss": 1.0172,
      "step": 11050
    },
    {
      "epoch": 0.04721963475392464,
      "grad_norm": 0.7032440304756165,
      "learning_rate": 2.7858485728935223e-05,
      "loss": 0.3032,
      "step": 11051
    },
    {
      "epoch": 0.04722390763735184,
      "grad_norm": 0.8294793963432312,
      "learning_rate": 2.7815757990087164e-05,
      "loss": 0.5022,
      "step": 11052
    },
    {
      "epoch": 0.04722818052077903,
      "grad_norm": 0.7858624458312988,
      "learning_rate": 2.7773030251239105e-05,
      "loss": 0.5647,
      "step": 11053
    },
    {
      "epoch": 0.047232453404206225,
      "grad_norm": 0.9677130579948425,
      "learning_rate": 2.7730302512391046e-05,
      "loss": 0.4028,
      "step": 11054
    },
    {
      "epoch": 0.047236726287633424,
      "grad_norm": 1.643721103668213,
      "learning_rate": 2.7687574773542984e-05,
      "loss": 1.1899,
      "step": 11055
    },
    {
      "epoch": 0.047240999171060616,
      "grad_norm": 3.1318023204803467,
      "learning_rate": 2.7644847034694925e-05,
      "loss": 0.7284,
      "step": 11056
    },
    {
      "epoch": 0.04724527205448781,
      "grad_norm": 0.6662039160728455,
      "learning_rate": 2.7602119295846863e-05,
      "loss": 0.2929,
      "step": 11057
    },
    {
      "epoch": 0.04724954493791501,
      "grad_norm": 0.828709065914154,
      "learning_rate": 2.7559391556998805e-05,
      "loss": 0.5031,
      "step": 11058
    },
    {
      "epoch": 0.0472538178213422,
      "grad_norm": 4.121049880981445,
      "learning_rate": 2.7516663818150742e-05,
      "loss": 0.9885,
      "step": 11059
    },
    {
      "epoch": 0.04725809070476939,
      "grad_norm": 0.8261808753013611,
      "learning_rate": 2.7473936079302684e-05,
      "loss": 0.5022,
      "step": 11060
    },
    {
      "epoch": 0.04726236358819658,
      "grad_norm": 4.198333263397217,
      "learning_rate": 2.743120834045462e-05,
      "loss": 1.1977,
      "step": 11061
    },
    {
      "epoch": 0.04726663647162378,
      "grad_norm": 2.080653667449951,
      "learning_rate": 2.7388480601606563e-05,
      "loss": 0.5872,
      "step": 11062
    },
    {
      "epoch": 0.047270909355050975,
      "grad_norm": 3.5712904930114746,
      "learning_rate": 2.7345752862758504e-05,
      "loss": 1.2015,
      "step": 11063
    },
    {
      "epoch": 0.04727518223847817,
      "grad_norm": 1.6545425653457642,
      "learning_rate": 2.7303025123910442e-05,
      "loss": 1.1897,
      "step": 11064
    },
    {
      "epoch": 0.047279455121905366,
      "grad_norm": 2.2234861850738525,
      "learning_rate": 2.7260297385062383e-05,
      "loss": 0.5847,
      "step": 11065
    },
    {
      "epoch": 0.04728372800533256,
      "grad_norm": 2.586824417114258,
      "learning_rate": 2.7217569646214324e-05,
      "loss": 0.5512,
      "step": 11066
    },
    {
      "epoch": 0.04728800088875975,
      "grad_norm": 1.0054199695587158,
      "learning_rate": 2.7174841907366265e-05,
      "loss": 0.4179,
      "step": 11067
    },
    {
      "epoch": 0.04729227377218695,
      "grad_norm": 0.786688506603241,
      "learning_rate": 2.7132114168518203e-05,
      "loss": 0.4836,
      "step": 11068
    },
    {
      "epoch": 0.04729654665561414,
      "grad_norm": 1.661117434501648,
      "learning_rate": 2.7089386429670145e-05,
      "loss": 1.1897,
      "step": 11069
    },
    {
      "epoch": 0.04730081953904133,
      "grad_norm": 0.6609689593315125,
      "learning_rate": 2.7046658690822082e-05,
      "loss": 0.2928,
      "step": 11070
    },
    {
      "epoch": 0.04730509242246853,
      "grad_norm": 0.6501774191856384,
      "learning_rate": 2.7003930951974024e-05,
      "loss": 0.3144,
      "step": 11071
    },
    {
      "epoch": 0.047309365305895724,
      "grad_norm": 4.088612079620361,
      "learning_rate": 2.696120321312596e-05,
      "loss": 0.9658,
      "step": 11072
    },
    {
      "epoch": 0.04731363818932292,
      "grad_norm": 2.5992271900177,
      "learning_rate": 2.6918475474277903e-05,
      "loss": 0.5574,
      "step": 11073
    },
    {
      "epoch": 0.047317911072750116,
      "grad_norm": 0.9008205533027649,
      "learning_rate": 2.687574773542984e-05,
      "loss": 0.4139,
      "step": 11074
    },
    {
      "epoch": 0.04732218395617731,
      "grad_norm": 0.7087684273719788,
      "learning_rate": 2.6833019996581782e-05,
      "loss": 0.3031,
      "step": 11075
    },
    {
      "epoch": 0.0473264568396045,
      "grad_norm": 0.8244801759719849,
      "learning_rate": 2.679029225773372e-05,
      "loss": 0.5029,
      "step": 11076
    },
    {
      "epoch": 0.0473307297230317,
      "grad_norm": 2.5022735595703125,
      "learning_rate": 2.674756451888566e-05,
      "loss": 0.6868,
      "step": 11077
    },
    {
      "epoch": 0.04733500260645889,
      "grad_norm": 4.051040172576904,
      "learning_rate": 2.67048367800376e-05,
      "loss": 0.9316,
      "step": 11078
    },
    {
      "epoch": 0.04733927548988608,
      "grad_norm": 2.8667659759521484,
      "learning_rate": 2.6662109041189543e-05,
      "loss": 1.6272,
      "step": 11079
    },
    {
      "epoch": 0.04734354837331328,
      "grad_norm": 3.01511812210083,
      "learning_rate": 2.661938130234148e-05,
      "loss": 0.7125,
      "step": 11080
    },
    {
      "epoch": 0.047347821256740474,
      "grad_norm": 3.154916286468506,
      "learning_rate": 2.6576653563493422e-05,
      "loss": 0.5398,
      "step": 11081
    },
    {
      "epoch": 0.047352094140167666,
      "grad_norm": 0.6968209743499756,
      "learning_rate": 2.653392582464536e-05,
      "loss": 0.289,
      "step": 11082
    },
    {
      "epoch": 0.047356367023594866,
      "grad_norm": 1.1260745525360107,
      "learning_rate": 2.64911980857973e-05,
      "loss": 0.9404,
      "step": 11083
    },
    {
      "epoch": 0.04736063990702206,
      "grad_norm": 1.584265112876892,
      "learning_rate": 2.644847034694924e-05,
      "loss": 0.5778,
      "step": 11084
    },
    {
      "epoch": 0.04736491279044925,
      "grad_norm": 0.8889167308807373,
      "learning_rate": 2.640574260810118e-05,
      "loss": 0.3969,
      "step": 11085
    },
    {
      "epoch": 0.04736918567387644,
      "grad_norm": 1.9640145301818848,
      "learning_rate": 2.636301486925312e-05,
      "loss": 0.5647,
      "step": 11086
    },
    {
      "epoch": 0.04737345855730364,
      "grad_norm": 3.011169910430908,
      "learning_rate": 2.632028713040506e-05,
      "loss": 0.7311,
      "step": 11087
    },
    {
      "epoch": 0.04737773144073083,
      "grad_norm": 4.974483966827393,
      "learning_rate": 2.6277559391556997e-05,
      "loss": 1.0722,
      "step": 11088
    },
    {
      "epoch": 0.047382004324158025,
      "grad_norm": 1.1268818378448486,
      "learning_rate": 2.623483165270894e-05,
      "loss": 0.9399,
      "step": 11089
    },
    {
      "epoch": 0.047386277207585224,
      "grad_norm": 0.7425822615623474,
      "learning_rate": 2.6192103913860876e-05,
      "loss": 0.3185,
      "step": 11090
    },
    {
      "epoch": 0.047390550091012416,
      "grad_norm": 3.130249261856079,
      "learning_rate": 2.6149376175012818e-05,
      "loss": 0.5239,
      "step": 11091
    },
    {
      "epoch": 0.04739482297443961,
      "grad_norm": 2.995997667312622,
      "learning_rate": 2.610664843616476e-05,
      "loss": 0.7975,
      "step": 11092
    },
    {
      "epoch": 0.04739909585786681,
      "grad_norm": 0.8048420548439026,
      "learning_rate": 2.60639206973167e-05,
      "loss": 0.3636,
      "step": 11093
    },
    {
      "epoch": 0.047403368741294,
      "grad_norm": 0.7942264676094055,
      "learning_rate": 2.6021192958468638e-05,
      "loss": 0.5654,
      "step": 11094
    },
    {
      "epoch": 0.04740764162472119,
      "grad_norm": 0.883428156375885,
      "learning_rate": 2.597846521962058e-05,
      "loss": 0.3808,
      "step": 11095
    },
    {
      "epoch": 0.04741191450814839,
      "grad_norm": 2.487908363342285,
      "learning_rate": 2.5935737480772517e-05,
      "loss": 0.673,
      "step": 11096
    },
    {
      "epoch": 0.04741618739157558,
      "grad_norm": 2.213691473007202,
      "learning_rate": 2.5893009741924458e-05,
      "loss": 0.5849,
      "step": 11097
    },
    {
      "epoch": 0.047420460275002775,
      "grad_norm": 5.070163249969482,
      "learning_rate": 2.5850282003076396e-05,
      "loss": 1.2105,
      "step": 11098
    },
    {
      "epoch": 0.047424733158429974,
      "grad_norm": 3.074157476425171,
      "learning_rate": 2.5807554264228337e-05,
      "loss": 1.2083,
      "step": 11099
    },
    {
      "epoch": 0.047429006041857166,
      "grad_norm": 1.846662998199463,
      "learning_rate": 2.576482652538028e-05,
      "loss": 0.4692,
      "step": 11100
    },
    {
      "epoch": 0.04743327892528436,
      "grad_norm": 0.9802079796791077,
      "learning_rate": 2.5722098786532216e-05,
      "loss": 0.4028,
      "step": 11101
    },
    {
      "epoch": 0.04743755180871156,
      "grad_norm": 2.064265727996826,
      "learning_rate": 2.5679371047684158e-05,
      "loss": 0.5702,
      "step": 11102
    },
    {
      "epoch": 0.04744182469213875,
      "grad_norm": 0.8206357359886169,
      "learning_rate": 2.5636643308836096e-05,
      "loss": 0.5029,
      "step": 11103
    },
    {
      "epoch": 0.04744609757556594,
      "grad_norm": 4.483541011810303,
      "learning_rate": 2.5593915569988037e-05,
      "loss": 1.0971,
      "step": 11104
    },
    {
      "epoch": 0.04745037045899314,
      "grad_norm": 2.5944786071777344,
      "learning_rate": 2.5551187831139978e-05,
      "loss": 0.5374,
      "step": 11105
    },
    {
      "epoch": 0.04745464334242033,
      "grad_norm": 0.7917030453681946,
      "learning_rate": 2.550846009229192e-05,
      "loss": 0.5647,
      "step": 11106
    },
    {
      "epoch": 0.047458916225847525,
      "grad_norm": 4.632527828216553,
      "learning_rate": 2.5465732353443857e-05,
      "loss": 1.2189,
      "step": 11107
    },
    {
      "epoch": 0.047463189109274724,
      "grad_norm": 1.129217267036438,
      "learning_rate": 2.5423004614595798e-05,
      "loss": 0.9397,
      "step": 11108
    },
    {
      "epoch": 0.047467461992701916,
      "grad_norm": 0.956105649471283,
      "learning_rate": 2.5380276875747736e-05,
      "loss": 0.3887,
      "step": 11109
    },
    {
      "epoch": 0.04747173487612911,
      "grad_norm": 0.7902234792709351,
      "learning_rate": 2.5337549136899677e-05,
      "loss": 0.5639,
      "step": 11110
    },
    {
      "epoch": 0.0474760077595563,
      "grad_norm": 0.9682021141052246,
      "learning_rate": 2.5294821398051615e-05,
      "loss": 0.3755,
      "step": 11111
    },
    {
      "epoch": 0.0474802806429835,
      "grad_norm": 0.8233858346939087,
      "learning_rate": 2.5252093659203556e-05,
      "loss": 0.5022,
      "step": 11112
    },
    {
      "epoch": 0.04748455352641069,
      "grad_norm": 0.8263148069381714,
      "learning_rate": 2.5209365920355494e-05,
      "loss": 0.5022,
      "step": 11113
    },
    {
      "epoch": 0.047488826409837884,
      "grad_norm": 0.7872154712677002,
      "learning_rate": 2.5166638181507435e-05,
      "loss": 0.5609,
      "step": 11114
    },
    {
      "epoch": 0.04749309929326508,
      "grad_norm": 1.956769347190857,
      "learning_rate": 2.5123910442659373e-05,
      "loss": 0.5647,
      "step": 11115
    },
    {
      "epoch": 0.047497372176692275,
      "grad_norm": 1.9035296440124512,
      "learning_rate": 2.5081182703811315e-05,
      "loss": 0.4591,
      "step": 11116
    },
    {
      "epoch": 0.04750164506011947,
      "grad_norm": 4.472154140472412,
      "learning_rate": 2.5038454964963252e-05,
      "loss": 1.0709,
      "step": 11117
    },
    {
      "epoch": 0.047505917943546666,
      "grad_norm": 0.8213608860969543,
      "learning_rate": 2.4995727226115197e-05,
      "loss": 0.502,
      "step": 11118
    },
    {
      "epoch": 0.04751019082697386,
      "grad_norm": 0.8887366056442261,
      "learning_rate": 2.4952999487267135e-05,
      "loss": 0.397,
      "step": 11119
    },
    {
      "epoch": 0.04751446371040105,
      "grad_norm": 4.9554924964904785,
      "learning_rate": 2.4910271748419076e-05,
      "loss": 1.0759,
      "step": 11120
    },
    {
      "epoch": 0.04751873659382825,
      "grad_norm": 3.003551721572876,
      "learning_rate": 2.4867544009571014e-05,
      "loss": 1.0546,
      "step": 11121
    },
    {
      "epoch": 0.04752300947725544,
      "grad_norm": 0.6226999163627625,
      "learning_rate": 2.4824816270722955e-05,
      "loss": 0.1896,
      "step": 11122
    },
    {
      "epoch": 0.047527282360682634,
      "grad_norm": 0.7067748308181763,
      "learning_rate": 2.4782088531874893e-05,
      "loss": 0.2889,
      "step": 11123
    },
    {
      "epoch": 0.04753155524410983,
      "grad_norm": 0.7929689288139343,
      "learning_rate": 2.4739360793026834e-05,
      "loss": 0.5631,
      "step": 11124
    },
    {
      "epoch": 0.047535828127537025,
      "grad_norm": 4.062931537628174,
      "learning_rate": 2.4696633054178772e-05,
      "loss": 0.9143,
      "step": 11125
    },
    {
      "epoch": 0.04754010101096422,
      "grad_norm": 4.878049850463867,
      "learning_rate": 2.4653905315330713e-05,
      "loss": 1.0392,
      "step": 11126
    },
    {
      "epoch": 0.047544373894391416,
      "grad_norm": 0.7337831854820251,
      "learning_rate": 2.461117757648265e-05,
      "loss": 0.3343,
      "step": 11127
    },
    {
      "epoch": 0.04754864677781861,
      "grad_norm": 0.8709261417388916,
      "learning_rate": 2.4568449837634592e-05,
      "loss": 0.3515,
      "step": 11128
    },
    {
      "epoch": 0.0475529196612458,
      "grad_norm": 0.8849308490753174,
      "learning_rate": 2.452572209878653e-05,
      "loss": 0.397,
      "step": 11129
    },
    {
      "epoch": 0.047557192544673,
      "grad_norm": 3.266458034515381,
      "learning_rate": 2.448299435993847e-05,
      "loss": 0.8114,
      "step": 11130
    },
    {
      "epoch": 0.04756146542810019,
      "grad_norm": 3.4136316776275635,
      "learning_rate": 2.444026662109041e-05,
      "loss": 1.1328,
      "step": 11131
    },
    {
      "epoch": 0.047565738311527384,
      "grad_norm": 2.2131187915802,
      "learning_rate": 2.4397538882242354e-05,
      "loss": 0.5752,
      "step": 11132
    },
    {
      "epoch": 0.04757001119495458,
      "grad_norm": 2.895895004272461,
      "learning_rate": 2.4354811143394292e-05,
      "loss": 1.65,
      "step": 11133
    },
    {
      "epoch": 0.047574284078381775,
      "grad_norm": 0.7961285710334778,
      "learning_rate": 2.4312083404546233e-05,
      "loss": 0.587,
      "step": 11134
    },
    {
      "epoch": 0.04757855696180897,
      "grad_norm": 0.7964154481887817,
      "learning_rate": 2.426935566569817e-05,
      "loss": 0.5631,
      "step": 11135
    },
    {
      "epoch": 0.04758282984523616,
      "grad_norm": 0.7977051138877869,
      "learning_rate": 2.4226627926850112e-05,
      "loss": 0.587,
      "step": 11136
    },
    {
      "epoch": 0.04758710272866336,
      "grad_norm": 1.866942286491394,
      "learning_rate": 2.4183900188002053e-05,
      "loss": 0.661,
      "step": 11137
    },
    {
      "epoch": 0.04759137561209055,
      "grad_norm": 2.4938318729400635,
      "learning_rate": 2.414117244915399e-05,
      "loss": 0.6721,
      "step": 11138
    },
    {
      "epoch": 0.04759564849551774,
      "grad_norm": 0.9954834580421448,
      "learning_rate": 2.4098444710305932e-05,
      "loss": 0.4509,
      "step": 11139
    },
    {
      "epoch": 0.04759992137894494,
      "grad_norm": 0.7577902674674988,
      "learning_rate": 2.405571697145787e-05,
      "loss": 0.3345,
      "step": 11140
    },
    {
      "epoch": 0.04760419426237213,
      "grad_norm": 0.9889734387397766,
      "learning_rate": 2.401298923260981e-05,
      "loss": 0.4178,
      "step": 11141
    },
    {
      "epoch": 0.047608467145799326,
      "grad_norm": 0.7197926640510559,
      "learning_rate": 2.397026149376175e-05,
      "loss": 0.3183,
      "step": 11142
    },
    {
      "epoch": 0.047612740029226525,
      "grad_norm": 0.8264428973197937,
      "learning_rate": 2.392753375491369e-05,
      "loss": 0.5006,
      "step": 11143
    },
    {
      "epoch": 0.04761701291265372,
      "grad_norm": 4.152171611785889,
      "learning_rate": 2.388480601606563e-05,
      "loss": 2.2718,
      "step": 11144
    },
    {
      "epoch": 0.04762128579608091,
      "grad_norm": 3.410705327987671,
      "learning_rate": 2.3842078277217573e-05,
      "loss": 1.1363,
      "step": 11145
    },
    {
      "epoch": 0.04762555867950811,
      "grad_norm": 0.6950890421867371,
      "learning_rate": 2.379935053836951e-05,
      "loss": 0.3031,
      "step": 11146
    },
    {
      "epoch": 0.0476298315629353,
      "grad_norm": 1.1579073667526245,
      "learning_rate": 2.3756622799521452e-05,
      "loss": 0.9592,
      "step": 11147
    },
    {
      "epoch": 0.04763410444636249,
      "grad_norm": 0.3082524240016937,
      "learning_rate": 2.371389506067339e-05,
      "loss": 0.0805,
      "step": 11148
    },
    {
      "epoch": 0.04763837732978969,
      "grad_norm": 0.777912974357605,
      "learning_rate": 2.367116732182533e-05,
      "loss": 0.305,
      "step": 11149
    },
    {
      "epoch": 0.04764265021321688,
      "grad_norm": 3.315420389175415,
      "learning_rate": 2.362843958297727e-05,
      "loss": 0.7382,
      "step": 11150
    },
    {
      "epoch": 0.047646923096644075,
      "grad_norm": 0.6695525646209717,
      "learning_rate": 2.358571184412921e-05,
      "loss": 0.293,
      "step": 11151
    },
    {
      "epoch": 0.047651195980071274,
      "grad_norm": 1.8533892631530762,
      "learning_rate": 2.3542984105281148e-05,
      "loss": 0.4687,
      "step": 11152
    },
    {
      "epoch": 0.04765546886349847,
      "grad_norm": 1.3891924619674683,
      "learning_rate": 2.350025636643309e-05,
      "loss": 0.5446,
      "step": 11153
    },
    {
      "epoch": 0.04765974174692566,
      "grad_norm": 1.9147486686706543,
      "learning_rate": 2.3457528627585027e-05,
      "loss": 0.4497,
      "step": 11154
    },
    {
      "epoch": 0.04766401463035286,
      "grad_norm": 0.7751450538635254,
      "learning_rate": 2.341480088873697e-05,
      "loss": 0.3182,
      "step": 11155
    },
    {
      "epoch": 0.04766828751378005,
      "grad_norm": 0.8055148124694824,
      "learning_rate": 2.3372073149888906e-05,
      "loss": 0.5631,
      "step": 11156
    },
    {
      "epoch": 0.04767256039720724,
      "grad_norm": 0.6599502563476562,
      "learning_rate": 2.3329345411040847e-05,
      "loss": 0.2779,
      "step": 11157
    },
    {
      "epoch": 0.04767683328063444,
      "grad_norm": 1.1559453010559082,
      "learning_rate": 2.328661767219279e-05,
      "loss": 0.93,
      "step": 11158
    },
    {
      "epoch": 0.04768110616406163,
      "grad_norm": 1.0602349042892456,
      "learning_rate": 2.324388993334473e-05,
      "loss": 0.4586,
      "step": 11159
    },
    {
      "epoch": 0.047685379047488825,
      "grad_norm": 4.915222644805908,
      "learning_rate": 2.3201162194496668e-05,
      "loss": 1.0508,
      "step": 11160
    },
    {
      "epoch": 0.04768965193091602,
      "grad_norm": 0.7993730306625366,
      "learning_rate": 2.315843445564861e-05,
      "loss": 0.4927,
      "step": 11161
    },
    {
      "epoch": 0.047693924814343217,
      "grad_norm": 0.7781915068626404,
      "learning_rate": 2.3115706716800547e-05,
      "loss": 0.5218,
      "step": 11162
    },
    {
      "epoch": 0.04769819769777041,
      "grad_norm": 4.642290115356445,
      "learning_rate": 2.3072978977952488e-05,
      "loss": 1.1954,
      "step": 11163
    },
    {
      "epoch": 0.0477024705811976,
      "grad_norm": 0.8071424961090088,
      "learning_rate": 2.3030251239104426e-05,
      "loss": 0.3641,
      "step": 11164
    },
    {
      "epoch": 0.0477067434646248,
      "grad_norm": 2.215134382247925,
      "learning_rate": 2.2987523500256367e-05,
      "loss": 0.5599,
      "step": 11165
    },
    {
      "epoch": 0.04771101634805199,
      "grad_norm": 1.9168832302093506,
      "learning_rate": 2.2944795761408305e-05,
      "loss": 0.4482,
      "step": 11166
    },
    {
      "epoch": 0.047715289231479184,
      "grad_norm": 2.0660006999969482,
      "learning_rate": 2.2902068022560246e-05,
      "loss": 0.5722,
      "step": 11167
    },
    {
      "epoch": 0.04771956211490638,
      "grad_norm": 0.8924984335899353,
      "learning_rate": 2.2859340283712184e-05,
      "loss": 0.3808,
      "step": 11168
    },
    {
      "epoch": 0.047723834998333575,
      "grad_norm": 1.1610281467437744,
      "learning_rate": 2.2816612544864125e-05,
      "loss": 0.93,
      "step": 11169
    },
    {
      "epoch": 0.04772810788176077,
      "grad_norm": 0.78301602602005,
      "learning_rate": 2.2773884806016063e-05,
      "loss": 0.3185,
      "step": 11170
    },
    {
      "epoch": 0.047732380765187966,
      "grad_norm": 1.586216688156128,
      "learning_rate": 2.2731157067168008e-05,
      "loss": 0.5696,
      "step": 11171
    },
    {
      "epoch": 0.04773665364861516,
      "grad_norm": 4.697032451629639,
      "learning_rate": 2.2688429328319946e-05,
      "loss": 1.1284,
      "step": 11172
    },
    {
      "epoch": 0.04774092653204235,
      "grad_norm": 1.1684662103652954,
      "learning_rate": 2.2645701589471887e-05,
      "loss": 0.9289,
      "step": 11173
    },
    {
      "epoch": 0.04774519941546955,
      "grad_norm": 0.8235464096069336,
      "learning_rate": 2.2602973850623825e-05,
      "loss": 0.3807,
      "step": 11174
    },
    {
      "epoch": 0.04774947229889674,
      "grad_norm": 3.5765132904052734,
      "learning_rate": 2.2560246111775766e-05,
      "loss": 0.9411,
      "step": 11175
    },
    {
      "epoch": 0.047753745182323934,
      "grad_norm": 4.4459075927734375,
      "learning_rate": 2.2517518372927707e-05,
      "loss": 1.0324,
      "step": 11176
    },
    {
      "epoch": 0.04775801806575113,
      "grad_norm": 4.1635422706604,
      "learning_rate": 2.2474790634079645e-05,
      "loss": 1.1026,
      "step": 11177
    },
    {
      "epoch": 0.047762290949178325,
      "grad_norm": 3.676400661468506,
      "learning_rate": 2.2432062895231586e-05,
      "loss": 1.0931,
      "step": 11178
    },
    {
      "epoch": 0.04776656383260552,
      "grad_norm": 0.7936290502548218,
      "learning_rate": 2.2389335156383524e-05,
      "loss": 0.555,
      "step": 11179
    },
    {
      "epoch": 0.047770836716032716,
      "grad_norm": 3.5591065883636475,
      "learning_rate": 2.2346607417535465e-05,
      "loss": 1.1489,
      "step": 11180
    },
    {
      "epoch": 0.04777510959945991,
      "grad_norm": 2.7378060817718506,
      "learning_rate": 2.2303879678687403e-05,
      "loss": 0.8692,
      "step": 11181
    },
    {
      "epoch": 0.0477793824828871,
      "grad_norm": 3.480785369873047,
      "learning_rate": 2.2261151939839344e-05,
      "loss": 0.8614,
      "step": 11182
    },
    {
      "epoch": 0.0477836553663143,
      "grad_norm": 1.0138319730758667,
      "learning_rate": 2.2218424200991282e-05,
      "loss": 0.5283,
      "step": 11183
    },
    {
      "epoch": 0.04778792824974149,
      "grad_norm": 2.5656256675720215,
      "learning_rate": 2.2175696462143227e-05,
      "loss": 0.5381,
      "step": 11184
    },
    {
      "epoch": 0.047792201133168684,
      "grad_norm": 3.4004294872283936,
      "learning_rate": 2.2132968723295165e-05,
      "loss": 1.0715,
      "step": 11185
    },
    {
      "epoch": 0.047796474016595876,
      "grad_norm": 3.293210029602051,
      "learning_rate": 2.2090240984447106e-05,
      "loss": 0.8085,
      "step": 11186
    },
    {
      "epoch": 0.047800746900023075,
      "grad_norm": 0.8106124997138977,
      "learning_rate": 2.2047513245599044e-05,
      "loss": 0.364,
      "step": 11187
    },
    {
      "epoch": 0.04780501978345027,
      "grad_norm": 4.168494701385498,
      "learning_rate": 2.2004785506750985e-05,
      "loss": 2.3043,
      "step": 11188
    },
    {
      "epoch": 0.04780929266687746,
      "grad_norm": 1.9027941226959229,
      "learning_rate": 2.1962057767902923e-05,
      "loss": 0.4243,
      "step": 11189
    },
    {
      "epoch": 0.04781356555030466,
      "grad_norm": 3.711672306060791,
      "learning_rate": 2.1919330029054864e-05,
      "loss": 1.0588,
      "step": 11190
    },
    {
      "epoch": 0.04781783843373185,
      "grad_norm": 3.0202250480651855,
      "learning_rate": 2.1876602290206802e-05,
      "loss": 1.0332,
      "step": 11191
    },
    {
      "epoch": 0.04782211131715904,
      "grad_norm": 2.063850164413452,
      "learning_rate": 2.1833874551358743e-05,
      "loss": 0.5563,
      "step": 11192
    },
    {
      "epoch": 0.04782638420058624,
      "grad_norm": 0.8900446891784668,
      "learning_rate": 2.179114681251068e-05,
      "loss": 0.3807,
      "step": 11193
    },
    {
      "epoch": 0.047830657084013434,
      "grad_norm": 1.889139175415039,
      "learning_rate": 2.1748419073662622e-05,
      "loss": 0.4549,
      "step": 11194
    },
    {
      "epoch": 0.047834929967440626,
      "grad_norm": 2.2707438468933105,
      "learning_rate": 2.170569133481456e-05,
      "loss": 0.5568,
      "step": 11195
    },
    {
      "epoch": 0.047839202850867825,
      "grad_norm": 0.40274760127067566,
      "learning_rate": 2.16629635959665e-05,
      "loss": 0.1478,
      "step": 11196
    },
    {
      "epoch": 0.04784347573429502,
      "grad_norm": 4.98848295211792,
      "learning_rate": 2.1620235857118442e-05,
      "loss": 1.1914,
      "step": 11197
    },
    {
      "epoch": 0.04784774861772221,
      "grad_norm": 4.69694709777832,
      "learning_rate": 2.1577508118270384e-05,
      "loss": 1.1072,
      "step": 11198
    },
    {
      "epoch": 0.04785202150114941,
      "grad_norm": 0.9819197058677673,
      "learning_rate": 2.153478037942232e-05,
      "loss": 0.4179,
      "step": 11199
    },
    {
      "epoch": 0.0478562943845766,
      "grad_norm": 0.664436936378479,
      "learning_rate": 2.1492052640574263e-05,
      "loss": 0.2928,
      "step": 11200
    },
    {
      "epoch": 0.04786056726800379,
      "grad_norm": 1.8896536827087402,
      "learning_rate": 2.14493249017262e-05,
      "loss": 0.6558,
      "step": 11201
    },
    {
      "epoch": 0.04786484015143099,
      "grad_norm": 0.7952480316162109,
      "learning_rate": 2.1406597162878142e-05,
      "loss": 0.5551,
      "step": 11202
    },
    {
      "epoch": 0.047869113034858184,
      "grad_norm": 1.5024056434631348,
      "learning_rate": 2.136386942403008e-05,
      "loss": 0.3696,
      "step": 11203
    },
    {
      "epoch": 0.047873385918285376,
      "grad_norm": 2.9072916507720947,
      "learning_rate": 2.132114168518202e-05,
      "loss": 1.68,
      "step": 11204
    },
    {
      "epoch": 0.047877658801712575,
      "grad_norm": 3.5941338539123535,
      "learning_rate": 2.127841394633396e-05,
      "loss": 0.9457,
      "step": 11205
    },
    {
      "epoch": 0.04788193168513977,
      "grad_norm": 0.7271329760551453,
      "learning_rate": 2.12356862074859e-05,
      "loss": 0.3181,
      "step": 11206
    },
    {
      "epoch": 0.04788620456856696,
      "grad_norm": 0.8047415018081665,
      "learning_rate": 2.1192958468637838e-05,
      "loss": 0.4915,
      "step": 11207
    },
    {
      "epoch": 0.04789047745199416,
      "grad_norm": 0.9732540845870972,
      "learning_rate": 2.115023072978978e-05,
      "loss": 0.4027,
      "step": 11208
    },
    {
      "epoch": 0.04789475033542135,
      "grad_norm": 0.5937208533287048,
      "learning_rate": 2.1107502990941717e-05,
      "loss": 0.256,
      "step": 11209
    },
    {
      "epoch": 0.04789902321884854,
      "grad_norm": 0.5779936909675598,
      "learning_rate": 2.106477525209366e-05,
      "loss": 0.256,
      "step": 11210
    },
    {
      "epoch": 0.047903296102275734,
      "grad_norm": 0.776843786239624,
      "learning_rate": 2.10220475132456e-05,
      "loss": 0.5539,
      "step": 11211
    },
    {
      "epoch": 0.047907568985702934,
      "grad_norm": 0.8074800372123718,
      "learning_rate": 2.097931977439754e-05,
      "loss": 0.4927,
      "step": 11212
    },
    {
      "epoch": 0.047911841869130126,
      "grad_norm": 0.823089063167572,
      "learning_rate": 2.0936592035549482e-05,
      "loss": 0.3807,
      "step": 11213
    },
    {
      "epoch": 0.04791611475255732,
      "grad_norm": 2.2471652030944824,
      "learning_rate": 2.089386429670142e-05,
      "loss": 0.5614,
      "step": 11214
    },
    {
      "epoch": 0.04792038763598452,
      "grad_norm": 0.795239269733429,
      "learning_rate": 2.085113655785336e-05,
      "loss": 0.5543,
      "step": 11215
    },
    {
      "epoch": 0.04792466051941171,
      "grad_norm": 3.58247447013855,
      "learning_rate": 2.08084088190053e-05,
      "loss": 0.9258,
      "step": 11216
    },
    {
      "epoch": 0.0479289334028389,
      "grad_norm": 1.057586908340454,
      "learning_rate": 2.076568108015724e-05,
      "loss": 0.4585,
      "step": 11217
    },
    {
      "epoch": 0.0479332062862661,
      "grad_norm": 1.1862379312515259,
      "learning_rate": 2.0722953341309178e-05,
      "loss": 0.982,
      "step": 11218
    },
    {
      "epoch": 0.04793747916969329,
      "grad_norm": 0.36507463455200195,
      "learning_rate": 2.068022560246112e-05,
      "loss": 0.0957,
      "step": 11219
    },
    {
      "epoch": 0.047941752053120484,
      "grad_norm": 1.9341264963150024,
      "learning_rate": 2.0637497863613057e-05,
      "loss": 0.5842,
      "step": 11220
    },
    {
      "epoch": 0.04794602493654768,
      "grad_norm": 2.9963533878326416,
      "learning_rate": 2.0594770124764998e-05,
      "loss": 0.7739,
      "step": 11221
    },
    {
      "epoch": 0.047950297819974876,
      "grad_norm": 1.595044732093811,
      "learning_rate": 2.0552042385916936e-05,
      "loss": 0.564,
      "step": 11222
    },
    {
      "epoch": 0.04795457070340207,
      "grad_norm": 0.7989533543586731,
      "learning_rate": 2.050931464706888e-05,
      "loss": 0.5551,
      "step": 11223
    },
    {
      "epoch": 0.04795884358682927,
      "grad_norm": 3.413386821746826,
      "learning_rate": 2.046658690822082e-05,
      "loss": 0.8086,
      "step": 11224
    },
    {
      "epoch": 0.04796311647025646,
      "grad_norm": 0.8017972707748413,
      "learning_rate": 2.042385916937276e-05,
      "loss": 0.3475,
      "step": 11225
    },
    {
      "epoch": 0.04796738935368365,
      "grad_norm": 0.88969486951828,
      "learning_rate": 2.0381131430524697e-05,
      "loss": 0.3807,
      "step": 11226
    },
    {
      "epoch": 0.04797166223711085,
      "grad_norm": 0.790736198425293,
      "learning_rate": 2.033840369167664e-05,
      "loss": 0.3475,
      "step": 11227
    },
    {
      "epoch": 0.04797593512053804,
      "grad_norm": 2.40315842628479,
      "learning_rate": 2.0295675952828576e-05,
      "loss": 0.6393,
      "step": 11228
    },
    {
      "epoch": 0.047980208003965234,
      "grad_norm": 0.6845375299453735,
      "learning_rate": 2.0252948213980518e-05,
      "loss": 0.2928,
      "step": 11229
    },
    {
      "epoch": 0.04798448088739243,
      "grad_norm": 3.4281368255615234,
      "learning_rate": 2.0210220475132456e-05,
      "loss": 1.0647,
      "step": 11230
    },
    {
      "epoch": 0.047988753770819625,
      "grad_norm": 3.079841375350952,
      "learning_rate": 2.0167492736284397e-05,
      "loss": 0.7834,
      "step": 11231
    },
    {
      "epoch": 0.04799302665424682,
      "grad_norm": 0.814549446105957,
      "learning_rate": 2.0124764997436335e-05,
      "loss": 0.3636,
      "step": 11232
    },
    {
      "epoch": 0.04799729953767402,
      "grad_norm": 4.157926559448242,
      "learning_rate": 2.0082037258588276e-05,
      "loss": 1.1907,
      "step": 11233
    },
    {
      "epoch": 0.04800157242110121,
      "grad_norm": 0.3665872812271118,
      "learning_rate": 2.0039309519740214e-05,
      "loss": 0.0957,
      "step": 11234
    },
    {
      "epoch": 0.0480058453045284,
      "grad_norm": 1.892403483390808,
      "learning_rate": 1.9996581780892155e-05,
      "loss": 0.6481,
      "step": 11235
    },
    {
      "epoch": 0.04801011818795559,
      "grad_norm": 0.8093202710151672,
      "learning_rate": 1.9953854042044093e-05,
      "loss": 0.4913,
      "step": 11236
    },
    {
      "epoch": 0.04801439107138279,
      "grad_norm": 2.7101171016693115,
      "learning_rate": 1.9911126303196037e-05,
      "loss": 0.6594,
      "step": 11237
    },
    {
      "epoch": 0.048018663954809984,
      "grad_norm": 0.6803621649742126,
      "learning_rate": 1.9868398564347975e-05,
      "loss": 0.2929,
      "step": 11238
    },
    {
      "epoch": 0.048022936838237176,
      "grad_norm": 2.3136537075042725,
      "learning_rate": 1.9825670825499916e-05,
      "loss": 0.6056,
      "step": 11239
    },
    {
      "epoch": 0.048027209721664375,
      "grad_norm": 0.8919440507888794,
      "learning_rate": 1.9782943086651854e-05,
      "loss": 0.3655,
      "step": 11240
    },
    {
      "epoch": 0.04803148260509157,
      "grad_norm": 1.985608696937561,
      "learning_rate": 1.9740215347803796e-05,
      "loss": 0.5543,
      "step": 11241
    },
    {
      "epoch": 0.04803575548851876,
      "grad_norm": 0.648371160030365,
      "learning_rate": 1.9697487608955733e-05,
      "loss": 0.2778,
      "step": 11242
    },
    {
      "epoch": 0.04804002837194596,
      "grad_norm": 0.40749695897102356,
      "learning_rate": 1.9654759870107675e-05,
      "loss": 0.1477,
      "step": 11243
    },
    {
      "epoch": 0.04804430125537315,
      "grad_norm": 0.7875248789787292,
      "learning_rate": 1.9612032131259612e-05,
      "loss": 0.3185,
      "step": 11244
    },
    {
      "epoch": 0.04804857413880034,
      "grad_norm": 0.7041695713996887,
      "learning_rate": 1.9569304392411554e-05,
      "loss": 0.309,
      "step": 11245
    },
    {
      "epoch": 0.04805284702222754,
      "grad_norm": 1.7052615880966187,
      "learning_rate": 1.952657665356349e-05,
      "loss": 1.2068,
      "step": 11246
    },
    {
      "epoch": 0.048057119905654734,
      "grad_norm": 0.5961642861366272,
      "learning_rate": 1.9483848914715433e-05,
      "loss": 0.256,
      "step": 11247
    },
    {
      "epoch": 0.048061392789081926,
      "grad_norm": 4.685266971588135,
      "learning_rate": 1.944112117586737e-05,
      "loss": 1.0685,
      "step": 11248
    },
    {
      "epoch": 0.048065665672509125,
      "grad_norm": 4.6909918785095215,
      "learning_rate": 1.9398393437019312e-05,
      "loss": 1.1046,
      "step": 11249
    },
    {
      "epoch": 0.04806993855593632,
      "grad_norm": 0.9148876667022705,
      "learning_rate": 1.9355665698171253e-05,
      "loss": 0.4139,
      "step": 11250
    },
    {
      "epoch": 0.04807421143936351,
      "grad_norm": 1.6078635454177856,
      "learning_rate": 1.9312937959323194e-05,
      "loss": 0.5694,
      "step": 11251
    },
    {
      "epoch": 0.04807848432279071,
      "grad_norm": 0.7834146618843079,
      "learning_rate": 1.9270210220475135e-05,
      "loss": 0.3323,
      "step": 11252
    },
    {
      "epoch": 0.0480827572062179,
      "grad_norm": 4.131053447723389,
      "learning_rate": 1.9227482481627073e-05,
      "loss": 0.9504,
      "step": 11253
    },
    {
      "epoch": 0.04808703008964509,
      "grad_norm": 0.7575769424438477,
      "learning_rate": 1.9184754742779015e-05,
      "loss": 0.5129,
      "step": 11254
    },
    {
      "epoch": 0.04809130297307229,
      "grad_norm": 1.9810210466384888,
      "learning_rate": 1.9142027003930952e-05,
      "loss": 0.5514,
      "step": 11255
    },
    {
      "epoch": 0.048095575856499484,
      "grad_norm": 0.8216245770454407,
      "learning_rate": 1.9099299265082894e-05,
      "loss": 0.3636,
      "step": 11256
    },
    {
      "epoch": 0.048099848739926676,
      "grad_norm": 0.8144463896751404,
      "learning_rate": 1.905657152623483e-05,
      "loss": 0.3636,
      "step": 11257
    },
    {
      "epoch": 0.04810412162335387,
      "grad_norm": 1.9132499694824219,
      "learning_rate": 1.9013843787386773e-05,
      "loss": 0.4323,
      "step": 11258
    },
    {
      "epoch": 0.04810839450678107,
      "grad_norm": 2.818084478378296,
      "learning_rate": 1.897111604853871e-05,
      "loss": 0.7712,
      "step": 11259
    },
    {
      "epoch": 0.04811266739020826,
      "grad_norm": 3.0180416107177734,
      "learning_rate": 1.8928388309690652e-05,
      "loss": 0.7773,
      "step": 11260
    },
    {
      "epoch": 0.04811694027363545,
      "grad_norm": 0.7132663130760193,
      "learning_rate": 1.888566057084259e-05,
      "loss": 0.303,
      "step": 11261
    },
    {
      "epoch": 0.04812121315706265,
      "grad_norm": 1.9082908630371094,
      "learning_rate": 1.884293283199453e-05,
      "loss": 0.4591,
      "step": 11262
    },
    {
      "epoch": 0.04812548604048984,
      "grad_norm": 0.6159643530845642,
      "learning_rate": 1.8800205093146472e-05,
      "loss": 0.2871,
      "step": 11263
    },
    {
      "epoch": 0.048129758923917035,
      "grad_norm": 1.013516902923584,
      "learning_rate": 1.8757477354298413e-05,
      "loss": 0.4509,
      "step": 11264
    },
    {
      "epoch": 0.048134031807344234,
      "grad_norm": 1.7221072912216187,
      "learning_rate": 1.871474961545035e-05,
      "loss": 1.2319,
      "step": 11265
    },
    {
      "epoch": 0.048138304690771426,
      "grad_norm": 4.617204666137695,
      "learning_rate": 1.8672021876602292e-05,
      "loss": 1.1592,
      "step": 11266
    },
    {
      "epoch": 0.04814257757419862,
      "grad_norm": 0.31004759669303894,
      "learning_rate": 1.862929413775423e-05,
      "loss": 0.0805,
      "step": 11267
    },
    {
      "epoch": 0.04814685045762582,
      "grad_norm": 1.8922648429870605,
      "learning_rate": 1.858656639890617e-05,
      "loss": 0.4475,
      "step": 11268
    },
    {
      "epoch": 0.04815112334105301,
      "grad_norm": 1.8797967433929443,
      "learning_rate": 1.854383866005811e-05,
      "loss": 0.6377,
      "step": 11269
    },
    {
      "epoch": 0.0481553962244802,
      "grad_norm": 4.100503444671631,
      "learning_rate": 1.850111092121005e-05,
      "loss": 0.9247,
      "step": 11270
    },
    {
      "epoch": 0.0481596691079074,
      "grad_norm": 0.4127979278564453,
      "learning_rate": 1.845838318236199e-05,
      "loss": 0.1477,
      "step": 11271
    },
    {
      "epoch": 0.04816394199133459,
      "grad_norm": 3.575820207595825,
      "learning_rate": 1.841565544351393e-05,
      "loss": 0.9063,
      "step": 11272
    },
    {
      "epoch": 0.048168214874761785,
      "grad_norm": 2.0935990810394287,
      "learning_rate": 1.8372927704665867e-05,
      "loss": 0.5676,
      "step": 11273
    },
    {
      "epoch": 0.048172487758188984,
      "grad_norm": 1.601425051689148,
      "learning_rate": 1.833019996581781e-05,
      "loss": 0.5569,
      "step": 11274
    },
    {
      "epoch": 0.048176760641616176,
      "grad_norm": 1.8765499591827393,
      "learning_rate": 1.8287472226969747e-05,
      "loss": 0.6283,
      "step": 11275
    },
    {
      "epoch": 0.04818103352504337,
      "grad_norm": 2.762047290802002,
      "learning_rate": 1.824474448812169e-05,
      "loss": 0.8503,
      "step": 11276
    },
    {
      "epoch": 0.04818530640847057,
      "grad_norm": 2.016758680343628,
      "learning_rate": 1.820201674927363e-05,
      "loss": 0.5701,
      "step": 11277
    },
    {
      "epoch": 0.04818957929189776,
      "grad_norm": 0.7897406816482544,
      "learning_rate": 1.815928901042557e-05,
      "loss": 0.3185,
      "step": 11278
    },
    {
      "epoch": 0.04819385217532495,
      "grad_norm": 0.578691303730011,
      "learning_rate": 1.8116561271577508e-05,
      "loss": 0.2559,
      "step": 11279
    },
    {
      "epoch": 0.04819812505875215,
      "grad_norm": 2.089148998260498,
      "learning_rate": 1.807383353272945e-05,
      "loss": 0.5645,
      "step": 11280
    },
    {
      "epoch": 0.04820239794217934,
      "grad_norm": 3.0507121086120605,
      "learning_rate": 1.8031105793881387e-05,
      "loss": 1.0133,
      "step": 11281
    },
    {
      "epoch": 0.048206670825606535,
      "grad_norm": 2.0921051502227783,
      "learning_rate": 1.798837805503333e-05,
      "loss": 0.6478,
      "step": 11282
    },
    {
      "epoch": 0.04821094370903373,
      "grad_norm": 2.0822954177856445,
      "learning_rate": 1.7945650316185266e-05,
      "loss": 0.5595,
      "step": 11283
    },
    {
      "epoch": 0.048215216592460926,
      "grad_norm": 0.8739038109779358,
      "learning_rate": 1.7902922577337207e-05,
      "loss": 0.3655,
      "step": 11284
    },
    {
      "epoch": 0.04821948947588812,
      "grad_norm": 3.6104934215545654,
      "learning_rate": 1.7860194838489145e-05,
      "loss": 0.9195,
      "step": 11285
    },
    {
      "epoch": 0.04822376235931531,
      "grad_norm": 1.1747227907180786,
      "learning_rate": 1.7817467099641086e-05,
      "loss": 0.9568,
      "step": 11286
    },
    {
      "epoch": 0.04822803524274251,
      "grad_norm": 0.7562475204467773,
      "learning_rate": 1.7774739360793024e-05,
      "loss": 0.3513,
      "step": 11287
    },
    {
      "epoch": 0.0482323081261697,
      "grad_norm": 0.9047563076019287,
      "learning_rate": 1.7732011621944966e-05,
      "loss": 0.3967,
      "step": 11288
    },
    {
      "epoch": 0.04823658100959689,
      "grad_norm": 1.9554582834243774,
      "learning_rate": 1.768928388309691e-05,
      "loss": 0.5994,
      "step": 11289
    },
    {
      "epoch": 0.04824085389302409,
      "grad_norm": 3.4300312995910645,
      "learning_rate": 1.7646556144248848e-05,
      "loss": 1.0782,
      "step": 11290
    },
    {
      "epoch": 0.048245126776451285,
      "grad_norm": 4.191042900085449,
      "learning_rate": 1.760382840540079e-05,
      "loss": 2.2693,
      "step": 11291
    },
    {
      "epoch": 0.04824939965987848,
      "grad_norm": 2.284545421600342,
      "learning_rate": 1.7561100666552727e-05,
      "loss": 0.5519,
      "step": 11292
    },
    {
      "epoch": 0.048253672543305676,
      "grad_norm": 4.860595226287842,
      "learning_rate": 1.751837292770467e-05,
      "loss": 1.0126,
      "step": 11293
    },
    {
      "epoch": 0.04825794542673287,
      "grad_norm": 3.660442590713501,
      "learning_rate": 1.7475645188856606e-05,
      "loss": 2.5193,
      "step": 11294
    },
    {
      "epoch": 0.04826221831016006,
      "grad_norm": 1.063380479812622,
      "learning_rate": 1.7432917450008547e-05,
      "loss": 0.4585,
      "step": 11295
    },
    {
      "epoch": 0.04826649119358726,
      "grad_norm": 1.4577398300170898,
      "learning_rate": 1.7390189711160485e-05,
      "loss": 0.5343,
      "step": 11296
    },
    {
      "epoch": 0.04827076407701445,
      "grad_norm": 4.091109752655029,
      "learning_rate": 1.7347461972312426e-05,
      "loss": 0.89,
      "step": 11297
    },
    {
      "epoch": 0.04827503696044164,
      "grad_norm": 0.7375052571296692,
      "learning_rate": 1.7304734233464364e-05,
      "loss": 0.318,
      "step": 11298
    },
    {
      "epoch": 0.04827930984386884,
      "grad_norm": 0.7954201698303223,
      "learning_rate": 1.7262006494616306e-05,
      "loss": 0.3185,
      "step": 11299
    },
    {
      "epoch": 0.048283582727296034,
      "grad_norm": 3.7234442234039307,
      "learning_rate": 1.7219278755768243e-05,
      "loss": 1.0737,
      "step": 11300
    },
    {
      "epoch": 0.048287855610723227,
      "grad_norm": 3.041923999786377,
      "learning_rate": 1.7176551016920185e-05,
      "loss": 1.0476,
      "step": 11301
    },
    {
      "epoch": 0.048292128494150426,
      "grad_norm": 1.0528035163879395,
      "learning_rate": 1.7133823278072122e-05,
      "loss": 0.4433,
      "step": 11302
    },
    {
      "epoch": 0.04829640137757762,
      "grad_norm": 7.95147705078125,
      "learning_rate": 1.7091095539224067e-05,
      "loss": 3.7488,
      "step": 11303
    },
    {
      "epoch": 0.04830067426100481,
      "grad_norm": 1.1417659521102905,
      "learning_rate": 1.7048367800376005e-05,
      "loss": 0.9069,
      "step": 11304
    },
    {
      "epoch": 0.04830494714443201,
      "grad_norm": 0.74334716796875,
      "learning_rate": 1.7005640061527946e-05,
      "loss": 0.3343,
      "step": 11305
    },
    {
      "epoch": 0.0483092200278592,
      "grad_norm": 1.5135163068771362,
      "learning_rate": 1.6962912322679884e-05,
      "loss": 0.3642,
      "step": 11306
    },
    {
      "epoch": 0.04831349291128639,
      "grad_norm": 1.1717900037765503,
      "learning_rate": 1.6920184583831825e-05,
      "loss": 0.9571,
      "step": 11307
    },
    {
      "epoch": 0.048317765794713585,
      "grad_norm": 3.2559173107147217,
      "learning_rate": 1.6877456844983763e-05,
      "loss": 0.6028,
      "step": 11308
    },
    {
      "epoch": 0.048322038678140784,
      "grad_norm": 1.196243405342102,
      "learning_rate": 1.6834729106135704e-05,
      "loss": 0.5726,
      "step": 11309
    },
    {
      "epoch": 0.048326311561567976,
      "grad_norm": 3.1296589374542236,
      "learning_rate": 1.6792001367287642e-05,
      "loss": 1.2233,
      "step": 11310
    },
    {
      "epoch": 0.04833058444499517,
      "grad_norm": 3.0249905586242676,
      "learning_rate": 1.6749273628439583e-05,
      "loss": 1.0062,
      "step": 11311
    },
    {
      "epoch": 0.04833485732842237,
      "grad_norm": 0.8109036087989807,
      "learning_rate": 1.670654588959152e-05,
      "loss": 0.4927,
      "step": 11312
    },
    {
      "epoch": 0.04833913021184956,
      "grad_norm": 4.854827404022217,
      "learning_rate": 1.6663818150743462e-05,
      "loss": 1.011,
      "step": 11313
    },
    {
      "epoch": 0.04834340309527675,
      "grad_norm": 3.5043811798095703,
      "learning_rate": 1.66210904118954e-05,
      "loss": 0.7534,
      "step": 11314
    },
    {
      "epoch": 0.04834767597870395,
      "grad_norm": 4.215142250061035,
      "learning_rate": 1.657836267304734e-05,
      "loss": 1.134,
      "step": 11315
    },
    {
      "epoch": 0.04835194886213114,
      "grad_norm": 4.854379177093506,
      "learning_rate": 1.6535634934199283e-05,
      "loss": 0.9861,
      "step": 11316
    },
    {
      "epoch": 0.048356221745558335,
      "grad_norm": 0.7996143698692322,
      "learning_rate": 1.6492907195351224e-05,
      "loss": 0.5552,
      "step": 11317
    },
    {
      "epoch": 0.048360494628985534,
      "grad_norm": 1.9621505737304688,
      "learning_rate": 1.6450179456503162e-05,
      "loss": 0.5346,
      "step": 11318
    },
    {
      "epoch": 0.048364767512412726,
      "grad_norm": 0.5969098210334778,
      "learning_rate": 1.6407451717655103e-05,
      "loss": 0.256,
      "step": 11319
    },
    {
      "epoch": 0.04836904039583992,
      "grad_norm": 0.971314549446106,
      "learning_rate": 1.636472397880704e-05,
      "loss": 0.4027,
      "step": 11320
    },
    {
      "epoch": 0.04837331327926712,
      "grad_norm": 0.7837936282157898,
      "learning_rate": 1.6321996239958982e-05,
      "loss": 0.3324,
      "step": 11321
    },
    {
      "epoch": 0.04837758616269431,
      "grad_norm": 2.276416063308716,
      "learning_rate": 1.627926850111092e-05,
      "loss": 0.5493,
      "step": 11322
    },
    {
      "epoch": 0.0483818590461215,
      "grad_norm": 4.198078155517578,
      "learning_rate": 1.623654076226286e-05,
      "loss": 1.1125,
      "step": 11323
    },
    {
      "epoch": 0.0483861319295487,
      "grad_norm": 3.1106367111206055,
      "learning_rate": 1.61938130234148e-05,
      "loss": 1.2119,
      "step": 11324
    },
    {
      "epoch": 0.04839040481297589,
      "grad_norm": 1.5860849618911743,
      "learning_rate": 1.615108528456674e-05,
      "loss": 0.5494,
      "step": 11325
    },
    {
      "epoch": 0.048394677696403085,
      "grad_norm": 1.8757784366607666,
      "learning_rate": 1.610835754571868e-05,
      "loss": 0.4714,
      "step": 11326
    },
    {
      "epoch": 0.048398950579830284,
      "grad_norm": 0.7063192129135132,
      "learning_rate": 1.606562980687062e-05,
      "loss": 0.2888,
      "step": 11327
    },
    {
      "epoch": 0.048403223463257476,
      "grad_norm": 2.5968847274780273,
      "learning_rate": 1.602290206802256e-05,
      "loss": 0.5612,
      "step": 11328
    },
    {
      "epoch": 0.04840749634668467,
      "grad_norm": 3.4237868785858154,
      "learning_rate": 1.5980174329174502e-05,
      "loss": 0.8193,
      "step": 11329
    },
    {
      "epoch": 0.04841176923011187,
      "grad_norm": 1.9509685039520264,
      "learning_rate": 1.5937446590326443e-05,
      "loss": 0.5335,
      "step": 11330
    },
    {
      "epoch": 0.04841604211353906,
      "grad_norm": 0.8328748345375061,
      "learning_rate": 1.589471885147838e-05,
      "loss": 0.4984,
      "step": 11331
    },
    {
      "epoch": 0.04842031499696625,
      "grad_norm": 2.306539535522461,
      "learning_rate": 1.5851991112630322e-05,
      "loss": 0.6004,
      "step": 11332
    },
    {
      "epoch": 0.048424587880393444,
      "grad_norm": 0.9788897633552551,
      "learning_rate": 1.580926337378226e-05,
      "loss": 0.3226,
      "step": 11333
    },
    {
      "epoch": 0.04842886076382064,
      "grad_norm": 4.168105125427246,
      "learning_rate": 1.57665356349342e-05,
      "loss": 2.2428,
      "step": 11334
    },
    {
      "epoch": 0.048433133647247835,
      "grad_norm": 1.8528742790222168,
      "learning_rate": 1.572380789608614e-05,
      "loss": 0.6213,
      "step": 11335
    },
    {
      "epoch": 0.04843740653067503,
      "grad_norm": 0.7983163595199585,
      "learning_rate": 1.568108015723808e-05,
      "loss": 0.5314,
      "step": 11336
    },
    {
      "epoch": 0.048441679414102226,
      "grad_norm": 2.731900215148926,
      "learning_rate": 1.5638352418390018e-05,
      "loss": 0.6588,
      "step": 11337
    },
    {
      "epoch": 0.04844595229752942,
      "grad_norm": 2.997781991958618,
      "learning_rate": 1.559562467954196e-05,
      "loss": 0.9889,
      "step": 11338
    },
    {
      "epoch": 0.04845022518095661,
      "grad_norm": 0.8146224617958069,
      "learning_rate": 1.5552896940693897e-05,
      "loss": 0.4929,
      "step": 11339
    },
    {
      "epoch": 0.04845449806438381,
      "grad_norm": 0.8090220093727112,
      "learning_rate": 1.551016920184584e-05,
      "loss": 0.4926,
      "step": 11340
    },
    {
      "epoch": 0.048458770947811,
      "grad_norm": 2.686702251434326,
      "learning_rate": 1.5467441462997776e-05,
      "loss": 0.6644,
      "step": 11341
    },
    {
      "epoch": 0.048463043831238194,
      "grad_norm": 2.2794477939605713,
      "learning_rate": 1.5424713724149717e-05,
      "loss": 0.5289,
      "step": 11342
    },
    {
      "epoch": 0.04846731671466539,
      "grad_norm": 0.9841382503509521,
      "learning_rate": 1.538198598530166e-05,
      "loss": 0.5283,
      "step": 11343
    },
    {
      "epoch": 0.048471589598092585,
      "grad_norm": 3.0794014930725098,
      "learning_rate": 1.5339258246453597e-05,
      "loss": 1.1633,
      "step": 11344
    },
    {
      "epoch": 0.04847586248151978,
      "grad_norm": 2.958782434463501,
      "learning_rate": 1.5296530507605538e-05,
      "loss": 0.9551,
      "step": 11345
    },
    {
      "epoch": 0.048480135364946976,
      "grad_norm": 1.8674734830856323,
      "learning_rate": 1.5253802768757479e-05,
      "loss": 0.4518,
      "step": 11346
    },
    {
      "epoch": 0.04848440824837417,
      "grad_norm": 3.7191569805145264,
      "learning_rate": 1.5211075029909418e-05,
      "loss": 1.083,
      "step": 11347
    },
    {
      "epoch": 0.04848868113180136,
      "grad_norm": 1.503660798072815,
      "learning_rate": 1.5168347291061358e-05,
      "loss": 0.3538,
      "step": 11348
    },
    {
      "epoch": 0.04849295401522856,
      "grad_norm": 3.339367628097534,
      "learning_rate": 1.5125619552213298e-05,
      "loss": 0.8116,
      "step": 11349
    },
    {
      "epoch": 0.04849722689865575,
      "grad_norm": 0.8080199956893921,
      "learning_rate": 1.5082891813365237e-05,
      "loss": 0.4927,
      "step": 11350
    },
    {
      "epoch": 0.048501499782082944,
      "grad_norm": 0.8093883395195007,
      "learning_rate": 1.5040164074517177e-05,
      "loss": 0.4919,
      "step": 11351
    },
    {
      "epoch": 0.04850577266551014,
      "grad_norm": 3.610023021697998,
      "learning_rate": 1.4997436335669118e-05,
      "loss": 0.9471,
      "step": 11352
    },
    {
      "epoch": 0.048510045548937335,
      "grad_norm": 0.7843474745750427,
      "learning_rate": 1.4954708596821057e-05,
      "loss": 0.3182,
      "step": 11353
    },
    {
      "epoch": 0.04851431843236453,
      "grad_norm": 2.6570324897766113,
      "learning_rate": 1.4911980857972997e-05,
      "loss": 0.6284,
      "step": 11354
    },
    {
      "epoch": 0.048518591315791726,
      "grad_norm": 0.7060085535049438,
      "learning_rate": 1.4869253119124936e-05,
      "loss": 0.2888,
      "step": 11355
    },
    {
      "epoch": 0.04852286419921892,
      "grad_norm": 4.706782817840576,
      "learning_rate": 1.4826525380276876e-05,
      "loss": 1.1245,
      "step": 11356
    },
    {
      "epoch": 0.04852713708264611,
      "grad_norm": 0.7425199747085571,
      "learning_rate": 1.4783797641428816e-05,
      "loss": 0.3342,
      "step": 11357
    },
    {
      "epoch": 0.0485314099660733,
      "grad_norm": 1.6994999647140503,
      "learning_rate": 1.4741069902580757e-05,
      "loss": 1.2418,
      "step": 11358
    },
    {
      "epoch": 0.0485356828495005,
      "grad_norm": 2.0913283824920654,
      "learning_rate": 1.4698342163732696e-05,
      "loss": 0.5853,
      "step": 11359
    },
    {
      "epoch": 0.04853995573292769,
      "grad_norm": 4.1741251945495605,
      "learning_rate": 1.4655614424884636e-05,
      "loss": 2.3044,
      "step": 11360
    },
    {
      "epoch": 0.048544228616354886,
      "grad_norm": 3.3226583003997803,
      "learning_rate": 1.4612886686036575e-05,
      "loss": 0.7832,
      "step": 11361
    },
    {
      "epoch": 0.048548501499782085,
      "grad_norm": 0.8213745355606079,
      "learning_rate": 1.4570158947188515e-05,
      "loss": 0.3636,
      "step": 11362
    },
    {
      "epoch": 0.04855277438320928,
      "grad_norm": 2.9156110286712646,
      "learning_rate": 1.4527431208340454e-05,
      "loss": 1.679,
      "step": 11363
    },
    {
      "epoch": 0.04855704726663647,
      "grad_norm": 0.733759343624115,
      "learning_rate": 1.4484703469492394e-05,
      "loss": 0.3343,
      "step": 11364
    },
    {
      "epoch": 0.04856132015006367,
      "grad_norm": 0.6000399589538574,
      "learning_rate": 1.4441975730644335e-05,
      "loss": 0.2559,
      "step": 11365
    },
    {
      "epoch": 0.04856559303349086,
      "grad_norm": 3.493828773498535,
      "learning_rate": 1.4399247991796275e-05,
      "loss": 0.8569,
      "step": 11366
    },
    {
      "epoch": 0.04856986591691805,
      "grad_norm": 4.169630527496338,
      "learning_rate": 1.4356520252948214e-05,
      "loss": 1.2079,
      "step": 11367
    },
    {
      "epoch": 0.04857413880034525,
      "grad_norm": 3.3159122467041016,
      "learning_rate": 1.4313792514100154e-05,
      "loss": 0.7917,
      "step": 11368
    },
    {
      "epoch": 0.04857841168377244,
      "grad_norm": 0.6568311452865601,
      "learning_rate": 1.4271064775252093e-05,
      "loss": 0.2637,
      "step": 11369
    },
    {
      "epoch": 0.048582684567199635,
      "grad_norm": 0.8217676281929016,
      "learning_rate": 1.4228337036404033e-05,
      "loss": 0.5218,
      "step": 11370
    },
    {
      "epoch": 0.048586957450626835,
      "grad_norm": 0.7952015995979309,
      "learning_rate": 1.4185609297555972e-05,
      "loss": 0.555,
      "step": 11371
    },
    {
      "epoch": 0.04859123033405403,
      "grad_norm": 2.261683225631714,
      "learning_rate": 1.4142881558707914e-05,
      "loss": 0.5529,
      "step": 11372
    },
    {
      "epoch": 0.04859550321748122,
      "grad_norm": 0.47437334060668945,
      "learning_rate": 1.4100153819859853e-05,
      "loss": 0.1694,
      "step": 11373
    },
    {
      "epoch": 0.04859977610090842,
      "grad_norm": 2.8934874534606934,
      "learning_rate": 1.4057426081011793e-05,
      "loss": 1.6858,
      "step": 11374
    },
    {
      "epoch": 0.04860404898433561,
      "grad_norm": 3.5867831707000732,
      "learning_rate": 1.4014698342163732e-05,
      "loss": 0.9219,
      "step": 11375
    },
    {
      "epoch": 0.0486083218677628,
      "grad_norm": 0.771479070186615,
      "learning_rate": 1.3971970603315672e-05,
      "loss": 0.3694,
      "step": 11376
    },
    {
      "epoch": 0.04861259475119,
      "grad_norm": 0.7141606211662292,
      "learning_rate": 1.3929242864467611e-05,
      "loss": 0.2888,
      "step": 11377
    },
    {
      "epoch": 0.04861686763461719,
      "grad_norm": 0.7986302971839905,
      "learning_rate": 1.3886515125619553e-05,
      "loss": 0.4926,
      "step": 11378
    },
    {
      "epoch": 0.048621140518044385,
      "grad_norm": 2.1260509490966797,
      "learning_rate": 1.3843787386771492e-05,
      "loss": 0.6325,
      "step": 11379
    },
    {
      "epoch": 0.048625413401471584,
      "grad_norm": 3.4365503787994385,
      "learning_rate": 1.3801059647923432e-05,
      "loss": 0.807,
      "step": 11380
    },
    {
      "epoch": 0.04862968628489878,
      "grad_norm": 1.1500301361083984,
      "learning_rate": 1.3758331909075371e-05,
      "loss": 0.9593,
      "step": 11381
    },
    {
      "epoch": 0.04863395916832597,
      "grad_norm": 3.5024025440216064,
      "learning_rate": 1.371560417022731e-05,
      "loss": 0.8648,
      "step": 11382
    },
    {
      "epoch": 0.04863823205175316,
      "grad_norm": 0.8124420642852783,
      "learning_rate": 1.3672876431379252e-05,
      "loss": 0.364,
      "step": 11383
    },
    {
      "epoch": 0.04864250493518036,
      "grad_norm": 4.146692276000977,
      "learning_rate": 1.3630148692531191e-05,
      "loss": 1.2005,
      "step": 11384
    },
    {
      "epoch": 0.04864677781860755,
      "grad_norm": 1.9126389026641846,
      "learning_rate": 1.3587420953683133e-05,
      "loss": 0.4581,
      "step": 11385
    },
    {
      "epoch": 0.048651050702034744,
      "grad_norm": 0.8181052803993225,
      "learning_rate": 1.3544693214835072e-05,
      "loss": 0.3807,
      "step": 11386
    },
    {
      "epoch": 0.04865532358546194,
      "grad_norm": 1.9449454545974731,
      "learning_rate": 1.3501965475987012e-05,
      "loss": 0.6108,
      "step": 11387
    },
    {
      "epoch": 0.048659596468889135,
      "grad_norm": 1.681644082069397,
      "learning_rate": 1.3459237737138951e-05,
      "loss": 1.2097,
      "step": 11388
    },
    {
      "epoch": 0.04866386935231633,
      "grad_norm": 0.7755276560783386,
      "learning_rate": 1.3416509998290891e-05,
      "loss": 0.5218,
      "step": 11389
    },
    {
      "epoch": 0.048668142235743526,
      "grad_norm": 0.7963577508926392,
      "learning_rate": 1.337378225944283e-05,
      "loss": 0.3476,
      "step": 11390
    },
    {
      "epoch": 0.04867241511917072,
      "grad_norm": 0.7914993762969971,
      "learning_rate": 1.3331054520594772e-05,
      "loss": 0.3476,
      "step": 11391
    },
    {
      "epoch": 0.04867668800259791,
      "grad_norm": 0.7981050610542297,
      "learning_rate": 1.3288326781746711e-05,
      "loss": 0.3474,
      "step": 11392
    },
    {
      "epoch": 0.04868096088602511,
      "grad_norm": 0.7923842668533325,
      "learning_rate": 1.324559904289865e-05,
      "loss": 0.3476,
      "step": 11393
    },
    {
      "epoch": 0.0486852337694523,
      "grad_norm": 3.442772626876831,
      "learning_rate": 1.320287130405059e-05,
      "loss": 0.8032,
      "step": 11394
    },
    {
      "epoch": 0.048689506652879494,
      "grad_norm": 0.783925473690033,
      "learning_rate": 1.316014356520253e-05,
      "loss": 0.3324,
      "step": 11395
    },
    {
      "epoch": 0.04869377953630669,
      "grad_norm": 2.7583067417144775,
      "learning_rate": 1.311741582635447e-05,
      "loss": 0.8785,
      "step": 11396
    },
    {
      "epoch": 0.048698052419733885,
      "grad_norm": 2.2776007652282715,
      "learning_rate": 1.3074688087506409e-05,
      "loss": 0.5665,
      "step": 11397
    },
    {
      "epoch": 0.04870232530316108,
      "grad_norm": 0.5697320103645325,
      "learning_rate": 1.303196034865835e-05,
      "loss": 0.2418,
      "step": 11398
    },
    {
      "epoch": 0.048706598186588276,
      "grad_norm": 4.481473445892334,
      "learning_rate": 1.298923260981029e-05,
      "loss": 1.0372,
      "step": 11399
    },
    {
      "epoch": 0.04871087107001547,
      "grad_norm": 1.8743561506271362,
      "learning_rate": 1.2946504870962229e-05,
      "loss": 0.4435,
      "step": 11400
    },
    {
      "epoch": 0.04871514395344266,
      "grad_norm": 0.976518452167511,
      "learning_rate": 1.2903777132114169e-05,
      "loss": 0.4026,
      "step": 11401
    },
    {
      "epoch": 0.04871941683686986,
      "grad_norm": 0.8070932626724243,
      "learning_rate": 1.2861049393266108e-05,
      "loss": 0.5196,
      "step": 11402
    },
    {
      "epoch": 0.04872368972029705,
      "grad_norm": 2.432159423828125,
      "learning_rate": 1.2818321654418048e-05,
      "loss": 0.6354,
      "step": 11403
    },
    {
      "epoch": 0.048727962603724244,
      "grad_norm": 2.849477767944336,
      "learning_rate": 1.2775593915569989e-05,
      "loss": 0.7746,
      "step": 11404
    },
    {
      "epoch": 0.04873223548715144,
      "grad_norm": 0.7007619142532349,
      "learning_rate": 1.2732866176721929e-05,
      "loss": 0.3031,
      "step": 11405
    },
    {
      "epoch": 0.048736508370578635,
      "grad_norm": 4.205887317657471,
      "learning_rate": 1.2690138437873868e-05,
      "loss": 1.1225,
      "step": 11406
    },
    {
      "epoch": 0.04874078125400583,
      "grad_norm": 1.5173823833465576,
      "learning_rate": 1.2647410699025808e-05,
      "loss": 0.3593,
      "step": 11407
    },
    {
      "epoch": 0.04874505413743302,
      "grad_norm": 0.7953720688819885,
      "learning_rate": 1.2604682960177747e-05,
      "loss": 0.3475,
      "step": 11408
    },
    {
      "epoch": 0.04874932702086022,
      "grad_norm": 3.328066349029541,
      "learning_rate": 1.2561955221329687e-05,
      "loss": 0.771,
      "step": 11409
    },
    {
      "epoch": 0.04875359990428741,
      "grad_norm": 4.635862827301025,
      "learning_rate": 1.2519227482481626e-05,
      "loss": 1.1465,
      "step": 11410
    },
    {
      "epoch": 0.0487578727877146,
      "grad_norm": 1.9409518241882324,
      "learning_rate": 1.2476499743633567e-05,
      "loss": 0.5353,
      "step": 11411
    },
    {
      "epoch": 0.0487621456711418,
      "grad_norm": 1.9432475566864014,
      "learning_rate": 1.2433772004785507e-05,
      "loss": 0.5432,
      "step": 11412
    },
    {
      "epoch": 0.048766418554568994,
      "grad_norm": 1.849234700202942,
      "learning_rate": 1.2391044265937447e-05,
      "loss": 0.6199,
      "step": 11413
    },
    {
      "epoch": 0.048770691437996186,
      "grad_norm": 3.0994815826416016,
      "learning_rate": 1.2348316527089386e-05,
      "loss": 1.1523,
      "step": 11414
    },
    {
      "epoch": 0.048774964321423385,
      "grad_norm": 1.585681438446045,
      "learning_rate": 1.2305588788241326e-05,
      "loss": 0.5513,
      "step": 11415
    },
    {
      "epoch": 0.04877923720485058,
      "grad_norm": 0.8145374059677124,
      "learning_rate": 1.2262861049393265e-05,
      "loss": 0.364,
      "step": 11416
    },
    {
      "epoch": 0.04878351008827777,
      "grad_norm": 4.910995006561279,
      "learning_rate": 1.2220133310545205e-05,
      "loss": 0.9886,
      "step": 11417
    },
    {
      "epoch": 0.04878778297170497,
      "grad_norm": 1.5780153274536133,
      "learning_rate": 1.2177405571697146e-05,
      "loss": 0.5339,
      "step": 11418
    },
    {
      "epoch": 0.04879205585513216,
      "grad_norm": 0.8790981769561768,
      "learning_rate": 1.2134677832849085e-05,
      "loss": 0.3656,
      "step": 11419
    },
    {
      "epoch": 0.04879632873855935,
      "grad_norm": 0.733704149723053,
      "learning_rate": 1.2091950094001027e-05,
      "loss": 0.3181,
      "step": 11420
    },
    {
      "epoch": 0.04880060162198655,
      "grad_norm": 2.6705243587493896,
      "learning_rate": 1.2049222355152966e-05,
      "loss": 0.6343,
      "step": 11421
    },
    {
      "epoch": 0.048804874505413744,
      "grad_norm": 3.602692127227783,
      "learning_rate": 1.2006494616304906e-05,
      "loss": 0.9289,
      "step": 11422
    },
    {
      "epoch": 0.048809147388840936,
      "grad_norm": 2.978724479675293,
      "learning_rate": 1.1963766877456845e-05,
      "loss": 0.9789,
      "step": 11423
    },
    {
      "epoch": 0.048813420272268135,
      "grad_norm": 1.1582891941070557,
      "learning_rate": 1.1921039138608786e-05,
      "loss": 0.93,
      "step": 11424
    },
    {
      "epoch": 0.04881769315569533,
      "grad_norm": 3.7211010456085205,
      "learning_rate": 1.1878311399760726e-05,
      "loss": 1.0763,
      "step": 11425
    },
    {
      "epoch": 0.04882196603912252,
      "grad_norm": 1.9279476404190063,
      "learning_rate": 1.1835583660912666e-05,
      "loss": 0.5293,
      "step": 11426
    },
    {
      "epoch": 0.04882623892254972,
      "grad_norm": 3.7228806018829346,
      "learning_rate": 1.1792855922064605e-05,
      "loss": 1.0939,
      "step": 11427
    },
    {
      "epoch": 0.04883051180597691,
      "grad_norm": 3.665889024734497,
      "learning_rate": 1.1750128183216545e-05,
      "loss": 0.7968,
      "step": 11428
    },
    {
      "epoch": 0.0488347846894041,
      "grad_norm": 2.9485628604888916,
      "learning_rate": 1.1707400444368484e-05,
      "loss": 0.9577,
      "step": 11429
    },
    {
      "epoch": 0.0488390575728313,
      "grad_norm": 0.7976746559143066,
      "learning_rate": 1.1664672705520424e-05,
      "loss": 0.4929,
      "step": 11430
    },
    {
      "epoch": 0.048843330456258494,
      "grad_norm": 0.7769657373428345,
      "learning_rate": 1.1621944966672365e-05,
      "loss": 0.3182,
      "step": 11431
    },
    {
      "epoch": 0.048847603339685686,
      "grad_norm": 2.592921257019043,
      "learning_rate": 1.1579217227824304e-05,
      "loss": 0.7777,
      "step": 11432
    },
    {
      "epoch": 0.04885187622311288,
      "grad_norm": 0.8112074136734009,
      "learning_rate": 1.1536489488976244e-05,
      "loss": 0.3635,
      "step": 11433
    },
    {
      "epoch": 0.04885614910654008,
      "grad_norm": 2.2589924335479736,
      "learning_rate": 1.1493761750128184e-05,
      "loss": 0.5456,
      "step": 11434
    },
    {
      "epoch": 0.04886042198996727,
      "grad_norm": 0.7875258326530457,
      "learning_rate": 1.1451034011280123e-05,
      "loss": 0.3182,
      "step": 11435
    },
    {
      "epoch": 0.04886469487339446,
      "grad_norm": 0.8368491530418396,
      "learning_rate": 1.1408306272432063e-05,
      "loss": 0.5319,
      "step": 11436
    },
    {
      "epoch": 0.04886896775682166,
      "grad_norm": 1.1459085941314697,
      "learning_rate": 1.1365578533584004e-05,
      "loss": 0.9358,
      "step": 11437
    },
    {
      "epoch": 0.04887324064024885,
      "grad_norm": 0.45699411630630493,
      "learning_rate": 1.1322850794735943e-05,
      "loss": 0.1581,
      "step": 11438
    },
    {
      "epoch": 0.048877513523676044,
      "grad_norm": 1.9258068799972534,
      "learning_rate": 1.1280123055887883e-05,
      "loss": 0.5277,
      "step": 11439
    },
    {
      "epoch": 0.04888178640710324,
      "grad_norm": 0.7928128242492676,
      "learning_rate": 1.1237395317039822e-05,
      "loss": 0.3327,
      "step": 11440
    },
    {
      "epoch": 0.048886059290530436,
      "grad_norm": 0.799166202545166,
      "learning_rate": 1.1194667578191762e-05,
      "loss": 0.4929,
      "step": 11441
    },
    {
      "epoch": 0.04889033217395763,
      "grad_norm": 1.9225503206253052,
      "learning_rate": 1.1151939839343702e-05,
      "loss": 0.4384,
      "step": 11442
    },
    {
      "epoch": 0.04889460505738483,
      "grad_norm": 0.8247442841529846,
      "learning_rate": 1.1109212100495641e-05,
      "loss": 0.5007,
      "step": 11443
    },
    {
      "epoch": 0.04889887794081202,
      "grad_norm": 2.087629795074463,
      "learning_rate": 1.1066484361647582e-05,
      "loss": 0.6358,
      "step": 11444
    },
    {
      "epoch": 0.04890315082423921,
      "grad_norm": 0.45838484168052673,
      "learning_rate": 1.1023756622799522e-05,
      "loss": 0.1581,
      "step": 11445
    },
    {
      "epoch": 0.04890742370766641,
      "grad_norm": 1.9253320693969727,
      "learning_rate": 1.0981028883951461e-05,
      "loss": 0.5915,
      "step": 11446
    },
    {
      "epoch": 0.0489116965910936,
      "grad_norm": 0.9480442404747009,
      "learning_rate": 1.0938301145103401e-05,
      "loss": 0.502,
      "step": 11447
    },
    {
      "epoch": 0.048915969474520794,
      "grad_norm": 2.599607467651367,
      "learning_rate": 1.089557340625534e-05,
      "loss": 0.6168,
      "step": 11448
    },
    {
      "epoch": 0.04892024235794799,
      "grad_norm": 0.8267966508865356,
      "learning_rate": 1.085284566740728e-05,
      "loss": 0.5007,
      "step": 11449
    },
    {
      "epoch": 0.048924515241375185,
      "grad_norm": 0.7826015949249268,
      "learning_rate": 1.0810117928559221e-05,
      "loss": 0.3324,
      "step": 11450
    },
    {
      "epoch": 0.04892878812480238,
      "grad_norm": 0.8091812133789062,
      "learning_rate": 1.076739018971116e-05,
      "loss": 0.3637,
      "step": 11451
    },
    {
      "epoch": 0.04893306100822958,
      "grad_norm": 0.79523766040802,
      "learning_rate": 1.07246624508631e-05,
      "loss": 0.4915,
      "step": 11452
    },
    {
      "epoch": 0.04893733389165677,
      "grad_norm": 3.3310036659240723,
      "learning_rate": 1.068193471201504e-05,
      "loss": 0.7586,
      "step": 11453
    },
    {
      "epoch": 0.04894160677508396,
      "grad_norm": 2.899751663208008,
      "learning_rate": 1.063920697316698e-05,
      "loss": 1.68,
      "step": 11454
    },
    {
      "epoch": 0.04894587965851116,
      "grad_norm": 0.7030167579650879,
      "learning_rate": 1.0596479234318919e-05,
      "loss": 0.2756,
      "step": 11455
    },
    {
      "epoch": 0.04895015254193835,
      "grad_norm": 2.5855233669281006,
      "learning_rate": 1.0553751495470858e-05,
      "loss": 0.6113,
      "step": 11456
    },
    {
      "epoch": 0.048954425425365544,
      "grad_norm": 4.2106709480285645,
      "learning_rate": 1.05110237566228e-05,
      "loss": 1.2377,
      "step": 11457
    },
    {
      "epoch": 0.048958698308792736,
      "grad_norm": 4.210503101348877,
      "learning_rate": 1.0468296017774741e-05,
      "loss": 0.9485,
      "step": 11458
    },
    {
      "epoch": 0.048962971192219935,
      "grad_norm": 0.8018895983695984,
      "learning_rate": 1.042556827892668e-05,
      "loss": 0.3637,
      "step": 11459
    },
    {
      "epoch": 0.04896724407564713,
      "grad_norm": 4.640298843383789,
      "learning_rate": 1.038284054007862e-05,
      "loss": 1.1345,
      "step": 11460
    },
    {
      "epoch": 0.04897151695907432,
      "grad_norm": 0.9731467962265015,
      "learning_rate": 1.034011280123056e-05,
      "loss": 0.3227,
      "step": 11461
    },
    {
      "epoch": 0.04897578984250152,
      "grad_norm": 0.7856044769287109,
      "learning_rate": 1.0297385062382499e-05,
      "loss": 0.3182,
      "step": 11462
    },
    {
      "epoch": 0.04898006272592871,
      "grad_norm": 0.8753224611282349,
      "learning_rate": 1.025465732353444e-05,
      "loss": 0.3808,
      "step": 11463
    },
    {
      "epoch": 0.0489843356093559,
      "grad_norm": 2.2771787643432617,
      "learning_rate": 1.021192958468638e-05,
      "loss": 0.5258,
      "step": 11464
    },
    {
      "epoch": 0.0489886084927831,
      "grad_norm": 3.0391972064971924,
      "learning_rate": 1.016920184583832e-05,
      "loss": 0.7913,
      "step": 11465
    },
    {
      "epoch": 0.048992881376210294,
      "grad_norm": 0.7321611642837524,
      "learning_rate": 1.0126474106990259e-05,
      "loss": 0.3181,
      "step": 11466
    },
    {
      "epoch": 0.048997154259637486,
      "grad_norm": 0.7252537608146667,
      "learning_rate": 1.0083746368142198e-05,
      "loss": 0.3344,
      "step": 11467
    },
    {
      "epoch": 0.049001427143064685,
      "grad_norm": 0.3672358989715576,
      "learning_rate": 1.0041018629294138e-05,
      "loss": 0.0957,
      "step": 11468
    },
    {
      "epoch": 0.04900570002649188,
      "grad_norm": 1.6963493824005127,
      "learning_rate": 9.998290890446077e-06,
      "loss": 1.2428,
      "step": 11469
    },
    {
      "epoch": 0.04900997290991907,
      "grad_norm": 0.7264339923858643,
      "learning_rate": 9.955563151598019e-06,
      "loss": 0.3343,
      "step": 11470
    },
    {
      "epoch": 0.04901424579334627,
      "grad_norm": 1.8292189836502075,
      "learning_rate": 9.912835412749958e-06,
      "loss": 0.6124,
      "step": 11471
    },
    {
      "epoch": 0.04901851867677346,
      "grad_norm": 5.020666599273682,
      "learning_rate": 9.870107673901898e-06,
      "loss": 1.1937,
      "step": 11472
    },
    {
      "epoch": 0.04902279156020065,
      "grad_norm": 0.3155629634857178,
      "learning_rate": 9.827379935053837e-06,
      "loss": 0.0854,
      "step": 11473
    },
    {
      "epoch": 0.04902706444362785,
      "grad_norm": 0.7801578640937805,
      "learning_rate": 9.784652196205777e-06,
      "loss": 0.4822,
      "step": 11474
    },
    {
      "epoch": 0.049031337327055044,
      "grad_norm": 3.135704278945923,
      "learning_rate": 9.741924457357716e-06,
      "loss": 0.8006,
      "step": 11475
    },
    {
      "epoch": 0.049035610210482236,
      "grad_norm": 2.9373281002044678,
      "learning_rate": 9.699196718509656e-06,
      "loss": 0.9495,
      "step": 11476
    },
    {
      "epoch": 0.049039883093909435,
      "grad_norm": 1.9288896322250366,
      "learning_rate": 9.656468979661597e-06,
      "loss": 0.4275,
      "step": 11477
    },
    {
      "epoch": 0.04904415597733663,
      "grad_norm": 0.7540820837020874,
      "learning_rate": 9.613741240813537e-06,
      "loss": 0.3343,
      "step": 11478
    },
    {
      "epoch": 0.04904842886076382,
      "grad_norm": 0.7824915647506714,
      "learning_rate": 9.571013501965476e-06,
      "loss": 0.4822,
      "step": 11479
    },
    {
      "epoch": 0.04905270174419101,
      "grad_norm": 0.8102773427963257,
      "learning_rate": 9.528285763117416e-06,
      "loss": 0.3636,
      "step": 11480
    },
    {
      "epoch": 0.04905697462761821,
      "grad_norm": 5.0115132331848145,
      "learning_rate": 9.485558024269355e-06,
      "loss": 1.1779,
      "step": 11481
    },
    {
      "epoch": 0.0490612475110454,
      "grad_norm": 4.755549907684326,
      "learning_rate": 9.442830285421295e-06,
      "loss": 1.1849,
      "step": 11482
    },
    {
      "epoch": 0.049065520394472595,
      "grad_norm": 0.7263043522834778,
      "learning_rate": 9.400102546573236e-06,
      "loss": 0.3181,
      "step": 11483
    },
    {
      "epoch": 0.049069793277899794,
      "grad_norm": 4.606436729431152,
      "learning_rate": 9.357374807725176e-06,
      "loss": 1.1006,
      "step": 11484
    },
    {
      "epoch": 0.049074066161326986,
      "grad_norm": 1.9024626016616821,
      "learning_rate": 9.314647068877115e-06,
      "loss": 0.5208,
      "step": 11485
    },
    {
      "epoch": 0.04907833904475418,
      "grad_norm": 0.8624812960624695,
      "learning_rate": 9.271919330029055e-06,
      "loss": 0.3514,
      "step": 11486
    },
    {
      "epoch": 0.04908261192818138,
      "grad_norm": 0.7012661099433899,
      "learning_rate": 9.229191591180994e-06,
      "loss": 0.3031,
      "step": 11487
    },
    {
      "epoch": 0.04908688481160857,
      "grad_norm": 4.734647274017334,
      "learning_rate": 9.186463852332934e-06,
      "loss": 1.1526,
      "step": 11488
    },
    {
      "epoch": 0.04909115769503576,
      "grad_norm": 2.0677192211151123,
      "learning_rate": 9.143736113484873e-06,
      "loss": 0.6113,
      "step": 11489
    },
    {
      "epoch": 0.04909543057846296,
      "grad_norm": 0.7980726957321167,
      "learning_rate": 9.101008374636814e-06,
      "loss": 0.4926,
      "step": 11490
    },
    {
      "epoch": 0.04909970346189015,
      "grad_norm": 0.7055113911628723,
      "learning_rate": 9.058280635788754e-06,
      "loss": 0.3089,
      "step": 11491
    },
    {
      "epoch": 0.049103976345317345,
      "grad_norm": 0.7818900346755981,
      "learning_rate": 9.015552896940694e-06,
      "loss": 0.5543,
      "step": 11492
    },
    {
      "epoch": 0.049108249228744544,
      "grad_norm": 0.9856182932853699,
      "learning_rate": 8.972825158092633e-06,
      "loss": 0.4178,
      "step": 11493
    },
    {
      "epoch": 0.049112522112171736,
      "grad_norm": 0.3193638324737549,
      "learning_rate": 8.930097419244573e-06,
      "loss": 0.0854,
      "step": 11494
    },
    {
      "epoch": 0.04911679499559893,
      "grad_norm": 1.707883358001709,
      "learning_rate": 8.887369680396512e-06,
      "loss": 1.2414,
      "step": 11495
    },
    {
      "epoch": 0.04912106787902613,
      "grad_norm": 0.7995705604553223,
      "learning_rate": 8.844641941548455e-06,
      "loss": 0.4915,
      "step": 11496
    },
    {
      "epoch": 0.04912534076245332,
      "grad_norm": 0.6980169415473938,
      "learning_rate": 8.801914202700395e-06,
      "loss": 0.2757,
      "step": 11497
    },
    {
      "epoch": 0.04912961364588051,
      "grad_norm": 3.4693856239318848,
      "learning_rate": 8.759186463852334e-06,
      "loss": 0.8355,
      "step": 11498
    },
    {
      "epoch": 0.04913388652930771,
      "grad_norm": 1.9253419637680054,
      "learning_rate": 8.716458725004274e-06,
      "loss": 0.5901,
      "step": 11499
    },
    {
      "epoch": 0.0491381594127349,
      "grad_norm": 1.1566317081451416,
      "learning_rate": 8.673730986156213e-06,
      "loss": 0.93,
      "step": 11500
    },
    {
      "epoch": 0.049142432296162095,
      "grad_norm": 0.9793301224708557,
      "learning_rate": 8.631003247308153e-06,
      "loss": 0.3226,
      "step": 11501
    },
    {
      "epoch": 0.049146705179589294,
      "grad_norm": 1.6927459239959717,
      "learning_rate": 8.588275508460092e-06,
      "loss": 1.2425,
      "step": 11502
    },
    {
      "epoch": 0.049150978063016486,
      "grad_norm": 2.6009416580200195,
      "learning_rate": 8.545547769612034e-06,
      "loss": 0.7922,
      "step": 11503
    },
    {
      "epoch": 0.04915525094644368,
      "grad_norm": 4.738394737243652,
      "learning_rate": 8.502820030763973e-06,
      "loss": 1.1549,
      "step": 11504
    },
    {
      "epoch": 0.04915952382987087,
      "grad_norm": 0.7782918214797974,
      "learning_rate": 8.460092291915913e-06,
      "loss": 0.305,
      "step": 11505
    },
    {
      "epoch": 0.04916379671329807,
      "grad_norm": 2.5809006690979004,
      "learning_rate": 8.417364553067852e-06,
      "loss": 0.7756,
      "step": 11506
    },
    {
      "epoch": 0.04916806959672526,
      "grad_norm": 2.3068509101867676,
      "learning_rate": 8.374636814219792e-06,
      "loss": 0.5293,
      "step": 11507
    },
    {
      "epoch": 0.04917234248015245,
      "grad_norm": 3.425786018371582,
      "learning_rate": 8.331909075371731e-06,
      "loss": 0.7958,
      "step": 11508
    },
    {
      "epoch": 0.04917661536357965,
      "grad_norm": 0.8247311115264893,
      "learning_rate": 8.28918133652367e-06,
      "loss": 0.3807,
      "step": 11509
    },
    {
      "epoch": 0.049180888247006845,
      "grad_norm": 1.6791232824325562,
      "learning_rate": 8.246453597675612e-06,
      "loss": 1.1857,
      "step": 11510
    },
    {
      "epoch": 0.04918516113043404,
      "grad_norm": 0.9806520342826843,
      "learning_rate": 8.203725858827552e-06,
      "loss": 0.3226,
      "step": 11511
    },
    {
      "epoch": 0.049189434013861236,
      "grad_norm": 1.0091716051101685,
      "learning_rate": 8.160998119979491e-06,
      "loss": 0.4509,
      "step": 11512
    },
    {
      "epoch": 0.04919370689728843,
      "grad_norm": 2.5803511142730713,
      "learning_rate": 8.11827038113143e-06,
      "loss": 0.7725,
      "step": 11513
    },
    {
      "epoch": 0.04919797978071562,
      "grad_norm": 0.7795440554618835,
      "learning_rate": 8.07554264228337e-06,
      "loss": 0.3182,
      "step": 11514
    },
    {
      "epoch": 0.04920225266414282,
      "grad_norm": 0.780276358127594,
      "learning_rate": 8.03281490343531e-06,
      "loss": 0.3182,
      "step": 11515
    },
    {
      "epoch": 0.04920652554757001,
      "grad_norm": 0.974176287651062,
      "learning_rate": 7.990087164587251e-06,
      "loss": 0.4028,
      "step": 11516
    },
    {
      "epoch": 0.0492107984309972,
      "grad_norm": 0.7041441202163696,
      "learning_rate": 7.94735942573919e-06,
      "loss": 0.3262,
      "step": 11517
    },
    {
      "epoch": 0.0492150713144244,
      "grad_norm": 1.6801536083221436,
      "learning_rate": 7.90463168689113e-06,
      "loss": 1.1857,
      "step": 11518
    },
    {
      "epoch": 0.049219344197851594,
      "grad_norm": 0.6956255435943604,
      "learning_rate": 7.86190394804307e-06,
      "loss": 0.2889,
      "step": 11519
    },
    {
      "epoch": 0.04922361708127879,
      "grad_norm": 2.8939173221588135,
      "learning_rate": 7.819176209195009e-06,
      "loss": 1.68,
      "step": 11520
    },
    {
      "epoch": 0.049227889964705986,
      "grad_norm": 1.9146875143051147,
      "learning_rate": 7.776448470346949e-06,
      "loss": 0.4147,
      "step": 11521
    },
    {
      "epoch": 0.04923216284813318,
      "grad_norm": 0.9841322898864746,
      "learning_rate": 7.733720731498888e-06,
      "loss": 0.3754,
      "step": 11522
    },
    {
      "epoch": 0.04923643573156037,
      "grad_norm": 1.157031536102295,
      "learning_rate": 7.69099299265083e-06,
      "loss": 0.93,
      "step": 11523
    },
    {
      "epoch": 0.04924070861498757,
      "grad_norm": 3.1290218830108643,
      "learning_rate": 7.648265253802769e-06,
      "loss": 0.7852,
      "step": 11524
    },
    {
      "epoch": 0.04924498149841476,
      "grad_norm": 0.810009241104126,
      "learning_rate": 7.605537514954709e-06,
      "loss": 0.4982,
      "step": 11525
    },
    {
      "epoch": 0.04924925438184195,
      "grad_norm": 2.8412864208221436,
      "learning_rate": 7.562809776106649e-06,
      "loss": 0.766,
      "step": 11526
    },
    {
      "epoch": 0.04925352726526915,
      "grad_norm": 1.8309496641159058,
      "learning_rate": 7.520082037258588e-06,
      "loss": 0.618,
      "step": 11527
    },
    {
      "epoch": 0.049257800148696344,
      "grad_norm": 1.5797308683395386,
      "learning_rate": 7.477354298410529e-06,
      "loss": 0.5483,
      "step": 11528
    },
    {
      "epoch": 0.049262073032123536,
      "grad_norm": 0.800032913684845,
      "learning_rate": 7.434626559562468e-06,
      "loss": 0.4927,
      "step": 11529
    },
    {
      "epoch": 0.04926634591555073,
      "grad_norm": 4.184720039367676,
      "learning_rate": 7.391898820714408e-06,
      "loss": 0.9244,
      "step": 11530
    },
    {
      "epoch": 0.04927061879897793,
      "grad_norm": 0.8024736642837524,
      "learning_rate": 7.349171081866348e-06,
      "loss": 0.4927,
      "step": 11531
    },
    {
      "epoch": 0.04927489168240512,
      "grad_norm": 0.705800473690033,
      "learning_rate": 7.306443343018288e-06,
      "loss": 0.2756,
      "step": 11532
    },
    {
      "epoch": 0.04927916456583231,
      "grad_norm": 2.844970464706421,
      "learning_rate": 7.263715604170227e-06,
      "loss": 0.774,
      "step": 11533
    },
    {
      "epoch": 0.04928343744925951,
      "grad_norm": 3.2736852169036865,
      "learning_rate": 7.220987865322168e-06,
      "loss": 0.616,
      "step": 11534
    },
    {
      "epoch": 0.0492877103326867,
      "grad_norm": 3.4164488315582275,
      "learning_rate": 7.178260126474107e-06,
      "loss": 0.77,
      "step": 11535
    },
    {
      "epoch": 0.049291983216113895,
      "grad_norm": 0.8009963035583496,
      "learning_rate": 7.135532387626047e-06,
      "loss": 0.4926,
      "step": 11536
    },
    {
      "epoch": 0.049296256099541094,
      "grad_norm": 0.8152291774749756,
      "learning_rate": 7.092804648777986e-06,
      "loss": 0.3635,
      "step": 11537
    },
    {
      "epoch": 0.049300528982968286,
      "grad_norm": 1.5265800952911377,
      "learning_rate": 7.050076909929927e-06,
      "loss": 0.3541,
      "step": 11538
    },
    {
      "epoch": 0.04930480186639548,
      "grad_norm": 1.9230958223342896,
      "learning_rate": 7.007349171081866e-06,
      "loss": 0.6042,
      "step": 11539
    },
    {
      "epoch": 0.04930907474982268,
      "grad_norm": 1.083054780960083,
      "learning_rate": 6.964621432233806e-06,
      "loss": 0.4746,
      "step": 11540
    },
    {
      "epoch": 0.04931334763324987,
      "grad_norm": 0.8013051748275757,
      "learning_rate": 6.921893693385746e-06,
      "loss": 0.4926,
      "step": 11541
    },
    {
      "epoch": 0.04931762051667706,
      "grad_norm": 0.8893489241600037,
      "learning_rate": 6.879165954537686e-06,
      "loss": 0.3969,
      "step": 11542
    },
    {
      "epoch": 0.04932189340010426,
      "grad_norm": 3.725335121154785,
      "learning_rate": 6.836438215689626e-06,
      "loss": 1.0614,
      "step": 11543
    },
    {
      "epoch": 0.04932616628353145,
      "grad_norm": 1.156729817390442,
      "learning_rate": 6.793710476841566e-06,
      "loss": 0.93,
      "step": 11544
    },
    {
      "epoch": 0.049330439166958645,
      "grad_norm": 1.922377586364746,
      "learning_rate": 6.750982737993506e-06,
      "loss": 0.6016,
      "step": 11545
    },
    {
      "epoch": 0.049334712050385844,
      "grad_norm": 2.847632646560669,
      "learning_rate": 6.7082549991454454e-06,
      "loss": 0.7645,
      "step": 11546
    },
    {
      "epoch": 0.049338984933813036,
      "grad_norm": 4.606451034545898,
      "learning_rate": 6.665527260297386e-06,
      "loss": 1.0967,
      "step": 11547
    },
    {
      "epoch": 0.04934325781724023,
      "grad_norm": 2.2566604614257812,
      "learning_rate": 6.622799521449325e-06,
      "loss": 0.5438,
      "step": 11548
    },
    {
      "epoch": 0.04934753070066743,
      "grad_norm": 1.5645414590835571,
      "learning_rate": 6.580071782601265e-06,
      "loss": 0.5313,
      "step": 11549
    },
    {
      "epoch": 0.04935180358409462,
      "grad_norm": 0.798200786113739,
      "learning_rate": 6.537344043753204e-06,
      "loss": 0.4915,
      "step": 11550
    },
    {
      "epoch": 0.04935607646752181,
      "grad_norm": 1.1585363149642944,
      "learning_rate": 6.494616304905145e-06,
      "loss": 0.9289,
      "step": 11551
    },
    {
      "epoch": 0.04936034935094901,
      "grad_norm": 2.7468760013580322,
      "learning_rate": 6.451888566057084e-06,
      "loss": 0.8698,
      "step": 11552
    },
    {
      "epoch": 0.0493646222343762,
      "grad_norm": 2.296055316925049,
      "learning_rate": 6.409160827209024e-06,
      "loss": 0.5314,
      "step": 11553
    },
    {
      "epoch": 0.049368895117803395,
      "grad_norm": 2.2724814414978027,
      "learning_rate": 6.366433088360964e-06,
      "loss": 0.5615,
      "step": 11554
    },
    {
      "epoch": 0.04937316800123059,
      "grad_norm": 0.7800760865211487,
      "learning_rate": 6.323705349512904e-06,
      "loss": 0.4597,
      "step": 11555
    },
    {
      "epoch": 0.049377440884657786,
      "grad_norm": 0.7063266038894653,
      "learning_rate": 6.280977610664843e-06,
      "loss": 0.2756,
      "step": 11556
    },
    {
      "epoch": 0.04938171376808498,
      "grad_norm": 1.9286572933197021,
      "learning_rate": 6.238249871816784e-06,
      "loss": 0.5989,
      "step": 11557
    },
    {
      "epoch": 0.04938598665151217,
      "grad_norm": 2.0667881965637207,
      "learning_rate": 6.195522132968723e-06,
      "loss": 0.6053,
      "step": 11558
    },
    {
      "epoch": 0.04939025953493937,
      "grad_norm": 1.5644888877868652,
      "learning_rate": 6.152794394120663e-06,
      "loss": 0.544,
      "step": 11559
    },
    {
      "epoch": 0.04939453241836656,
      "grad_norm": 0.7787741422653198,
      "learning_rate": 6.110066655272602e-06,
      "loss": 0.48,
      "step": 11560
    },
    {
      "epoch": 0.049398805301793754,
      "grad_norm": 0.8894311189651489,
      "learning_rate": 6.067338916424543e-06,
      "loss": 0.3968,
      "step": 11561
    },
    {
      "epoch": 0.04940307818522095,
      "grad_norm": 0.7866961359977722,
      "learning_rate": 6.024611177576483e-06,
      "loss": 0.4621,
      "step": 11562
    },
    {
      "epoch": 0.049407351068648145,
      "grad_norm": 3.0333070755004883,
      "learning_rate": 5.981883438728423e-06,
      "loss": 0.7735,
      "step": 11563
    },
    {
      "epoch": 0.04941162395207534,
      "grad_norm": 0.7813712358474731,
      "learning_rate": 5.939155699880363e-06,
      "loss": 0.4597,
      "step": 11564
    },
    {
      "epoch": 0.049415896835502536,
      "grad_norm": 3.6376068592071533,
      "learning_rate": 5.8964279610323025e-06,
      "loss": 0.9605,
      "step": 11565
    },
    {
      "epoch": 0.04942016971892973,
      "grad_norm": 3.423046112060547,
      "learning_rate": 5.853700222184242e-06,
      "loss": 0.7719,
      "step": 11566
    },
    {
      "epoch": 0.04942444260235692,
      "grad_norm": 0.7798848152160645,
      "learning_rate": 5.8109724833361825e-06,
      "loss": 0.5847,
      "step": 11567
    },
    {
      "epoch": 0.04942871548578412,
      "grad_norm": 1.6995636224746704,
      "learning_rate": 5.768244744488122e-06,
      "loss": 1.2414,
      "step": 11568
    },
    {
      "epoch": 0.04943298836921131,
      "grad_norm": 0.6591573357582092,
      "learning_rate": 5.7255170056400615e-06,
      "loss": 0.2927,
      "step": 11569
    },
    {
      "epoch": 0.049437261252638504,
      "grad_norm": 2.736145496368408,
      "learning_rate": 5.682789266792002e-06,
      "loss": 0.8611,
      "step": 11570
    },
    {
      "epoch": 0.0494415341360657,
      "grad_norm": 1.1879717111587524,
      "learning_rate": 5.6400615279439415e-06,
      "loss": 0.9799,
      "step": 11571
    },
    {
      "epoch": 0.049445807019492895,
      "grad_norm": 0.7834291458129883,
      "learning_rate": 5.597333789095881e-06,
      "loss": 0.3182,
      "step": 11572
    },
    {
      "epoch": 0.04945007990292009,
      "grad_norm": 2.5907182693481445,
      "learning_rate": 5.5546060502478205e-06,
      "loss": 0.6182,
      "step": 11573
    },
    {
      "epoch": 0.049454352786347286,
      "grad_norm": 0.8019440770149231,
      "learning_rate": 5.511878311399761e-06,
      "loss": 0.3323,
      "step": 11574
    },
    {
      "epoch": 0.04945862566977448,
      "grad_norm": 0.814879834651947,
      "learning_rate": 5.4691505725517004e-06,
      "loss": 0.3475,
      "step": 11575
    },
    {
      "epoch": 0.04946289855320167,
      "grad_norm": 0.7834210395812988,
      "learning_rate": 5.42642283370364e-06,
      "loss": 0.4597,
      "step": 11576
    },
    {
      "epoch": 0.04946717143662887,
      "grad_norm": 0.6358984708786011,
      "learning_rate": 5.38369509485558e-06,
      "loss": 0.2503,
      "step": 11577
    },
    {
      "epoch": 0.04947144432005606,
      "grad_norm": 2.3333981037139893,
      "learning_rate": 5.34096735600752e-06,
      "loss": 0.6106,
      "step": 11578
    },
    {
      "epoch": 0.049475717203483253,
      "grad_norm": 3.4172134399414062,
      "learning_rate": 5.2982396171594594e-06,
      "loss": 0.7629,
      "step": 11579
    },
    {
      "epoch": 0.049479990086910446,
      "grad_norm": 1.1589505672454834,
      "learning_rate": 5.2555118783114e-06,
      "loss": 0.93,
      "step": 11580
    },
    {
      "epoch": 0.049484262970337645,
      "grad_norm": 0.8153036832809448,
      "learning_rate": 5.21278413946334e-06,
      "loss": 0.3635,
      "step": 11581
    },
    {
      "epoch": 0.04948853585376484,
      "grad_norm": 0.6742430329322815,
      "learning_rate": 5.17005640061528e-06,
      "loss": 0.3351,
      "step": 11582
    },
    {
      "epoch": 0.04949280873719203,
      "grad_norm": 4.716888904571533,
      "learning_rate": 5.12732866176722e-06,
      "loss": 1.1221,
      "step": 11583
    },
    {
      "epoch": 0.04949708162061923,
      "grad_norm": 0.6771883368492126,
      "learning_rate": 5.08460092291916e-06,
      "loss": 0.2777,
      "step": 11584
    },
    {
      "epoch": 0.04950135450404642,
      "grad_norm": 1.921076774597168,
      "learning_rate": 5.041873184071099e-06,
      "loss": 0.4194,
      "step": 11585
    },
    {
      "epoch": 0.04950562738747361,
      "grad_norm": 0.8072856068611145,
      "learning_rate": 4.999145445223039e-06,
      "loss": 0.4925,
      "step": 11586
    },
    {
      "epoch": 0.04950990027090081,
      "grad_norm": 0.6612213850021362,
      "learning_rate": 4.956417706374979e-06,
      "loss": 0.2926,
      "step": 11587
    },
    {
      "epoch": 0.049514173154328,
      "grad_norm": 0.7765220403671265,
      "learning_rate": 4.913689967526919e-06,
      "loss": 0.3695,
      "step": 11588
    },
    {
      "epoch": 0.049518446037755195,
      "grad_norm": 1.1633729934692383,
      "learning_rate": 4.870962228678858e-06,
      "loss": 0.9293,
      "step": 11589
    },
    {
      "epoch": 0.049522718921182395,
      "grad_norm": 0.8043052554130554,
      "learning_rate": 4.8282344898307986e-06,
      "loss": 0.4912,
      "step": 11590
    },
    {
      "epoch": 0.04952699180460959,
      "grad_norm": 0.6232666969299316,
      "learning_rate": 4.785506750982738e-06,
      "loss": 0.1821,
      "step": 11591
    },
    {
      "epoch": 0.04953126468803678,
      "grad_norm": 0.6901424527168274,
      "learning_rate": 4.742779012134678e-06,
      "loss": 0.2929,
      "step": 11592
    },
    {
      "epoch": 0.04953553757146398,
      "grad_norm": 0.8065394759178162,
      "learning_rate": 4.700051273286618e-06,
      "loss": 0.4919,
      "step": 11593
    },
    {
      "epoch": 0.04953981045489117,
      "grad_norm": 0.8043619990348816,
      "learning_rate": 4.6573235344385576e-06,
      "loss": 0.4913,
      "step": 11594
    },
    {
      "epoch": 0.04954408333831836,
      "grad_norm": 0.8699935078620911,
      "learning_rate": 4.614595795590497e-06,
      "loss": 0.3514,
      "step": 11595
    },
    {
      "epoch": 0.04954835622174556,
      "grad_norm": 0.8042023181915283,
      "learning_rate": 4.571868056742437e-06,
      "loss": 0.4919,
      "step": 11596
    },
    {
      "epoch": 0.04955262910517275,
      "grad_norm": 0.7841998934745789,
      "learning_rate": 4.529140317894377e-06,
      "loss": 0.3182,
      "step": 11597
    },
    {
      "epoch": 0.049556901988599945,
      "grad_norm": 0.6910198926925659,
      "learning_rate": 4.4864125790463165e-06,
      "loss": 0.2929,
      "step": 11598
    },
    {
      "epoch": 0.049561174872027144,
      "grad_norm": 4.509385585784912,
      "learning_rate": 4.443684840198256e-06,
      "loss": 1.074,
      "step": 11599
    },
    {
      "epoch": 0.04956544775545434,
      "grad_norm": 2.0732157230377197,
      "learning_rate": 4.400957101350197e-06,
      "loss": 0.6172,
      "step": 11600
    },
    {
      "epoch": 0.04956972063888153,
      "grad_norm": 2.926711082458496,
      "learning_rate": 4.358229362502137e-06,
      "loss": 1.7037,
      "step": 11601
    },
    {
      "epoch": 0.04957399352230873,
      "grad_norm": 2.9335153102874756,
      "learning_rate": 4.315501623654076e-06,
      "loss": 0.9531,
      "step": 11602
    },
    {
      "epoch": 0.04957826640573592,
      "grad_norm": 0.7624151110649109,
      "learning_rate": 4.272773884806017e-06,
      "loss": 0.5539,
      "step": 11603
    },
    {
      "epoch": 0.04958253928916311,
      "grad_norm": 0.7783326506614685,
      "learning_rate": 4.230046145957956e-06,
      "loss": 0.5543,
      "step": 11604
    },
    {
      "epoch": 0.049586812172590304,
      "grad_norm": 2.928635597229004,
      "learning_rate": 4.187318407109896e-06,
      "loss": 0.9536,
      "step": 11605
    },
    {
      "epoch": 0.0495910850560175,
      "grad_norm": 0.7081269025802612,
      "learning_rate": 4.144590668261835e-06,
      "loss": 0.3031,
      "step": 11606
    },
    {
      "epoch": 0.049595357939444695,
      "grad_norm": 1.9091929197311401,
      "learning_rate": 4.101862929413776e-06,
      "loss": 0.5266,
      "step": 11607
    },
    {
      "epoch": 0.04959963082287189,
      "grad_norm": 0.7849518656730652,
      "learning_rate": 4.059135190565715e-06,
      "loss": 0.3053,
      "step": 11608
    },
    {
      "epoch": 0.049603903706299086,
      "grad_norm": 1.8213446140289307,
      "learning_rate": 4.016407451717655e-06,
      "loss": 0.6193,
      "step": 11609
    },
    {
      "epoch": 0.04960817658972628,
      "grad_norm": 2.9048383235931396,
      "learning_rate": 3.973679712869595e-06,
      "loss": 1.6789,
      "step": 11610
    },
    {
      "epoch": 0.04961244947315347,
      "grad_norm": 3.0269808769226074,
      "learning_rate": 3.930951974021535e-06,
      "loss": 0.7665,
      "step": 11611
    },
    {
      "epoch": 0.04961672235658067,
      "grad_norm": 5.045557975769043,
      "learning_rate": 3.888224235173474e-06,
      "loss": 1.1984,
      "step": 11612
    },
    {
      "epoch": 0.04962099524000786,
      "grad_norm": 1.1674573421478271,
      "learning_rate": 3.845496496325415e-06,
      "loss": 0.9288,
      "step": 11613
    },
    {
      "epoch": 0.049625268123435054,
      "grad_norm": 0.8031442761421204,
      "learning_rate": 3.8027687574773546e-06,
      "loss": 0.4913,
      "step": 11614
    },
    {
      "epoch": 0.04962954100686225,
      "grad_norm": 0.8055473566055298,
      "learning_rate": 3.760041018629294e-06,
      "loss": 0.4913,
      "step": 11615
    },
    {
      "epoch": 0.049633813890289445,
      "grad_norm": 0.8113223910331726,
      "learning_rate": 3.717313279781234e-06,
      "loss": 0.3476,
      "step": 11616
    },
    {
      "epoch": 0.04963808677371664,
      "grad_norm": 3.6588737964630127,
      "learning_rate": 3.674585540933174e-06,
      "loss": 2.5538,
      "step": 11617
    },
    {
      "epoch": 0.049642359657143836,
      "grad_norm": 3.120178699493408,
      "learning_rate": 3.6318578020851136e-06,
      "loss": 0.7951,
      "step": 11618
    },
    {
      "epoch": 0.04964663254057103,
      "grad_norm": 1.562088131904602,
      "learning_rate": 3.5891300632370536e-06,
      "loss": 0.5321,
      "step": 11619
    },
    {
      "epoch": 0.04965090542399822,
      "grad_norm": 0.6632938385009766,
      "learning_rate": 3.546402324388993e-06,
      "loss": 0.2926,
      "step": 11620
    },
    {
      "epoch": 0.04965517830742542,
      "grad_norm": 0.8069930672645569,
      "learning_rate": 3.503674585540933e-06,
      "loss": 0.3323,
      "step": 11621
    },
    {
      "epoch": 0.04965945119085261,
      "grad_norm": 0.36417651176452637,
      "learning_rate": 3.460946846692873e-06,
      "loss": 0.0908,
      "step": 11622
    },
    {
      "epoch": 0.049663724074279804,
      "grad_norm": 2.354736804962158,
      "learning_rate": 3.418219107844813e-06,
      "loss": 0.6241,
      "step": 11623
    },
    {
      "epoch": 0.049667996957707,
      "grad_norm": 0.7871845960617065,
      "learning_rate": 3.375491368996753e-06,
      "loss": 0.3049,
      "step": 11624
    },
    {
      "epoch": 0.049672269841134195,
      "grad_norm": 2.7368903160095215,
      "learning_rate": 3.332763630148693e-06,
      "loss": 0.8663,
      "step": 11625
    },
    {
      "epoch": 0.04967654272456139,
      "grad_norm": 1.1920140981674194,
      "learning_rate": 3.2900358913006324e-06,
      "loss": 0.9543,
      "step": 11626
    },
    {
      "epoch": 0.049680815607988586,
      "grad_norm": 2.1116175651550293,
      "learning_rate": 3.2473081524525724e-06,
      "loss": 0.6019,
      "step": 11627
    },
    {
      "epoch": 0.04968508849141578,
      "grad_norm": 0.7994380593299866,
      "learning_rate": 3.204580413604512e-06,
      "loss": 0.4892,
      "step": 11628
    },
    {
      "epoch": 0.04968936137484297,
      "grad_norm": 0.9662365317344666,
      "learning_rate": 3.161852674756452e-06,
      "loss": 0.3752,
      "step": 11629
    },
    {
      "epoch": 0.04969363425827016,
      "grad_norm": 0.872620701789856,
      "learning_rate": 3.119124935908392e-06,
      "loss": 0.3385,
      "step": 11630
    },
    {
      "epoch": 0.04969790714169736,
      "grad_norm": 0.8070736527442932,
      "learning_rate": 3.0763971970603314e-06,
      "loss": 0.4919,
      "step": 11631
    },
    {
      "epoch": 0.049702180025124554,
      "grad_norm": 0.6389594078063965,
      "learning_rate": 3.0336694582122714e-06,
      "loss": 0.3042,
      "step": 11632
    },
    {
      "epoch": 0.049706452908551746,
      "grad_norm": 1.7038159370422363,
      "learning_rate": 2.9909417193642113e-06,
      "loss": 1.2414,
      "step": 11633
    },
    {
      "epoch": 0.049710725791978945,
      "grad_norm": 0.8043270707130432,
      "learning_rate": 2.9482139805161513e-06,
      "loss": 0.4915,
      "step": 11634
    },
    {
      "epoch": 0.04971499867540614,
      "grad_norm": 0.8464949131011963,
      "learning_rate": 2.9054862416680912e-06,
      "loss": 0.381,
      "step": 11635
    },
    {
      "epoch": 0.04971927155883333,
      "grad_norm": 1.900189995765686,
      "learning_rate": 2.8627585028200308e-06,
      "loss": 0.5178,
      "step": 11636
    },
    {
      "epoch": 0.04972354444226053,
      "grad_norm": 3.0173003673553467,
      "learning_rate": 2.8200307639719707e-06,
      "loss": 0.7555,
      "step": 11637
    },
    {
      "epoch": 0.04972781732568772,
      "grad_norm": 1.9077000617980957,
      "learning_rate": 2.7773030251239103e-06,
      "loss": 0.5872,
      "step": 11638
    },
    {
      "epoch": 0.04973209020911491,
      "grad_norm": 1.8275947570800781,
      "learning_rate": 2.7345752862758502e-06,
      "loss": 0.6195,
      "step": 11639
    },
    {
      "epoch": 0.04973636309254211,
      "grad_norm": 2.099576234817505,
      "learning_rate": 2.69184754742779e-06,
      "loss": 0.5941,
      "step": 11640
    },
    {
      "epoch": 0.049740635975969304,
      "grad_norm": 3.4119746685028076,
      "learning_rate": 2.6491198085797297e-06,
      "loss": 0.7643,
      "step": 11641
    },
    {
      "epoch": 0.049744908859396496,
      "grad_norm": 2.9118921756744385,
      "learning_rate": 2.60639206973167e-06,
      "loss": 1.6788,
      "step": 11642
    },
    {
      "epoch": 0.049749181742823695,
      "grad_norm": 1.8721826076507568,
      "learning_rate": 2.56366433088361e-06,
      "loss": 0.4351,
      "step": 11643
    },
    {
      "epoch": 0.04975345462625089,
      "grad_norm": 0.7100139856338501,
      "learning_rate": 2.5209365920355496e-06,
      "loss": 0.2756,
      "step": 11644
    },
    {
      "epoch": 0.04975772750967808,
      "grad_norm": 2.286864995956421,
      "learning_rate": 2.4782088531874896e-06,
      "loss": 0.5244,
      "step": 11645
    },
    {
      "epoch": 0.04976200039310528,
      "grad_norm": 1.9136054515838623,
      "learning_rate": 2.435481114339429e-06,
      "loss": 0.5225,
      "step": 11646
    },
    {
      "epoch": 0.04976627327653247,
      "grad_norm": 0.7894788980484009,
      "learning_rate": 2.392753375491369e-06,
      "loss": 0.3049,
      "step": 11647
    },
    {
      "epoch": 0.04977054615995966,
      "grad_norm": 0.806310772895813,
      "learning_rate": 2.350025636643309e-06,
      "loss": 0.4919,
      "step": 11648
    },
    {
      "epoch": 0.04977481904338686,
      "grad_norm": 0.8064512610435486,
      "learning_rate": 2.3072978977952485e-06,
      "loss": 0.4913,
      "step": 11649
    },
    {
      "epoch": 0.049779091926814054,
      "grad_norm": 0.7941012382507324,
      "learning_rate": 2.2645701589471885e-06,
      "loss": 0.3181,
      "step": 11650
    },
    {
      "epoch": 0.049783364810241246,
      "grad_norm": 1.0082908868789673,
      "learning_rate": 2.221842420099128e-06,
      "loss": 0.4339,
      "step": 11651
    },
    {
      "epoch": 0.049787637693668445,
      "grad_norm": 0.8058892488479614,
      "learning_rate": 2.1791146812510684e-06,
      "loss": 0.3476,
      "step": 11652
    },
    {
      "epoch": 0.04979191057709564,
      "grad_norm": 1.2154344320297241,
      "learning_rate": 2.1363869424030084e-06,
      "loss": 0.5958,
      "step": 11653
    },
    {
      "epoch": 0.04979618346052283,
      "grad_norm": 3.615149974822998,
      "learning_rate": 2.093659203554948e-06,
      "loss": 0.9469,
      "step": 11654
    },
    {
      "epoch": 0.04980045634395002,
      "grad_norm": 0.7075073719024658,
      "learning_rate": 2.050931464706888e-06,
      "loss": 0.2757,
      "step": 11655
    },
    {
      "epoch": 0.04980472922737722,
      "grad_norm": 4.724367618560791,
      "learning_rate": 2.0082037258588274e-06,
      "loss": 1.1203,
      "step": 11656
    },
    {
      "epoch": 0.04980900211080441,
      "grad_norm": 1.5682684183120728,
      "learning_rate": 1.9654759870107674e-06,
      "loss": 0.5328,
      "step": 11657
    },
    {
      "epoch": 0.049813274994231604,
      "grad_norm": 0.6628804802894592,
      "learning_rate": 1.9227482481627073e-06,
      "loss": 0.2926,
      "step": 11658
    },
    {
      "epoch": 0.049817547877658803,
      "grad_norm": 0.7787501811981201,
      "learning_rate": 1.880020509314647e-06,
      "loss": 0.5793,
      "step": 11659
    },
    {
      "epoch": 0.049821820761085996,
      "grad_norm": 0.7776301503181458,
      "learning_rate": 1.837292770466587e-06,
      "loss": 0.5543,
      "step": 11660
    },
    {
      "epoch": 0.04982609364451319,
      "grad_norm": 2.090336799621582,
      "learning_rate": 1.7945650316185268e-06,
      "loss": 0.5871,
      "step": 11661
    },
    {
      "epoch": 0.04983036652794039,
      "grad_norm": 0.6803082823753357,
      "learning_rate": 1.7518372927704665e-06,
      "loss": 0.2778,
      "step": 11662
    },
    {
      "epoch": 0.04983463941136758,
      "grad_norm": 0.791191577911377,
      "learning_rate": 1.7091095539224065e-06,
      "loss": 0.3182,
      "step": 11663
    },
    {
      "epoch": 0.04983891229479477,
      "grad_norm": 3.114452600479126,
      "learning_rate": 1.6663818150743465e-06,
      "loss": 0.7927,
      "step": 11664
    },
    {
      "epoch": 0.04984318517822197,
      "grad_norm": 0.44576194882392883,
      "learning_rate": 1.6236540762262862e-06,
      "loss": 0.1476,
      "step": 11665
    },
    {
      "epoch": 0.04984745806164916,
      "grad_norm": 1.8104469776153564,
      "learning_rate": 1.580926337378226e-06,
      "loss": 0.6052,
      "step": 11666
    },
    {
      "epoch": 0.049851730945076354,
      "grad_norm": 2.7436861991882324,
      "learning_rate": 1.5381985985301657e-06,
      "loss": 0.8749,
      "step": 11667
    },
    {
      "epoch": 0.04985600382850355,
      "grad_norm": 0.7046850323677063,
      "learning_rate": 1.4954708596821057e-06,
      "loss": 0.3261,
      "step": 11668
    },
    {
      "epoch": 0.049860276711930746,
      "grad_norm": 2.286618947982788,
      "learning_rate": 1.4527431208340456e-06,
      "loss": 0.5185,
      "step": 11669
    },
    {
      "epoch": 0.04986454959535794,
      "grad_norm": 0.8088492155075073,
      "learning_rate": 1.4100153819859854e-06,
      "loss": 0.4919,
      "step": 11670
    },
    {
      "epoch": 0.04986882247878514,
      "grad_norm": 2.9122025966644287,
      "learning_rate": 1.3672876431379251e-06,
      "loss": 1.6789,
      "step": 11671
    },
    {
      "epoch": 0.04987309536221233,
      "grad_norm": 0.8797289133071899,
      "learning_rate": 1.3245599042898649e-06,
      "loss": 0.3515,
      "step": 11672
    },
    {
      "epoch": 0.04987736824563952,
      "grad_norm": 2.921485424041748,
      "learning_rate": 1.281832165441805e-06,
      "loss": 1.7043,
      "step": 11673
    },
    {
      "epoch": 0.04988164112906672,
      "grad_norm": 3.323981285095215,
      "learning_rate": 1.2391044265937448e-06,
      "loss": 0.7663,
      "step": 11674
    },
    {
      "epoch": 0.04988591401249391,
      "grad_norm": 3.5475358963012695,
      "learning_rate": 1.1963766877456845e-06,
      "loss": 0.7772,
      "step": 11675
    },
    {
      "epoch": 0.049890186895921104,
      "grad_norm": 0.7859125733375549,
      "learning_rate": 1.1536489488976243e-06,
      "loss": 0.48,
      "step": 11676
    },
    {
      "epoch": 0.0498944597793483,
      "grad_norm": 3.120661497116089,
      "learning_rate": 1.110921210049564e-06,
      "loss": 0.7817,
      "step": 11677
    },
    {
      "epoch": 0.049898732662775495,
      "grad_norm": 1.5664974451065063,
      "learning_rate": 1.0681934712015042e-06,
      "loss": 0.5342,
      "step": 11678
    },
    {
      "epoch": 0.04990300554620269,
      "grad_norm": 2.334407329559326,
      "learning_rate": 1.025465732353444e-06,
      "loss": 0.6124,
      "step": 11679
    },
    {
      "epoch": 0.04990727842962988,
      "grad_norm": 1.9065977334976196,
      "learning_rate": 9.827379935053837e-07,
      "loss": 0.5818,
      "step": 11680
    },
    {
      "epoch": 0.04991155131305708,
      "grad_norm": 2.7379157543182373,
      "learning_rate": 9.400102546573235e-07,
      "loss": 0.861,
      "step": 11681
    },
    {
      "epoch": 0.04991582419648427,
      "grad_norm": 2.836526870727539,
      "learning_rate": 8.972825158092634e-07,
      "loss": 0.7457,
      "step": 11682
    },
    {
      "epoch": 0.04992009707991146,
      "grad_norm": 0.7627593278884888,
      "learning_rate": 8.545547769612032e-07,
      "loss": 0.5538,
      "step": 11683
    },
    {
      "epoch": 0.04992436996333866,
      "grad_norm": 0.8081608414649963,
      "learning_rate": 8.118270381131431e-07,
      "loss": 0.4919,
      "step": 11684
    },
    {
      "epoch": 0.049928642846765854,
      "grad_norm": 2.898524761199951,
      "learning_rate": 7.690992992650828e-07,
      "loss": 0.932,
      "step": 11685
    },
    {
      "epoch": 0.049932915730193046,
      "grad_norm": 1.1885590553283691,
      "learning_rate": 7.263715604170228e-07,
      "loss": 0.9536,
      "step": 11686
    },
    {
      "epoch": 0.049937188613620245,
      "grad_norm": 4.185385704040527,
      "learning_rate": 6.836438215689626e-07,
      "loss": 2.3039,
      "step": 11687
    },
    {
      "epoch": 0.04994146149704744,
      "grad_norm": 1.7249594926834106,
      "learning_rate": 6.409160827209025e-07,
      "loss": 1.2668,
      "step": 11688
    },
    {
      "epoch": 0.04994573438047463,
      "grad_norm": 1.4182896614074707,
      "learning_rate": 5.981883438728423e-07,
      "loss": 0.5331,
      "step": 11689
    },
    {
      "epoch": 0.04995000726390183,
      "grad_norm": 0.8058089017868042,
      "learning_rate": 5.55460605024782e-07,
      "loss": 0.3475,
      "step": 11690
    },
    {
      "epoch": 0.04995428014732902,
      "grad_norm": 2.106339693069458,
      "learning_rate": 5.12732866176722e-07,
      "loss": 0.6047,
      "step": 11691
    },
    {
      "epoch": 0.04995855303075621,
      "grad_norm": 5.121445655822754,
      "learning_rate": 4.7000512732866177e-07,
      "loss": 2.9914,
      "step": 11692
    },
    {
      "epoch": 0.04996282591418341,
      "grad_norm": 5.037858486175537,
      "learning_rate": 4.272773884806016e-07,
      "loss": 1.1834,
      "step": 11693
    },
    {
      "epoch": 0.049967098797610604,
      "grad_norm": 0.7760893702507019,
      "learning_rate": 3.845496496325414e-07,
      "loss": 0.305,
      "step": 11694
    },
    {
      "epoch": 0.049971371681037796,
      "grad_norm": 3.4770753383636475,
      "learning_rate": 3.418219107844813e-07,
      "loss": 0.8334,
      "step": 11695
    },
    {
      "epoch": 0.049975644564464995,
      "grad_norm": 3.028756856918335,
      "learning_rate": 2.9909417193642113e-07,
      "loss": 0.7721,
      "step": 11696
    },
    {
      "epoch": 0.04997991744789219,
      "grad_norm": 0.7849178314208984,
      "learning_rate": 2.56366433088361e-07,
      "loss": 0.305,
      "step": 11697
    },
    {
      "epoch": 0.04998419033131938,
      "grad_norm": 1.1631656885147095,
      "learning_rate": 2.136386942403008e-07,
      "loss": 0.9293,
      "step": 11698
    },
    {
      "epoch": 0.04998846321474658,
      "grad_norm": 1.5196293592453003,
      "learning_rate": 1.7091095539224064e-07,
      "loss": 0.3562,
      "step": 11699
    },
    {
      "epoch": 0.04999273609817377,
      "grad_norm": 3.643738031387329,
      "learning_rate": 1.281832165441805e-07,
      "loss": 1.257,
      "step": 11700
    }
  ],
  "logging_steps": 1,
  "max_steps": 11702,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1365084093115008.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
